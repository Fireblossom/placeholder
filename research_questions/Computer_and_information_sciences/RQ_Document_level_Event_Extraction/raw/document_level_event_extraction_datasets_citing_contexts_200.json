{
  "report_info": {
    "title": "DOCUMENT-LEVEL EVENT EXTRACTION DATASETS - CITING PAPERS 论文引用上下文分析报告",
    "query": "Document-level Event Extraction Datasets - Citing Papers",
    "generated_at": "2025-09-12 00:24:50",
    "total_papers_queried": 143,
    "papers_with_contexts": 108,
    "papers_without_contexts": 35
  },
  "query_statistics": {
    "total_ids": 143,
    "found_ids": 108,
    "not_found_ids": [
      269588198,
      257268255,
      259418040,
      277403109,
      277093054,
      265300708,
      271908679,
      269592227,
      272371883,
      269750393,
      269398868,
      272790173,
      257176439,
      252569165,
      276969721,
      269987857,
      265467642,
      248371142,
      266885866,
      264894494,
      229015826,
      272744507,
      271980137,
      272079941,
      272585841,
      269889930,
      263828907,
      257118327,
      279595265,
      256058939,
      256793212,
      248246981,
      222210854,
      274514716,
      268563434
    ],
    "total_results": 1294,
    "query_time_seconds": 1.9003217220306396
  },
  "papers_data": {
    "250390839": {
      "citing_paper_info": {
        "title": "DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction",
        "abstract": "Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.",
        "year": 2022,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "152439499",
            "name": "Meihan Tong"
          },
          {
            "authorId": "2113744169",
            "name": "Bin Xu"
          },
          {
            "authorId": "2118512998",
            "name": "Shuai Wang"
          },
          {
            "authorId": "2175603382",
            "name": "Meihuan Han"
          },
          {
            "authorId": "145014675",
            "name": "Yixin Cao"
          },
          {
            "authorId": "1737159260",
            "name": "Jiangqi Zhu"
          },
          {
            "authorId": "2175598253",
            "name": "Siyu Chen"
          },
          {
            "authorId": "145779862",
            "name": "Lei Hou"
          },
          {
            "authorId": "2133353675",
            "name": "Juanzi Li"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 10,
        "unique_cited_count": 10,
        "influential_count": 0,
        "detailed_records_count": 10
      },
      "cited_papers": [
        "215737171",
        "197544867",
        "218630327",
        "198953378",
        "3867049",
        "3719231",
        "30370175",
        "19235598",
        "4803246",
        "207853145"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3719231,
          "isinfluential": false,
          "contexts": [
            "Fol-456 lowing (Kowsari et al., 2019), we use Precision(P),457 Recall(R) and Macro-F1 score as the evaluation458 metrics.459\nOverall Performance Table 4 shows the experi-460 mental results under the normal and cross-domain461 settings, from which we have the following ob-462\nservations: 1) Compared with TextCNN, trans- 463 former based models (BERT, ALBERT, DistillBert, 464 RoBERTa) perform better, which are pre-trained on 465 a large-scale unsupervised corpus and have more 466 background semantic knowledge to rely on.",
            "Baselines We adopt a CNN-based method and442 various pre-trained transformer-based methods as443 our baselines, including: 1) TextCNN (Kim, 2014)444 uses different sizes CNN kernels to extract key in-445 formation in text for classification.",
            "Baselines We adopt a CNN-based method and 442 various pre-trained transformer-based methods as 443 our baselines, including: 1) TextCNN (Kim, 2014) 444 uses different sizes CNN kernels to extract key in445 formation in text for classification."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Convolutional Neural Networks",
            "abstract": "This chapter introduces the first deep learning architecture of the book, convolutional neural networks. It starts with redefining the way a logistic regression accepts data, and defines 1D and 2D convolutional layers as a natural extension of the logistic regression. The chapter also details on how to connect the layers and dimensionality problems. The local receptive field is introduced as a core concept of any convolutional architecture and the connections with the vanishing gradient problem is explored. Also the idea of padding is introduced in the visual setting, as well as the stride of the local receptive field. Pooling is also explored in the general setting and as max-pooling. A complete convolutional neural network for classifying MNIST is then presented in Keras code, and all the details of the code are presented as comments and illustrations. The final section of the chapter presents modifications needed to adapt convolutional networks, which are primarily visual classificators, to work with text and language.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "3422664",
                "name": "Sandro Skansi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3867049,
          "isinfluential": false,
          "contexts": [
            "Following (Hamborg et al., 2018; Hsi, 2018), we focus on main event classification, so Stage 1 is a single-label classification task."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Giveme5W: Main Event Retrieval from News Articles by Extraction of the Five Journalistic W Questions",
            "abstract": "",
            "year": 2018,
            "venue": "iConference",
            "authors": [
              {
                "authorId": "3461253",
                "name": "Felix Hamborg"
              },
              {
                "authorId": "40305390",
                "name": "Soeren Lachnit"
              },
              {
                "authorId": "3021925",
                "name": "M. Schubotz"
              },
              {
                "authorId": "2293218",
                "name": "Thomas Hepp"
              },
              {
                "authorId": "145151838",
                "name": "Bela Gipp"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4803246,
          "isinfluential": false,
          "contexts": [
            "tive works are EventKG (Gottschalk and Demidova, 2018), Event Wiki (Ge et al."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "EventKG: A Multilingual Event-Centric Temporal Knowledge Graph",
            "abstract": "One of the key requirements to facilitate semantic analytics of information regarding contemporary and historical events on the Web, in the news and in social media is the availability of reference knowledge repositories containing comprehensive representations of events and temporal relations. Existing knowledge graphs, with popular examples including DBpedia, YAGO and Wikidata, focus mostly on entity-centric information and are insufficient in terms of their coverage and completeness with respect to events and temporal relations. EventKG presented in this paper is a multilingual event-centric temporal knowledge graph that addresses this gap. EventKG incorporates over 690 thousand contemporary and historical events and over 2.3 million temporal relations extracted from several large-scale knowledge graphs and semi-structured sources and makes them available through a canonical representation.",
            "year": 2018,
            "venue": "Extended Semantic Web Conference",
            "authors": [
              {
                "authorId": "87344858",
                "name": "Simon Gottschalk"
              },
              {
                "authorId": "122585639",
                "name": "Elena Demidova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19235598,
          "isinfluential": false,
          "contexts": [
            "Based on them, various superior models have been proposed to improve the sentence-level EE and have achieved great success (Orr et al., 2018; Nguyen and Grishman, 2018; Tong et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Graph Convolutional Networks With Argument-Aware Pooling for Event Detection",
            "abstract": "\n \n The current neural network models for event detection have only considered the sequential representation of sentences. Syntactic representations have not been explored in this area although they provide an effective mechanism to directly link words to their informative context for event detection in the sentences. In this work, we investigate a convolutional neural network based on dependency trees to perform event detection. We propose a novel pooling method that relies on entity mentions to aggregate the convolution vectors. The extensive experiments demonstrate the benefits of the dependency-based convolutional neural networks and the entity mention-based pooling method for event detection. We achieve the state-of-the-art performance on widely used datasets with both perfect and predicted entity mentions.\n \n",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 30370175,
          "isinfluential": false,
          "contexts": [
            "Following Vargas-Vera and Motta (2004), we refine the initial query in BERT_QA with argument ontology knowledge obtained from Oxford dictionary (Dictionary, 1989)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Oxford English Dictionary.",
            "abstract": "",
            "year": 2008,
            "venue": "Isis; an international review devoted to the history of science and its cultural influences",
            "authors": [
              {
                "authorId": "153186391",
                "name": "A. Hughes"
              }
            ]
          }
        },
        {
          "citedcorpusid": 197544867,
          "isinfluential": false,
          "contexts": [
            "The representative works are Event Logic Graph (Ding et al., 2019) and Giveme5W1H (Hamborg et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ELG: An Event Logic Graph",
            "abstract": "The evolution and development of events have their own basic principles, which make events happen sequentially. Therefore, the discovery of such evolutionary patterns among events are of great value for event prediction, decision-making and scenario design of dialog systems. However, conventional knowledge graph mainly focuses on the entities and their relations, which neglects the real world events. In this paper, we present a novel type of knowledge base - Event Logic Graph (ELG), which can reveal evolutionary patterns and development logics of real world events. Specifically, ELG is a directed cyclic graph, whose nodes are events, and edges stand for the sequential, causal, conditional or hypernym-hyponym (is-a) relations between events. We constructed two domain ELG: financial domain ELG, which consists of more than 1.5 million of event nodes and more than 1.8 million of directed edges, and travel domain ELG, which consists of about 30 thousand of event nodes and more than 234 thousand of directed edges. Experimental results show that ELG is effective for the task of script event prediction.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2117434160",
                "name": "Xiao Ding"
              },
              {
                "authorId": "2145415526",
                "name": "Zhongyang Li"
              },
              {
                "authorId": "40282288",
                "name": "Ting Liu"
              },
              {
                "authorId": "2069173694",
                "name": "Kuo Liao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": false,
          "contexts": [
            "5) RoBERTa (Liu et al., 2019) is built on BERT and trains with much larger mini-batches and learning rates."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "RAMS(Ebner et al., 2020) limits the scope of the arguments in a 5-sentence window around its event trigger, which is not in line with the actual application, and the number of the argument types in RAMS is only 65, which is quite limited.",
            ", 2021) and RAMS(Ebner et al., 2020) consist of 246/9,124 documents with only 59/65 argument types, and most of the arguments in the two datasets are shared among",
            "RAMS(Ebner et al., 2020) limits the scope of the arguments in a 5-sentence window around its event trigger, which is not in line with the ac-056 tual application, and the number of the argument types in RAMS is only 65, which is quite limited."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215737171,
          "isinfluential": false,
          "contexts": [
            "We adopt Longformer (Beltagy et al., 2020) as encoder for the (doc) baseline, and BERT-base for the other baselines."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Longformer: The Long-Document Transformer",
            "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              },
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "2527954",
                "name": "Arman Cohan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "Following prior work (Du and Cardie, 2020b), we use Head noun phrase Match (HM) and Exact Match (EM) as two evaluation metrics."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        }
      ]
    },
    "269214164": {
      "citing_paper_info": {
        "title": "CMNEE:A Large-Scale Document-Level Event Extraction Dataset Based on Open-Source Chinese Military News",
        "abstract": "Extracting structured event knowledge, including event triggers and corresponding arguments, from military texts is fundamental to many applications, such as intelligence analysis and decision assistance. However, event extraction in the military field faces the data scarcity problem, which impedes the research of event extraction models in this domain. To alleviate this problem, we propose CMNEE, a large-scale, document-level open-source Chinese Military News Event Extraction dataset. It contains 17,000 documents and 29,223 events, which are all manually annotated based on a pre-defined schema for the military domain including 8 event types and 11 argument role types. We designed a two-stage, multi-turns annotation strategy to ensure the quality of CMNEE and reproduced several state-of-the-art event extraction models with a systematic evaluation. The experimental results on CMNEE fall shorter than those on other domain datasets obviously, which demonstrates that event extraction for military domain poses unique challenges and requires further research efforts. Our code and data can be obtained from https://github.com/Mzzzhu/CMNEE. Keywords: Corpus,Information Extraction, Information Retrieval, Knowledge Discovery/Representation",
        "year": 2024,
        "venue": "International Conference on Language Resources and Evaluation",
        "authors": [
          {
            "authorId": "2297636985",
            "name": "Mengna Zhu"
          },
          {
            "authorId": "2117883657",
            "name": "Zijie Xu"
          },
          {
            "authorId": "10673612",
            "name": "Kaisheng Zeng"
          },
          {
            "authorId": "2297186713",
            "name": "Kaiming Xiao"
          },
          {
            "authorId": "2297374593",
            "name": "Mao Wang"
          },
          {
            "authorId": "1596819256",
            "name": "Wenjun Ke"
          },
          {
            "authorId": "2284447693",
            "name": "Hongbin Huang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 25,
        "unique_cited_count": 23,
        "influential_count": 1,
        "detailed_records_count": 25
      },
      "cited_papers": [
        "254877171",
        "252089843",
        "264814421",
        "216562779",
        "233219850",
        "262825274",
        "154230030",
        "2563759",
        "216562330",
        "119308902",
        "207853145",
        "235254286",
        "250390839",
        "154992508",
        "235458429",
        "226262283",
        "252569165",
        "237353175",
        "258378242",
        "265659367",
        "196201373",
        "11986411",
        "213428800"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2563759,
          "isinfluential": false,
          "contexts": [
            "Experiments demonstrate that CMNEE is challenging and event extraction in military domain remains an open issue.",
            "There are also some datasets oriented to other specific domains, such as biomedical domain (Pyysalo et al., 2013), literary domain (Sims et al., 2019), terror-ist attack events (Grishman and Sundheim, 1996), breaking news (Fu et al., 2010), etc."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Cancer Genetics (CG) task of BioNLP Shared Task 2013",
            "abstract": "",
            "year": 2013,
            "venue": "BioNLP@ACL",
            "authors": [
              {
                "authorId": "1708916",
                "name": "S. Pyysalo"
              },
              {
                "authorId": "2095533089",
                "name": "Tomoko Ohta"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11986411,
          "isinfluential": false,
          "contexts": [
            "There are also some datasets oriented to other specific domains, such as biomedical domain (Pyysalo et al., 2013), literary domain (Sims et al., 2019), terror-ist attack events (Grishman and Sundheim, 1996), breaking news (Fu et al., 2010), etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Message Understanding Conference- 6: A Brief History",
            "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",
            "year": 1996,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "Moreover, most existing event extraction datasets are oriented towards general (Li et al., 2020, 2021a) or financial domains (Han et al., 2022a; Zheng et al., 2019).",
            "ChfinAnn (Zheng et al., 2019) is constructed using distant supervision to assist in the construction, with a sizable scale, but it does not contain event trigger information, and can only be used for the event argument extraction task.",
            ", MAVEN (Wang et al., 2020), DuEE (Li et al., 2020), MNEE (Huang et al., 2023b) and document-level event extraction datasets, RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021b), Duee-fin (Han et al., 2022b), ChfinAnn (Zheng et al., 2019), Do-cEE (Tong et al., 2022"
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154230030,
          "isinfluential": false,
          "contexts": [
            "Extraction of military events from documents is crucial to downstream applications such as intelligence analysis (Santucci, 2022; Bang, 2016; Freed-man, 1983; Ivanov, 2011), decision making assistance (Skryabina et al., 2020), and strategic planning (Schrodt, 2012; Sankar, 2023; Lyu, 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Pitfalls in Military Quantitative Intelligence Analysis: Incident Reporting in a Low Intensity Conflict",
            "abstract": "",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "2610553",
                "name": "Martin Bang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 154992508,
          "isinfluential": false,
          "contexts": [
            "Extraction of military events from documents is crucial to downstream applications such as intelligence analysis (Santucci, 2022; Bang, 2016; Freed-man, 1983; Ivanov, 2011), decision making assistance (Skryabina et al., 2020), and strategic planning (Schrodt, 2012; Sankar, 2023; Lyu, 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Precedents, Progress, and Prospects in Political Event Data",
            "abstract": "",
            "year": 2012,
            "venue": "",
            "authors": [
              {
                "authorId": "9804292",
                "name": "Philip A. Schrodt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196201373,
          "isinfluential": false,
          "contexts": [
            "There are also some datasets oriented to other specific domains, such as biomedical domain (Pyysalo et al., 2013), literary domain (Sims et al., 2019), terror-ist attack events (Grishman and Sundheim, 1996), breaking news (Fu et al., 2010), etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Literary Event Detection",
            "abstract": "In this work we present a new dataset of literary events—events that are depicted as taking place within the imagined space of a novel. While previous work has focused on event detection in the domain of contemporary news, literature poses a number of complications for existing systems, including complex narration, the depiction of a broad array of mental states, and a strong emphasis on figurative language. We outline the annotation decisions of this new dataset and compare several models for predicting events; the best performing model, a bidirectional LSTM with BERT token representations, achieves an F1 score of 73.9. We then apply this model to a corpus of novels split across two dimensions—prestige and popularity—and demonstrate that there are statistically significant differences in the distribution of events for prestige.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "36978794",
                "name": "Matthew Sims"
              },
              {
                "authorId": "2109283816",
                "name": "Jongho Park"
              },
              {
                "authorId": "2168134",
                "name": "David Bamman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "For the document-level event extraction task, the commonly used datasets are RAMS (Ebner et al., 2020) and WikiEvents (Li et al., 2021a), with a smaller number of documents, 9124 and 246, respectively.",
            ", MAVEN (Wang et al., 2020), DuEE (Li et al., 2020), MNEE (Huang et al., 2023b) and document-level event extraction datasets, RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021b), Duee-fin (Han et al., 2022b), ChfinAnn (Zheng et al., 2019), Do-cEE (Tong et al., 2022"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 213428800,
          "isinfluential": false,
          "contexts": [
            "Extraction of military events from documents is crucial to downstream applications such as intelligence analysis (Santucci, 2022; Bang, 2016; Freed-man, 1983; Ivanov, 2011), decision making assistance (Skryabina et al., 2020), and strategic planning (Schrodt, 2012; Sankar, 2023; Lyu, 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The role of emergency preparedness exercises in the response to a mass casualty terrorist incident: A mixed methods study",
            "abstract": "",
            "year": 2020,
            "venue": "International Journal of Disaster Risk Reduction",
            "authors": [
              {
                "authorId": "3642959",
                "name": "E. Skryabina"
              },
              {
                "authorId": "40907838",
                "name": "Naomi Betts"
              },
              {
                "authorId": "144468674",
                "name": "G. Reedy"
              },
              {
                "authorId": "50740892",
                "name": "P. Riley"
              },
              {
                "authorId": "3878110",
                "name": "R. Amlȏt"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "(Du and Cardie, 2020) converted the event extraction task into the natural question answering task."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562779,
          "isinfluential": true,
          "contexts": [
            "Evaluation Following the widely-used setting, we report the micro Precision, Recall, and F-1 scores for event detection and event argument extraction as our evaluation metrics (Wang et al., 2020; Tong et al., 2022).",
            "Attack” category, while the MAVEN dataset includes categories like “Attack” and “Defending”.",
            ", MAVEN (Wang et al., 2020), DuEE (Li et al., 2020), MNEE (Huang et al., 2023b) and document-level event extraction datasets, RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021b), Duee-fin (Han et al., 2022b), ChfinAnn (Zheng et al., 2019), Do-cEE (Tong et al., 2022",
            "MAVEN (Wang et al., 2020) is the largest event detection dataset, which defines 168 event types and labels 19,640 events."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MAVEN: A Massive General Domain Event Detection Dataset",
            "abstract": "Event detection (ED), which identifies event trigger words and classifies event types according to contexts, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Small scale of existing datasets is not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Limited event types of existing datasets lead to the trained models cannot be easily adapted to general-domain scenarios. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 117,200 event mention instances, and 207 event types. MAVEN alleviates the lack of data problem and covers much more general event types. Besides the dataset, we reproduce the recent state-of-the-art ED models and conduct a thorough evaluation for these models on MAVEN. The experimental results and empirical analyses show that existing ED methods cannot achieve promising results as on the small datasets, which suggests ED in real world remains a challenging task and requires further research efforts. The dataset and baseline code will be released in the future to promote this field.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "1659764899",
                "name": "Wangyi Jiang"
              },
              {
                "authorId": "151185222",
                "name": "Rong Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "…text, which is typically separated into two subtasks: event detection and event argument extraction (Ahn, 2006; Yang et al., 2019; Xu et al., 2023; Wang et al., 2021b; Shi et al., 2023; Yang et al., 2023; Wan et al., 2023; Liu et al., 2020; Chu et al., 2023; Peng et al., 2023; Wang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "Moreover, most existing event extraction datasets are oriented towards general (Li et al., 2020, 2021a) or financial domains (Han et al., 2022a; Zheng et al., 2019).",
            "For the document-level event extraction task, the commonly used datasets are RAMS (Ebner et al., 2020) and WikiEvents (Li et al., 2021a), with a smaller number of documents, 9124 and 246, respectively.",
            ", MAVEN (Wang et al., 2020), DuEE (Li et al., 2020), MNEE (Huang et al., 2023b) and document-level event extraction datasets, RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021b), Duee-fin (Han et al., 2022b), ChfinAnn (Zheng et al., 2019), Do-cEE (Tong et al., 2022"
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235254286,
          "isinfluential": false,
          "contexts": [
            "…text, which is typically separated into two subtasks: event detection and event argument extraction (Ahn, 2006; Yang et al., 2019; Xu et al., 2023; Wang et al., 2021b; Shi et al., 2023; Yang et al., 2023; Wan et al., 2023; Liu et al., 2020; Chu et al., 2023; Peng et al., 2023; Wang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLEVE: Contrastive Pre-training for Event Extraction",
            "abstract": "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2149202150",
                "name": "Yankai Lin"
              },
              {
                "authorId": "2055765060",
                "name": "Lei Hou"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "6 ) TEXT2EVENT (Lu et al., 2021) is a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237353175,
          "isinfluential": false,
          "contexts": [
            "Most of these models use sentence-level text for analysis, and while sentence-level event extraction research is relatively mature (Gao, 2021; Hsu et al., 2021; Huang et al., 2023a), this technology struggles when directly applied to document-level event extraction tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Natural Language Generation",
            "abstract": "Event extraction (EE), the task that identiﬁes event triggers and their arguments in text, is usually formulated as a classiﬁcation or structured prediction problem. Such models usually reduce labels to numeric identiﬁers, making them unable to take advantage of label semantics (e.g. an event type named A RREST is related to words like arrest , detain , or appre-hend ). This prevents the generalization to new event types. In this work, we formulate EE as a natural language generation task and pro-pose G EN EE, a model that not only captures complex dependencies within an event but also generalizes well to unseen or rare event types. Given a passage and an event type, G EN EE is trained to generate a natural sentence following a predeﬁned template for that event type. The generated output is then decoded into trigger and argument predictions. The autoregressive generation process naturally models the dependencies among the predictions — each new word predicted depends on those already generated in the output sentence. Using carefully designed input prompts during generation, G EN EE is able to capture label semantics, which enables the generalization to new event types. Empirical results show that our model achieves strong performance on event extraction tasks under all zero-shot, few-shot, and high-resource scenarios. Especially, in the high-resource setting, G EN EE outperforms the state-of-the-art model on argument extraction and gets competitive results with the current best on end-to-end EE tasks.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "3256207",
                "name": "Elizabeth Boschee"
              },
              {
                "authorId": "123937952",
                "name": "Scott Miller"
              },
              {
                "authorId": "145603129",
                "name": "P. Natarajan"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250390839,
          "isinfluential": false,
          "contexts": [
            "Evaluation Following the widely-used setting, we report the micro Precision, Recall, and F-1 scores for event detection and event argument extraction as our evaluation metrics (Wang et al., 2020; Tong et al., 2022).",
            ", MAVEN (Wang et al., 2020), DuEE (Li et al., 2020), MNEE (Huang et al., 2023b) and document-level event extraction datasets, RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021b), Duee-fin (Han et al., 2022b), ChfinAnn (Zheng et al., 2019), Do-cEE (Tong et al., 2022"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction",
            "abstract": "Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152439499",
                "name": "Meihan Tong"
              },
              {
                "authorId": "2113744169",
                "name": "Bin Xu"
              },
              {
                "authorId": "2118512998",
                "name": "Shuai Wang"
              },
              {
                "authorId": "2175603382",
                "name": "Meihuan Han"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "1737159260",
                "name": "Jiangqi Zhu"
              },
              {
                "authorId": "2175598253",
                "name": "Siyu Chen"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252089843,
          "isinfluential": false,
          "contexts": [
            "Currently, the FewFC dataset is the main dataset used for the study of overlapping events (Sheng et al., 2021; Cao et al., 2022), but the size of this dataset is relatively small, in which only 18% of the instances contain overlapping events, and its annotation is based on sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction",
            "abstract": "Event extraction (EE) is an essential task of information extraction, which aims to extract structured event information from unstructured text. Most prior work focuses on extracting flat events while neglecting overlapped or nested ones. A few models for overlapped and nested EE includes several successive stages to extract event triggers and arguments,which suffer from error propagation. Therefore, we design a simple yet effective tagging scheme and model to formulate EE as word-word relation recognition, called OneEE. The relations between trigger or argument words are simultaneously recognized in one stage with parallel grid tagging, thus yielding a very fast event extraction speed. The model is equipped with an adaptive event fusion module to generate event-aware representations and a distance-aware predictor to integrate relative distance information for word-word relation recognition, which are empirically demonstrated to be effective mechanisms. Experiments on 3 overlapped and nested EE benchmarks, namely FewFC, Genia11, and Genia13, show that OneEE achieves the state-of-the-art (SOTA) results. Moreover, the inference speed of OneEE is faster than those of baselines in the same condition, and can be further substantially improved since it supports parallel inference.",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "101749938",
                "name": "H. Cao"
              },
              {
                "authorId": "2000217741",
                "name": "Jingye Li"
              },
              {
                "authorId": "2159755533",
                "name": "Fangfang Su"
              },
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              },
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "1957924118",
                "name": "Shengqiong Wu"
              },
              {
                "authorId": "2132446579",
                "name": "Bobo Li"
              },
              {
                "authorId": "2116735539",
                "name": "Liang Zhao"
              },
              {
                "authorId": "145628086",
                "name": "Donghong Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252569165,
          "isinfluential": false,
          "contexts": [
            "Moreover, most existing event extraction datasets are oriented towards general (Li et al., 2020, 2021a) or financial domains (Han et al., 2022a; Zheng et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DuEE-Fin: A Large-Scale Dataset for Document-Level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "Natural Language Processing and Chinese Computing",
            "authors": [
              {
                "authorId": "2159829320",
                "name": "Cuiyun Han"
              },
              {
                "authorId": "2144141948",
                "name": "Jinchuan Zhang"
              },
              {
                "authorId": "2118335351",
                "name": "Xinyu Li"
              },
              {
                "authorId": "2186424675",
                "name": "Guojin Xu"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "40572919",
                "name": "Zengfeng Zeng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254877171,
          "isinfluential": false,
          "contexts": [
            "Recently, there have been some efforts using ChatGPT for data annotation in areas like Named Entity Recognition and Relation Extraction, as indicated in references (Ding et al., 2022; Goel et al., 2023; Zhang et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Is GPT-3 a Good Data Annotator?",
            "abstract": "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2064493724",
                "name": "Bosheng Ding"
              },
              {
                "authorId": "2084609980",
                "name": "Chengwei Qin"
              },
              {
                "authorId": "2145314839",
                "name": "Linlin Liu"
              },
              {
                "authorId": "1996394",
                "name": "Lidong Bing"
              },
              {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
              },
              {
                "authorId": "1728712",
                "name": "Boyang Albert Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258378242,
          "isinfluential": false,
          "contexts": [
            "…text, which is typically separated into two subtasks: event detection and event argument extraction (Ahn, 2006; Yang et al., 2019; Xu et al., 2023; Wang et al., 2021b; Shi et al., 2023; Yang et al., 2023; Wan et al., 2023; Liu et al., 2020; Chu et al., 2023; Peng et al., 2023; Wang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Hybrid Detection and Generation Framework with Separate Encoders for Event Extraction",
            "abstract": "The event extraction task typically consists of event detection and event argument extraction. Most previous work models these two subtasks with shared representation by multiple classification tasks or a unified generative approach. In this paper, we revisit this pattern and propose to use independent encoders to model event detection and event argument extraction, respectively, and use the output of event detection to construct the input of event argument extraction. In addition, we use token-level features to precisely control the fusion between two encoders to achieve joint bridging training rather than directly reusing representations between different tasks. Through a series of careful experiments, we demonstrate the importance of avoiding feature interference of different tasks and the importance of joint bridging training. We achieved competitive results on standard benchmarks (ACE05-E, ACE05-E+, and ERE-EN) and established a solid baseline.",
            "year": 2023,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2067725506",
                "name": "Ge Shi"
              },
              {
                "authorId": "2352398881",
                "name": "Yunyue Su"
              },
              {
                "authorId": "2115772983",
                "name": "Yongliang Ma"
              },
              {
                "authorId": "2152177024",
                "name": "Ming Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262825274,
          "isinfluential": false,
          "contexts": [
            "…text, which is typically separated into two subtasks: event detection and event argument extraction (Ahn, 2006; Yang et al., 2019; Xu et al., 2023; Wang et al., 2021b; Shi et al., 2023; Yang et al., 2023; Wan et al., 2023; Liu et al., 2020; Chu et al., 2023; Peng et al., 2023; Wang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding",
            "abstract": "Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an event understanding toolkit OmniEvent, which features three desiderata: (1) Comprehensive. OmniEvent supports mainstream modeling paradigms of all the event understanding tasks and the processing of 15 widely-used English and Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous evaluation pitfalls reported in Peng et al. (2023), which ensures fair comparisons between different models. (3) Easy-to-use. OmniEvent is designed to be easily used by users with varying needs. We provide off-the-shelf models that can be directly deployed as web services. The modular framework also enables users to easily implement and evaluate new event understanding models with OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publicly released along with the demonstration website and video (https://omnievent.xlore.cn/).",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "47837854",
                "name": "Hao Peng"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "2218965360",
                "name": "Feng Yao"
              },
              {
                "authorId": "2200687506",
                "name": "Zimu Wang"
              },
              {
                "authorId": "2107153643",
                "name": "C. Zhu"
              },
              {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 264814421,
          "isinfluential": false,
          "contexts": [
            "Recently, there have been some efforts using ChatGPT for data annotation in areas like Named Entity Recognition and Relation Extraction, as indicated in references (Ding et al., 2022; Goel et al., 2023; Zhang et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "LLMaAA: Making Large Language Models as Active Annotators",
            "abstract": "Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2110064476",
                "name": "Ruoyu Zhang"
              },
              {
                "authorId": "32732179",
                "name": "Yanzeng Li"
              },
              {
                "authorId": "2264313369",
                "name": "Yongliang Ma"
              },
              {
                "authorId": "2262091009",
                "name": "Ming Zhou"
              },
              {
                "authorId": "2260649310",
                "name": "Lei Zou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265659367,
          "isinfluential": false,
          "contexts": [
            "Experiments demonstrate that CMNEE is challenging and event extraction in military domain remains an open issue."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "LLMs Accelerate Annotation for Medical Information Extraction",
            "abstract": "The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare.",
            "year": 2023,
            "venue": "ML4H@NeurIPS",
            "authors": [
              {
                "authorId": "2269736191",
                "name": "Akshay Goel"
              },
              {
                "authorId": "2204956989",
                "name": "Almog Gueta"
              },
              {
                "authorId": "2269735644",
                "name": "Omry Gilon"
              },
              {
                "authorId": "2269762842",
                "name": "Chang Liu"
              },
              {
                "authorId": "2101317386",
                "name": "Sofia Erell"
              },
              {
                "authorId": "2269759759",
                "name": "Lan Huong Nguyen"
              },
              {
                "authorId": "2269757864",
                "name": "Xiaohong Hao"
              },
              {
                "authorId": "2269734365",
                "name": "Bolous Jaber"
              },
              {
                "authorId": "2269733442",
                "name": "Shashir Reddy"
              },
              {
                "authorId": "2269734359",
                "name": "Rupesh Kartha"
              },
              {
                "authorId": "2269735887",
                "name": "Jean Steiner"
              },
              {
                "authorId": "40142021",
                "name": "Itay Laish"
              },
              {
                "authorId": "46609506",
                "name": "Amir Feder"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Most of these models use sentence-level text for analysis, and while sentence-level event extraction research is relatively mature (Gao, 2021; Hsu et al., 2021; Huang et al., 2023a), this technology struggles when directly applied to document-level event extraction tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Extraction of military events from documents is crucial to downstream applications such as intelligence analysis (Santucci, 2022; Bang, 2016; Freed-man, 1983; Ivanov, 2011), decision making assistance (Skryabina et al., 2020), and strategic planning (Schrodt, 2012; Sankar, 2023; Lyu, 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "269762511": {
      "citing_paper_info": {
        "title": "CFinDEE: A Chinese Fine-Grained Financial Dataset for Document-Level Event Extraction",
        "abstract": "Document-level event extraction faces numerous challenges in accurately modeling real-world financial scenarios, particularly due to the inadequacies in existing datasets regarding data scale and fine-grained annotations. The development of datasets is a crucial factor in driving research progress; therefore, we present a high-quality Chinese document-level event extraction dataset, CFinDEE. This dataset, grounded in real-world financial news, defines 22 event types and 116 argument roles, annotating 26,483 events and 107,096 event arguments. CFinDEE aims to address these shortcomings by providing more comprehensive annotations and data augmentation, offering richer resources for document-level event extraction in the financial domain. CFinDEE extends data both horizontally and vertically, where horizontal expansion enriches the types of financial events, enhancing the diversity of the dataset; vertical expansion, by increasing the scale of the data, effectively boosts the practical value of the dataset. Experiments conducted on multiple advanced models have validated the high applicability and effectiveness of the CFinDEE dataset for document-level event extraction tasks in the financial field.",
        "year": 2024,
        "venue": "The Web Conference",
        "authors": [
          {
            "authorId": "2276406518",
            "name": "Tian Zhang"
          },
          {
            "authorId": "2276000576",
            "name": "Maofu Liu"
          },
          {
            "authorId": "2301267842",
            "name": "Bingying Zhou"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 6,
        "influential_count": 0,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "51871198",
        "259120735",
        "266163877",
        "252569165",
        "207853145",
        "252819354"
      ],
      "citation_details": [
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "To comprehensively evaluate our dataset and demonstrate its potential in financial DEE tasks, we selected the following models as baselines: • DCFEE [19] employs an argument-completion strategy and critical event detection techniques to generate document-levelevent records.",
            "This comparison spans sentence-level datasets (such as ACE2005 [2] and DuEE [10]) to document-level datasets (such as MUC-4 [4], RAMS [3], DCFEE [19], ChFinAnn [23], FEED [7] and DuEE-Fin [5]).",
            "In 2018, Yang et al. [19] constructed the first document-level Chinese event extraction dataset in the financial field, DCFEE, using a distant supervision approach."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "This comparison spans sentence-level datasets (such as ACE2005 [2] and DuEE [10]) to document-level datasets (such as MUC-4 [4], RAMS [3], DCFEE [19], ChFinAnn [23], FEED [7] and DuEE-Fin [5])."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252569165,
          "isinfluential": false,
          "contexts": [
            "As illustrated in Figure 1, the arguments for financial profit events annotated in the DuEE-Fin dataset [5] are dispersed across multiple sentences.",
            "This comparison spans sentence-level datasets (such as ACE2005 [2] and DuEE [10]) to document-level datasets (such as MUC-4 [4], RAMS [3], DCFEE [19], ChFinAnn [23], FEED [7] and DuEE-Fin [5])."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DuEE-Fin: A Large-Scale Dataset for Document-Level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "Natural Language Processing and Chinese Computing",
            "authors": [
              {
                "authorId": "2159829320",
                "name": "Cuiyun Han"
              },
              {
                "authorId": "2144141948",
                "name": "Jinchuan Zhang"
              },
              {
                "authorId": "2118335351",
                "name": "Xinyu Li"
              },
              {
                "authorId": "2186424675",
                "name": "Guojin Xu"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "40572919",
                "name": "Zengfeng Zeng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252819354,
          "isinfluential": false,
          "contexts": [
            "For event types with fewer samples, few-shot learning methods can be adopted, with current solutions including data augmentation [13], prompt learning [8], transfer learning [12], meta-learning [20], generative adversarial networks [16], and sample weighting [15], among others."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "KiPT: Knowledge-injected Prompt Tuning for Event Detection",
            "abstract": "",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2145536897",
                "name": "Haochen Li"
              },
              {
                "authorId": "8046305",
                "name": "Tong Mo"
              },
              {
                "authorId": "2118734401",
                "name": "Hongcheng Fan"
              },
              {
                "authorId": "2154810345",
                "name": "Jingkun Wang"
              },
              {
                "authorId": "2167466487",
                "name": "Jiaxi Wang"
              },
              {
                "authorId": "2118587613",
                "name": "Fuhao Zhang"
              },
              {
                "authorId": "2139261376",
                "name": "Weiping Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259120735,
          "isinfluential": false,
          "contexts": [
            "In 2022, Ren et al. [14] introduced a fine-grained event extraction dataset named IREE from an investment perspective, comprising five major news categories and 59 types of risk events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "IREE: A Fine-Grained Dataset for Chinese Event Extraction in Investment Research",
            "abstract": "",
            "year": 2022,
            "venue": "China Conference on Knowledge Graph and Semantic Computing",
            "authors": [
              {
                "authorId": "1588802656",
                "name": "Junxiang Ren"
              },
              {
                "authorId": "2143244328",
                "name": "Sibo Wang"
              },
              {
                "authorId": "2124707961",
                "name": "Ruilin Song"
              },
              {
                "authorId": "2150921883",
                "name": "Yuejiao Wu"
              },
              {
                "authorId": "2118545335",
                "name": "Yizhou Gao"
              },
              {
                "authorId": "2219779771",
                "name": "Borong An"
              },
              {
                "authorId": "2186372522",
                "name": "Zhen Cheng"
              },
              {
                "authorId": "2186632380",
                "name": "Guoqiang Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266163877,
          "isinfluential": false,
          "contexts": [
            "• IPGPF [6] eliminates the dependence on the generation order of argument roles by parallel generating event arguments and iteratively generating event records while adopting a pre-filling strategy to mitigate training deficiencies and zero precision issues in a parallel generation."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An Iteratively Parallel Generation Method with the Pre-Filling Strategy for Document-level Event Extraction",
            "abstract": ",",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2185459693",
                "name": "Guanhua Huang"
              },
              {
                "authorId": "2274917073",
                "name": "Runxin Xu"
              },
              {
                "authorId": "2274218927",
                "name": "Ying Zeng"
              },
              {
                "authorId": "2273549520",
                "name": "Jiaze Chen"
              },
              {
                "authorId": "2273577882",
                "name": "Zhouwang Yang"
              },
              {
                "authorId": "2255989628",
                "name": "Weinan E"
              }
            ]
          }
        }
      ]
    },
    "272659306": {
      "citing_paper_info": {
        "title": "EADRE: Event-type Aware Dynamic Representation of Entities in Document-level Event Extraction",
        "abstract": "Document-level event extraction aims to identify event types and arguments from one document. However, existing methods fail to consider semantic distinctions between multiple mentions of one entity and ignore dynamic representation of entities across multiple events simultaneously. Therefore, the models cannot capture flexible and specific entity representations in different event types. In this article, we propose EADRE (Event-type-Aware Dynamic Representation of Entities). Specifically, we use cross-attention between mentions and event-type prototypes to obtain event-type-aware mention features. Then, we propose ASGate (Adaptive Soft Gate), which adaptively selects mention features to reduce the influence of event-unrelated mentions. EADRE introduces no more than 1% new parameters compared with the base model and has good transportability. Experiments on two public datasets show that EADRE improves the performance of multi-event extraction by 2.6% and 3.1%, as well as outperforms previous state-of-the-art baselines by 0.2% and 1.6%, with lower resource consumption without the use of pre-trained models. Further experimental analysis shows that EADRE significantly improves extraction performance in O2M and M2M multi-event scenarios.",
        "year": 2024,
        "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
        "authors": [
          {
            "authorId": "2268601957",
            "name": "Guangjun Zhang"
          },
          {
            "authorId": "2268625712",
            "name": "Hu Zhang"
          },
          {
            "authorId": "2268631246",
            "name": "Ru Li"
          },
          {
            "authorId": "2274159318",
            "name": "Hongye Tan"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 8,
        "influential_count": 2,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "249431954",
        "216562330",
        "14339673",
        "13756489",
        "226262283",
        "248780177",
        "252569165",
        "211529034"
      ],
      "citation_details": [
        {
          "citedcorpusid": 13756489,
          "isinfluential": true,
          "contexts": [
            "We use a vanilla Transformer [18] as the encoder and employ a CRF (Conditional Random Field) to classify the token representations into entity labels.",
            "As shown in Figure 2, EADRE first encodes each sentence in D through Transformer [18] and extracts entity mentions at the sentence level (Section 3.1)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Chen et al. [2] proposed a dynamic multi-pool convolutional neural network to uncover clues at the sentence level, employing dynamic pooling layers to retain more information about event trigger words and arguments."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211529034,
          "isinfluential": false,
          "contexts": [
            "Most existing approaches [1, 3, 12, 14, 15, 19, 22] focus on Sentence-level Event Extraction (SEE) ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Biomedical Event Extraction as Multi-turn Question Answering",
            "abstract": "Biomedical event extraction from natural text is a challenging task as it searches for complex and often nested structures describing specific relationships between multiple molecular entities, such as genes, proteins, or cellular components. It usually is implemented by a complex pipeline of individual tools to solve the different relation extraction subtasks. We present an alternative approach where the detection of relationships between entities is described uniformly as questions, which are iteratively answered by a question answering (QA) system based on the domain-specific language model SciBERT. This model outperforms two strong baselines in two biomedical event extraction corpora in a Knowledge Base Population setting, and also achieves competitive performance in BioNLP challenge evaluation settings.",
            "year": 2020,
            "venue": "International Workshop on Health Text Mining and Information Analysis",
            "authors": [
              {
                "authorId": "47120472",
                "name": "Xinglong Wang"
              },
              {
                "authorId": "20308468",
                "name": "Leon Weber"
              },
              {
                "authorId": "1693022",
                "name": "U. Leser"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie [4] extracted trigger words and arguments in a fragmentary manner by defining problem templates for trigger words and event roles."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "Liu et al. [10] proposed an unsupervised question generation method, which can avoid the problem of inadequate semantics resulting from template generation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248780177,
          "isinfluential": false,
          "contexts": [
            "In contrast, for ChatGLM-6B , we conducted P-tuning-v2 [11] using the encoder-decoder framework with all training data."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
            "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2111312892",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2065056119",
                "name": "Kaixuan Ji"
              },
              {
                "authorId": "1998914086",
                "name": "Yicheng Fu"
              },
              {
                "authorId": "1403621152",
                "name": "W. Tam"
              },
              {
                "authorId": "66395694",
                "name": "Zhengxiao Du"
              },
              {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
              },
              {
                "authorId": "2109541439",
                "name": "Jie Tang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249431954,
          "isinfluential": false,
          "contexts": [
            "Liang et al. [9] propose ReDEE, a relation-enhanced DEE model, which strengthens the relationships between parameters by introducing an additional relational extraction task.",
            "— ReDEE [9] introduces the relational extraction task to enhance the relationship between arguments.",
            "” Previous methods [9, 23, 25, 27, 28] divided DEE into three tasks: mention extraction , event detection , and event record extraction ."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction",
            "abstract": "In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117874913",
                "name": "Yuan Liang"
              },
              {
                "authorId": "3435199",
                "name": "Zhuoxuan Jiang"
              },
              {
                "authorId": "2168284967",
                "name": "Di Yin"
              },
              {
                "authorId": "2064646914",
                "name": "Bo Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252569165,
          "isinfluential": false,
          "contexts": [
            "We conducted experiments on two public DEE datasets including ChiFinAnn [27] and DuEE-Fin [5]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "DuEE-Fin: A Large-Scale Dataset for Document-Level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "Natural Language Processing and Chinese Computing",
            "authors": [
              {
                "authorId": "2159829320",
                "name": "Cuiyun Han"
              },
              {
                "authorId": "2144141948",
                "name": "Jinchuan Zhang"
              },
              {
                "authorId": "2118335351",
                "name": "Xinyu Li"
              },
              {
                "authorId": "2186424675",
                "name": "Guojin Xu"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "40572919",
                "name": "Zengfeng Zeng"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "Then, following the approach proposed by Zhu et al. [28], we select a set of roles R k for the k -th event type based on the importance scores of the roles.",
            "Zhu et al. [28] propose a more efficient DEE model, PTPCG, which constructs a pruned complete graph based on pseudo-trigger words and makes the model learn this graph structure during training.",
            "Finally, we use the methods based on a pseudo-trigger-aware pruned complete graph [28] to obtain event records for each event (Section 3.4).",
            "— PTPCG [28] proposes a pseudo-trigger-aware pruned complete graph and extracting different combinations of arguments from it efficiently.",
            "” Previous methods [9, 23, 25, 27, 28] divided DEE into three tasks: mention extraction , event detection , and event record extraction .",
            "We adopt a pruning-based fully connected graph method [28] to obtain different argument combinations to extract event records.",
            "Due to insufficient robustness of the models [28], these links cannot be predicted correctly, leading to decreased accuracy in adjacency matrix prediction."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "249431954": {
      "citing_paper_info": {
        "title": "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction",
        "abstract": "In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.",
        "year": 2022,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2117874913",
            "name": "Yuan Liang"
          },
          {
            "authorId": "3435199",
            "name": "Zhuoxuan Jiang"
          },
          {
            "authorId": "2168284967",
            "name": "Di Yin"
          },
          {
            "authorId": "2064646914",
            "name": "Bo Ren"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 11,
        "influential_count": 5,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "11187670",
        "6452487",
        "2617020",
        "231985811",
        "14339673",
        "19220240",
        "196178503",
        "1320606",
        "52967399",
        "235458429",
        "119308902"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "Previously a lot of works((Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event occurrences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2617020,
          "isinfluential": false,
          "contexts": [
            "To train the above four components, we leverage the multi-task learning method (Collobert and Weston, 2008) and integrate the four corresponding loss functions together as the following:"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A unified architecture for natural language processing: deep neural networks with multitask learning",
            "abstract": "",
            "year": 2008,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2939803",
                "name": "R. Collobert"
              },
              {
                "authorId": "145183709",
                "name": "J. Weston"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": true,
          "contexts": [
            "Previously a lot of works((Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event…",
            "Previously a lot of works((Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event occurrences.",
            "Most of the previous methods focus on sentencelevel event extraction (SEE) (Ahn, 2006; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Zhao et al., 2018; Sha et al., 2018; Yan et al., 2019; Du and Cardie, 2020; Li\n*These authors contributed equally to this work\net…",
            "Then a joint model is created to extract triggers and arguments simultaneously via multi-task learning (Nguyen et al., 2016; Sha et al., 2018)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11187670,
          "isinfluential": false,
          "contexts": [
            "Previously a lot of works((Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event occurrences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Using Document Level Cross-Event Inference to Improve Event Extraction",
            "abstract": "",
            "year": 2010,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39524110",
                "name": "Shasha Liao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": true,
          "contexts": [
            "For example, a neural pipeline model is proposed to identify triggers first and then extracts roles and arguments (Chen et al., 2015).",
            "Previously a lot of works((Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event…",
            "Previously a lot of works((Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)) deal with event extraction in two stages: firstly, trigger words are detected, which are usually nouns or verbs that clearly express event occurrences.",
            "Most of the previous methods focus on sentencelevel event extraction (SEE) (Ahn, 2006; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Zhao et al., 2018; Sha et al., 2018; Yan et al., 2019; Du and Cardie, 2020; Li\n*These authors contributed equally to this work\net…"
          ],
          "intents": [
            "--",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19220240,
          "isinfluential": false,
          "contexts": [
            "…event extraction (SEE) (Ahn, 2006; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Zhao et al., 2018; Sha et al., 2018; Yan et al., 2019; Du and Cardie, 2020; Li\n*These authors contributed equally to this work\net al., 2020; Paolini et al., 2021; Lu et…",
            "Then a joint model is created to extract triggers and arguments simultaneously via multi-task learning (Nguyen et al., 2016; Sha et al., 2018)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction",
            "abstract": "\n \n Event extraction plays an important role in natural language processing (NLP) applications including question answering and information retrieval. Traditional event extraction relies heavily on lexical and syntactic features, which require intensive human engineering and may not generalize to different datasets. Deep neural networks, on the other hand, are able to automatically learn underlying features, but existing networks do not make full use of syntactic relations. In this paper, we propose a novel dependency bridge recurrent neural network (dbRNN) for event extraction. We build our model upon a recurrent neural network, but enhance it with dependency bridges, which carry syntactically related information when modeling each word.We illustrates that simultaneously applying tree structure and sequence structure in RNN brings much better performance than only uses sequential RNN. In addition, we use a tensor layer to simultaneously capture the various types of latent interaction between candidate arguments as well as identify/classify all arguments of an event. Experiments show that our approach achieves competitive results compared with previous work.\n \n",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "39058310",
                "name": "Lei Sha"
              },
              {
                "authorId": "2053324591",
                "name": "Feng Qian"
              },
              {
                "authorId": "39488576",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Specifically, we use the BERT (Devlin et al., 2019) encoder pre-trained in Roberta setting (Liu et al.",
            "Specifically, we use the BERT (Devlin et al., 2019) encoder pre-trained in Roberta setting (Liu et al., 2019).",
            "We use BERT encoder in the EER component for fine-tuning and Roberta-chinese-wwm (Yiming et al., 2020) as the pre-trained model."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": true,
          "contexts": [
            "To iteratively generate every argument for a specific event type, we refer to the entity-based directed acyclic graph (EDAG) method (Zheng et al., 2019).",
            "In our experiments we adopt two public Chinese datasets, i.e. ChiFinAnn (Zheng et al., 2019) and DuEE-fin (Li, 2021).",
            "Recently, document-level event extraction (DEE) attracts great attention from both academic and industrial communities, and is regarded as a promising direction to tackle the above issues (Yang et al., 2018; Zheng et al., 2019; Xu et al., 2021b; Yang et al., 2021; Zhu et al., 2021).",
            "Later, an innovative end-to-end model Doc2EDAG, is proposed (Zheng et al., 2019), which can generate event records via an entity-based directed acyclic graph to fulfill the document-level event extraction effectively.",
            "For example, Pledger and Pledgee in the EquityPledge event could have a relation named as Pledge2Pledgee, and the order of head and tail entities is determined by the pre-order of event arguments (Zheng et al., 2019).",
            "2) Doc2EDAG (Zheng et al., 2019), proposed an end-to-end model which transforms DEE as directly filling event tables with entity-based path expending.",
            "Based on Doc2EDAG, there are some variants appearing.",
            "ChiFinAnn (Zheng et al., 2019) and DuEE-fin (Li, 2021)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            ", 2018), pre-trained language models (Yang et al., 2019), and explicit external knowledge (Liu et al.",
            "To utilize more knowledge, some studies propose to leverage document contexts (Chen et al., 2018; Zhao et al., 2018), pre-trained language models (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231985811,
          "isinfluential": true,
          "contexts": [
            "To predict the argument relations in this step, we adopt the structured self attention network (Xu et al., 2021a) which is the latest method for document-level relation extraction.",
            "Recently, document-level event extraction (DEE) attracts great attention from both academic and industrial communities, and is regarded as a promising direction to tackle the above issues (Yang et al., 2018; Zheng et al., 2019; Xu et al., 2021b; Yang et al., 2021; Zhu et al., 2021).",
            "Compared with the ground truth, our model correctly predicts all event arguments except one, while GIT only captures one event, with an argument missed.",
            "A.4 Case Study Figure 4 shows the prediction results of our model and the best baseline model GIT on the example in Figure 1.",
            "4) GIT (Xu et al., 2021b), a model using heterogeneous graph interaction network as encoder and maintaining a global tracker during the decoding process.",
            "we adopt the structured self attention network (Xu et al., 2021a) which is the latest method for document-level relation extraction.",
            "For instance, GIT (Xu et al., 2021b) designs a heterogeneous graph interaction network to capture global interaction information among different sentences and entity mentions."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "--",
            "--",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction",
            "abstract": "Entities, as the essential elements in relation extraction tasks, exhibit certain structure. In this work, we formulate such entity structure as distinctive dependencies between mention pairs. We then propose SSAN, which incorporates these structural dependencies within the standard self-attention mechanism and throughout the overall encoding stage. Specifically, we design two alternative transformation modules inside each self-attention building block to produce attentive biases so as to adaptively regularize its attention flow. Our experiments demonstrate the usefulness of the proposed entity structure and the effectiveness of SSAN. It significantly outperforms competitive baselines, achieving new state-of-the-art results on three popular document-level relation extraction datasets. We further provide ablation and visualization to show how the entity structure guides the model for better relation extraction. Our code is publicly available.",
            "year": 2021,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1754285124",
                "name": "Benfeng Xu"
              },
              {
                "authorId": "143906199",
                "name": "Quan Wang"
              },
              {
                "authorId": "8020700",
                "name": "Yajuan Lyu"
              },
              {
                "authorId": "2116512598",
                "name": "Yong Zhu"
              },
              {
                "authorId": "1855978",
                "name": "Zhendong Mao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "…and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Zhao et al., 2018; Sha et al., 2018; Yan et al., 2019; Du and Cardie, 2020; Li\n*These authors contributed equally to this work\net al., 2020; Paolini et al., 2021; Lu et al., 2021), extracting events from a single sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "A.1 Distribution of Event Type DuEE-fin Table 8 shows the complete event type and corresponding distribution of DuEE-fin dataset.",
            "When it comes to DuEE-fin, a similar phenomenon can be observed that both the RAATs can contribute positively to our model.",
            "In our experiments we adopt two public Chinese datasets, i.e. ChiFinAnn (Zheng et al., 2019) and DuEE-fin (Li, 2021).",
            "The DuEE-fin dataset has 13 different event types and its test set includes a large size of document samples that do not have any event records, which both make it more complicated.",
            "By our statistics, the training sets of ChiFinAnn and DuEE-fin have about 98.0% and 98.9% records that scatter across sentences respectively.",
            "Table 4 shows the comparison results of our model with baselines on the developing set of\nDuEE-fin and its online testing.",
            "We conduct several offline evaluations for ChiFinAnn, but only an online test for DuEE-fin.",
            "DuEE-fin is also from the financial domain with around 11,900 documents in total."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "273993349": {
      "citing_paper_info": {
        "title": "Are Triggers Needed for Document-Level Event Extraction?",
        "abstract": "Most existing work on event extraction has focused on sentence-level texts and presumes the identification of a trigger-span -- a word or phrase in the input that evokes the occurrence of an event of interest. Event arguments are then extracted with respect to the trigger. Indeed, triggers are treated as integral to, and trigger detection as an essential component of, event extraction. In this paper, we provide the first investigation of the role of triggers for the more difficult and much less studied task of document-level event extraction. We analyze their usefulness in multiple end-to-end and pipelined transformer-based event extraction models for three document-level event extraction datasets, measuring performance using triggers of varying quality (human-annotated, LLM-generated, keyword-based, and random). We find that whether or not systems benefit from explicitly extracting triggers depends both on dataset characteristics (i.e. the typical number of events per document) and task-specific information available during extraction (i.e. natural language event schemas). Perhaps surprisingly, we also observe that the mere existence of triggers in the input, even random ones, is important for prompt-based in-context learning approaches to the task.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "65877664",
            "name": "Shaden Shaar"
          },
          {
            "authorId": "2330418836",
            "name": "Wayne Chen"
          },
          {
            "authorId": "2330397941",
            "name": "Maitreyi Chatterjee"
          },
          {
            "authorId": "2160611160",
            "name": "Barry Wang"
          },
          {
            "authorId": "2331171152",
            "name": "Wenting Zhao"
          },
          {
            "authorId": "2064285348",
            "name": "Claire Cardie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 40,
        "unique_cited_count": 36,
        "influential_count": 5,
        "detailed_records_count": 40
      },
      "cited_papers": [
        "235253912",
        "269214164",
        "248512779",
        "258967833",
        "2114517",
        "218470122",
        "259370571",
        "52967399",
        "237363266",
        "2257053",
        "207847663",
        "250390839",
        "204838007",
        "51871198",
        "202539496",
        "216562779",
        "247155069",
        "196178503",
        "231602921",
        "14365335",
        "14339673",
        "235097664",
        "233219850",
        "258887711",
        "1053009",
        "203701085",
        "52822214",
        "220047190",
        "13756489",
        "207853145",
        "1320606",
        "252873525",
        "14492070",
        "208031598",
        "247084444",
        "248780414"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1053009,
          "isinfluential": false,
          "contexts": [
            "Document-level event extraction systems utilize drastically different techniques: earlier methods relied on pattern matching (Riloff, 1993; Riloff and Jones, 1999), while more modern approaches use deep learning to represent event components such as triggers and entities (Wad-den et al., 2019; Wang…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping",
            "abstract": "Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories.",
            "year": 1999,
            "venue": "AAAI/IAAI",
            "authors": [
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              },
              {
                "authorId": "144494993",
                "name": "R. Jones"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "The vast majority of that research focuses on sentence-level event extraction (Ji and Grishman, 2008; Li et al., 2013; Yang et al., 2019; Wadden et al., 2019, i.a.): given a text and a predefined set of event type s E , each e ∈ E with its own set of arguments A e (also known as roles ), find all…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "The vast majority of that research focuses on sentence-level event extraction (Ji and Grishman, 2008; Li et al., 2013; Yang et al., 2019; Wadden et al., 2019, i.a.): given a text and a predefined set of event type s E , each e ∈ E with its own set of arguments A e (also known as roles ), find all…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2257053,
          "isinfluential": false,
          "contexts": [
            "Document-level event extraction systems utilize drastically different techniques: earlier methods relied on pattern matching (Riloff, 1993; Riloff and Jones, 1999), while more modern approaches use deep learning to represent event components such as triggers and entities (Wad-den et al., 2019; Wang…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatically Constructing a Dictionary for Information Extraction Tasks",
            "abstract": "Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary.",
            "year": 1993,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Most recently, the task has been framed as sequence-to-sequence generation (i.e. TANL (Paolini et al., 2021), GTT (Du et al., 2021) and D EGREE (Hsu et al., 2022)) built upon powerful transformer (Vaswani et al., 2017) language models.",
            "We analyze whether using triggers improves event extraction performance in multiple end-to-end and pipelined transformer-based (Vaswani et al., 2017) event extraction models for three document-level event extraction datasets."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "We primarily focus on TANL (Paolini et al., 2021) and G EN IE (Li et al., 2021) that model the task via sequence generation, though other methods that rely on careful manipulation and learning of embeddings also exist (Chen et al., 2015; Yang et al., 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14365335,
          "isinfluential": false,
          "contexts": [
            "Docu-ment-level event extraction was introduced in the Message Understanding Conferences ( MUC ) (Sundheim, 1991) as a task to identify occurrences of predefined event types and fill the associated template roles by extracting entity mentions from the text or categorizing aspects of the event.",
            "Similar to previous work, we perform our analyses on the MUC (Sundheim, 1991), WikiEvents (Li et al., 2021) and CMNEE (Zhu et al., 2024) datasets."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Overview of the Third Message Understanding Evaluation and Conference",
            "abstract": "The Naval Ocean Systems Center (NOSC) has conducted the third in a series of evaluations of English text analysis systems. These evaluations are intended to advance our understanding of the merits of current text analysis techniques, as applied to the performance of a realistic information extraction task. The latest one is also intended to provide insight into information retrieval technology (document retrieval and categorization) used instead of or in concert with language understanding technology. The inputs to the analysis/extraction process consist of naturally-occurring texts that were obtained in the form of electronic messages. The outputs of the process are a set of templates or semantic frames resembling the contents of a partially formatted database.",
            "year": 1991,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14492070,
          "isinfluential": false,
          "contexts": [
            "Note that event trigger-span identification is not required as part of the output; indeed, most datasets for the task do not include trigger-span annotations (Soboroff, 2023; Jain et al., 2020; Pavlick et al., 2016)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "The Gun Violence Database: A new task and data set for NLP",
            "abstract": "We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem.",
            "year": 2016,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2949185",
                "name": "Ellie Pavlick"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "34741133",
                "name": "Xiaoman Pan"
              },
              {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "Yang et al. (2018b) show that providing rationales as extra supervision enhances a model’s capability to perform multi-hop reasoning.",
            "Existing datasets such as CMNEE and DCFEE (Yang et al., 2018a) are in Chinese (therefore likely require translation before use) while others such as RAMS (Ebner et al., 2020) and Doc-EE (Tong et al., 2022) do not exhibit traits of document level datasets ( RAMS has 1 event per document and…",
            "Existing datasets such as CMNEE and DCFEE (Yang et al., 2018a) are in Chinese (therefore likely require translation before use) while others such as RAMS (Ebner et al., 2020) and Doc-EE (Tong et al., 2022) do not exhibit traits of document level datasets ( RAMS has 1 event per document and “doc-ument\" are restricted to 5 sentence windows surrounding the trigger while Doc-EE has only one event per document)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52822214,
          "isinfluential": false,
          "contexts": [
            "Yang et al. (2018b) show that providing rationales as extra supervision enhances a model’s capability to perform multi-hop reasoning."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
            "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
              },
              {
                "authorId": "50531624",
                "name": "Peng Qi"
              },
              {
                "authorId": "35097114",
                "name": "Saizheng Zhang"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              },
              {
                "authorId": "50056360",
                "name": "William W. Cohen"
              },
              {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "GTT (Du et al., 2021) is an end-to-end model that takes a document and an ordered list of the possible event types and generates filled templates for all event types in a single pass through BERT (Devlin et al., 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "The vast majority of that research focuses on sentence-level event extraction (Ji and Grishman, 2008; Li et al., 2013; Yang et al., 2019; Wadden et al., 2019, i.a.): given a text and a predefined set of event type s E , each e ∈ E with its own set of arguments A e (also known as roles ), find all…",
            "We primarily focus on TANL (Paolini et al., 2021) and G EN IE (Li et al., 2021) that model the task via sequence generation, though other methods that rely on careful manipulation and learning of embeddings also exist (Chen et al., 2015; Yang et al., 2019)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "The vast majority of that research focuses on sentence-level event extraction (Ji and Grishman, 2008; Li et al., 2013; Yang et al., 2019; Wadden et al., 2019, i.a.): given a text and a predefined set of event type s E , each e ∈ E with its own set of arguments A e (also known as roles ), find all…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203701085,
          "isinfluential": false,
          "contexts": [
            "There is a long history of research in Natural Language Processing (NLP) on the topic of event extraction (Grishman, 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Twenty-five years of information extraction",
            "abstract": "Abstract Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.",
            "year": 2019,
            "venue": "Natural Language Engineering",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "As shown in Figure 2, TANL fine-tunes a T5-large transformer (Raffel et al., 2020) to complete event extraction in two phases: 1) event detection, and 2) argument extraction."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207847663,
          "isinfluential": false,
          "contexts": [
            "DeYoung et al. (2020) assert that rationales can function as explanations, helping end users evaluate a system’s trustworthiness."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
            "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "48727916",
                "name": "Jay DeYoung"
              },
              {
                "authorId": "49837811",
                "name": "Sarthak Jain"
              },
              {
                "authorId": "8937909",
                "name": "Nazneen Rajani"
              },
              {
                "authorId": "51172373",
                "name": "Eric P. Lehman"
              },
              {
                "authorId": "2228109",
                "name": "Caiming Xiong"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "1912476",
                "name": "Byron C. Wallace"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": true,
          "contexts": [
            "In cases where they denote a (verb or noun) predicate for the event, they will serve as an anchor to localize argument extraction (Ebner et al., 2020).",
            "Other event extraction datasets include RAMS (Ebner et al., 2020), CMNEE (Zhu et al., 2024), WikiEvents (Li et al., 2021), and MAVEN (Wang et al., 2020).",
            "…datasets such as CMNEE and DCFEE (Yang et al., 2018a) are in Chinese (therefore likely require translation before use) while others such as RAMS (Ebner et al., 2020) and Doc-EE (Tong et al., 2022) do not exhibit traits of document level datasets ( RAMS has 1 event per document and “doc-ument\"…",
            "Existing datasets such as CMNEE and DCFEE (Yang et al., 2018a) are in Chinese (therefore likely require translation before use) while others such as RAMS (Ebner et al., 2020) and Doc-EE (Tong et al., 2022) do not exhibit traits of document level datasets ( RAMS has 1 event per document and “doc-ument\" are restricted to 5 sentence windows surrounding the trigger while Doc-EE has only one event per document)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208031598,
          "isinfluential": false,
          "contexts": [
            "Prior work treats triggers as integral to accurate event extraction (M’hamdi et al., 2019; Tong et al., 2020; Lin et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Contextualized Cross-Lingual Event Trigger Extraction with Minimal Resources",
            "abstract": "Event trigger extraction is an information extraction task of practical utility, yet it is challenging due to the difficulty of disambiguating word sense meaning. Previous approaches rely extensively on hand-crafted language-specific features and are applied mainly to English for which annotated datasets and Natural Language Processing (NLP) tools are available. However, the availability of such resources varies from one language to another. Recently, contextualized Bidirectional Encoder Representations from Transformers (BERT) models have established state-of-the-art performance for a variety of NLP tasks. However, there has not been much effort in exploring language transfer using BERT for event extraction. In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms language-specific models for both English and Chinese. Our work is the first to experiment with two event architecture variants in a cross-lingual setting, to show the effectiveness of contextualized embeddings obtained using BERT, and to explore and analyze its performance on Arabic.",
            "year": 2019,
            "venue": "Conference on Computational Natural Language Learning",
            "authors": [
              {
                "authorId": "1411352299",
                "name": "Meryem M'hamdi"
              },
              {
                "authorId": "2052513135",
                "name": "Marjorie Freedman"
              },
              {
                "authorId": "143823227",
                "name": "Jonathan May"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562779,
          "isinfluential": false,
          "contexts": [
            "Other event extraction datasets include RAMS (Ebner et al., 2020), CMNEE (Zhu et al., 2024), WikiEvents (Li et al., 2021), and MAVEN (Wang et al., 2020)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "MAVEN: A Massive General Domain Event Detection Dataset",
            "abstract": "Event detection (ED), which identifies event trigger words and classifies event types according to contexts, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Small scale of existing datasets is not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Limited event types of existing datasets lead to the trained models cannot be easily adapted to general-domain scenarios. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 117,200 event mention instances, and 207 event types. MAVEN alleviates the lack of data problem and covers much more general event types. Besides the dataset, we reproduce the recent state-of-the-art ED models and conduct a thorough evaluation for these models on MAVEN. The experimental results and empirical analyses show that existing ED methods cannot achieve promising results as on the small datasets, which suggests ED in real world remains a challenging task and requires further research efforts. The dataset and baseline code will be released in the future to promote this field.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "1659764899",
                "name": "Wangyi Jiang"
              },
              {
                "authorId": "151185222",
                "name": "Rong Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218470122,
          "isinfluential": false,
          "contexts": [
            "Note that event trigger-span identification is not required as part of the output; indeed, most datasets for the task do not include trigger-span annotations (Soboroff, 2023; Jain et al., 2020; Pavlick et al., 2016)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "SciREX: A Challenge Dataset for Document-Level Information Extraction",
            "abstract": "Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49837811",
                "name": "Sarthak Jain"
              },
              {
                "authorId": "15292561",
                "name": "Madeleine van Zuylen"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              },
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220047190,
          "isinfluential": false,
          "contexts": [
            "Prior work treats triggers as integral to accurate event extraction (M’hamdi et al., 2019; Tong et al., 2020; Lin et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Improving Event Detection via Open-domain Trigger Knowledge",
            "abstract": "Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152439499",
                "name": "Meihan Tong"
              },
              {
                "authorId": "143876183",
                "name": "Bin Xu"
              },
              {
                "authorId": "2118512998",
                "name": "Shuai Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "2109935759",
                "name": "Jun Xie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231602921,
          "isinfluential": true,
          "contexts": [
            "Specifically, we study triggers in conjunction with four recent sequence-to-sequence transformer-based approaches to document-level event extraction : TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021).",
            "We primarily focus on TANL (Paolini et al., 2021) and G EN IE (Li et al., 2021) that model the task via sequence generation, though other methods that rely on careful manipulation and learning of embeddings also exist (Chen et al., 2015; Yang et al., 2019).",
            "An example of the prompt can be found in Appendix B. TANL (Paolini et al., 2021) is a pipeline-based model that takes only the document as input.",
            "We study the performance of four state-of-the-art sequence-to-sequence style systems for document-level event extraction: TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021) and compare them against prompt-based baselines using GPT-4 O and…",
            "Our work focuses primarily on such methods due to the flexibility of sequence-to-sequence frameworks (Paolini et al., 2021) and because we believe future methods will increasingly utilize constantly improving LLMs. Event Argument Extraction.",
            "Most recently, the task has been framed as sequence-to-sequence generation (i.e. TANL (Paolini et al., 2021), GTT (Du et al., 2021) and D EGREE (Hsu et al., 2022)) built upon powerful transformer (Vaswani et al., 2017) language models."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Structured Prediction as Translation between Augmented Natural Languages",
            "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",
            "year": 2021,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1950846",
                "name": "Giovanni Paolini"
              },
              {
                "authorId": "2095707",
                "name": "Ben Athiwaratkun"
              },
              {
                "authorId": "49186783",
                "name": "Jason Krone"
              },
              {
                "authorId": "2115889791",
                "name": "Jie Ma"
              },
              {
                "authorId": "16163297",
                "name": "A. Achille"
              },
              {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
              },
              {
                "authorId": "1790831",
                "name": "C. D. Santos"
              },
              {
                "authorId": "144028698",
                "name": "Bing Xiang"
              },
              {
                "authorId": "1715959",
                "name": "Stefano Soatto"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": true,
          "contexts": [
            "G EN IE (Li et al., 2021) was originally proposed for argument extraction.",
            "Specifically, we study triggers in conjunction with four recent sequence-to-sequence transformer-based approaches to document-level event extraction : TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021).",
            "We primarily focus on TANL (Paolini et al., 2021) and G EN IE (Li et al., 2021) that model the task via sequence generation, though other methods that rely on careful manipulation and learning of embeddings also exist (Chen et al., 2015; Yang et al., 2019).",
            "Similar to previous work, we perform our analyses on the MUC (Sundheim, 1991), WikiEvents (Li et al., 2021) and CMNEE (Zhu et al., 2024) datasets.",
            "Argument extraction is a subtask of event extraction that requires identifying fillers for the roles associated with the event given an event type as well as its trigger span (Li et al., 2021; Ren et al., 2023; Ma et al., 2022).",
            "Other event extraction datasets include RAMS (Ebner et al., 2020), CMNEE (Zhu et al., 2024), WikiEvents (Li et al., 2021), and MAVEN (Wang et al., 2020).",
            "Li et al. (2021), for example, find that the arguments of close to 40% of events in their Wikipedia-based dataset appear in sentences other than the event trigger sentence.",
            "…of four state-of-the-art sequence-to-sequence style systems for document-level event extraction: TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021) and compare them against prompt-based baselines using GPT-4 O and GPT-4 O - MINI ."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235097664,
          "isinfluential": true,
          "contexts": [
            "Specifically, we study triggers in conjunction with four recent sequence-to-sequence transformer-based approaches to document-level event extraction : TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021).",
            "GTT (Du et al., 2021) is an end-to-end model that takes a document and an ordered list of the possible event types and generates filled templates for all event types in a single pass through BERT (Devlin et al., 2019).",
            "Following previous work (Du et al., 2021; Wang et al., 2023a), we restricted event templates to contain 6 out of the original 24 roles — EVENT - TYPE , PER - PETRATOR , PERPETRATOR - ORGANIZATION , TAR -",
            "…the performance of four state-of-the-art sequence-to-sequence style systems for document-level event extraction: TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021) and compare them against prompt-based baselines using GPT-4 O and GPT-4 O…",
            "Most recently, the task has been framed as sequence-to-sequence generation (i.e. TANL (Paolini et al., 2021), GTT (Du et al., 2021) and D EGREE (Hsu et al., 2022)) built upon powerful transformer (Vaswani et al., 2017) language models."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Template Filling with Generative Transformers",
            "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": false,
          "contexts": [
            "…utilize drastically different techniques: earlier methods relied on pattern matching (Riloff, 1993; Riloff and Jones, 1999), while more modern approaches use deep learning to represent event components such as triggers and entities (Wad-den et al., 2019; Wang et al., 2023c; Xu et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237363266,
          "isinfluential": false,
          "contexts": [
            "Prior work treats triggers as integral to accurate event extraction (M’hamdi et al., 2019; Tong et al., 2020; Lin et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues",
            "abstract": "The goal of dialogue relation extraction (DRE) is to identify the relation between two entities in a given dialogue. During conversations, speakers may expose their relations to certain entities by explicit or implicit clues, such evidences called “triggers”. However, trigger annotations may not be always available for the target data, so it is challenging to leverage such information for enhancing the performance. Therefore, this paper proposes to learn how to identify triggers from the data with trigger annotations and then transfers the trigger-finding capability to other datasets for better performance. The experiments show that the proposed approach is capable of improving relation extraction performance of unseen relations and also demonstrate the transferability of our proposed trigger-finding model across different domains and datasets.",
            "year": 2021,
            "venue": "SIGDIAL Conferences",
            "authors": [
              {
                "authorId": "2147344624",
                "name": "Po-Wei Lin"
              },
              {
                "authorId": "27629426",
                "name": "Shang-Yu Su"
              },
              {
                "authorId": "2144862809",
                "name": "Yun-Nung Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": false,
          "contexts": [
            "Argument extraction is a subtask of event extraction that requires identifying fillers for the roles associated with the event given an event type as well as its trigger span (Li et al., 2021; Ren et al., 2023; Ma et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247155069,
          "isinfluential": false,
          "contexts": [
            "This follows findings in recent work (Min et al., 2022) and suggests that learning the concept of a trigger, even without proper examples, meaningfully supports extraction."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
            "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48872685",
                "name": "Sewon Min"
              },
              {
                "authorId": "2156533327",
                "name": "Xinxi Lyu"
              },
              {
                "authorId": "14487640",
                "name": "Ari Holtzman"
              },
              {
                "authorId": "2347956",
                "name": "Mikel Artetxe"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248512779,
          "isinfluential": true,
          "contexts": [
            "Most recently, the task has been framed as sequence-to-sequence generation (i.e. TANL (Paolini et al., 2021), GTT (Du et al., 2021) and D EGREE (Hsu et al., 2022)) built upon powerful transformer (Vaswani et al., 2017) language models.",
            "Specifically, we study triggers in conjunction with four recent sequence-to-sequence transformer-based approaches to document-level event extraction : TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021).",
            "D EGREE (Hsu et al., 2022) is an end-to-end model that takes a document, a natural language template for one event type E and extra information about events of type E (i.e. type definitions, example triggers); it generates filled natural language templates (that include identified triggers) for all…",
            "…of four state-of-the-art sequence-to-sequence style systems for document-level event extraction: TANL (Paolini et al., 2021), GTT (Du et al., 2021), D EGREE (Hsu et al., 2022), and G EN IE (Li et al., 2021) and compare them against prompt-based baselines using GPT-4 O and GPT-4 O - MINI ."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DEGREE: A Data-Efficient Generation-Based Event Extraction Model",
            "abstract": "Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage DEGREE and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "3256207",
                "name": "Elizabeth Boschee"
              },
              {
                "authorId": "123937952",
                "name": "Scott Miller"
              },
              {
                "authorId": "2104644641",
                "name": "Premkumar Natarajan"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248780414,
          "isinfluential": false,
          "contexts": [
            "We follow the matching algorithm of Das et al. (2022) to align and score the predicted templates 6 .",
            "Unfortunately, the document-level task setting poses significant challenges (Das et al., 2022)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatic Error Analysis for Document-level Information Extraction",
            "abstract": "Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2148357499",
                "name": "Aliva Das"
              },
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2160611160",
                "name": "Barry Wang"
              },
              {
                "authorId": "46749263",
                "name": "Kejian Shi"
              },
              {
                "authorId": "2216587688",
                "name": "J. Gu"
              },
              {
                "authorId": "2052386937",
                "name": "Thomas J. Porter"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250390839,
          "isinfluential": false,
          "contexts": [
            "Additionally, some more elaborate prompting-based approaches such as C ODE 4S TRUCT (Wang et al., 2022) warrant testing given the rapidly developing capabilities of LLMs.",
            "Existing datasets such as CMNEE and DCFEE (Yang et al., 2018a) are in Chinese (therefore likely require translation before use) while others such as RAMS (Ebner et al., 2020) and Doc-EE (Tong et al., 2022) do not exhibit traits of document level datasets ( RAMS has 1 event per document and “doc-ument\" are restricted to 5 sentence windows surrounding the trigger while Doc-EE has only one event per document).",
            "…(Yang et al., 2018a) are in Chinese (therefore likely require translation before use) while others such as RAMS (Ebner et al., 2020) and Doc-EE (Tong et al., 2022) do not exhibit traits of document level datasets ( RAMS has 1 event per document and “doc-ument\" are restricted to 5 sentence…"
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction",
            "abstract": "Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152439499",
                "name": "Meihan Tong"
              },
              {
                "authorId": "2113744169",
                "name": "Bin Xu"
              },
              {
                "authorId": "2118512998",
                "name": "Shuai Wang"
              },
              {
                "authorId": "2175603382",
                "name": "Meihuan Han"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "1737159260",
                "name": "Jiangqi Zhu"
              },
              {
                "authorId": "2175598253",
                "name": "Siyu Chen"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252873525,
          "isinfluential": false,
          "contexts": [
            "For example, I TER X (Chen et al., 2023) formulates event extraction as a Markov decision process while P ROCNET builds a graph over entity representations to be parsed into events (Wang et al., 2023c)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Iterative Document-level Information Extraction via Imitation Learning",
            "abstract": "We present a novel iterative extraction model, IterX, for extracting complex relations, or templates, i.e., N-tuples representing a mapping from named slots to spans of text within a document. Documents may feature zero or more instances of a template of any given type, and the task of template extraction entails identifying the templates in a document and extracting each template’s slot values. Our imitation learning approach casts the problem as a Markov decision process (MDP), and relieves the need to use predefined template orders to train an extractor. It leads to state-of-the-art results on two established benchmarks – 4-ary relation extraction on SciREX and template extraction on MUC-4 – as well as a strong baseline on the new BETTER Granular task.",
            "year": 2022,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "104375103",
                "name": "Yunmo Chen"
              },
              {
                "authorId": "1447311948",
                "name": "William Gantt Walden"
              },
              {
                "authorId": "2061383963",
                "name": "Weiwei Gu"
              },
              {
                "authorId": "40364920",
                "name": "Tongfei Chen"
              },
              {
                "authorId": "1738705340",
                "name": "Aaron Steven White"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258887711,
          "isinfluential": false,
          "contexts": [
            "LLM triggers Recent work has demonstrated that GPT performs well on some information extraction tasks (Wang et al., 2023b; Wadhwa et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Code4Struct: Code Generation for Few-Shot Event Structure Prediction",
            "abstract": "Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code.We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks.As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code.This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints.We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts.Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12% absolute F1.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2144803999",
                "name": "Xingyao Wang"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2072975661",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258967833,
          "isinfluential": false,
          "contexts": [
            "For example, I TER X (Chen et al., 2023) formulates event extraction as a Markov decision process while P ROCNET builds a graph over entity representations to be parsed into events (Wang et al., 2023c).",
            "…utilize drastically different techniques: earlier methods relied on pattern matching (Riloff, 1993; Riloff and Jones, 1999), while more modern approaches use deep learning to represent event components such as triggers and entities (Wad-den et al., 2019; Wang et al., 2023c; Xu et al., 2021)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization",
            "abstract": "Document-level multi-event extraction aims to extract the structural information from a given document automatically. Most recent approaches usually involve two steps: (1) modeling entity interactions; (2) decoding entity interactions into events. However, such approaches ignore a global view of inter-dependency of multiple events. Moreover, an event is decoded by iteratively merging its related entities as arguments, which might suffer from error propagation and is computationally inefficient. In this paper, we propose an alternative approach for document-level multi-event extraction with event proxy nodes and Hausdorff distance minimization. The event proxy nodes, representing pseudo-events, are able to build connections with other event proxy nodes, essentially capturing global information. The Hausdorff distance makes it possible to compare the similarity between the set of predicted events and the set of ground-truth events. By directly minimizing Hausdorff distance, the model is trained towards the global optimum directly, which improves performance and reduces training time. Experimental results show that our model outperforms previous state-of-the-art method in F1-score on two datasets with only a fraction of training time.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2144706657",
                "name": "Xinyu Wang"
              },
              {
                "authorId": "145096580",
                "name": "Lin Gui"
              },
              {
                "authorId": "1390509967",
                "name": "Yulan He"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259370571,
          "isinfluential": false,
          "contexts": [
            "Argument extraction is a subtask of event extraction that requires identifying fillers for the roles associated with the event given an event type as well as its trigger span (Li et al., 2021; Ren et al., 2023; Ma et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation",
            "abstract": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "47184362",
                "name": "Yanan Cao"
              },
              {
                "authorId": "2075394870",
                "name": "Ping Guo"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2185915076",
                "name": "Wei Ma"
              },
              {
                "authorId": "1390641501",
                "name": "Zheng Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269214164,
          "isinfluential": false,
          "contexts": [
            "Other event extraction datasets include RAMS (Ebner et al., 2020), CMNEE (Zhu et al., 2024), WikiEvents (Li et al., 2021), and MAVEN (Wang et al., 2020).",
            "Similar to previous work, we perform our analyses on the MUC (Sundheim, 1991), WikiEvents (Li et al., 2021) and CMNEE (Zhu et al., 2024) datasets."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "CMNEE:A Large-Scale Document-Level Event Extraction Dataset Based on Open-Source Chinese Military News",
            "abstract": "Extracting structured event knowledge, including event triggers and corresponding arguments, from military texts is fundamental to many applications, such as intelligence analysis and decision assistance. However, event extraction in the military field faces the data scarcity problem, which impedes the research of event extraction models in this domain. To alleviate this problem, we propose CMNEE, a large-scale, document-level open-source Chinese Military News Event Extraction dataset. It contains 17,000 documents and 29,223 events, which are all manually annotated based on a pre-defined schema for the military domain including 8 event types and 11 argument role types. We designed a two-stage, multi-turns annotation strategy to ensure the quality of CMNEE and reproduced several state-of-the-art event extraction models with a systematic evaluation. The experimental results on CMNEE fall shorter than those on other domain datasets obviously, which demonstrates that event extraction for military domain poses unique challenges and requires further research efforts. Our code and data can be obtained from https://github.com/Mzzzhu/CMNEE. Keywords: Corpus,Information Extraction, Information Retrieval, Knowledge Discovery/Representation",
            "year": 2024,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2297636985",
                "name": "Mengna Zhu"
              },
              {
                "authorId": "2117883657",
                "name": "Zijie Xu"
              },
              {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
              },
              {
                "authorId": "2297186713",
                "name": "Kaiming Xiao"
              },
              {
                "authorId": "2297374593",
                "name": "Mao Wang"
              },
              {
                "authorId": "1596819256",
                "name": "Wenjun Ke"
              },
              {
                "authorId": "2284447693",
                "name": "Hongbin Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "LLM triggers Recent work has demonstrated that GPT performs well on some information extraction tasks (Wang et al., 2023b; Wadhwa et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Triggers , as defined in Walker et al. (2005), refer to text spans (verbs, nouns, or adjectives) that most clearly express the occurrence of one or more events."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Note that event trigger-span identification is not required as part of the output; indeed, most datasets for the task do not include trigger-span annotations (Soboroff, 2023; Jain et al., 2020; Pavlick et al., 2016)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Most recently, Chen et al. (2022) demonstrate the potential of rationale-based NLP systems on adversarial inputs."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "271116589": {
      "citing_paper_info": {
        "title": "RAUIE: A Relation-Augmented Document-Level Event Extraction Model Based on UIE",
        "abstract": "In document-level event extraction tasks, multiple event types often coexist within a document (the multi-event problem), and arguments may play different roles across different events (the argument intertwining problem). Existing methods often suffer from significantly reduced event extraction accuracy, mainly when focusing on scenarios with scarce annotated data and imbalanced event-type distributions. This paper emphasizes enhancing the correlation between event triggers and arguments to address these challenges. Therefore, we propose the Relation-Augmented UIE Model (RADIE), which introduces entity relationships into prompts using prompt-based learning and designs a relation-augmentation algorithm to improve the accuracy and completeness of event extraction. Experimental results demonstrate that this approach significantly outperforms baseline methods, mainly showcasing excellent performance on datasets with long-tail distributions.",
        "year": 2024,
        "venue": "2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)",
        "authors": [
          {
            "authorId": "2310859786",
            "name": "Yiyuan Liang"
          },
          {
            "authorId": "2311482569",
            "name": "Hong Jia"
          },
          {
            "authorId": "2118462956",
            "name": "Licai Wang"
          },
          {
            "authorId": "50818197",
            "name": "Yangchen Huang"
          },
          {
            "authorId": "2310915854",
            "name": "Zhe Yang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "248780216",
        "426998",
        "14665391",
        "247619149"
      ],
      "citation_details": [
        {
          "citedcorpusid": 426998,
          "isinfluential": false,
          "contexts": [
            "Reference [7] employed Markov Random Field for sequence labeling, alleviating issues such as low-quality event corpus and a high proportion of pseudo-trigger words."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Modeling of Trigger Identification and Event Type Determination in Chinese Event Extraction",
            "abstract": "",
            "year": 2012,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "47470867",
                "name": "Peifeng Li"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              },
              {
                "authorId": "2895159",
                "name": "H. Diao"
              },
              {
                "authorId": "143740945",
                "name": "Guodong Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14665391,
          "isinfluential": false,
          "contexts": [
            "Then, the text to be extracted is matched against these patterns to obtain extraction results, such as AutoSlog-TS extracts pattern recognizers[5]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Introduction to the Sundance and AutoSlog Systems",
            "abstract": ",",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              },
              {
                "authorId": "2053634833",
                "name": "W. Phillips"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247619149,
          "isinfluential": false,
          "contexts": [
            "Reference [13] Inspired by the abovementioned research, this paper adopts the UIE model for text feature learning[14].",
            "Additionally, we utilize rejection mechanisms for data augmentation [14]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unified Structure Generation for Universal Information Extraction",
            "abstract": "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2154976399",
                "name": "Qing Liu"
              },
              {
                "authorId": "40495683",
                "name": "Dai Dai"
              },
              {
                "authorId": "2107521158",
                "name": "Xinyan Xiao"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "2149181702",
                "name": "Hua Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248780216,
          "isinfluential": false,
          "contexts": [
            "…information in natural language text can be transformed into structured forms understandable and processable by computers, thereby providing structured information for downstream tasks such as knowledge graph construction [1], machine reading comprehension [2], and knowledge question answering [3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BEEDS: Large-Scale Biomedical Event Extraction using Distant Supervision and Question Answering",
            "abstract": "Automatic extraction of event structures from text is a promising way to extract important facts from the evergrowing amount of biomedical literature. We propose BEEDS, a new approach on how to mine event structures from PubMed based on a question-answering paradigm. Using a three-step pipeline comprising a document retriever, a document reader, and an entity normalizer, BEEDS is able to fully automatically extract event triples involving a query protein or gene and to store this information directly in a knowledge base. BEEDS applies a transformer-based architecture for event extraction and uses distant supervision to augment the scarce training data in event mining. In a knowledge base population setting, it outperforms a strong baseline in finding post-translational modification events consisting of enzyme-substrate-site triples while achieving competitive results in extracting binary relations consisting of protein-protein and protein-site interactions.",
            "year": 2022,
            "venue": "Workshop on Biomedical Natural Language Processing",
            "authors": [
              {
                "authorId": "47120472",
                "name": "Xinglong Wang"
              },
              {
                "authorId": "1693022",
                "name": "U. Leser"
              },
              {
                "authorId": "20308468",
                "name": "Leon Weber"
              }
            ]
          }
        }
      ]
    },
    "258998173": {
      "citing_paper_info": {
        "title": "Document-level Event Extraction - A Survey of Methods and Applications",
        "abstract": "Event extraction aims to detect occurrences of specified types and extract corresponding event arguments from unstructured data input, which is an integral part of information extraction [1]. Many downstream tasks, such as text summarization, causal relationship identification, and event reasoning, make extensive use of event extraction. However, most of the existing research on event extraction remains at the sentence level. Document-level event extraction is still under exploration and lacks relevant review. Focusing on the task of document-level event extraction, this survey first divides the existing technologies into three categories and introduces the most representative models; Then, we list some commonly used datasets for document-level event extraction with their usage messages; Finally, we analyse the current research gaps of document-level event extraction and future research trends.",
        "year": 2023,
        "venue": "Journal of Physics: Conference Series",
        "authors": [
          {
            "authorId": "2144831701",
            "name": "Qi Liu"
          },
          {
            "authorId": "2218656359",
            "name": "Zhen Luan"
          },
          {
            "authorId": "31328338",
            "name": "Kunlong Wang"
          },
          {
            "authorId": "2219054579",
            "name": "Ye Zou"
          },
          {
            "authorId": "143830417",
            "name": "Bing Liu"
          },
          {
            "authorId": "2219063788",
            "name": "Yang Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "19220240",
        "3469394",
        "245934663",
        "218501728",
        "233219850"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3469394,
          "isinfluential": false,
          "contexts": [
            "Automated techniques can extract information from clinical medical data that is useful for scientific study [12], increasing the productivity of researchers and hastening the advancement of drug development."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "SSEL-ADE: A semi-supervised ensemble learning framework for extracting adverse drug events from social media",
            "abstract": "",
            "year": 2017,
            "venue": "Artif. Intell. Medicine",
            "authors": [
              {
                "authorId": "46701354",
                "name": "J. Liu"
              },
              {
                "authorId": "2494598",
                "name": "Songzheng Zhao"
              },
              {
                "authorId": "2096527",
                "name": "G. Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19220240,
          "isinfluential": false,
          "contexts": [
            "[7] uses a dependency bridge to enhance its information representation when modelling each word."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction",
            "abstract": "\n \n Event extraction plays an important role in natural language processing (NLP) applications including question answering and information retrieval. Traditional event extraction relies heavily on lexical and syntactic features, which require intensive human engineering and may not generalize to different datasets. Deep neural networks, on the other hand, are able to automatically learn underlying features, but existing networks do not make full use of syntactic relations. In this paper, we propose a novel dependency bridge recurrent neural network (dbRNN) for event extraction. We build our model upon a recurrent neural network, but enhance it with dependency bridges, which carry syntactically related information when modeling each word.We illustrates that simultaneously applying tree structure and sequence structure in RNN brings much better performance than only uses sequential RNN. In addition, we use a tensor layer to simultaneously capture the various types of latent interaction between candidate arguments as well as identify/classify all arguments of an event. Experiments show that our approach achieves competitive results compared with previous work.\n \n",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "39058310",
                "name": "Lei Sha"
              },
              {
                "authorId": "2053324591",
                "name": "Feng Qian"
              },
              {
                "authorId": "39488576",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218501728,
          "isinfluential": false,
          "contexts": [
            "[8] proposed Weakly Aligned Structured Embedding (WASE), a structured representation of semantic information extracted from textual and visual data encoded into a common embedding space."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross-media Structured Common Space for Multimedia Event Extraction",
            "abstract": "We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3361240",
                "name": "Manling Li"
              },
              {
                "authorId": "2778637",
                "name": "Alireza Zareian"
              },
              {
                "authorId": "145653969",
                "name": "Qi Zeng"
              },
              {
                "authorId": "153188991",
                "name": "Spencer Whitehead"
              },
              {
                "authorId": "152347526",
                "name": "Di Lu"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "9546964",
                "name": "Shih-Fu Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "[3] put out a document-level neural event argument extraction model, which describes tasks as conditional generation following event templates."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245934663,
          "isinfluential": false,
          "contexts": [
            "4 focus of public opinion more quickly, some researchers [10] are committed to automatically extracting various types of events from vast internet electronic texts."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Target-dependent Event Detection: A New Task to Event Extraction from News",
            "abstract": "Event extraction aims to detect events and extract event arguments. However, various events are not only too nuanced and complex to distinguish, but also involve multiple entities in the real-world scenario, especially in the financial field. This brings a great challenge to the current event extraction. To address these problems, previous event-centric methods detect events first and then extract arguments. Due to the diversity and complexity of events, event detection has a low performance, which is unfit for the huge amount of news in the real world. Given that the performance of named entity recognition (NER) is satisfactory, we shift our perspective from event-centric to target-centric view. In this paper, we propose a new task: target-dependent event detection (TDED), which aims to extract target entities and detect their corresponding events. We also propose a semantic and syntactic aware approach to support thousands of target entity extraction first and dozens of event types detection, that can be applied to massive corpora. Experimental results on a real-world Chinese financial dataset demonstrate that our model outperforms previous methods, especially in complex scenarios.",
            "year": 2021,
            "venue": "2021 IEEE International Conference on Big Data (Big Data)",
            "authors": [
              {
                "authorId": "2146334451",
                "name": "Tiantian Zhang"
              },
              {
                "authorId": "49723181",
                "name": "Xin Mao"
              },
              {
                "authorId": "2151285629",
                "name": "Dejian Li"
              },
              {
                "authorId": "2153935133",
                "name": "Meirong Ma"
              },
              {
                "authorId": "1491238705",
                "name": "Haonan Yuan"
              },
              {
                "authorId": "1490772341",
                "name": "Jianchao Zhu"
              },
              {
                "authorId": "49284832",
                "name": "Man Lan"
              }
            ]
          }
        }
      ]
    },
    "245123950": {
      "citing_paper_info": {
        "title": "Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph",
        "abstract": "Most previous studies of document-level event extraction mainly focus on building argument chains in an autoregressive way, which achieves a certain success but is inefficient in both training and inference.\n\nIn contrast to the previous studies, we propose a fast and lightweight model named as PTPCG.\n\nIn our model, we design a novel strategy for event argument combination together with a non-autoregressive decoding algorithm via pruned complete graphs, which are constructed under the guidance of the automatically selected pseudo triggers.\n\nCompared to the previous systems, our system achieves competitive results with 19.8% of parameters and much lower resource consumption, taking only 3.8% GPU hours for training and up to 8.5 times faster for inference.\n\nBesides, our model shows superior compatibility for the datasets with (or without) triggers and the pseudo triggers can be the supplements for annotated triggers to make further improvements.\n\nCodes are available at https://github.com/Spico197/DocEE .",
        "year": 2021,
        "venue": "International Joint Conference on Artificial Intelligence",
        "authors": [
          {
            "authorId": "1914586128",
            "name": "Tong Zhu"
          },
          {
            "authorId": "51912474",
            "name": "Xiaoye Qu"
          },
          {
            "authorId": "48993675",
            "name": "Wenliang Chen"
          },
          {
            "authorId": "2243403387",
            "name": "Zhefeng Wang"
          },
          {
            "authorId": "2422046",
            "name": "Baoxing Huai"
          },
          {
            "authorId": "1677643972",
            "name": "N. Yuan"
          },
          {
            "authorId": "1390813134",
            "name": "Min Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 7,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "1820089",
        "52967399",
        "1915014",
        "13886709",
        "10910955",
        "4249277",
        "220048375"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1820089,
          "isinfluential": false,
          "contexts": [
            "Our PTPCG is an end-to-end model with joint training and scheduled sampling [Bengio et al. , 2015] strategy."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
            "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.",
            "year": 2015,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1751569",
                "name": "Samy Bengio"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "3111912",
                "name": "N. Jaitly"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1915014,
          "isinfluential": false,
          "contexts": [
            "For a document D , a bidirectional long short-term memory (BiLSTM) network [Hochreiter and Schmidhuber, 1997] is used to encode each sentence s i into token-wise hidden states ( h i , . . . , h ) , where h i and | s i | is the length of i -th sentence."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Long Short-Term Memory",
            "abstract": "",
            "year": 1997,
            "venue": "Neural Computation",
            "authors": [
              {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
              },
              {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4249277,
          "isinfluential": false,
          "contexts": [
            "1 Event Detection For a document D, a bidirectional long short-term memory (BiLSTM) network [Hochreiter and Schmidhuber, 1997] is used to encode each sentence si into token-wise hidden states"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Neural Computation",
            "abstract": "",
            "year": 1989,
            "venue": "Artificial Intelligence",
            "authors": [
              {
                "authorId": "98241663",
                "name": "Mark J van Rossum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10910955,
          "isinfluential": false,
          "contexts": [
            "a low annotation quality since the large-scale datasets are usually generated via distantly supervised (DS) alignment with existing knowledge bases (KB) (Mintz et al. 2009; Chen et al. 2017)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distant supervision for relation extraction without labeled data",
            "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.",
            "year": 2009,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "36181176",
                "name": "Mike D. Mintz"
              },
              {
                "authorId": "87299088",
                "name": "Steven Bills"
              },
              {
                "authorId": "144621026",
                "name": "R. Snow"
              },
              {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13886709,
          "isinfluential": false,
          "contexts": [
            "Otherwise, Bron-Kerbosch (BK) algo-rithm [Bron and Kerbosch, 1973] is applied ﬁrst to ﬁnd all possible cliques (step 1 in Figure 3)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Algorithm 457: finding all cliques of an undirected graph",
            "abstract": "",
            "year": 1973,
            "venue": "",
            "authors": [
              {
                "authorId": "35220065",
                "name": "C. Bron"
              },
              {
                "authorId": "17132266",
                "name": "J. Kerbosch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "We believe it is the future direction to improve the similarity calculation and adjacent matrix prediction. et al. , 2019 ] as sentence representations and exploit relations between sentences via graph neural networks [ Veli ˇ ckovi ´ c et al. , 2018 ] to help identify ﬁxed number of combinations."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220048375,
          "isinfluential": false,
          "contexts": [
            "…EE (SEE) which focuses on building trigger-centered trees [Chen et al. , 2015; Nguyen et al. , 2016; Liu et al. , 2018; Wadden et al. , 2019; Lin et al. , 2020], document-level EE (DEE) is to decode ar-gument combinations from abundant entities across multiple sentences and ﬁll these…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Joint Neural Model for Information Extraction with Global Features",
            "abstract": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "143857288",
                "name": "Fei Huang"
              },
              {
                "authorId": "3008832",
                "name": "Lingfei Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Our PTPCG is an end-to-end model with joint training and scheduled sampling [Bengio et al., 2015] strategy.",
            "Our PTPCG is an end-to-end model with joint training and scheduled sampling [Bengio et al. , 2015] strategy."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "278115197": {
      "citing_paper_info": {
        "title": "Document-level event extraction framework based on prompt learning",
        "abstract": "Currently, most research on event extraction adopts a fine-tuning approach to pre-trained language models based on downstream tasks, which may lead to inconsistencies between upstream and downstream tasks of the pre-trained models. To address these issues, a document-level event extraction model based on prompt learning is proposed. First, a named entity recognition module based on a multi-head attention mechanism is used to identify entities. Then, to avoid the use of trigger words and reduce data annotation difficulty, a strategy is proposed to quantify the importance of different argument roles for event types. Finally, the event argument extraction task is restructured into a conditional text generation task through a prompt-based event argument extraction module. Experimental comparisons are conducted on three publicly available document-level event extraction datasets, demonstrating the effectiveness of the proposed framework.",
        "year": 2025,
        "venue": "Other Conferences",
        "authors": [
          {
            "authorId": "2306365414",
            "name": "Rui Wang"
          },
          {
            "authorId": "2314157013",
            "name": "Jiaoli Liu"
          },
          {
            "authorId": "2357553799",
            "name": "Yu Yan"
          },
          {
            "authorId": "2357624718",
            "name": "Liwei Zang"
          },
          {
            "authorId": "2299739145",
            "name": "Huimin Wang"
          },
          {
            "authorId": "2306073160",
            "name": "Jianyi Liu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 1,
        "unique_cited_count": 1,
        "influential_count": 0,
        "detailed_records_count": 1
      },
      "cited_papers": [
        "35386653"
      ],
      "citation_details": [
        {
          "citedcorpusid": 35386653,
          "isinfluential": false,
          "contexts": [
            "Liu et al. 6 designed an end-to-end event extraction model that uses an attention mechanism to capture the semantic relationship between event elements and candidate triggers."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms",
            "abstract": "This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        }
      ]
    },
    "264146058": {
      "citing_paper_info": {
        "title": "Type-Aware Decoding Via Explicitly Aggregating Event Information for Document-Level Event Extraction",
        "abstract": "Document-level event extraction (DEE) faces two main challenges: arguments-scattering and multi-event. Although previous methods attempt to address these challenges, they overlook the interference of event-unrelated sentences during event detection and neglect the mutual interference of different event roles during argument extraction. Therefore, this paper proposes a novel Schema-based Explicitly Aggregating (SEA) model to address these limitations. SEA aggregates event information into event type and role representations, enabling the decoding of event records based on specific type-aware representations. By detecting each event based on its event type representation, SEA mitigates the interference caused by event-unrelated information. Furthermore, SEA extracts arguments for each role based on its role-aware representations, reducing mutual interference between different roles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA outperforms the SOTA methods.",
        "year": 2023,
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "authors": [
          {
            "authorId": "2258709862",
            "name": "Gang Zhao"
          },
          {
            "authorId": "2204885006",
            "name": "Yidong Shi"
          },
          {
            "authorId": "116829010",
            "name": "Shudong Lu"
          },
          {
            "authorId": "2259271568",
            "name": "Xinjie Yang"
          },
          {
            "authorId": "51490462",
            "name": "Guanting Dong"
          },
          {
            "authorId": "2258782529",
            "name": "Jian Xu"
          },
          {
            "authorId": "2258780338",
            "name": "Xiaocheng Gong"
          },
          {
            "authorId": "2258893446",
            "name": "Si Li"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 7,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "243865619",
        "235458429",
        "219683473",
        "248512779",
        "13756489",
        "247619149",
        "6628106"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "We set the learning rate to 5e-5, the batch size to 64, and choose Adam [22] as our optimizer."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Given a document D = { s i } |D| i =1 , where s i is the i th sentence containing | s i | tokens, we utilize a Transformer [16] to encode s i following [7, 8, 9]: (1) Next, we obtain the sentence representation S i ∈ R d by max-pooling (cid:101) s i and adding the sentence position embedding: S i =…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            "After that, we employ a CRF [17] layer to recognize entities as candidate event arguments and get the entity recognition loss L er : where ˆ y s i is the golden label sequence of s i ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "Most previous works of Event Extraction [11, 12, 13, 4, 5] concentrate on Sentence-level Event Extraction (SEE) based on the ACE 2005 [14] dataset.",
            "In contrast to sentence-level event extraction [2, 3, 4, 5, 6] which extracts events within a sentence, DEE faces two specific challenges: arguments-scattering and multi-event."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865619,
          "isinfluential": false,
          "contexts": [
            "Document-level Event Extraction (DEE) aims to detect events and extract event arguments of pre-defined types from documents, which plays important roles in various fields, such as question answering [1], financial analysis, speech understanding, etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations",
            "abstract": "Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines’ ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1%, 63.3% and 83.5% for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0%, 79.6%, 100% respectively), highlighting our dataset as a challenging benchmark.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "51518773",
                "name": "Rujun Han"
              },
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "145478138",
                "name": "Jiao Sun"
              },
              {
                "authorId": "66500474",
                "name": "J. Baylón"
              },
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247619149,
          "isinfluential": false,
          "contexts": [
            "In contrast to sentence-level event extraction [2, 3, 4, 5, 6] which extracts events within a sentence, DEE faces two specific challenges: arguments-scattering and multi-event."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unified Structure Generation for Universal Information Extraction",
            "abstract": "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2154976399",
                "name": "Qing Liu"
              },
              {
                "authorId": "40495683",
                "name": "Dai Dai"
              },
              {
                "authorId": "2107521158",
                "name": "Xinyan Xiao"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "2149181702",
                "name": "Hua Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248512779,
          "isinfluential": false,
          "contexts": [
            "Most previous works of Event Extraction [11, 12, 13, 4, 5] concentrate on Sentence-level Event Extraction (SEE) based on the ACE 2005 [14] dataset.",
            "In contrast to sentence-level event extraction [2, 3, 4, 5, 6] which extracts events within a sentence, DEE faces two specific challenges: arguments-scattering and multi-event."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DEGREE: A Data-Efficient Generation-Based Event Extraction Model",
            "abstract": "Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage DEGREE and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "3256207",
                "name": "Elizabeth Boschee"
              },
              {
                "authorId": "123937952",
                "name": "Scott Miller"
              },
              {
                "authorId": "2104644641",
                "name": "Premkumar Natarajan"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "DuEE-fin [19] is a recently released public financial DEE dataset comprising 13 event types and 92 event roles extracted from common financial events.",
            "Following [10], we evaluate our method on two widely-used DEE datasets: ChFinAnn [7] and DuEE-fin [19]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "269804126": {
      "citing_paper_info": {
        "title": "DEIE: Benchmarking Document-level Event Information Extraction with a Large-scale Chinese News Dataset",
        "abstract": "",
        "year": 2024,
        "venue": "International Conference on Language Resources and Evaluation",
        "authors": [
          {
            "authorId": "2109978994",
            "name": "Yubing Ren"
          },
          {
            "authorId": "2257354181",
            "name": "Yanan Cao"
          },
          {
            "authorId": "2274084234",
            "name": "Hao Li"
          },
          {
            "authorId": "2201603227",
            "name": "Yingjie Li"
          },
          {
            "authorId": "2301745311",
            "name": "Zixuan ZM Ma"
          },
          {
            "authorId": "2257017651",
            "name": "Fang Fang"
          },
          {
            "authorId": "2075394870",
            "name": "Ping Guo"
          },
          {
            "authorId": "2302816454",
            "name": "Wei Ma"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 21,
        "unique_cited_count": 19,
        "influential_count": 1,
        "detailed_records_count": 21
      },
      "cited_papers": [
        "119308902",
        "207853145",
        "341734",
        "14625075",
        "3030259",
        "202539496",
        "15926286",
        "204960716",
        "11137563",
        "202770954",
        "243865143",
        "266177132",
        "216562779",
        "202120592",
        "237491754",
        "8143782",
        "38234032",
        "243865639",
        "17716605"
      ],
      "citation_details": [
        {
          "citedcorpusid": 341734,
          "isinfluential": false,
          "contexts": [
            "Adhering to the TimeML specification (Pustejovsky et al., 2003a, 2010), temporal relation datasets such as TimeBank (James Pustejovsky and others, 2006) and TempEval (Ver-hagen et al., 2009, 2010; UzZaman et al., 2013) have been constructed."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TimeML: Robust Specification of Event and Temporal Expressions in Text",
            "abstract": "In this paper we provide a description of TimeML, a rich specification language for event and temporal expressions in natural language text, developed in the context of the AQUAINT program on Question Answering Systems. Unlik em ost previous work on event annotation, TimeML capture st hree distinct phenomena in temporal markup: (1) it systematically anchors event predicates to a broad range of temporally denotating expressions; (2) it orders event expressions in text relative to one another, both intrasententially and in discourse; and (3) it allows for a delayed (underspecified) interpretation of partially determined temporal expressions. We demonstrate the expressiveness of TimeML for a broad range of syntactic and semantic contexts, including aspectual predication, modal subordination, and an initial treatment of lexical and constructional causation in text.",
            "year": 2003,
            "venue": "New Directions in Question Answering",
            "authors": [
              {
                "authorId": "1707726",
                "name": "J. Pustejovsky"
              },
              {
                "authorId": "145326430",
                "name": "J. Castaño"
              },
              {
                "authorId": "2319612",
                "name": "Robert Ingria"
              },
              {
                "authorId": "144327698",
                "name": "R. Saurí"
              },
              {
                "authorId": "1718590",
                "name": "R. Gaizauskas"
              },
              {
                "authorId": "2024838",
                "name": "A. Setzer"
              },
              {
                "authorId": "40457061",
                "name": "G. Katz"
              },
              {
                "authorId": "9215251",
                "name": "Dragomir R. Radev"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3030259,
          "isinfluential": false,
          "contexts": [
            "Leveraging temporal insights, causal relation datasets (Do et al., 2011; Mirza et al., 2014; Mostafazadeh et al., 2016; Dunietz et al., 2017; Caselli and Vossen, 2017; Tan et al., 2022) have emerged."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Minimally Supervised Event Causality Identification",
            "abstract": "",
            "year": 2011,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145625456",
                "name": "Q. Do"
              },
              {
                "authorId": "2988645",
                "name": "Yee Seng Chan"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8143782,
          "isinfluential": false,
          "contexts": [
            "Datasets like Cancer Genetics, EPM, GENIA2011, GENIA2013, Pathway Curation, and MLEE (Pyysalo et al., 2013; Ohta et al., 2011; Van Landeghem et al., 2013) are tailored exclusively for the biological domain."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Epigenetics and Post-translational Modifications (EPI) task of BioNLP Shared Task 2011",
            "abstract": "",
            "year": 2011,
            "venue": "BioNLP@ACL",
            "authors": [
              {
                "authorId": "2095533089",
                "name": "Tomoko Ohta"
              },
              {
                "authorId": "1708916",
                "name": "S. Pyysalo"
              },
              {
                "authorId": "1737901",
                "name": "Junichi Tsujii"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11137563,
          "isinfluential": false,
          "contexts": [
            "The CEC (Fu et al., 2010) dataset, being auto-annotated, encompasses 332 emergency news documents, which is insufficient to capture the vast array of real-world events.",
            "Chinese Emergency Corpus (CEC) (Fu et al., 2010) is a Chinese dataset pertinent to emergencies, which provides 332 documents covering 5 categories."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Study of Chinese Event Taggability",
            "abstract": "",
            "year": 2010,
            "venue": "2010 Second International Conference on Communication Software and Networks",
            "authors": [
              {
                "authorId": "1896549",
                "name": "Jianfeng Fu"
              },
              {
                "authorId": "2157221925",
                "name": "Wei Liu"
              },
              {
                "authorId": "2109308296",
                "name": "Zongtian Liu"
              },
              {
                "authorId": "47474799",
                "name": "Sha-sha Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14625075,
          "isinfluential": false,
          "contexts": [
            "Adhering to the TimeML specification (Pustejovsky et al., 2003a, 2010), temporal relation datasets such as TimeBank (James Pustejovsky and others, 2006) and TempEval (Ver-hagen et al., 2009, 2010; UzZaman et al., 2013) have been constructed."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ISO-TimeML: An International Standard for Semantic Annotation",
            "abstract": "",
            "year": 2010,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "1707726",
                "name": "J. Pustejovsky"
              },
              {
                "authorId": "34895244",
                "name": "Kiyong Lee"
              },
              {
                "authorId": "1786202",
                "name": "H. Bunt"
              },
              {
                "authorId": "1714893",
                "name": "Laurent Romary"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15926286,
          "isinfluential": false,
          "contexts": [
            "We measure the inter-annotator agreements (IAA) of the event argument/summary/relation annotation between two annotators with Cohen’s Kappa (Cohen, 1960)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Coefficient of Agreement for Nominal Scales",
            "abstract": "",
            "year": 1960,
            "venue": "",
            "authors": [
              {
                "authorId": "145670758",
                "name": "Jacob Cohen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17716605,
          "isinfluential": false,
          "contexts": [
            "Leveraging temporal insights, causal relation datasets (Do et al., 2011; Mirza et al., 2014; Mostafazadeh et al., 2016; Dunietz et al., 2017; Caselli and Vossen, 2017; Tan et al., 2022) have emerged."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations",
            "abstract": "Language of cause and effect captures an essential component of the semantics of a text. However, causal language is also intertwined with other semantic relations, such as temporal precedence and correlation. This makes it difficult to determine when causation is the primary intended meaning. This paper presents BECauSE 2.0, a new version of the BECauSE corpus with exhaustively annotated expressions of causal language, but also seven semantic relations that are frequently co-present with causation. The new corpus shows high inter-annotator agreement, and yields insights both about the linguistic expressions of causation and about the process of annotating co-present semantic relations.",
            "year": 2017,
            "venue": "LAW@ACL",
            "authors": [
              {
                "authorId": "2076678",
                "name": "Jesse Dunietz"
              },
              {
                "authorId": "1686960",
                "name": "Lori S. Levin"
              },
              {
                "authorId": "143712374",
                "name": "J. Carbonell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 38234032,
          "isinfluential": false,
          "contexts": [
            "Leveraging temporal insights, causal relation datasets (Do et al., 2011; Mirza et al., 2014; Mostafazadeh et al., 2016; Dunietz et al., 2017; Caselli and Vossen, 2017; Tan et al., 2022) have emerged.",
            "A few datasets (Caselli and Vossen, 2017; Ning et al., 2018; Wang et al., 2022) annotate both temporal and causal relations concurrently."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction",
            "abstract": "This paper reports on the Event StoryLine Corpus (ESC) v1.0, a new benchmark dataset for the temporal and causal relation detection. By developing this dataset, we also introduce a new task, the StoryLine Extraction from news data, which aims at extracting and classifying events relevant for stories, from across news documents spread in time and clustered around a single seminal event or topic. In addition to describing the dataset, we also report on three baselines systems whose results show the complexity of the task and suggest directions for the development of more robust systems.",
            "year": 2017,
            "venue": "NEWS@ACL",
            "authors": [
              {
                "authorId": "1864635",
                "name": "Tommaso Caselli"
              },
              {
                "authorId": "1791713",
                "name": "P. Vossen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "ChFinAnn (Zheng et al., 2019) is restricted to the financial domain, featuring 5 event types and 35 argument types.",
            "ChFinAnn (Zheng et al., 2019) is nar-rowly defined, featuring 5 event types and 35 ar-gument types within the Chinese financial domain."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202120592,
          "isinfluential": false,
          "contexts": [
            "We reproduce four representative neural models and report their performances on Deie, including (1) BERT base (Ethayarajh, 2019), a widely-used PLM, we adopt it as the backbone and build classification models on top of it."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
            "abstract": "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "10324691",
                "name": "Kawin Ethayarajh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "…as the foundation for the development of numerous models aimed at improving sentence-level event extraction, resulting in significant achievements in this field (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020; Wang et al., 2021; Lu et al., 2021; Liu et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202770954,
          "isinfluential": false,
          "contexts": [
            "…as the foundation for the development of numerous models aimed at improving sentence-level event extraction, resulting in significant achievements in this field (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020; Wang et al., 2021; Lu et al., 2021; Liu et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HMEAE: Hierarchical Modular Event Argument Extraction",
            "abstract": "Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              },
              {
                "authorId": "2108485135",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1384550891",
                "name": "Xiang Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "(1) BART base (Lewis et al., 2020), a widely-used sequence-to-sequence model for generation tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": true,
          "contexts": [
            "RAMS (Ebner et al., 2020) narrows the context to a mere 5 sentences, which poses a challenge to encapsulating document-level event details.",
            "RAMS (Ebner et al., 2020) includes 3,993 annotated documents sourced from news, featuring 139 event types and 65 roles.",
            "We show the main statistics of Deie and compare them with some existing widely-used document-level event datasets in Table 2, including MUC-4, WikiEvents, RAMS, ChFinAnn, DuEE-Fin and DocEE."
          ],
          "intents": [
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562779,
          "isinfluential": false,
          "contexts": [
            "MAVEN (Wang et al., 2020) concentrates on annotating event triggers and contains 168 types within 11,832 sentences.",
            "(2) MAVEN-ERE (Wang et al., 2022), which provides simple but strong baselines for 4 event relation extraction tasks."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "MAVEN: A Massive General Domain Event Detection Dataset",
            "abstract": "Event detection (ED), which identifies event trigger words and classifies event types according to contexts, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Small scale of existing datasets is not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Limited event types of existing datasets lead to the trained models cannot be easily adapted to general-domain scenarios. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 117,200 event mention instances, and 207 event types. MAVEN alleviates the lack of data problem and covers much more general event types. Besides the dataset, we reproduce the recent state-of-the-art ED models and conduct a thorough evaluation for these models on MAVEN. The experimental results and empirical analyses show that existing ED methods cannot achieve promising results as on the small datasets, which suggests ED in real world remains a challenging task and requires further research efforts. The dataset and baseline code will be released in the future to promote this field.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "1659764899",
                "name": "Wangyi Jiang"
              },
              {
                "authorId": "151185222",
                "name": "Rong Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237491754,
          "isinfluential": false,
          "contexts": [
            "(2) CPT (Shao et al., 2021), a Chinese language understanding and generation model."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation",
            "abstract": "In this paper, we take the advantage of previous pre-trained models (PTMs) and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different from previous Chinese PTMs, CPT is designed to utilize the shared knowledge between natural language understanding (NLU) and natural language generation (NLG) to boost the performance. CPT consists of three parts: a shared encoder, an understanding decoder, and a generation decoder. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multi-task pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model. Moreover, the unbalanced Transformer saves the computational and storage cost, which makes CPT competitive and greatly accelerates the inference of text generation. Experimental results on a wide range of Chinese NLU and NLG tasks show the effectiveness of CPT.",
            "year": 2021,
            "venue": "Science China Information Sciences",
            "authors": [
              {
                "authorId": "95329799",
                "name": "Yunfan Shao"
              },
              {
                "authorId": "2056612192",
                "name": "Zhichao Geng"
              },
              {
                "authorId": "2130193933",
                "name": "Yitao Liu"
              },
              {
                "authorId": "2087363104",
                "name": "Junqi Dai"
              },
              {
                "authorId": "2156128119",
                "name": "Fei Yang"
              },
              {
                "authorId": "2126496490",
                "name": "Li Zhe"
              },
              {
                "authorId": "1679542",
                "name": "H. Bao"
              },
              {
                "authorId": "1767521",
                "name": "Xipeng Qiu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865143,
          "isinfluential": false,
          "contexts": [
            "Researchers have made a lot of progress in this field (Zhang et al., 2020; Xu et al., 2021; Huang and Jia, 2021; Ren et al., 2022; Ma et al., 2022; Xu et al., 2022; Du and Cardie, 2020; Liu et al., 2021b; Wei et al., 2021; Li et al., 2021; Ren et al., 2023; Li et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction",
            "abstract": "Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach — it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865639,
          "isinfluential": false,
          "contexts": [
            "(3) Relative-Time (Wen and Ji, 2021 (3) For casual relation extraction, this performance is far from perfect, thus suggesting the challenges for event casual identification and presenting ample research opportunities to improve the performance in the future."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Utilizing Relative Event Time to Enhance Event-Event Temporal Relation Extraction",
            "abstract": "Event time is one of the most important features for event-event temporal relation extraction. However, explicit event time information in text is sparse. For example, only about 20% of event mentions in TimeBank-Dense have event-time links. In this paper, we propose a joint model for event-event temporal relation classification and an auxiliary task, relative event time prediction, which predicts the event time as real numbers. We adopt the Stack-Propagation framework to incorporate predicted relative event time for temporal relation classification and keep the differentiability. Our experiments on MATRES dataset show that our model can significantly improve the RoBERTa-based baseline and achieve state-of-the-art performance.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "4428136",
                "name": "Haoyang Wen"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266177132,
          "isinfluential": false,
          "contexts": [
            "Researchers have made a lot of progress in this field (Zhang et al., 2020; Xu et al., 2021; Huang and Jia, 2021; Ren et al., 2022; Ma et al., 2022; Xu et al., 2022; Du and Cardie, 2020; Liu et al., 2021b; Wei et al., 2021; Li et al., 2021; Ren et al., 2023; Li et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction",
            "abstract": ",",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2274084234",
                "name": "Hao Li"
              },
              {
                "authorId": "2257354181",
                "name": "Yanan Cao"
              },
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "2257017651",
                "name": "Fang Fang"
              },
              {
                "authorId": "2273820905",
                "name": "Lanxue Zhang"
              },
              {
                "authorId": "2201603227",
                "name": "Yingjie Li"
              },
              {
                "authorId": "2274051475",
                "name": "Shi Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The Text Analysis Conference (TAC) has introduced three benchmark datasets (Stephanie Strassel and others, 2014, 2016): TAC-KBP 2015, TAC-KBP 2016, and TAC-KBP 2017, featuring 9, 8, and 8 event types, along with 38, 18, and 18 event subtypes respectively."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "For instance, * Yanan Cao is corresponding author MUC-4 dataset (McLean, 1992) encompasses 1,700 documents addressing 4 event types, and these types are close to each other and confined to the terrorist attack topic.",
            "The MUC-4 dataset (McLean, 1992), centered on Latin American terrorism news articles, comprises 1,700 documents that span 4 event types and 5 argument types."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "252648205": {
      "citing_paper_info": {
        "title": "DocEE: A Large-Scale Dataset for Document-level Event Extraction",
        "abstract": "Event extraction (EE) is the task of identi-001 fying events and their types, along with the 002 involved arguments. Despite the great suc-003 cess in sentence-level event extraction, events 004 are more naturally presented in the form of 005 document, with event arguments scattering in 006 multiple sentences. However, a major bar-007 rier to promote document-level event extrac-008 tion has been the lack of large-scale and prac-009 tical training and evaluation datasets. In this 010 paper, we present DocEE, a new document-011 level EE dataset including 20,000+ events, 012 100,000+ arguments. We highlight three fea-013 tures: large-scale annotations, ﬁne-grained 014 event arguments and application-oriented set-015 tings. Experiments show that even SOTA mod-016 els show inferior performance on DocEE, espe-017 cially in cross-domain settings, indicating that 018 DocEE is still a challenging task. We will pub-019 lish DocEE upon acceptance. 020",
        "year": 2021,
        "venue": "",
        "authors": []
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "3867049",
        "202537206",
        "11986411",
        "3719231"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3719231,
          "isinfluential": false,
          "contexts": [
            "We adopt CNN-based method and various 425 transformer-based methods as our baselines, in426 cluding: 1) TextCNN (Kim, 2014) uses different 427 sizes CNN kernels to extract key information in 428 text for classification."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Convolutional Neural Networks",
            "abstract": "This chapter introduces the first deep learning architecture of the book, convolutional neural networks. It starts with redefining the way a logistic regression accepts data, and defines 1D and 2D convolutional layers as a natural extension of the logistic regression. The chapter also details on how to connect the layers and dimensionality problems. The local receptive field is introduced as a core concept of any convolutional architecture and the connections with the vanishing gradient problem is explored. Also the idea of padding is introduced in the visual setting, as well as the stride of the local receptive field. Pooling is also explored in the general setting and as max-pooling. A complete convolutional neural network for classifying MNIST is then presented in Keras code, and all the details of the code are presented as comments and illustrations. The final section of the chapter presents modifications needed to adapt convolutional networks, which are primarily visual classificators, to work with text and language.",
            "year": 2018,
            "venue": "",
            "authors": [
              {
                "authorId": "3422664",
                "name": "Sandro Skansi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3867049,
          "isinfluential": false,
          "contexts": [
            "Following (Hamborg et al., 2018), The main event refers to the event reﬂected in the title and mainly described in the article."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Giveme5W: Main Event Retrieval from News Articles by Extraction of the Five Journalistic W Questions",
            "abstract": "",
            "year": 2018,
            "venue": "iConference",
            "authors": [
              {
                "authorId": "3461253",
                "name": "Felix Hamborg"
              },
              {
                "authorId": "40305390",
                "name": "Soeren Lachnit"
              },
              {
                "authorId": "3021925",
                "name": "M. Schubotz"
              },
              {
                "authorId": "2293218",
                "name": "Thomas Hepp"
              },
              {
                "authorId": "145151838",
                "name": "Bela Gipp"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11986411,
          "isinfluential": false,
          "contexts": [
            "MUC-060 4(Grishman and Sundheim, 1996) consists of only 1700 news articles based on an ontology of 4 event types and 5 argument types."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Message Understanding Conference- 6: A Brief History",
            "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",
            "year": 1996,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202537206,
          "isinfluential": false,
          "contexts": [
            "In reality, events usually appear in the form of doc-043 umentary descriptions, and the arguments involved in the event are also scattered in various sentences (Hamborg et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Giveme5W1H: A Universal System for Extracting Main Events from News Articles",
            "abstract": "Event extraction from news articles is a commonly required prerequisite for various tasks, such as article summarization, article clustering, and news aggregation. Due to the lack of universally applicable and publicly available methods tailored to news datasets, many researchers redundantly implement event extraction methods for their own projects. The journalistic 5W1H questions are capable of describing the main event of an article, i.e., by answering who did what, when, where, why, and how. We provide an in-depth description of an improved version of Giveme5W1H, a system that uses syntactic and domain-specific rules to automatically extract the relevant phrases from English news articles to provide answers to these 5W1H questions. Given the answers to these questions, the system determines an article's main event. In an expert evaluation with three assessors and 120 articles, we determined an overall precision of p=0.73, and p=0.82 for answering the first four W questions, which alone can sufficiently summarize the main event reported on in a news article. We recently made our system publicly available, and it remains the only universal open-source 5W1H extractor capable of being applied to a wide range of use cases in news analysis.",
            "year": 2019,
            "venue": "INRA@RecSys",
            "authors": [
              {
                "authorId": "3461253",
                "name": "Felix Hamborg"
              },
              {
                "authorId": "2587724",
                "name": "Corinna Breitinger"
              },
              {
                "authorId": "145151838",
                "name": "Bela Gipp"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Wikipedia contains two kinds of events: historical events and timeline events (Hienert and Luciano, 2012)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Besides, we provide a good evaluation scenario for testing the ability of the pre-trained models that aim at handling long text understand-107 ing, such as longformer(Beltagy et al., 2020) and ∞ -former(Martins et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "WikiEvents(Li et al., 2021) con-065 sists of only 246 documents."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "272575907": {
      "citing_paper_info": {
        "title": "Document-level Overlapping Event Extraction via ChatGPT Leading Decision and Cross-Sentence Entity Representation Enhancements",
        "abstract": "Document-level event extraction is still a challenge when the document contains multiple events with multiple overlapped arguments. In the ChFinAnn dataset, over 37% of documents match this situation. However, existing works have not yet clearly interpreted the Overlapping Events. There is also no targeted solution to the problem. To address the above problems, in this paper, we first categorize and define the Overlapping Event, and then give a practical framework to solve the problem of Overlapping Event extraction. We propose a new metric called Argument Correlation to calculate the likelihood of overlapping event. We propose a parallel model DESIA-CHAT, which introduces ChatGPT as the leading model to distinguish Regular and Overlapping Events. Besides, for the issue of Cross-Sentence argument overlap, we enhanced the entity representation to capture complete features. We conducted experiments on two datasets(ChFinAnn and Assevent). Experimental results show that our method achieves new state-of-the-art performance on the 2 datasets.",
        "year": 2024,
        "venue": "IEEE International Joint Conference on Neural Network",
        "authors": [
          {
            "authorId": "2320840027",
            "name": "Dong Yu"
          },
          {
            "authorId": "2321149280",
            "name": "JiaWen Cao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 1,
        "unique_cited_count": 1,
        "influential_count": 0,
        "detailed_records_count": 1
      },
      "cited_papers": [
        "1915014"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1915014,
          "isinfluential": false,
          "contexts": [
            "In order to better model the semantic relationship between entities, we use another BiLSTM ([5]) to get the final set of entity representation e = { e i } ei =1 which contains more complete context information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Long Short-Term Memory",
            "abstract": "",
            "year": 1997,
            "venue": "Neural Computation",
            "authors": [
              {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
              },
              {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
              }
            ]
          }
        }
      ]
    },
    "280190723": {
      "citing_paper_info": {
        "title": "Few-Shot Document-Level Event Argument Extraction Based on Multi-Agent Systems",
        "abstract": "Event Argument Extraction (EAE) is a fundamental task in Natural Language Processing (NLP) that aims to identify and extract role-specific arguments from event templates in textual data. Although significant progress has been made in sentence-level extraction, real-world events are often spread across multiple sentences, requiring document-level extraction models. These models usually rely on fully annotated datasets, but because annotation is costly and time-consuming, Few-Shot Learning (FSL) has emerged as a promising alternative for addressing the lack of labeled data. However, traditional FSL models struggle to effectively capture long document contexts and address the challenge of highly imbalanced label distributions, making Few-Shot Document-level Event Argument Extraction especially difficult. Recently, in-context learning (ICL) approaches using large language models (LLMs) have shown promise in addressing few-shot learning tasks. However, constructing suitable context examples for such tasks requires careful design, and the diverse types of event arguments in EAE add complexity to selecting examples for ICL. To address these challenges, we propose a multi-agent system-based approach for few-shot document-level event argument extraction. Our method uses three specialized LLM-driven agents to automatically carry out iterative extraction and result optimization. Experiments show that by constructing basic prompt templates and leveraging agent collaboration to enhance LLMs' context comprehension, our approach outperforms traditional models without requiring any training. This provides a scalable solution for FSL and various NLP applications.",
        "year": 2025,
        "venue": "2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)",
        "authors": [
          {
            "authorId": "2372372145",
            "name": "Jiayu Yu"
          },
          {
            "authorId": "2308064445",
            "name": "Yantuan Xian"
          },
          {
            "authorId": "2116733312",
            "name": "Zheng Yu"
          },
          {
            "authorId": "2346250599",
            "name": "Hongbin Wang"
          },
          {
            "authorId": "2308047292",
            "name": "Yuxin Huang"
          },
          {
            "authorId": null,
            "name": "Yan Xiang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 22,
        "unique_cited_count": 21,
        "influential_count": 1,
        "detailed_records_count": 22
      },
      "cited_papers": [
        "233219850",
        "9631585",
        "5046366",
        "204915992",
        "252090194",
        "21695474",
        "17825977",
        "258998173",
        "265149752",
        "275901979",
        "248505827",
        "225039792",
        "233988541",
        "247155069",
        "258967387",
        "259950998",
        "202786163",
        "218971783",
        "119308902",
        "207853145",
        "250390839"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5046366,
          "isinfluential": false,
          "contexts": [
            "In recent research, [25] set up an agent as an extractor for the event extraction task, using a generative adversarial network (GAN) to accept inputs from experts, extractors, and the current state to estimate rewards."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction with Generative Adversarial Imitation Learning",
            "abstract": "We propose a new method for event extraction (EE) task based on an imitation learning framework, specifically, inverse reinforcement learning (IRL) via generative adversarial network (GAN). The GAN estimates proper rewards according to the difference between the actions committed by the expert (or ground truth) and the agent among complicated states in the environment. EE task benefits from these dynamic rewards because instances and labels yield to various extents of difficulty and the gains are expected to be diverse -- e.g., an ambiguous but correctly detected trigger or argument should receive high gains -- while the traditional RL models usually neglect such differences and pay equal attention on all instances. Moreover, our experiments also demonstrate that the proposed framework outperforms state-of-the-art methods, without explicit feature engineering.",
            "year": 2018,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2111626",
                "name": "Tongtao Zhang"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9631585,
          "isinfluential": false,
          "contexts": [
            "For instance, extracted event information can aid in building knowledge bases [1], extracting interactions between biological molecules in biomedical literature [2], and measuring economic indicators from social media events in the economic domain [3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Overview of BioNLP’09 Shared Task on Event Extraction",
            "abstract": "The paper presents the design and implementation of the BioNLP'09 Shared Task, and reports the final results with analysis. The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges.",
            "year": 2009,
            "venue": "BioNLP@HLT-NAACL",
            "authors": [
              {
                "authorId": "1785622",
                "name": "Jin-Dong Kim"
              },
              {
                "authorId": "2095533089",
                "name": "Tomoko Ohta"
              },
              {
                "authorId": "1708916",
                "name": "S. Pyysalo"
              },
              {
                "authorId": "34803801",
                "name": "Yoshinobu Kano"
              },
              {
                "authorId": "1737901",
                "name": "Junichi Tsujii"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17825977,
          "isinfluential": false,
          "contexts": [
            "For the event extraction task, [24] proposed an Ontology-based Fuzzy Event Extraction (OFEE) agent to extract events from Chinese electronic news summaries."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Ontology-based fuzzy event extraction agent for Chinese e-news summarization",
            "abstract": "",
            "year": 2003,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "2155591290",
                "name": "Chang-Shing Lee"
              },
              {
                "authorId": "3294118",
                "name": "Yea-Juan Chen"
              },
              {
                "authorId": "2117958",
                "name": "Zhi-Wei Jian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21695474,
          "isinfluential": false,
          "contexts": [
            "For instance, extracted event information can aid in building knowledge bases [1], extracting interactions between biological molecules in biomedical literature [2], and measuring economic indicators from social media events in the economic domain [3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "EventWiki: A Knowledge Base of Major Events",
            "abstract": "",
            "year": 2018,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2547492",
                "name": "Tao Ge"
              },
              {
                "authorId": "145500855",
                "name": "Lei Cui"
              },
              {
                "authorId": "39488576",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              },
              {
                "authorId": "49807919",
                "name": "Furu Wei"
              },
              {
                "authorId": "143849609",
                "name": "M. Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "[16] transforms event argument extraction into the generation of a directed acyclic graph based on trigger words and introduces the ChFinAnn dataset, built on Chinese financial corpora."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202786163,
          "isinfluential": false,
          "contexts": [
            "For instance, extracted event information can aid in building knowledge bases [1], extracting interactions between biological molecules in biomedical literature [2], and measuring economic indicators from social media events in the economic domain [3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study",
            "abstract": "Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policymakers. We argue that events reported in streaming news can be used as “micro-sensors” for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socio-economic indicators with events. We empirically demonstrate strong correlation between ECIM values to several representative indicators in socio-economic research.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1875233",
                "name": "Bonan Min"
              },
              {
                "authorId": "2118080357",
                "name": "Xiaoxi Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204915992,
          "isinfluential": false,
          "contexts": [
            "Formally, the task definition of few-shot document-level event argument extraction [6] builds on previous research on few-shot sentence-level event detection [26]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection",
            "abstract": "Event detection (ED), a sub-task of event extraction, involves identifying triggers and categorizing event mentions. Existing methods primarily rely upon supervised learning and require large-scale labeled event datasets which are unfortunately not readily available in many real-life applications. In this paper, we consider and reformulate the ED task with limited labeled data as a Few-Shot Learning problem. We propose a Dynamic-Memory-Based Prototypical Network (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learn better prototypes for event types, but also produce more robust sentence encodings for event mentions. Differing from vanilla prototypical networks simply computing event prototypes by averaging, which only consume event mentions once, our model is more robust and is capable of distilling contextual information from event mentions for multiple times due to the multi-hop mechanism of DMNs. The experiments show that DMB-PN not only deals with sample scarcity better than a series of baseline models but also performs more robustly when the variety of event types is relatively large and the instance quantity is extremely small.",
            "year": 2019,
            "venue": "Web Search and Data Mining",
            "authors": [
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "1388742072",
                "name": "Jiaojian Kang"
              },
              {
                "authorId": "2118158068",
                "name": "Yichi Zhang"
              },
              {
                "authorId": "2155468731",
                "name": "Wei Zhang"
              },
              {
                "authorId": "1729778",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "The RAMS [17] dataset annotates cross-sentence implicit arguments for 9,124 events from news, covering a wide range of event types."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218971783,
          "isinfluential": false,
          "contexts": [
            "With their massive training datasets and numerous model parameters, LLMs have demonstrated the ability to handle complex tasks [8–12], making it possible to perform event argument extraction in a single step."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039792,
          "isinfluential": false,
          "contexts": [
            "[20] formulated argument extraction as a question answering problem, effectively leveraging task-related knowledge from pretrained reading comprehension models to improve few-shot performance."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction",
            "abstract": "We study the problem of event extraction from text data, which requires both detecting target event types and their arguments. Typically, both the event detection and argument detection subtasks are formulated as supervised sequence labeling problems. We argue that the event extraction models so trained are inherently label-hungry, and can generalize poorly across domains and text genres.We propose a reading comprehension framework for event extraction.Specifically, we formulate event detection as a textual entailment prediction problem, and argument detection as a question answer-ing problem. By constructing proper query templates, our approach can effectively distill rich knowledge about tasks and label semantics from pretrained reading comprehension models. Moreover, our model can be fine-tuned with a small amount of data to boost its performance. Our experiment results show that our method performs strongly for zero-shot and few-shot event extraction, and it achieves state-of-the-art performance on the ACE 2005 benchmark when trained with full supervision.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2054599518",
                "name": "Rui Feng"
              },
              {
                "authorId": "2118573410",
                "name": "Jie Yuan"
              },
              {
                "authorId": "2152737282",
                "name": "Chao Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "Additionally, [5] converts argument extraction into a conditional generation task and proposes the WIKIEVENTS dataset.",
            "Without employing document-level models, it is impossible to fully capture all event information [5]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233988541,
          "isinfluential": false,
          "contexts": [
            "Other studies have used meta-learning [21] and entailment [22] models to explore effective solutions for the event argument extraction (EAE) task when training samples are significantly reduced."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Meta Learning for Event Argument Extraction via Domain-Specific Information Enhanced",
            "abstract": "",
            "year": 2020,
            "venue": "China Conference on Knowledge Graph and Semantic Computing",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247155069,
          "isinfluential": false,
          "contexts": [
            "…Communication Engineering (NNICE) learning (ICL) involve combining the query question (the input requiring label prediction) with context examples of related cases to form prompt inputs that guide the LLMs’ predictions [13], enhancing LLMs’ capabilities in solving event argument extraction tasks."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
            "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48872685",
                "name": "Sewon Min"
              },
              {
                "authorId": "2156533327",
                "name": "Xinxi Lyu"
              },
              {
                "authorId": "14487640",
                "name": "Ari Holtzman"
              },
              {
                "authorId": "2347956",
                "name": "Mikel Artetxe"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248505827,
          "isinfluential": false,
          "contexts": [
            "Other studies have used meta-learning [21] and entailment [22] models to explore effective solutions for the event argument extraction (EAE) task when training samples are significantly reduced."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning",
            "abstract": "Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents respectively, while achieving the same performance as with full training. More importantly, we show that recasting EAE as entailment alleviates the dependency on schemas, which has been a road-block for transferring annotations between domains. Thanks to the entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer. Our analysis shows that the key to good results is the use of several entailment datasets to pre-train the entailment model. Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument type is needed, and comparable results can be achieved with users with different level of expertise.",
            "year": 2022,
            "venue": "NAACL-HLT",
            "authors": [
              {
                "authorId": "1724648481",
                "name": "Oscar Sainz"
              },
              {
                "authorId": "1404791152",
                "name": "Itziar Gonzalez-Dios"
              },
              {
                "authorId": "1715983",
                "name": "Oier Lopez de Lacalle"
              },
              {
                "authorId": "1875233",
                "name": "Bonan Min"
              },
              {
                "authorId": "1733049",
                "name": "Eneko Agirre"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250390839,
          "isinfluential": false,
          "contexts": [
            "[18] proposed DocEE, a large-scale, manually annotated document-level event extraction dataset."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction",
            "abstract": "Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152439499",
                "name": "Meihan Tong"
              },
              {
                "authorId": "2113744169",
                "name": "Bin Xu"
              },
              {
                "authorId": "2118512998",
                "name": "Shuai Wang"
              },
              {
                "authorId": "2175603382",
                "name": "Meihuan Han"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "1737159260",
                "name": "Jiangqi Zhu"
              },
              {
                "authorId": "2175598253",
                "name": "Siyu Chen"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252090194,
          "isinfluential": true,
          "contexts": [
            "Formally, the task definition of few-shot document-level event argument extraction [6] builds on previous research on few-shot sentence-level event detection [26].",
            "We conducted experiments on the FewDocAE [6] dataset, specifically designed for few-shot document-level event argument extraction.",
            "Recently, a few-shot document-level event argument extraction task was proposed, employing the novel N-Way-D-Doc sampling algorithm to reconstruct the related dataset FewDocAE [6].",
            "Moreover, traditional methods heavily rely on fully annotated datasets [6], which are often infeasible due to the high cost and time required for event annotation.",
            "Therefore, [6] proposed a few-shot event argument extraction task at the document level and introduced a novel N-Way-D-Doc sampling strategy to reconstruct the corresponding dataset, FewDocAE."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Few-Shot Document-Level Event Argument Extraction",
            "abstract": "Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is limited in production.To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing document-level event extraction dataset. We first define the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling instead of the traditional N-Way-K-Shot strategy. Then we adjust the current document-level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find this novel task to be very challenging with substantively low performance. Considering FewDocAE is closely related to practical use under low-resource regimes, we hope this benchmark encourages more research in this direction. Our data and codes will be available online.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2145170944",
                "name": "Xianjun Yang"
              },
              {
                "authorId": "2140021277",
                "name": "Yujie Lu"
              },
              {
                "authorId": "21038849",
                "name": "Linda Petzold"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258967387,
          "isinfluential": false,
          "contexts": [
            "[19] reformulates the task as a link prediction problem on Abstract Meaning Representation (AMR) graphs."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "An AMR-based Link Prediction Approach for Document-level Event Argument Extraction",
            "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2145435513",
                "name": "Yuqing Yang"
              },
              {
                "authorId": "3187768",
                "name": "Qipeng Guo"
              },
              {
                "authorId": "12040998",
                "name": "Xiangkun Hu"
              },
              {
                "authorId": "39939186",
                "name": "Yue Zhang"
              },
              {
                "authorId": "1767521",
                "name": "Xipeng Qiu"
              },
              {
                "authorId": "1852415",
                "name": "Zheng Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258998173,
          "isinfluential": false,
          "contexts": [
            "Despite significant progress at the sentence level, event information in real-world texts is often dispersed across multiple sentences [4]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction - A Survey of Methods and Applications",
            "abstract": "Event extraction aims to detect occurrences of specified types and extract corresponding event arguments from unstructured data input, which is an integral part of information extraction [1]. Many downstream tasks, such as text summarization, causal relationship identification, and event reasoning, make extensive use of event extraction. However, most of the existing research on event extraction remains at the sentence level. Document-level event extraction is still under exploration and lacks relevant review. Focusing on the task of document-level event extraction, this survey first divides the existing technologies into three categories and introduces the most representative models; Then, we list some commonly used datasets for document-level event extraction with their usage messages; Finally, we analyse the current research gaps of document-level event extraction and future research trends.",
            "year": 2023,
            "venue": "Journal of Physics: Conference Series",
            "authors": [
              {
                "authorId": "2144831701",
                "name": "Qi Liu"
              },
              {
                "authorId": "2218656359",
                "name": "Zhen Luan"
              },
              {
                "authorId": "31328338",
                "name": "Kunlong Wang"
              },
              {
                "authorId": "2219054579",
                "name": "Ye Zou"
              },
              {
                "authorId": "143830417",
                "name": "Bing Liu"
              },
              {
                "authorId": "2219063788",
                "name": "Yang Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259950998,
          "isinfluential": false,
          "contexts": [
            "With their massive training datasets and numerous model parameters, LLMs have demonstrated the ability to handle complex tasks [8–12], making it possible to perform event argument extraction in a single step."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2113243762",
                "name": "Hugo Touvron"
              },
              {
                "authorId": "143792623",
                "name": "Louis Martin"
              },
              {
                "authorId": "2059203763",
                "name": "Kevin R. Stone"
              },
              {
                "authorId": "2214809450",
                "name": "Peter Albert"
              },
              {
                "authorId": "2634674",
                "name": "Amjad Almahairi"
              },
              {
                "authorId": "2223764353",
                "name": "Yasmine Babaei"
              },
              {
                "authorId": "2223756247",
                "name": "Niko-lay Bashlykov"
              },
              {
                "authorId": "47505161",
                "name": "Soumya Batra"
              },
              {
                "authorId": "51229603",
                "name": "Prajjwal Bhargava"
              },
              {
                "authorId": "2116473",
                "name": "Shruti Bhosale"
              },
              {
                "authorId": "2023469",
                "name": "D. Bikel"
              },
              {
                "authorId": "2040305955",
                "name": "Lukas Blecher"
              },
              {
                "authorId": "66286536",
                "name": "Cris-tian Cantón Ferrer"
              },
              {
                "authorId": "2108267192",
                "name": "Moya Chen"
              },
              {
                "authorId": "7153363",
                "name": "Guillem Cucurull"
              },
              {
                "authorId": "71039937",
                "name": "David Esiobu"
              },
              {
                "authorId": "2166312768",
                "name": "Jude Fernandes"
              },
              {
                "authorId": "2223974989",
                "name": "J. Fu"
              },
              {
                "authorId": "2223742000",
                "name": "Wenyin Fu"
              },
              {
                "authorId": "2223748737",
                "name": "Brian Fuller"
              },
              {
                "authorId": "2107063269",
                "name": "Cynthia Gao"
              },
              {
                "authorId": "28554843",
                "name": "Vedanuj Goswami"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "4305645",
                "name": "A. Hartshorn"
              },
              {
                "authorId": "2195458",
                "name": "Saghar Hosseini"
              },
              {
                "authorId": "2132302721",
                "name": "Rui Hou"
              },
              {
                "authorId": "2065277797",
                "name": "Hakan Inan"
              },
              {
                "authorId": "2059886128",
                "name": "Marcin Kardas"
              },
              {
                "authorId": "2190957318",
                "name": "Viktor Kerkez"
              },
              {
                "authorId": "2072010",
                "name": "Madian Khabsa"
              },
              {
                "authorId": "2207049",
                "name": "Isabel M. Kloumann"
              },
              {
                "authorId": "2294453195",
                "name": "A. Korenev"
              },
              {
                "authorId": "2146367061",
                "name": "Punit Singh Koura"
              },
              {
                "authorId": "114952298",
                "name": "M. Lachaux"
              },
              {
                "authorId": "46183616",
                "name": "Thibaut Lavril"
              },
              {
                "authorId": "2223749565",
                "name": "Jenya Lee"
              },
              {
                "authorId": "2145259939",
                "name": "Diana Liskovich"
              },
              {
                "authorId": "1768032",
                "name": "Yinghai Lu"
              },
              {
                "authorId": "3375249",
                "name": "Yuning Mao"
              },
              {
                "authorId": "1490887583",
                "name": "Xavier Martinet"
              },
              {
                "authorId": "39980906",
                "name": "Todor Mihaylov"
              },
              {
                "authorId": "3047561",
                "name": "Pushkar Mishra"
              },
              {
                "authorId": "2322981055",
                "name": "Igor Molybog"
              },
              {
                "authorId": "40383658",
                "name": "Yixin Nie"
              },
              {
                "authorId": "38579672",
                "name": "Andrew Poulton"
              },
              {
                "authorId": "39906022",
                "name": "J. Reizenstein"
              },
              {
                "authorId": "150282885",
                "name": "Rashi Rungta"
              },
              {
                "authorId": "1859294",
                "name": "Kalyan Saladi"
              },
              {
                "authorId": "14279694",
                "name": "Alan Schelten"
              },
              {
                "authorId": "2214818043",
                "name": "Ruan Silva"
              },
              {
                "authorId": "51324296",
                "name": "Eric Michael Smith"
              },
              {
                "authorId": "2066074360",
                "name": "R. Subramanian"
              },
              {
                "authorId": "2112782199",
                "name": "Xia Tan"
              },
              {
                "authorId": "71292072",
                "name": "Binh Tang"
              },
              {
                "authorId": "2110697298",
                "name": "Ross Taylor"
              },
              {
                "authorId": "2110032535",
                "name": "Adina Williams"
              },
              {
                "authorId": "2223770369",
                "name": "Jian Xiang Kuan"
              },
              {
                "authorId": "2214843767",
                "name": "Puxin Xu"
              },
              {
                "authorId": "14701107",
                "name": "Zhengxu Yan"
              },
              {
                "authorId": "121929334",
                "name": "Iliyan Zarov"
              },
              {
                "authorId": "2108473229",
                "name": "Yuchen Zhang"
              },
              {
                "authorId": "144270981",
                "name": "Angela Fan"
              },
              {
                "authorId": "2165660870",
                "name": "M. Kambadur"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "2166043087",
                "name": "Aur'elien Rodriguez"
              },
              {
                "authorId": "1962768",
                "name": "Robert Stojnic"
              },
              {
                "authorId": "2068070",
                "name": "Sergey Edunov"
              },
              {
                "authorId": "2073456043",
                "name": "Thomas Scialom"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265149752,
          "isinfluential": false,
          "contexts": [
            "Additionally, using multiple documents as examples can easily exceed the context length limitations of LLMs [14]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction",
            "abstract": "In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in diverse tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2111825433",
                "name": "Hanzhang Zhou"
              },
              {
                "authorId": "2266439699",
                "name": "Junlang Qian"
              },
              {
                "authorId": "2112599636",
                "name": "Zijian Feng"
              },
              {
                "authorId": "2266420870",
                "name": "Hui Lu"
              },
              {
                "authorId": "2166589821",
                "name": "Zixiao Zhu"
              },
              {
                "authorId": "2128504277",
                "name": "Kezhi Mao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 275901979,
          "isinfluential": false,
          "contexts": [
            "AI agents are regarded as artificial entities capable of perceiving their surroundings using sensors, making decisions, and taking responsive actions using actuators [23]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "The rise and potential of large language model based agents: a survey",
            "abstract": "",
            "year": 2025,
            "venue": "Science China Information Sciences",
            "authors": [
              {
                "authorId": "2218237934",
                "name": "Zhiheng Xi"
              },
              {
                "authorId": "2240538633",
                "name": "Wenxiang Chen"
              },
              {
                "authorId": "2240675422",
                "name": "Xin Guo"
              },
              {
                "authorId": "2241408708",
                "name": "Wei He"
              },
              {
                "authorId": "2240473116",
                "name": "Yiwen Ding"
              },
              {
                "authorId": "2240450431",
                "name": "Boyang Hong"
              },
              {
                "authorId": "2240574799",
                "name": "Ming Zhang"
              },
              {
                "authorId": "2240465418",
                "name": "Junzhe Wang"
              },
              {
                "authorId": "2219131195",
                "name": "Senjie Jin"
              },
              {
                "authorId": "2240446306",
                "name": "Enyu Zhou"
              },
              {
                "authorId": "2058585152",
                "name": "Rui Zheng"
              },
              {
                "authorId": "2241140630",
                "name": "Xiaoran Fan"
              },
              {
                "authorId": "2342462770",
                "name": "Xiao Wang"
              },
              {
                "authorId": "2222630539",
                "name": "Limao Xiong"
              },
              {
                "authorId": "2212175381",
                "name": "Yuhao Zhou"
              },
              {
                "authorId": "2240703461",
                "name": "Weiran Wang"
              },
              {
                "authorId": "2240482661",
                "name": "Changhao Jiang"
              },
              {
                "authorId": "51192034",
                "name": "Yicheng Zou"
              },
              {
                "authorId": "2344064177",
                "name": "Xiangyang Liu"
              },
              {
                "authorId": "2155273086",
                "name": "Zhangyue Yin"
              },
              {
                "authorId": "2042683163",
                "name": "Shihan Dou"
              },
              {
                "authorId": "2320464491",
                "name": "Rongxiang Weng"
              },
              {
                "authorId": "2240455955",
                "name": "Wenjuan Qin"
              },
              {
                "authorId": "2342240913",
                "name": "Yongyan Zheng"
              },
              {
                "authorId": "2282972251",
                "name": "Xipeng Qiu"
              },
              {
                "authorId": "2257129989",
                "name": "Xuanjing Huang"
              },
              {
                "authorId": "2287225519",
                "name": "Qi Zhang"
              },
              {
                "authorId": "2333592096",
                "name": "Tao Gui"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "To demonstrate that our method can be applied to different LLMs, we selected Llama3 (70B) [27] and Gemma2 (27B) [28]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "278488523": {
      "citing_paper_info": {
        "title": "A dataset for document level Chinese financial event extraction",
        "abstract": "Financial event modeling is fundamental to financial investment decisions and risk management, crucial for the stability and growth of financial institutions, and helps ensure the stability and quality of people’s lives. Utilizing state-of-the-art natural language processing techniques for automated financial event extraction addresses the inefficiencies and high costs associated with traditional event identification and modeling, which rely heavily on financial domain experts. However, existing datasets fail to tackle the issues with long documents in practical situations. To address this, we first propose DocFEE, a large-scale Document-level Chinese Financial Event Extraction dataset. It reflects the length of announcement documents and the long-distance dependencies of event arguments in real-world scenarios.",
        "year": 2025,
        "venue": "Scientific Data",
        "authors": [
          {
            "authorId": "1763402",
            "name": "Yubo Chen"
          },
          {
            "authorId": "144137458",
            "name": "Tong Zhou"
          },
          {
            "authorId": "2361908443",
            "name": "Sirui Li"
          },
          {
            "authorId": "2286766381",
            "name": "Jun Zhao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "252569165",
        "251640549",
        "216562779",
        "246241684",
        "268417143"
      ],
      "citation_details": [
        {
          "citedcorpusid": 216562779,
          "isinfluential": false,
          "contexts": [
            "Recent natural language processing advancements have enabled automated extraction and modeling of financial-related events from unstructured texts 17–24 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "MAVEN: A Massive General Domain Event Detection Dataset",
            "abstract": "Event detection (ED), which identifies event trigger words and classifies event types according to contexts, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Small scale of existing datasets is not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Limited event types of existing datasets lead to the trained models cannot be easily adapted to general-domain scenarios. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 117,200 event mention instances, and 207 event types. MAVEN alleviates the lack of data problem and covers much more general event types. Besides the dataset, we reproduce the recent state-of-the-art ED models and conduct a thorough evaluation for these models on MAVEN. The experimental results and empirical analyses show that existing ED methods cannot achieve promising results as on the small datasets, which suggests ED in real world remains a challenging task and requires further research efforts. The dataset and baseline code will be released in the future to promote this field.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "1659764899",
                "name": "Wangyi Jiang"
              },
              {
                "authorId": "151185222",
                "name": "Rong Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246241684,
          "isinfluential": false,
          "contexts": [
            "The existing dataset does not take into account real-world length public announcements for the financial event extraction task 7,15,27,28 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "FEED: A Chinese Financial Event Extraction Dataset Constructed by Distant Supervision",
            "abstract": "As an essential task in information extraction, event extraction (EE) provides abundant and valuable structured information and has been shown to be useful sources of background knowledge for applications in various domains, such as finance, legislation, health, etc. However, extracting events from domain documents is challenging since relevant information of multiple events is often scattered across multiple sentences. To this end, we release a large-scale Chinese financial event extraction dataset FEED, consisting of 31,748 documents on five financial event types derived from the Chinese financial portals, which considers the case of event arguments scattered in multiple sentences and one document containing multiple events. In order to construct FEED dataset, we first extract candidate events from financial announcements by Fonduer. Then we build an event knowledge base using weakly supervised classification, and finally label events via distant supervision. We also verify the usability of FEED and the distinguishability on baseline models. Experimental results show that FEED is challenging for existing event extraction methods, which indicates that Chinese financial event extraction remains an open problem and requires further efforts. All details and resources about FEED and event knowledge base are released at https://github.com/seukgcode/FEED.",
            "year": 2021,
            "venue": "IJCKG",
            "authors": [
              {
                "authorId": "91119265",
                "name": "Guozheng Li"
              },
              {
                "authorId": "2155303586",
                "name": "Peng Wang"
              },
              {
                "authorId": "2153624361",
                "name": "Jiafeng Xie"
              },
              {
                "authorId": "2087801075",
                "name": "Ruilong Cui"
              },
              {
                "authorId": "2152158770",
                "name": "Zhenkai Deng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251640549,
          "isinfluential": false,
          "contexts": [
            "Such capabilities form a foundational basis for proactive risk alerting and trend forecasting 25,26 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event prediction within directional change framework using a CNN-LSTM model",
            "abstract": "Financial forecasting has always been an intriguing research area in the field of finance. The widely accepted approach to forecast financial data is to perform predictions using time series data. In time series analysis, sampling the financial data with a predefined frequency (e.g. hourly, daily) leads to an uneven and discontinued data flow. Directional Change is a newly proposed approach that replaces physical time within the financial data and establishes an event-driven framework. With the emergence of the machine and deep learning-based methods, researchers have utilised them in financial time series. These techniques have shown to outperform conventional approaches. This paper aims to employ the CNN-LSTM model to investigate its predictive competence within the Directional Change (DC) framework to predict DC event prices. To obtain this objective, we first create the tick bars/candles of the GBPUSD, EURUSD, USDCHF, and USDCAD tick prices from January to August 2019. Then, the DC-based summaries of the selected tick bar/candle for each currency pair will be generated and fed to the CNN-LSTM model. The CNN-LSTM network architecture incorporates the robustness of Convolutional Neural Network (CNN) in feature extraction and Long Short-Term Memory (LSTM) in predicting sequential data. The results suggest that the performance of the CNN-LSTM model improves significantly within the DC framework.",
            "year": 2022,
            "venue": "Neural computing & applications (Print)",
            "authors": [
              {
                "authorId": "2181889405",
                "name": "Ahoora Rostamian"
              },
              {
                "authorId": "1401003374",
                "name": "J. O'Hara"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252569165,
          "isinfluential": false,
          "contexts": [
            "The existing dataset does not take into account real-world length public announcements for the financial event extraction task 7,15,27,28 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "DuEE-Fin: A Large-Scale Dataset for Document-Level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "Natural Language Processing and Chinese Computing",
            "authors": [
              {
                "authorId": "2159829320",
                "name": "Cuiyun Han"
              },
              {
                "authorId": "2144141948",
                "name": "Jinchuan Zhang"
              },
              {
                "authorId": "2118335351",
                "name": "Xinyu Li"
              },
              {
                "authorId": "2186424675",
                "name": "Guojin Xu"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "40572919",
                "name": "Zengfeng Zeng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268417143,
          "isinfluential": false,
          "contexts": [
            "Recent natural language processing advancements have enabled automated extraction and modeling of financial-related events from unstructured texts 17–24 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CEAN: Contrastive Event Aggregation Network with LLM-based Augmentation for Event Extraction",
            "abstract": "Event Extraction is a crucial yet arduous task in natural language processing (NLP), as its performance is significantly hindered by laborious data annotation. Given this challenge, recent research has predominantly focused on two approaches: pretraining task-oriented models for event extraction and employing data augmentation techniques. These methods involve integrating external knowledge, semantic structures, or artificially generated samples using large language models (LLMs). However, their performances can be compromised due to two fundamental issues. Firstly, the alignment between the introduced knowledge and event extraction knowledge is crucial. Secondly, the introduction of data noise during the augmentation is unavoidable and can mislead the model’s convergence. To address these issues, we propose a Contrastive Event Aggregation Network with LLM-based Augmentation to promote low-resource learning and reduce data noise for event extraction. Different from the existing methods introducing linguistic knowledge into data augmentation, an event aggregation network is established to introduce event knowledge into supervised learning by constructing adaptively-updated semantic representation for trigger and argument. For LLM-based augmentation, we design a new scheme including a multi-pattern rephrasing paradigm and a data-free composing paradigm. Instead of directly using augmentation samples in the supervised task, we introduce span-level contrastive learning to reduce data noise. Experiments on the ACE2005 and ERE-EN demonstrate that our proposed approach achieves new state-of-the-art results on both of the two datasets.",
            "year": 2024,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2291367085",
                "name": "Zihao Meng"
              },
              {
                "authorId": "2149917875",
                "name": "Tao Liu"
              },
              {
                "authorId": "2291931827",
                "name": "Heng Zhang"
              },
              {
                "authorId": "2291423704",
                "name": "Kai Feng"
              },
              {
                "authorId": "2292307128",
                "name": "Peng Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "These are critical not only for ensuring the stability and growth of financial institutions 1,2 but also for safeguarding the overall security and sustainable development of both financial markets and global economies 3,4 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The existing dataset does not take into account real-world length public announcements for the financial event extraction task 7,15,27,28 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The financial expert summarizes and extracts various predefined events by comprehending and analyzing financial documents 12 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "These are critical not only for ensuring the stability and growth of financial institutions 1,2 but also for safeguarding the overall security and sustainable development of both financial markets and global economies 3,4 ."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "51871198": {
      "citing_paper_info": {
        "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
        "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
        "year": 2018,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "144955350",
            "name": "Hang Yang"
          },
          {
            "authorId": "1763402",
            "name": "Yubo Chen"
          },
          {
            "authorId": "2200096",
            "name": "Kang Liu"
          },
          {
            "authorId": "2116643823",
            "name": "Yang Xiao"
          },
          {
            "authorId": "1390572170",
            "name": "Jun Zhao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 6,
        "influential_count": 1,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "12108307",
        "1320606",
        "2778800",
        "1942185",
        "8174613",
        "17984630"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "Hybrid event-extraction methods combine statistical methods and pattern-based methods together (Jungermann and Morik, 2008), (Bjorne et al., 2010)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1942185,
          "isinfluential": false,
          "contexts": [
            "Hybrid event-extraction methods combine statistical methods and pattern-based methods together (Jungermann and Morik, 2008), (Bjorne et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Enhanced Services for Targeted Information Retrieval by Event Extraction and Data Mining",
            "abstract": "",
            "year": 2008,
            "venue": "LWA",
            "authors": [
              {
                "authorId": "1954152",
                "name": "F. Jungermann"
              },
              {
                "authorId": "1752599",
                "name": "K. Morik"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2778800,
          "isinfluential": false,
          "contexts": [
            "DS has proved its effectiveness in automatically labeling data for Relation Extraction (Zeng et al., 2015) and Event Extraction (Chen et al., 2017).",
            "DS has proved its effectiveness in automatically labeling data for Relation Extraction (Zeng et al., 2015) and Event Extraction (Chen et al."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks",
            "abstract": "Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",
            "year": 2015,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8174613,
          "isinfluential": false,
          "contexts": [
            "The current EE approaches can be mainly classified into statistical methods, pattern-based method and hybrid method (Hogenboom et al., 2016)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
            "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3016273",
                "name": "Jiatao Gu"
              },
              {
                "authorId": "11955007",
                "name": "Zhengdong Lu"
              },
              {
                "authorId": "49404233",
                "name": "Hang Li"
              },
              {
                "authorId": "2052674293",
                "name": "V. Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": true,
          "contexts": [
            "We make use of Distance Supervision (DS) which has been validated to generate labeled data for EE (Chen et al., 2017) to automatically generate large-scaled annotated data.",
            "Event Extraction (EE), a challenging task in Nature Language Processing (NLP), aims at discovering event mentions 3 and extracting events which contain event triggers 4 and event arguments 5 from texts.",
            "DS has proved its effectiveness in automatically labeling data for Relation Extraction (Zeng et al., 2015) and Event Extraction (Chen et al., 2017).",
            "Figure 2 describes the architecture of our proposed DCFEE framework which primarily involves the following two components: (i) Data Generation, which makes use of DS to automatically label event mention from the whole documen-t (document-level data) and annotate triggers and arguments from event mention (sentence-level data); (ii) EE system, which contains Sentence-level Event Extraction (SEE) supported by sentence-level labeled data and Document-level Event Extraction (DEE) supported by document-level labeled data.",
            "In order to improve recall, there are two main research directions: build relatively complete pattern library and use a semi-automatic method to build trigger dictionary (Chen et al., 2017), (Gu et al.",
            "In order to improve recall, there are two main research directions: build relatively complete pattern library and use a semi-automatic method to build trigger dictionary (Chen et al., 2017), (Gu et al., 2016).",
            "Figure 4 depicts the overall architecture of the EE system proposed in this paper which primarily involves the following two components: The sentence-level Event Extraction (SEE) purposes to extract event arguments and event triggers from one sentence; The document-level Event Extraction (DEE) aims to extract event arguments from the whole document based on a key event detection model and an arguments-completion strategy."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "--",
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17984630,
          "isinfluential": false,
          "contexts": [
            "Hybrid event-extraction methods combine statistical methods and pattern-based methods together (Jungermann and Morik, 2008), (Bjorne et al., 2010).",
            "Hybrid event-extraction method-s combine statistical methods and pattern-based methods together (Jungermann and Morik, 2008), (Bjorne et al., 2010)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Complex event extraction at PubMed scale",
            "abstract": "Motivation: There has recently been a notable shift in biomedical information extraction (IE) from relation models toward the more expressive event model, facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources. The event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction. A recent collaborative evaluation demonstrated the potential of event extraction systems, yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large-scale extraction. Results: This study considers event-based IE at PubMed scale. We introduce a system combining publicly available, state-of-the-art methods for domain parsing, named entity recognition and event extraction, and test the system on a representative 1% sample of all PubMed citations. We present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity, event extraction from the entire PubMed is feasible. We further illustrate the value of the extraction approach through a number of analyses of the extracted information. Availability: The event detection system and extracted data are open source licensed and available at http://bionlp.utu.fi/. Contact: jari.bjorne@utu.fi",
            "year": 2010,
            "venue": "Bioinform.",
            "authors": [
              {
                "authorId": "1761599",
                "name": "Jari Björne"
              },
              {
                "authorId": "1694491",
                "name": "Filip Ginter"
              },
              {
                "authorId": "1708916",
                "name": "S. Pyysalo"
              },
              {
                "authorId": "1737901",
                "name": "Junichi Tsujii"
              },
              {
                "authorId": "1680811",
                "name": "T. Salakoski"
              }
            ]
          }
        }
      ]
    },
    "249191641": {
      "citing_paper_info": {
        "title": "EA2E: Improving Consistency with Event Awareness for Document-Level Argument Extraction",
        "abstract": "Events are inter-related in documents. Motivated by the one-sense-per-discourse theory, we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events, which will further cause discrepancy for downstream applications such as event knowledge base population, question answering, and hypothesis generation. In this work, we formulate event argument consistency as the constraints from event-event relations under the document-level setting. To improve consistency we introduce the Event-Aware Argument Extraction (EA$^2$E) model with augmented context for training and inference. Experiment results on WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E compared to baseline methods.",
        "year": 2022,
        "venue": "NAACL-HLT",
        "authors": [
          {
            "authorId": "145653969",
            "name": "Qi Zeng"
          },
          {
            "authorId": "2167027235",
            "name": "Qiusi Zhan"
          },
          {
            "authorId": "2072975828",
            "name": "Heng Ji"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "131886",
        "233219850",
        "218630327"
      ],
      "citation_details": [
        {
          "citedcorpusid": 131886,
          "isinfluential": false,
          "contexts": [
            "Motivated by the one-sense-per-discourse theory (Gale et al., 1992) that mentions of an ambiguous word usually tend to share the same sense in a given discourse, we hypothesize that a participant tends to play consistent roles across multiple events in the same document .",
            "Motivated by the one-sense-per-discourse theory (Gale et al., 1992) that mentions of an ambiguous word usually tend to share the same sense in a given discourse, we hypothesize that a participant tends to play consistent roles across multiple events in the same document."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "One Sense Per Discourse",
            "abstract": "It is well-known that there are polysemous words like sentence whose \"meaning\" or \"sense\" depends on the context of use. We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). As this work was nearing completion, we observed a very strong discourse effect. That is, if a polysemous word such as sentence appears two or more times in a well-written discourse, it is extremely likely that they will all share the same sense. This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%). This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm. In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint.",
            "year": 1992,
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "authors": [
              {
                "authorId": "34938639",
                "name": "W. Gale"
              },
              {
                "authorId": "2244184",
                "name": "Kenneth Ward Church"
              },
              {
                "authorId": "1693517",
                "name": "David Yarowsky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            ", 2021) and (Du and Cardie, 2020) are designed for Role-filler Entity Extraction (REE) task under the assumption that one generic template is produced for each document, while our work focuses on extracting arguments for multiple events for each document."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "It is a practically more useful but more challenging task than sentence-level Ar-gument Extraction (Nguyen et al., 2016; Wadden et al., 2019; Lin et al., 2020) because in a typical long input document events usually scatter across multiple sentences and are inherently connected.",
            "We compare EA 2 E with document-level BART-Gen (Li et al., 2021), sentence-level ONEIE (Lin et al., 2020) and BERT-CRF (Shi and Lin, 2019)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        }
      ]
    },
    "273185680": {
      "citing_paper_info": {
        "title": "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering",
        "abstract": "As an essential task in information extraction (IE), Event-Event Causal Relation Extraction (ECRE) aims to identify and classify the causal relationships between event mentions in natural language texts. However, existing research on ECRE has highlighted two critical challenges, including the lack of document-level modeling and causal hallucinations. In this paper, we propose a Knowledge-guided binary Question Answering (KnowQA) method with event structures for ECRE, consisting of two stages: Event Structure Construction and Binary Question Answering. We conduct extensive experiments under both zero-shot and fine-tuning settings with large language models (LLMs) on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the usefulness of event structures on document-level ECRE and the effectiveness of KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only the effectiveness but also the high generalizability and low inconsistency of our method, particularly when with complete event structures after fine-tuning the models.",
        "year": 2024,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2200687506",
            "name": "Zimu Wang"
          },
          {
            "authorId": "2324921937",
            "name": "Lei Xia"
          },
          {
            "authorId": "2284640298",
            "name": "Wei Wang"
          },
          {
            "authorId": "2325116028",
            "name": "Xinya Du"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 19,
        "unique_cited_count": 18,
        "influential_count": 1,
        "detailed_records_count": 19
      },
      "cited_papers": [
        "235313431",
        "258297899",
        "262825274",
        "235097376",
        "236460024",
        "235166504",
        "226262283",
        "2172129",
        "235254286",
        "249010869",
        "270199894",
        "260063107",
        "268249143",
        "259858871",
        "220484653",
        "244678529",
        "226254029",
        "250390907"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2172129,
          "isinfluential": false,
          "contexts": [
            "Du and Ji (2022); Choudhary and Du (2024) leverage end-to-end deep learning-based methods (Du et al., 2017; Du and Cardie, 2018) for generating QA pairs for representing events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning to Ask: Neural Question Generation for Reading Comprehension",
            "abstract": "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e.,, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1931349",
                "name": "Junru Shao"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220484653,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie (2020); Liu et al. (2020a) utilize heuristic-based methods for generating questions.",
            "…recent advancements have leveraged PLMs and introduce semantic structures (Tran Phu and Nguyen, 2021; Hu et al., 2023a), external knowledge (Liu et al., 2020b; Cao et al., 2021), and data augmentation (Zuo et al., 2020, 2021) approaches and investigated the potency of ECI with LLMs (Gao et…"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Knowledge Enhanced Event Causality Identification with Mention Masking Generalizations",
            "abstract": "Identifying causal relations of events is a crucial language understanding task. Despite many efforts for this task, existing methods lack the ability to adopt background knowledge, and they typically generalize poorly to new, previously unseen data. In this paper, we present a new method for event causality identification, aiming to address limitations of previous methods. On the one hand, our model can leverage external knowledge for reasoning, which can greatly enrich the representation of events; On the other hand, our model can mine event-agnostic, context-specific patterns, via a mechanism called event mention masking generalization, which can greatly enhance the ability of our model to handle new, previously unseen cases. In experiments, we evaluate our model on three benchmark datasets and show our model outperforms previous methods by a significant margin. Moreover, we perform 1) cross-topic adaptation, 2) exploiting unseen predicates, and 3) cross-task adaptation to evaluate the generalization ability of our model. Experimental results show that our model demonstrates a definite advantage over previous methods.",
            "year": 2020,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "48211977",
                "name": "Jian Liu"
              },
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226254029,
          "isinfluential": false,
          "contexts": [
            "Cause Effect language logical/temporal reasoning (Yang et al., 2020, 2023, 2024)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Duration Prediction via Time-aware Pre-training",
            "abstract": "End-to-end models in NLP rarely encode external world knowledge about length of time. We introduce two effective models for duration prediction, which incorporate external knowledge by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit where the duration value falls in (R-PRED); and the other predicts the exact duration value (E-PRED). Our best model – E-PRED, substantially outperforms previous work, and captures duration information more accurately than R-PRED. We also demonstrate our models are capable of duration prediction in the unsupervised setting, outperforming the baselines.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2124477940",
                "name": "Zonglin Yang"
              },
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie (2020); Liu et al. (2020a) utilize heuristic-based methods for generating questions.",
            "…recent advancements have leveraged PLMs and introduce semantic structures (Tran Phu and Nguyen, 2021; Hu et al., 2023a), external knowledge (Liu et al., 2020b; Cao et al., 2021), and data augmentation (Zuo et al., 2020, 2021) approaches and investigated the potency of ECI with LLMs (Gao et…"
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235097376,
          "isinfluential": false,
          "contexts": [
            "Afterwards, we follow previous IE toolkits (Wen et al., 2021; Du et al., 2022) to extract the event arguments with BART-Gen (Li et al., 2021), a generative model for document-level event argument extraction (EAE)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System",
            "abstract": "We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "4428136",
                "name": "Haoyang Wen"
              },
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "145242558",
                "name": "T. Lai"
              },
              {
                "authorId": "34741133",
                "name": "Xiaoman Pan"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "48030192",
                "name": "Xudong Lin"
              },
              {
                "authorId": "2108536188",
                "name": "Ben Zhou"
              },
              {
                "authorId": "2118482058",
                "name": "Manling Li"
              },
              {
                "authorId": "34269118",
                "name": "Haoyu Wang"
              },
              {
                "authorId": "2111112132",
                "name": "Hongming Zhang"
              },
              {
                "authorId": "3099583",
                "name": "Xiaodong Yu"
              },
              {
                "authorId": "2101316346",
                "name": "Alexander Dong"
              },
              {
                "authorId": "2108330537",
                "name": "Zhenhailong Wang"
              },
              {
                "authorId": "51135899",
                "name": "Y. Fung"
              },
              {
                "authorId": "51234098",
                "name": "Piyush Mishra"
              },
              {
                "authorId": "1904906987",
                "name": "Qing Lyu"
              },
              {
                "authorId": "35399640",
                "name": "Dídac Surís"
              },
              {
                "authorId": "2108342501",
                "name": "Brian Chen"
              },
              {
                "authorId": "1783500",
                "name": "S. Brown"
              },
              {
                "authorId": "145755155",
                "name": "Martha Palmer"
              },
              {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
              },
              {
                "authorId": "1856025",
                "name": "Carl Vondrick"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              },
              {
                "authorId": "9546964",
                "name": "Shih-Fu Chang"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235166504,
          "isinfluential": true,
          "contexts": [
            "The event arguments and their single-hop relations are obtained from relevant IE models (Peng et al., 2023b; Li et al., 2021; Eberts and Ulges, 2021).",
            "Event Argument Extraction & Joint Entity and Relation Extraction We adopted the pre-trained models released by BART-Gen 6 (Li et al., 2021) and JEREX 7 (Eberts and Ulges, 2021) to conduct EAE and joint entity and relation extraction on our datasets, separately.",
            "We adopt the templates defined by Li et al. (2021) to extract arguments for the event mentions.",
            "Afterwards, we follow previous IE toolkits (Wen et al., 2021; Du et al., 2022) to extract the event arguments with BART-Gen (Li et al., 2021), a generative model for document-level event argument extraction (EAE).",
            "Event Detection We trained an event detection using the OmniEvent toolkit (Peng et al., 2023b) on the WikiEvents dataset (Li et al., 2021), and we selected CLEVE (Wang et al., 2021) as the PLM.",
            "Following the previous research in EE (Du and Cardie, 2020; Deng et al., 2021), we define our Event-Event Causal Relation Extraction (ECRE) task as two subtasks: Event Causality Identification (ECI) , which identifies the existence of causal relationships between event mentions, and Causal Relation…",
            "We train an event detection model us-ing the CLEVE (Wang et al., 2021) PLM on the WikiEvents dataset (Li et al., 2021) to classify the event mentions to their most likely belonged event type in the KAIROS ontology."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "OntoED: Low-resource Event Detection with Ontology Embedding",
            "abstract": "Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED framework entitled OntoED with ontology embedding. We enrich event ontology with linkages among event types, and further induce more event-event correlations. Based on the event ontology, OntoED can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types. Furthermore, OntoED can be applied to new unseen event types, by establishing linkages to existing ones. Experiments indicate that OntoED is more predominant and robust than previous approaches to ED, especially in data-scarce scenarios.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "1942197948",
                "name": "Luoqiu Li"
              },
              {
                "authorId": "2155551120",
                "name": "Hui Chen"
              },
              {
                "authorId": "32602115",
                "name": "Huaixiao Tou"
              },
              {
                "authorId": "2108266952",
                "name": "Mosha Chen"
              },
              {
                "authorId": "2087380523",
                "name": "Fei Huang"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235254286,
          "isinfluential": false,
          "contexts": [
            "Event Detection We trained an event detection using the OmniEvent toolkit (Peng et al., 2023b) on the WikiEvents dataset (Li et al., 2021), and we selected CLEVE (Wang et al., 2021) as the PLM.",
            "We train an event detection model us-ing the CLEVE (Wang et al., 2021) PLM on the WikiEvents dataset (Li et al., 2021) to classify the event mentions to their most likely belonged event type in the KAIROS ontology."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CLEVE: Contrastive Pre-training for Event Extraction",
            "abstract": "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2149202150",
                "name": "Yankai Lin"
              },
              {
                "authorId": "2055765060",
                "name": "Lei Hou"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313431,
          "isinfluential": false,
          "contexts": [
            "…advancements have leveraged PLMs and introduce semantic structures (Tran Phu and Nguyen, 2021; Hu et al., 2023a), external knowledge (Liu et al., 2020b; Cao et al., 2021), and data augmentation (Zuo et al., 2020, 2021) approaches and investigated the potency of ECI with LLMs (Gao et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification",
            "abstract": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460024,
          "isinfluential": false,
          "contexts": [
            "…advancements have leveraged PLMs and introduce semantic structures (Tran Phu and Nguyen, 2021; Hu et al., 2023a), external knowledge (Liu et al., 2020b; Cao et al., 2021), and data augmentation (Zuo et al., 2020, 2021) approaches and investigated the potency of ECI with LLMs (Gao et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks",
            "abstract": "Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244678529,
          "isinfluential": false,
          "contexts": [
            "We extend the definition proposed by Automatic Content Extraction (ACE), consisting of event mentions and event arguments (Frisoni et al., 2021), with the single-hop relationships of arguments to enrich their information in contexts, as examples shown in Figure 3."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Survey on Event Extraction for Natural Language Understanding: Riding the Biomedical Literature Wave",
            "abstract": "Motivation: The scientific literature embeds an enormous amount of relational knowledge, encompassing interactions between biomedical entities, like proteins, drugs, and symptoms. To cope with the ever-increasing number of publications, researchers are experiencing a surge of interest in extracting valuable, structured, concise, and unambiguous information from plain texts. With the development of deep learning, the granularity of information extraction is evolving from entities and pairwise relations to events. Events can model complex interactions involving multiple participants having a specific semantic role, also handling nested and overlapping definitions. After being studied for years, automatic event extraction is on the road to significantly impact biology in a wide range of applications, from knowledge base enrichment to the formulation of new research hypotheses. Results: This paper provides a comprehensive and up-to-date survey on the link between event extraction and natural language understanding, focusing on the biomedical domain. First, we establish a flexible event definition, summarizing the terminological efforts conducted in various areas. Second, we present the event extraction task, the related challenges, and the available annotated corpora. Third, we deeply explore the most representative methods and present an analysis of the current state-of-the-art, accompanied by performance discussion. To help researchers navigate the avalanche of event extraction works, we provide a detailed taxonomy for classifying the contributions proposed by the community. Fourth, we compare solutions applied in biomedicine with those evaluated in other domains, identifying research opportunities and providing insights for strategies not yet explored. Finally, we discuss applications and our envisions about future perspectives, moving the needle on explainability and knowledge injection.",
            "year": 2021,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "1841141158",
                "name": "Giacomo Frisoni"
              },
              {
                "authorId": "143853729",
                "name": "G. Moro"
              },
              {
                "authorId": "1680758",
                "name": "A. Carbonaro"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249010869,
          "isinfluential": false,
          "contexts": [
            "Afterwards, we follow previous IE toolkits (Wen et al., 2021; Du et al., 2022) to extract the event arguments with BART-Gen (Li et al., 2021), a generative model for document-level event argument extraction (EAE)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios",
            "abstract": "We introduce RESIN-11, a new schema-guided event extraction&prediction framework that can be applied to a large variety of newsworthy scenarios. The framework consists of two parts: (1) an open-domain end-to-end multimedia multilingual information extraction system with weak-supervision and zero-shot learningbased techniques. (2) schema matching and schema-guided event prediction based on our curated schema library. We build a demo website based on our dockerized system and schema library publicly available for installation (https://github.com/RESIN-KAIROS/RESIN-11). We also include a video demonstrating the system.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2116461591",
                "name": "Zixuan Zhang"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "144808890",
                "name": "Pengfei Yu"
              },
              {
                "authorId": "2108986414",
                "name": "Hongwei Wang"
              },
              {
                "authorId": "145242558",
                "name": "T. Lai"
              },
              {
                "authorId": "48030192",
                "name": "Xudong Lin"
              },
              {
                "authorId": "2166112050",
                "name": "Iris Liu"
              },
              {
                "authorId": "2108536188",
                "name": "Ben Zhou"
              },
              {
                "authorId": "4428136",
                "name": "Haoyang Wen"
              },
              {
                "authorId": "2118482058",
                "name": "Manling Li"
              },
              {
                "authorId": "153060461",
                "name": "Darryl Hannan"
              },
              {
                "authorId": "46665218",
                "name": "Jie Lei"
              },
              {
                "authorId": "51270689",
                "name": "Hyounghun Kim"
              },
              {
                "authorId": "3372941",
                "name": "Rotem Dror"
              },
              {
                "authorId": "34269118",
                "name": "Haoyu Wang"
              },
              {
                "authorId": "145666891",
                "name": "Michael Regan"
              },
              {
                "authorId": "145653969",
                "name": "Qi Zeng"
              },
              {
                "authorId": "1904906987",
                "name": "Qing Lyu"
              },
              {
                "authorId": "2110963190",
                "name": "Charles Yu"
              },
              {
                "authorId": "48870109",
                "name": "Carl N. Edwards"
              },
              {
                "authorId": "2149111828",
                "name": "Xiaomeng Jin"
              },
              {
                "authorId": "1381900594",
                "name": "Yizhu Jiao"
              },
              {
                "authorId": "51203051",
                "name": "Ghazaleh Kazeminejad"
              },
              {
                "authorId": "2052036545",
                "name": "Zhenhailong Wang"
              },
              {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
              },
              {
                "authorId": "1856025",
                "name": "Carl Vondrick"
              },
              {
                "authorId": "143977268",
                "name": "Mohit Bansal"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              },
              {
                "authorId": "2111759643",
                "name": "Jiawei Han"
              },
              {
                "authorId": "2122374530",
                "name": "Shih-Fu Chang"
              },
              {
                "authorId": "145755155",
                "name": "Martha Palmer"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250390907,
          "isinfluential": false,
          "contexts": [
            "In the single-turn QA strategy, we make use of the prompt proposed by previous work (Man et al., 2022; Gao et al., 2023) and incorporate the event structures of the two event mentions into the prompt."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Causality Identification via Generation of Important Context Words",
            "abstract": "An important problem of Information Extraction involves Event Causality Identification (ECI) that seeks to identify causal relation between pairs of event mentions. Prior models for ECI have mainly solved the problem using the classification framework that does not explore prediction/generation of important context words from input sentences for causal recognition. In this work, we consider the words along the dependency path between the two event mentions in the dependency tree as the important context words for ECI. We introduce dependency path generation as a complementary task for ECI, which can be solved jointly with causal label prediction to improve the performance. To facilitate the multi-task learning, we cast ECI into a generation problem that aims to generate both causal relation and dependency path words from input sentence. In addition, we propose to use the REINFORCE algorithm to train our generative model where novel reward functions are designed to capture both causal prediction accuracy and generation quality. The experiments on two benchmark datasets demonstrate state-of-the-art performance of the proposed model for ECI.",
            "year": 2022,
            "venue": "STARSEM",
            "authors": [
              {
                "authorId": "2027979466",
                "name": "Hieu Man"
              },
              {
                "authorId": "2114751778",
                "name": "Minh Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258297899,
          "isinfluential": false,
          "contexts": [
            "An intuitive option is to utilize LLMs for this procedure; however, since they have been found to be insufficient for IE (Li et al., 2023; Peng et al., 2023a), we adopt PLM-based approaches to construct event structures, consisting of three steps: Event Detection , Event Argument Extraction , and…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
            "abstract": "The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2485552",
                "name": "Bo Li"
              },
              {
                "authorId": "2215216578",
                "name": "Gexiang Fang"
              },
              {
                "authorId": "2152917615",
                "name": "Yang Yang"
              },
              {
                "authorId": "117898431",
                "name": "Quansen Wang"
              },
              {
                "authorId": "145235149",
                "name": "Wei Ye"
              },
              {
                "authorId": "2118223372",
                "name": "Wen Zhao"
              },
              {
                "authorId": "1705434",
                "name": "Shikun Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259858871,
          "isinfluential": false,
          "contexts": [
            "In addition, QA-based methods have also been investigated in temporal relation extraction (Cohen and Bar, 2023) and ECI (Gao et al., 2023), or retrieving useful background knowledge to improve event causality recognition (Kruengkrai et al., 2017; Kad-owaki et al., 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Temporal Relation Classification using Boolean Question Answering",
            "abstract": "Classifying temporal relations between a pair of events is crucial to natural language understanding and a well-known natural language processing task. Given a document and two event mentions, the task is aimed at finding which one started first. We propose an efficient approach for temporal relation classification (TRC) using a boolean question answering (QA) model which we fine-tune on questions that we carefully design based on the TRC annotation guidelines, thereby mimicking the way human annotators approach the task. Our new QA-based TRC model outperforms previous state-of-the-art results by 2.4%.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2220400048",
                "name": "O. Cohen"
              },
              {
                "authorId": "50143541",
                "name": "Kfir Bar"
              }
            ]
          }
        },
        {
          "citedcorpusid": 260063107,
          "isinfluential": false,
          "contexts": [
            "Unlike previous work that relys heavily on semantic structures, we leverage cross-task knowledge to construct document-level event structures to enrich event information, motivated by the effectiveness of cross-task knowledge in IE (Lin et al., 2020; Jin et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Toward Consistent and Informative Event-Event Temporal Relation Extraction",
            "abstract": "Event-event temporal relation extraction aims to extract the temporal order between a pair of event mentions, which is usually used to construct temporal event graphs. However, event graphs generated by existing methods are usually globally inconsistent (event graphs containing cycles), semantically irrelevant (two unrelated events having temporal links), and context unaware (neglecting neighborhood information of an event node). In this paper, we propose a novel event-event temporal relation extraction method to address these limitations. Our model combines a pretrained language model and a graph neural network to output event embeddings, which captures the contextual information of event graphs. Moreover, to achieve global consistency and semantic relevance, (1) event temporal order should be in accordance with the norm of their embeddings, and (2) two events have temporal relation only if their embeddings are close enough. Experimental results on a real-world event dataset demonstrate that our method achieves state-of-the-art performance and generates high-quality event graphs.",
            "year": 2023,
            "venue": "MATCHING",
            "authors": [
              {
                "authorId": "2149111828",
                "name": "Xiaomeng Jin"
              },
              {
                "authorId": "4428136",
                "name": "Haoyang Wen"
              },
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 262825274,
          "isinfluential": false,
          "contexts": [
            "The event arguments and their single-hop relations are obtained from relevant IE models (Peng et al., 2023b; Li et al., 2021; Eberts and Ulges, 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding",
            "abstract": "Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an event understanding toolkit OmniEvent, which features three desiderata: (1) Comprehensive. OmniEvent supports mainstream modeling paradigms of all the event understanding tasks and the processing of 15 widely-used English and Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous evaluation pitfalls reported in Peng et al. (2023), which ensures fair comparisons between different models. (3) Easy-to-use. OmniEvent is designed to be easily used by users with varying needs. We provide off-the-shelf models that can be directly deployed as web services. The modular framework also enables users to easily implement and evaluate new event understanding models with OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publicly released along with the demonstration website and video (https://omnievent.xlore.cn/).",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "47837854",
                "name": "Hao Peng"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "2218965360",
                "name": "Feng Yao"
              },
              {
                "authorId": "2200687506",
                "name": "Zimu Wang"
              },
              {
                "authorId": "2107153643",
                "name": "C. Zhu"
              },
              {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268249143,
          "isinfluential": false,
          "contexts": [
            "We compared the performance of KnowQA against the following state-of-the-art baselines from existing ECRE research: (1) PLM (Tran Phu and Nguyen, 2021) 6) HOTECI (Man et al., 2024b) leverages optimal transport to select the most important words and sentences from full documents; (7) GIMC (He et al., 2024) constructs a heterogeneous graph interaction network to model long-distance dependencies between events.",
            "…(1) PLM (Tran Phu and Nguyen, 2021) 6) HOTECI (Man et al., 2024b) leverages optimal transport to select the most important words and sentences from full documents; (7) GIMC (He et al., 2024) constructs a heterogeneous graph interaction network to model long-distance dependencies between events."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning",
            "abstract": "Event Causality Identification (ECI) refers to the detection of causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource languages, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over a document. Then, to improve cross-lingual transferability of causal knowledge learned from the source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms the previous state-of-the-art model by 9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios respectively. Notably, in the multilingual scenario, our zero-shot framework even exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance.",
            "year": 2024,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2268906494",
                "name": "Zhitao He"
              },
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "2290024603",
                "name": "Zhiqiang Zhang"
              },
              {
                "authorId": "2273904059",
                "name": "Mengshu Sun"
              },
              {
                "authorId": "2269147239",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270199894,
          "isinfluential": false,
          "contexts": [
            "This phenomenon contributes to low precision and high recall of LLMs on this task, which severely hampers their performance in this field (Gao et al., 2023; Liu et al., 2024)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Identifying while Learning for Document Event Causality Identification",
            "abstract": "Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of identifying after learning paradigm, where events' representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignoring causal direction. In this paper, we take care of the causal direction and propose a new identifying while learning mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events' representations for boosting next round of causality identification. To this end, this paper designs an *iterative learning and identifying framework*: In each iteration, we construct an event causality graph, on which events' causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.",
            "year": 2024,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2304744193",
                "name": "Cheng Liu"
              },
              {
                "authorId": "2052727822",
                "name": "Wei Xiang"
              },
              {
                "authorId": "2156645305",
                "name": "Bang Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "To ensure the richness of classification, we adopt the KAIROS 2 ontology, a superset of ACE 2005 (Walker et al., 2006) that consists of 50 event types and 59 argument roles 3 , to classify the event mentions."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "272368002": {
      "citing_paper_info": {
        "title": "Enhancing Document-level Argument Extraction with Definition-augmented Heuristic-driven Prompting for LLMs",
        "abstract": "Event Argument Extraction (EAE) is pivotal for extracting structured information from unstructured text, yet it remains challenging due to the complexity of real-world document-level EAE. We propose a novel Definition-augmented Heuristic-driven Prompting (DHP) method to enhance the performance of Large Language Models (LLMs) in document-level EAE. Our method integrates argument extraction-related definitions and heuristic rules to guide the extraction process, reducing error propagation and improving task accuracy. We also employ the Chain-of-Thought (CoT) method to simulate human reasoning, breaking down complex problems into manageable sub-problems. Experiments have shown that our method achieves a certain improvement in performance over existing prompting methods and few-shot supervised learning on document-level EAE datasets. The DHP method enhances the generalization capability of LLMs and reduces reliance on large annotated datasets, offering a novel research perspective for document-level EAE.",
        "year": 2024,
        "venue": "Pacific Asia Conference on Language, Information and Computation",
        "authors": [
          {
            "authorId": "2319580599",
            "name": "Tongyue Sun"
          },
          {
            "authorId": "2319974963",
            "name": "Jiayi Xiao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 11,
        "influential_count": 3,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "220048375",
        "252683303",
        "14339673",
        "269613809",
        "218971783",
        "247519241",
        "249017743",
        "256389694",
        "254408772",
        "239998651",
        "52816033"
      ],
      "citation_details": [
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "The majority of previous studies posit that events are articulated solely within a single sentence, hence their primary focus has been on sentence-level information extraction (Chen et al., 2015; Liu et al., 2018; Zhou et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52816033,
          "isinfluential": false,
          "contexts": [
            "The majority of previous studies posit that events are articulated solely within a single sentence, hence their primary focus has been on sentence-level information extraction (Chen et al., 2015; Liu et al., 2018; Zhou et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
            "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2730687",
                "name": "Zhunchen Luo"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218971783,
          "isinfluential": true,
          "contexts": [
            "We propose new perspectives and methods, solving the example selection problem from the new perspective of Definition-Enhanced Prompt-ing Heuristic Method, promoting explicit heuristic learning in ICL.",
            "In contrast to the traditional reliance on vast corpora, the incorporation of In-Context Learning (ICL) within LLMs has emerged as a transformative approach (Brown et al., 2020; Zhou et al., 2022, 2023; Wang et al., 2024).",
            "Gonen et al. (2023) have noted that the performance of ICL is highly sensitive to the selection of examples.",
            "ICL adeptly diminishes the necessity for extensive datasets by leveraging a modest collection of examples, serving as illustrative prompts for both inputs and outputs.",
            "(Agrawal et al., 2022) have employed LLMs in clinical Event Argument Extraction (EAE) using standard prompts that do not involve any reasoning strategies, while research on prompting strategies specifically tailored for the EAE task is scarce, with only (Zhou et al., 2024) exploring the promising and challenging research direction of reducing the dependence on specific large-scale training datasets through ICL, thereby enhancing the generalization capability of LLMs in EAE tasks.",
            "This implies that well-designed prompts and heuristic rules can effectively enhance ICL performance without the need for fine-tuning on task-specific datasets.",
            "Utilizing inputs that include document content, task definitions, argument extraction rules, and identified event types and triggers, we constructed a definition-driven heuristic ICL.",
            "This capability is particularly useful in ICL, as LLMs are always faced with unseen samples and unseen classes (Zhou et al., 2024).",
            "The In-Context Learning (ICL) (Brown et al., 2020) methodology is designed to expedite the adaptability of language models across various tasks, necessitating minimal or no prior data (Wei et al., 2022; Kojima et al., 2022).",
            "In ICL, heuristics are used to select or design examples (demonstrations) that can guide the model to make correct predictions(Zhou et al., 2024).",
            "This includes the choice of prompt templates, the selection of context examples, and the order of examples (Zhao et al., 2021; Lu et al., 2022), as well as the selection of examples and the format of inference steps (Zhang et al., 2022b; Fu et al., 2022; Zhang et al., 2022a), which collectively impact the application of ICL on LLMs.",
            "The performance of ICL is highly sensitive to specific settings, necessitating the selection of appropriate contextual information and the optimization of the model’s training process."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220048375,
          "isinfluential": true,
          "contexts": [
            "Document-level EAE commonly relies on manual domain and pattern annotation for supervised learning models (Xiang and Wang, 2019; Lin et al., 2020; Li et al., 2022; Liu et al., 2022; Hsu et al., 2022; Liu et al., 2023).",
            "4 Related works 4.1 Document-level EAE Document-level EAE commonly relies on manual domain and pattern annotation for supervised learning models (Xiang and Wang, 2019; Lin et al., 2020; Li et al., 2022; Liu et al., 2022; Hsu et al., 2022; Liu et al., 2023)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Joint Neural Model for Information Extraction with Global Features",
            "abstract": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "143857288",
                "name": "Fei Huang"
              },
              {
                "authorId": "3008832",
                "name": "Lingfei Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 239998651,
          "isinfluential": false,
          "contexts": [
            "CoT has proven particularly adept at tackling complex reasoning challenges, encompassing arithmetic and commonsense reasoning (Cobbe et al., 2021; Wei et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Training Verifiers to Solve Math Word Problems",
            "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "6062736",
                "name": "K. Cobbe"
              },
              {
                "authorId": "13622184",
                "name": "Vineet Kosaraju"
              },
              {
                "authorId": "2275251620",
                "name": "Mo Bavarian"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "35450887",
                "name": "Heewoo Jun"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3407285",
                "name": "Matthias Plappert"
              },
              {
                "authorId": "2065005836",
                "name": "Jerry Tworek"
              },
              {
                "authorId": "2052366271",
                "name": "Jacob Hilton"
              },
              {
                "authorId": "7406311",
                "name": "Reiichiro Nakano"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "47971768",
                "name": "John Schulman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247519241,
          "isinfluential": false,
          "contexts": [
            "For instance, InstructGPT (Ouyang et al., 2022) and ChatGLM (Du et al., 2022) have excelled in diverse downstream applications such as dialogue systems and text summarization generation through metic-ulously crafted instructions."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
            "abstract": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "66395694",
                "name": "Zhengxiao Du"
              },
              {
                "authorId": "5606742",
                "name": "Yujie Qian"
              },
              {
                "authorId": "2111312892",
                "name": "Xiao Liu"
              },
              {
                "authorId": "145573466",
                "name": "Ming Ding"
              },
              {
                "authorId": "40125294",
                "name": "J. Qiu"
              },
              {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
              },
              {
                "authorId": "2109541439",
                "name": "Jie Tang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249017743,
          "isinfluential": false,
          "contexts": [
            "The In-Context Learning (ICL) (Brown et al., 2020) methodology is designed to expedite the adaptability of language models across various tasks, necessitating minimal or no prior data (Wei et al., 2022; Kojima et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Large Language Models are Zero-Shot Reasoners",
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "year": 2022,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2081836120",
                "name": "Takeshi Kojima"
              },
              {
                "authorId": "2046135",
                "name": "S. Gu"
              },
              {
                "authorId": "1557386977",
                "name": "Machel Reid"
              },
              {
                "authorId": "2153732825",
                "name": "Yutaka Matsuo"
              },
              {
                "authorId": "1715282",
                "name": "Yusuke Iwasawa"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252683303,
          "isinfluential": false,
          "contexts": [
            "…templates, the selection of context examples, and the order of examples (Zhao et al., 2021; Lu et al., 2022), as well as the selection of examples and the format of inference steps (Zhang et al., 2022b; Fu et al., 2022; Zhang et al., 2022a), which collectively impact the application of ICL on LLMs."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Complexity-Based Prompting for Multi-Step Reasoning",
            "abstract": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
            "year": 2022,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "46956602",
                "name": "Yao Fu"
              },
              {
                "authorId": "2293471",
                "name": "Hao-Chun Peng"
              },
              {
                "authorId": "48229640",
                "name": "Ashish Sabharwal"
              },
              {
                "authorId": "48323507",
                "name": "Peter Clark"
              },
              {
                "authorId": "2236429",
                "name": "Tushar Khot"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254408772,
          "isinfluential": false,
          "contexts": [
            "Gonen et al. (2023) have noted that the performance of ICL is highly sensitive to the selection of examples."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
            "abstract": "Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1821892",
                "name": "Hila Gonen"
              },
              {
                "authorId": "1900163",
                "name": "Srini Iyer"
              },
              {
                "authorId": "3443287",
                "name": "Terra Blevins"
              },
              {
                "authorId": "144365875",
                "name": "Noah A. Smith"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 256389694,
          "isinfluential": false,
          "contexts": [
            "…of LLMs on data from different domains, which is crucial in real-world applications where large amounts of annotated data may be difficult to obtain (Tong et al., 2022; Luo et al., 2023), we tested the model performance under the Cross domain-settings of the DocEE dataset, as shown in Table 2."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Closer Look at Few-shot Classification Again",
            "abstract": "Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions. Code and pre-trained models (in PyTorch) are available at https://github.com/Frankluox/CloserLookAgainFewShot.",
            "year": 2023,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2115611493",
                "name": "Xu Luo"
              },
              {
                "authorId": "1664776313",
                "name": "Hao Wu"
              },
              {
                "authorId": "2116921931",
                "name": "Ji Zhang"
              },
              {
                "authorId": "2671321",
                "name": "Lianli Gao"
              },
              {
                "authorId": "2212230799",
                "name": "Jing Xu"
              },
              {
                "authorId": "2346105",
                "name": "Jingkuan Song"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269613809,
          "isinfluential": false,
          "contexts": [
            "It is noteworthy that due to the relatively high cost of Deepseek-v2-chat, its evaluation was limited to a subset of the dataset.",
            "The experiments were conducted using two large language models: the publicly available Deepseek-v2-chat (Liu et al., 2024) and Llama3."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
            "year": 2024,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "144485528",
                "name": "Zhihong Shao"
              },
              {
                "authorId": "10780897",
                "name": "Damai Dai"
              },
              {
                "authorId": "2278834796",
                "name": "Daya Guo"
              },
              {
                "authorId": "2156640188",
                "name": "Bo Liu (Benjamin Liu)"
              },
              {
                "authorId": "2243360876",
                "name": "Zihan Wang"
              },
              {
                "authorId": "2238628841",
                "name": "Huajian Xin"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "When applied to tasks that do not inherently require reasoning, the CoT method risks simplifying the multi-step reasoning process into a potentially in-adequate single-step, thereby undermining its full potential (Shum et al., 2023; Zhou et al., 2024).",
            "For the Heuristics-driven CoT part, we mainly follow the settings and definitions proposed by Zhou et al. (2024) and Wei et al. (2022) to guide the model along a specific logical path, thereby improving the accuracy of event argument extraction.",
            "Here, we present the replication of results based on the CoT prompting method by Zhou et al. (2024), which represents one of the few excellent prompting strategies specifically tailored for the Event Argument Extraction task in LLMs.",
            "Moreover, cognitive research has found that compared to complex methods, humans use heuristics as an effective cognitive pathway to achieve more accurate reasoning (Gigerenzer and Gaissmaier, 2011; Hogarth and Karelaia, 2007; Zhou et al., 2024).",
            "As similar results presented in the studies by Wei et al. (2022) and Zhou et al. (2024), paralleling this human cognitive strategy, we enable LLMs to learn from explicit heuristics to enhance reasoning.",
            "Within the domain of few-shot learning, our comparative analysis is grounded on the performance data from a limited number of samples as previously reported by Liu et al. (2023) and Zhou et al. (2024).",
            "In ICL, heuristics are used to select or design examples (demonstrations) that can guide the model to make correct predictions(Zhou et al., 2024).",
            "For the assessment, we follow the metrics outlined in (Ma et al., 2022; Zhou et al., 2024), which are the F1 score for argument identification (Arg-I) and the F1 score for argument classification (Arg-C).",
            "Humans use heuristics as an effective cognitive pathway, which often leads to more accurate reasoning than complex methods (Gigerenzer and Gaissmaier, 2011; Hogarth and Karelaia, 2007; Zhou et al., 2024).",
            "Furthermore, recent studies (Lin et al., 2023; Zhou et al., 2024) have expanded the application of LLMs in complex tasks like event extraction by ingeniously constructing prompts, highlighting the broad prospects of LLMs in the EAE domain."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "248780414": {
      "citing_paper_info": {
        "title": "Automatic Error Analysis for Document-level Information Extraction",
        "abstract": "Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.",
        "year": 2022,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2148357499",
            "name": "Aliva Das"
          },
          {
            "authorId": "13728923",
            "name": "Xinya Du"
          },
          {
            "authorId": "2160611160",
            "name": "Barry Wang"
          },
          {
            "authorId": "46749263",
            "name": "Kejian Shi"
          },
          {
            "authorId": "2216587688",
            "name": "J. Gu"
          },
          {
            "authorId": "2052386937",
            "name": "Thomas J. Porter"
          },
          {
            "authorId": "1748501",
            "name": "Claire Cardie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 19,
        "unique_cited_count": 15,
        "influential_count": 2,
        "detailed_records_count": 19
      },
      "cited_papers": [
        "218551201",
        "216562330",
        "203701085",
        "202712654",
        "247797575",
        "6341459",
        "16925224",
        "198118773",
        "14672517",
        "235097664",
        "236460259",
        "220048375",
        "216869183",
        "196199409",
        "35264612"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6341459,
          "isinfluential": false,
          "contexts": [
            "We converted the optional templates to required templates and removed the subtypes of the incidents as done in previous work (Chambers, 2013; Du et al., 2021b) We use the tuning data as training data and reserve 10% of the test data, i.e. 12 examples, to create a development/validation set.",
            "We converted the optional templates to required templates and removed the subtypes of the incidents as done in previous work (Chambers, 2013; Du et al., 2021b) so that the dataset is transformed into standardized templates."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Schema Induction with a Probabilistic Entity-Driven Model",
            "abstract": "Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).",
            "year": 2013,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1729918",
                "name": "Nathanael Chambers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14672517,
          "isinfluential": false,
          "contexts": [
            "…(Vilar et al., 2006; Zhou et al., 2008; Farrús et al., 2010; Kholy and Habash, 2011; Ze-man et al., 2011; Popovi´c and Ney, 2011), coreference resolution (Uryupina, 2008; Kummerfeld and Klein, 2013; Martschat and Strube, 2014; Martschat et al., 2015) and parsing (Kummerfeld et al., 2012)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Error Analysis for Learning-based Coreference Resolution",
            "abstract": "",
            "year": 2008,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "143835479",
                "name": "Olga Uryupina"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16925224,
          "isinfluential": false,
          "contexts": [
            "…(Vilar et al., 2006; Zhou et al., 2008; Farrús et al., 2010; Kholy and Habash, 2011; Zeman et al., 2011; Popović and Ney, 2011), coreference resolution (Uryupina, 2008; Kummerfeld and Klein, 2013; Martschat and Strube, 2014; Martschat et al., 2015) and parsing (Kummerfeld et al., 2012).",
            ", 2011; Popović and Ney, 2011), coreference resolution (Uryupina, 2008; Kummerfeld and Klein, 2013; Martschat and Strube, 2014; Martschat et al., 2015) and parsing (Kummerfeld et al."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Recall Error Analysis for Coreference Resolution",
            "abstract": "We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2417706",
                "name": "Sebastian Martschat"
              },
              {
                "authorId": "31380436",
                "name": "M. Strube"
              }
            ]
          }
        },
        {
          "citedcorpusid": 35264612,
          "isinfluential": false,
          "contexts": [
            "Valls-Vargas et al. (2017) proposed a framework for studying how different errors propagate through an IE system; however, the framework can only be used for pipelined systems, not end-to-end ones."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Error Analysis in an Automated Narrative Information Extraction Pipeline",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
            "authors": [
              {
                "authorId": "2551007",
                "name": "Josep Valls-Vargas"
              },
              {
                "authorId": "35187206",
                "name": "Jichen Zhu"
              },
              {
                "authorId": "1722671",
                "name": "Santiago Ontañón"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196199409,
          "isinfluential": false,
          "contexts": [
            "testing like Errudite (Wu et al., 2019), CHECKLIST (Ribeiro et al.",
            "Recently, generalized automated error analysis frameworks involving human-in-the-loop testing like Errudite (Wu et al., 2019), CHECK-\nLIST (Ribeiro et al., 2020), CrossCheck (Arendt et al., 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al., 2020).",
            "Recently, generalized automated error analysis frameworks involving human-in-the-loop testing like Errudite (Wu et al., 2019), CHECK-\nLIST (Ribeiro et al., 2020), CrossCheck (Arendt et al., 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine…"
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Errudite: Scalable, Reproducible, and Testable Error Analysis",
            "abstract": "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35232494",
                "name": "Tongshuang Sherry Wu"
              },
              {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
              },
              {
                "authorId": "1803140",
                "name": "Jeffrey Heer"
              },
              {
                "authorId": "1780531",
                "name": "Daniel S. Weld"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198118773,
          "isinfluential": false,
          "contexts": [
            "To date, for example, there has been no attempt to directly compare the error landscape and distribution of\n2See, for example, Zhang et al. (2019), Du and Cardie (2020) and Lin et al. (2020) for within-sentence event extraction; Akbik et al. (2018), and Akbik et al. (2019) for named entity…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Extracting Entities and Events as a Single Task Using a Transition-Based Neural Model",
            "abstract": "The task of event extraction contains subtasks including detections for entity mentions, event triggers and argument roles. Traditional methods solve them as a pipeline, which does not make use of task correlation for their mutual benefits. There have been recent efforts towards building a joint model for all tasks. However, due to technical challenges, there has not been work predicting the joint output structure as a single task. We build a first model to this end using a neural transition-based framework, incrementally predicting complex joint structures in a state-transition process. Results on standard benchmarks show the benefits of the joint model, which gives the best result in the literature.",
            "year": 2019,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "47538865",
                "name": "Junchi Zhang"
              },
              {
                "authorId": "2246101",
                "name": "Yanxia Qin"
              },
              {
                "authorId": "1591125925",
                "name": "Yue Zhang"
              },
              {
                "authorId": "2898327",
                "name": "Mengchi Liu"
              },
              {
                "authorId": "145628086",
                "name": "Donghong Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202712654,
          "isinfluential": false,
          "contexts": [
            ", 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al.",
            "L IST (Ribeiro et al., 2020), CrossCheck (Arendt et al., 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models",
            "abstract": "Neural NLP models are increasingly accurate but are imperfect and opaque—they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit’s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp.org/interpret.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145217343",
                "name": "Eric Wallace"
              },
              {
                "authorId": "1388109456",
                "name": "Jens Tuyls"
              },
              {
                "authorId": "49606614",
                "name": "Junlin Wang"
              },
              {
                "authorId": "17097887",
                "name": "Sanjay Subramanian"
              },
              {
                "authorId": "40642935",
                "name": "Matt Gardner"
              },
              {
                "authorId": "34650964",
                "name": "Sameer Singh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203701085,
          "isinfluential": false,
          "contexts": [
            "Although information extraction (IE) research has almost uniformly focused on sentence-level relation and event extraction (Grishman, 2019), the earliest research in the area formulated the task at the document level."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Twenty-five years of information extraction",
            "abstract": "Abstract Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.",
            "year": 2019,
            "venue": "Natural Language Engineering",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "To date, for example, there has been no attempt to directly compare the error landscape and distribution of\n2See, for example, Zhang et al. (2019), Du and Cardie (2020) and Lin et al. (2020) for within-sentence event extraction; Akbik et al. (2018), and Akbik et al. (2019) for named entity…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216869183,
          "isinfluential": false,
          "contexts": [
            ", 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al., 2020).",
            "L IST (Ribeiro et al., 2020), CrossCheck (Arendt et al., 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task",
            "abstract": "TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3413117",
                "name": "Christoph Alt"
              },
              {
                "authorId": "3449621",
                "name": "Aleksandra Gabryszak"
              },
              {
                "authorId": "36943315",
                "name": "Leonhard Hennig"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218551201,
          "isinfluential": false,
          "contexts": [
            "L IST (Ribeiro et al., 2020), CrossCheck (Arendt et al., 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
            "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
              },
              {
                "authorId": "35232494",
                "name": "Tongshuang Sherry Wu"
              },
              {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
              },
              {
                "authorId": "34650964",
                "name": "Sameer Singh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220048375,
          "isinfluential": false,
          "contexts": [
            "…has been no attempt to directly compare the error landscape and distribution of\n2See, for example, Zhang et al. (2019), Du and Cardie (2020) and Lin et al. (2020) for within-sentence event extraction; Akbik et al. (2018), and Akbik et al. (2019) for named entity recognition (NER); and Zhang et…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Joint Neural Model for Information Extraction with Global Features",
            "abstract": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "143857288",
                "name": "Fei Huang"
              },
              {
                "authorId": "3008832",
                "name": "Lingfei Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235097664,
          "isinfluential": true,
          "contexts": [
            "In fact, the problem of document-level information extraction has only recently begun to be revisited (Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of end-to-end neural network techniques that have…",
            "All other hyperparameters are set as in Du et al. (2021b).",
            "Next, we employ the error analysis framework in a comparison of two state-of-the-art documentlevel neural template-filling approaches, DyGIE++ (Wadden et al., 2019) and GTT (Du et al., 2021b), across three template-filling datasets (SciREX, ProMED (Patwardhan and Riloff, 2009)3, and MUC-4).",
            "We converted the optional templates to required templates and removed the subtypes of the incidents as done in previous work (Chambers, 2013; Du et al., 2021b) so that the dataset is transformed into standardized templates."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Template Filling with Generative Transformers",
            "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": false,
          "contexts": [
            "Thus, we would also like to improve the time complexity of our template (and mention) matching algorithms using an approach like bipartite matching (Yang et al., 2021).",
            "…extraction has only recently begun to be revisited (Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of end-to-end neural network techniques that have been so successful on their…",
            "(Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of endto-end neural network techniques that have been so successful on their sentence-level counterparts."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247797575,
          "isinfluential": false,
          "contexts": [
            "…information extraction has only recently begun to be revis-ited (Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of end-to-end neural network techniques that have been so successful on their…",
            "(Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of endto-end neural network techniques that have been so successful on their sentence-level counterparts."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards More Intelligent Extraction of Information from Documents",
            "abstract": "In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.",
            "year": 2021,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": 247797575,
          "isinfluential": false,
          "contexts": [
            "…information extraction has only recently begun to be revis-ited (Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of end-to-end neural network techniques that have been so successful on their…",
            "(Quirk and Poon, 2017; Jain et al., 2020; Du et al., 2021b,a; Li et al., 2021; Du, 2021; Yang et al., 2021) in part in an attempt to test the power of endto-end neural network techniques that have been so successful on their sentence-level counterparts."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards More Intelligent Extraction of Information from Documents",
            "abstract": "In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.",
            "year": 2021,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "As in Jurafsky and Martin (2021), we will refer to document-level information extraction tasks as template-ﬁlling tasks and use the term going forward to refer to both event extraction and document-level relation extraction tasks."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In this work, we first introduce a framework for automating error analysis for document-level event and relation extraction, casting both as instances of a general role-filling, or template-filling task (Jurafsky and Martin, 2021).",
            "As in Jurafsky and Martin (2021), we will refer to document-level information extraction tasks as template-ﬁlling tasks and use the term going forward to refer to both event extraction and document-level relation extraction tasks."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "We ﬁrst discuss the results of DyGIE++ and GTT on SciREX, ProMED, and MUC-4; and then examine the performance of these newer neural models on the 1992 MUC-4 dataset vs. a few of the best-performing IE systems at the time.",
            "MUC-4 (MUC-4, 1992) consists of newswire describing terrorist incidents in Latin America provided by the FBIS (Federal Broadcast Information Services).",
            "Table 2 shows the results of evaluating DyGIE++ and GTT on the SciREX, ProMED, and MUC-4 datasets.",
            "In our experiments, we train and test two neural-based IE models, described brieﬂy below, on the MUC-4, ProMED, and SciREX datasets.",
            "Consider, for example, the ﬁrst large-scale (for the time) evaluations of IE systems — e.g. MUC-3 (1991) and MUC-4 (1992).",
            "From the error count results in Figure 4, we see that GTT makes fewer Missing Template errors than DyGIE++ on the MUC-4 dataset (86 vs. 97).",
            "Next, we employ the error analysis framework in a comparison of two state-of-the-art document-level neural template-ﬁlling approaches, DyGIE++ (Wadden et al., 2019) and GTT (Du et al., 2021b), across three template-ﬁlling datasets (SciREX, ProMED (Patwardhan and Riloff, 2009) 3 , and MUC-4).",
            "We can see that all models perform substantially worse on sci-entiﬁc texts (ProMED, SciREX) as compared to news (MUC-4) , likely because the model base is pretrained for general-purpose NLP applications (BERT) or there are not enough examples of scientiﬁc-style text in the pretraining corpus (SciB-ERT).",
            "For the MUC-4 and SciREX datasets, GTT is run for 20 epochs, while for ProMED it is run for 36 epochs, to adjust for the smaller size of the dataset.",
            "Table 3 presents the precision, recall, and F1 performance on the MUC-4 dataset for early models from 1992 alongside those of the more recent DyGIE++ and GTT models.",
            "Finally, in an attempt to gauge progress in the information extraction ﬁeld over the past 30 years, we employ the framework to compare the performance of four of the original MUC-4 systems with the two newer deep-learning approaches to document-level IE.",
            "Aside from the original MUC-4 evaluation scoring reports (Chinchor, 1991), which included counts of missing and spurious role ﬁller errors, there have been very few attempts at understanding the types of errors made by IE systems and grounding those errors linguistically."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "275587538": {
      "citing_paper_info": {
        "title": "SHR: Enhancing Event Argument Extraction Ability of Large language Models with Simple-Hard Refining",
        "abstract": "Event Argument Extraction (EAE) aims to identify and extract key information such as entities, times, and locations related to specific events from text and serves as a fundamental task for many NLP applications. Recent researches have utilized large language models (LLMs) for EAE, effectively addressing the resource-intensive nature of annotating training datasets for this task. However, when performing EAE on longer texts (document-level EAE), the presence of descriptions unrelated to the events within document-level EAE can lead LLMs to identify incorrect arguments. To address this issue, we propose Simple-Hard Refining: a novel prompt framework that segments EAE into straightforward and complex extraction tasks. Based on the complexity of inference, we divide EAE task into simple-argument extraction and hard-argument extraction. By utilizing a chain of prompt to perform simple and hard argument extraction sequentially, noise introduced by irrelevant description for simple-argument extraction can be effectively alleviated. Furthermore, we explore the potential of LLMs to furnish dependable explanations for their extraction outcomes. We design an explanation-based prompting method that involves a three-step explanation process: relevant sentence extraction, argument role semantic analysis, and argument role entity localization. This method further enhances the extraction accuracy at each stage of the framework. Our experiments demonstrate that our method achieves state-of-the-art performance, surpassing various baselines that utilize LLMs for the EAE task. Ablation studies further verify the effectiveness of each stage of our framework and show the ability of our proposed approach to effectively mitigate noise. Our work contributes to the structured extraction of event argument information using LLMs.",
        "year": 2024,
        "venue": "BigData Congress [Services Society]",
        "authors": [
          {
            "authorId": "2340595493",
            "name": "Jinghan Wu"
          },
          {
            "authorId": "2165165357",
            "name": "Chunhong Zhang"
          },
          {
            "authorId": "2335621104",
            "name": "Zheng Hu"
          },
          {
            "authorId": "2331147529",
            "name": "Jibin Yu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 15,
        "influential_count": 4,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "233219850",
        "252762275",
        "257205763",
        "9776219",
        "218971783",
        "202539496",
        "246426909",
        "248496614",
        "246411621",
        "207853145",
        "259000027",
        "269762049",
        "247595263",
        "269900068",
        "231632658"
      ],
      "citation_details": [
        {
          "citedcorpusid": 9776219,
          "isinfluential": false,
          "contexts": [
            "Additionally, we validated the effectiveness of argument explanation based prompting on sentence-level EAE datasets using the ACE05 [25].",
            "We conduct experiments using the ACE05 dataset [25] to demonstrate that our argument explanation-based prompting method can enhance the argument extraction capabilities of large language models."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation",
            "abstract": "",
            "year": 2004,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2862682",
                "name": "G. Doddington"
              },
              {
                "authorId": "2449760",
                "name": "A. Mitchell"
              },
              {
                "authorId": "2282719",
                "name": "Mark A. Przybocki"
              },
              {
                "authorId": "1744313",
                "name": "L. Ramshaw"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              },
              {
                "authorId": "1732071",
                "name": "R. Weischedel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "The preprocessing of ACE05 follows the methodology outlined by [18]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": true,
          "contexts": [
            "A traditional approach of EAE is to employ classification techniques to identify potential text spans and assign roles to them [1]–[3].",
            "When performing document-level EAE [1], descriptions unrelated to the target arguments constitute noise in EAE.",
            "For the evaluation of the document-level EAE task, we adopt the RAMS dataset [1] .",
            "Traditional methods [1], [6]–[8] primarily rely on supervised learning, which requires a large annotated training dataset."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218971783,
          "isinfluential": false,
          "contexts": [
            "5-turbo-instruct [16], [17].",
            "Due to its complexity and the limited availability of annotated data, prompt engineering of large language models (LLMs) [16], [17], which have become increasingly powerful, serve as an effective alternative [15].",
            "5-turbo [16], [17] is selected as the base model."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231632658,
          "isinfluential": true,
          "contexts": [
            "The results of ICL method [20] are significantly lower than those of other methods, indicating that carefully crafted instructions and explanations are more crucial compared to a rich sample repository and similar examples.",
            "One common approach [20] is to select ICL examples involves choosing the nearest neighbors of input instances based on similarity metrics.",
            "Following [20], when constructing ICL examples, the samples with the highest textual similarity are selected from datas within the training set that share the same event type with the input data.",
            "…Simple-Hard Refining method with several state-of-the-art base-line prompting methods: Standard Prompting used in [13], Chain-of-Thought prompting [9], In-Context Learning method KATE [20], the self-consistency method [12], and the HD-LoA prompting [15] designed specifically for document-level EAE.",
            "Through ICL [20], [23], argument explanation based prompting enables the large language model to give explanations for the extraction process of each argument as the provided one-shot example do.",
            "Table I shows that our method outperforms the Standard [13], CoT [9], ICL method [20], self-consistency method [12] and HD-LoA [15] baseline models at every stage, except for the argument-explanation-based-prompting performance when using gpt-3.",
            "Recently, many approaches [20]–[22] have focused on enhancing the capabilities of large models by selecting appropriate examples."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "What Makes Good In-Context Examples for GPT-3?",
            "abstract": "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3’s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3’s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).",
            "year": 2021,
            "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",
            "authors": [
              {
                "authorId": "8807538",
                "name": "Jiachang Liu"
              },
              {
                "authorId": "19178763",
                "name": "Dinghan Shen"
              },
              {
                "authorId": "48378494",
                "name": "Yizhe Zhang"
              },
              {
                "authorId": "66648221",
                "name": "Bill Dolan"
              },
              {
                "authorId": "145006560",
                "name": "L. Carin"
              },
              {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "Recent developments have seen a trend towards redefining EAE as a question-answering task [4], [5], [26] or text generation task [8], [30].",
            "Traditional methods [1], [6]–[8] primarily rely on supervised learning, which requires a large annotated training dataset."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246411621,
          "isinfluential": true,
          "contexts": [
            "Chain of thought prompting [9] comprises a sequence of intermediate reasoning steps that substantially enhances the capacity of large language models to tackle complex reasoning tasks.",
            "When applying existing EAE prompt engineering methods [9], [13], [15] to LLMs, LLMs need to determine whether multi-sentence reasoning is required, which leads LLMs to analyze sentences unrelated to argu-ment extraction, thereby introducing additional noise into the extraction process.",
            "…Simple-Hard Refining method with several state-of-the-art base-line prompting methods: Standard Prompting used in [13], Chain-of-Thought prompting [9], In-Context Learning method KATE [20], the self-consistency method [12], and the HD-LoA prompting [15] designed specifically for document-level…",
            "Table I shows that our method outperforms the Standard [13], CoT [9], ICL method [20], self-consistency method [12] and HD-LoA [15] baseline models at every stage, except for the argument-explanation-based-prompting performance when using gpt-3.",
            "Our method’s performance in the first stage already surpasses that of CoT [9] and HD-LoA [15], indicating that argument explanation-based prompting is more effective for the EAE task compared to these approaches.",
            "It has been widely demonstrated that enhancing the reasoning ability of large language models by generating detailed thought processes improves their performance [9].",
            "Chain-of-thought prompting (CoT) [9] and its subsequent work [10], [11] has improved the capability of large language models to address complex problems by producing step-by-step solutions."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "119640649",
                "name": "Jason Wei"
              },
              {
                "authorId": "1524732527",
                "name": "Xuezhi Wang"
              },
              {
                "authorId": "1714772",
                "name": "Dale Schuurmans"
              },
              {
                "authorId": "40377863",
                "name": "Maarten Bosma"
              },
              {
                "authorId": "2226805",
                "name": "Ed H. Chi"
              },
              {
                "authorId": "144956443",
                "name": "F. Xia"
              },
              {
                "authorId": "1998340269",
                "name": "Quoc Le"
              },
              {
                "authorId": "65855107",
                "name": "Denny Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246426909,
          "isinfluential": false,
          "contexts": [
            "5-turbo-instruct [16], [17].",
            "Due to its complexity and the limited availability of annotated data, prompt engineering of large language models (LLMs) [16], [17], which have become increasingly powerful, serve as an effective alternative [15].",
            "5-turbo [16], [17] is selected as the base model."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Training language models to follow instructions with human feedback",
            "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "year": 2022,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31793034",
                "name": "Long Ouyang"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2115903168",
                "name": "Xu Jiang"
              },
              {
                "authorId": "2061137049",
                "name": "Diogo Almeida"
              },
              {
                "authorId": "2064084601",
                "name": "Carroll L. Wainwright"
              },
              {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
              },
              {
                "authorId": null,
                "name": "Chong Zhang"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "2117680841",
                "name": "Katarina Slama"
              },
              {
                "authorId": "2064770039",
                "name": "Alex Ray"
              },
              {
                "authorId": "47971768",
                "name": "John Schulman"
              },
              {
                "authorId": "2052366271",
                "name": "Jacob Hilton"
              },
              {
                "authorId": "2151735262",
                "name": "Fraser Kelton"
              },
              {
                "authorId": "2142365973",
                "name": "Luke E. Miller"
              },
              {
                "authorId": "2151735251",
                "name": "Maddie Simens"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "2930640",
                "name": "Peter Welinder"
              },
              {
                "authorId": "145791315",
                "name": "P. Christiano"
              },
              {
                "authorId": "2990741",
                "name": "Jan Leike"
              },
              {
                "authorId": "49407415",
                "name": "Ryan J. Lowe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247595263,
          "isinfluential": true,
          "contexts": [
            "For each argument, soft-hard refining framework requires at most three rounds of requests, yet both Arg-I and Arg-C outperform the self-consistency method [12], which requires five rounds.",
            "…Simple-Hard Refining method with several state-of-the-art base-line prompting methods: Standard Prompting used in [13], Chain-of-Thought prompting [9], In-Context Learning method KATE [20], the self-consistency method [12], and the HD-LoA prompting [15] designed specifically for document-level EAE.",
            "The self-consistency strategy [12] leverages the intuition that a complex reasoning problem can often be solved correctly through various different paths.",
            "Table I shows that our method outperforms the Standard [13], CoT [9], ICL method [20], self-consistency method [12] and HD-LoA [15] baseline models at every stage, except for the argument-explanation-based-prompting performance when using gpt-3.",
            "5-turbo as the base model, where it is marginally less effective compared to the Standard method [13] and self-consistency method [12].",
            "Self-consistency [12] utilizes the idea that a complex problem generally has multiple distinct approaches that converge on the same correct answer and employs majority voting to finalize the answer selection.",
            "QA-without-explanation-sc [12] represents the results obtained by conducting multiple rounds of experiments on QA-without-explanation and utilizing majority voting on the extraction results."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1524732527",
                "name": "Xuezhi Wang"
              },
              {
                "authorId": "119640649",
                "name": "Jason Wei"
              },
              {
                "authorId": "50319359",
                "name": "D. Schuurmans"
              },
              {
                "authorId": "1998340269",
                "name": "Quoc Le"
              },
              {
                "authorId": "2226805",
                "name": "Ed H. Chi"
              },
              {
                "authorId": "65855107",
                "name": "Denny Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": false,
          "contexts": [
            "A traditional approach of EAE is to employ classification techniques to identify potential text spans and assign roles to them [1]–[3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252762275,
          "isinfluential": false,
          "contexts": [
            "Chain-of-thought prompting (CoT) [9] and its subsequent work [10], [11] has improved the capability of large language models to address complex problems by producing step-by-step solutions."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatic Chain of Thought Prompting in Large Language Models",
            "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",
            "year": 2022,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "3322871",
                "name": "Zhuosheng Zhang"
              },
              {
                "authorId": "2085709",
                "name": "Aston Zhang"
              },
              {
                "authorId": "1701799",
                "name": "Mu Li"
              },
              {
                "authorId": "78088877",
                "name": "Alexander J. Smola"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257205763,
          "isinfluential": false,
          "contexts": [
            "Chain-of-thought prompting (CoT) [9] and its subsequent work [10], [11] has improved the capability of large language models to address complex problems by producing step-by-step solutions."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
            "abstract": "Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%). The code is available at https://github.com/SHUMKASHUN/Automate-CoT.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2121340452",
                "name": "Kashun Shum"
              },
              {
                "authorId": "50826757",
                "name": "Shizhe Diao"
              },
              {
                "authorId": "2146324423",
                "name": "Tong Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259000027,
          "isinfluential": false,
          "contexts": [
            "Following previous approaches [6], [7], we utilized the argument identification F1 score (Arg-I) and the argument classification F1 score (Arg-C) as our evaluation metrics.",
            "There are also prompt-based methodologies [6], [7] that harness slotted prompts to extract arguments in a generative slot-filling paradigm."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Revisiting Event Argument Extraction: Can EAE Models Learn Better When Being Aware of Event Co-occurrences?",
            "abstract": "Event co-occurrences have been proved effective for event extraction (EE) in previous studies, but have not been considered for event argument extraction (EAE) recently. In this paper, we try to fill this gap between EE research and EAE research, by highlighting the question that “Can EAE models learn better when being aware of event co-occurrences?”. To answer this question, we reformulate EAE as a problem of table generation and extend a SOTA prompt-based EAE model into a non-autoregressive generation framework, called TabEAE, which is able to extract the arguments of multiple events in parallel. Under this framework, we experiment with 3 different training-inference schemes on 4 datasets (ACE05, RAMS, WikiEvents and MLEE) and discover that via training the model to extract all events in parallel, it can better distinguish the semantic boundary of each event and its ability to extract single event gets substantially improved. Experimental results show that our method achieves new state-of-the-art performance on the 4 datasets. Our code is avilable at https://github.com/Stardust-hyx/TabEAE.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2163738241",
                "name": "Yuxin He"
              },
              {
                "authorId": "2146556479",
                "name": "Jing-Hao Hu"
              },
              {
                "authorId": "2987453",
                "name": "Buzhou Tang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269762049,
          "isinfluential": false,
          "contexts": [
            "Data augmentation techniques [27], [28] have also been widely applied to enhance EAE performance."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Targeted Augmentation for Low-Resource Event Extraction",
            "abstract": "Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples. Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance). This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence. Extensive experimental results demonstrate the effectiveness of the proposed paradigm. Furthermore, identified limitations are discussed, shedding light on areas for future improvement.",
            "year": 2024,
            "venue": "NAACL-HLT",
            "authors": [
              {
                "authorId": "2238971842",
                "name": "Sijia Wang"
              },
              {
                "authorId": "2301269508",
                "name": "Lifu Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269900068,
          "isinfluential": false,
          "contexts": [
            "Recent developments have seen a trend towards redefining EAE as a question-answering task [4], [5], [26] or text generation task [8], [30]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards Better Question Generation in QA-Based Event Extraction",
            "abstract": "Event Extraction (EE) is an essential information extraction task that aims to extract event-related information from unstructured texts. The paradigm of this task has shifted from conventional classification-based methods to more contemporary question-answering-based (QA-based) approaches. However, in QA-based EE, the quality of the questions dramatically affects the extraction accuracy, and how to generate high-quality questions for QA-based EE remains a challenge. In this work, to tackle this challenge, we suggest four criteria to evaluate the quality of a question and propose a reinforcement learning method, RLQG, for QA-based EE that can generate generalizable, high-quality, and context-dependent questions and provides clear guidance to QA models. The extensive experiments conducted on ACE and RAMS datasets have strongly validated our approach's effectiveness, which also demonstrates its robustness in scenarios with limited training data. The corresponding code of RLQG is released for further research.",
            "year": 2024,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2302353772",
                "name": "Zijin Hong"
              },
              {
                "authorId": "2302188059",
                "name": "Jian Liu"
              }
            ]
          }
        }
      ]
    },
    "227923674": {
      "citing_paper_info": {
        "title": "Reconstructing Event Regions for Event Extraction via Graph Attention Networks",
        "abstract": "Event information is usually scattered across multiple sentences within a document. The local sentence-level event extractors often yield many noisy event role filler extractions in the absence of a broader view of the document-level context. Filtering spurious extractions and aggregating event information in a document remains a challenging problem. Following the observation that a document has several relevant event regions densely populated with event role fillers, we build graphs with candidate role filler extractions enriched by sentential embeddings as nodes, and use graph attention networks to identify event regions in a document and aggregate event information. We characterize edges between candidate extractions in a graph into rich vector representations to facilitate event region identification. The experimental results on two datasets of two languages show that our approach yields new state-of-the-art performance for the challenging event extraction task.",
        "year": 2020,
        "venue": "AACL",
        "authors": [
          {
            "authorId": "2901524",
            "name": "Pei Chen"
          },
          {
            "authorId": "1845787839",
            "name": "Hang Yang"
          },
          {
            "authorId": "77397868",
            "name": "Kang Liu"
          },
          {
            "authorId": "40372969",
            "name": "Ruihong Huang"
          },
          {
            "authorId": "152829071",
            "name": "Yubo Chen"
          },
          {
            "authorId": "1799672",
            "name": "Taifeng Wang"
          },
          {
            "authorId": "1390572170",
            "name": "Jun Zhao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 14,
        "influential_count": 2,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "14339673",
        "14493443",
        "19220240",
        "3126319",
        "15894892",
        "10827006",
        "6644751",
        "1942185",
        "2257053",
        "18462140",
        "3519188",
        "19224644",
        "5749336",
        "5458500"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1942185,
          "isinfluential": false,
          "contexts": [
            "Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classi-ﬁed into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Enhanced Services for Targeted Information Retrieval by Event Extraction and Data Mining",
            "abstract": "",
            "year": 2008,
            "venue": "LWA",
            "authors": [
              {
                "authorId": "1954152",
                "name": "F. Jungermann"
              },
              {
                "authorId": "1752599",
                "name": "K. Morik"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2257053,
          "isinfluential": false,
          "contexts": [
            "Systems like AutoSlog (Riloff et al., 1993) and AutoSlog-TS (Riloff, 1996) directly applied regular patterns to extract role ﬁllers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automatically Constructing a Dictionary for Information Extraction Tasks",
            "abstract": "Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary.",
            "year": 1993,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3126319,
          "isinfluential": false,
          "contexts": [
            "Event Extraction (EE), a challenging task in Natural Language Processing, aims to extract key types of information (aka event roles , e.g., perpetrators and victims of an attack event) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer (Yang et al., 2003) and Summarizing (Filatova and Hatzivassiloglou, 2004).",
            "…aims to extract key types of information (aka event roles , e.g., perpetrators and victims of an attack event) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer (Yang et al., 2003) and Summarizing (Filatova and Hatzivassiloglou, 2004)."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Structured use of external knowledge for event-based open domain question answering",
            "abstract": "",
            "year": 2003,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "10237116",
                "name": "G. Yang"
              },
              {
                "authorId": "144078686",
                "name": "Tat-Seng Chua"
              },
              {
                "authorId": "50695187",
                "name": "Shuguang Wang"
              },
              {
                "authorId": "46235155",
                "name": "Chun-Keat Koh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3519188,
          "isinfluential": false,
          "contexts": [
            "Until recent years, researchers (Hsi, 2018; Yang et al., 2018; Zheng et al., 2019) began to utilize multiple neural-based methods to solve the task."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction for Document-Level Structured Summarization",
            "abstract": "Event extraction has been well studied for more than two decades, through both the lens of document-level and sentence-level event extraction. However, event extraction methods to date do not yet offer a satisfactory solution to providing concise, structured, document-level summaries of events in news articles. Prior work on document-level event extraction methods have focused on highly specific domains, often with great reliance on handcrafted rules. Such approaches do not generalize well to new domains. In contrast, sentence-level event extraction methods have applied to a much wider variety of domains, but generate output at such fine-grained details that they cannot offer good document-level summaries of events. In this thesis, we propose a new framework for extracting document-level event summaries called macro-events, unifying together aspects of both information extraction and text summarization. The goal of this work is to extract concise, structured representations of documents that can clearly outline the main event of interest and all the necessary argument fillers to describe the event. Unlike work in abstractive and extractive summarization, we seek to create template-based, structured summaries, rather than plain text summaries. We propose three novel methods to address the macro-event extraction task. First, we introduce a structured prediction model based on the Learning to Search framework for jointly learning argument fillers both across and within event argument slots. Second, we propose a multi-layer neural network that is trained directly on macro-event annotated data. Finally, we propose a deep learning method that treats the problem as machine comprehension, which does not require training with any on-domain macro-event labeled data. Our experimental results on a variety of domains show that such algorithms can achieve stronger performance on this task compared to existing baseline approaches. On average across all datasets, neural networks can achieve a 1.76% and 3.96% improvement on micro-averaged and macro-averaged F1 respectively over baseline approaches, while Learning to Search achieves a 3.87% and 5.10% improvement over baseline approaches on the same metrics. Furthermore, under scenarios of limited training data, we find that machine comprehension models can offer very strong performance compared to directly supervised algorithms, while requiring very little human effort to adapt to new domains. January 31, 2018 DRAFT",
            "year": 2022,
            "venue": "",
            "authors": [
              {
                "authorId": "37332445",
                "name": "Andrew Hsi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5458500,
          "isinfluential": true,
          "contexts": [
            "Although the constructed graphs do not precisely demonstrate the original event regions, the GNNs models will synthesize comprehensive context from such connections to enforce each can-didate’s representations, identify the noises, and reconstruct the original regions as a result.",
            "By now, some works (Schlichtkrull et al., 2018; Vashishth et al., 2019) have successfully applied GNNs to model the document-level information within texts and achieved state-of-the-art performance.",
            "In order to deal with graphs with different edge types, relational GNNs (Schlichtkrull et al., 2018; Marcheggiani and Titov, 2017; Vashishth et al., 2019; Bast-ings et al., 2017) try to use separate weights for different edges.",
            "Traditional neural networks such as Convolu-tional Neural Networks and Recursive Neural Networks are hard to deal with graphical data structures, so many graph-based neural networks (GNNs) emerge (Gori et al., 2005; Bruna et al., 2013; Kipf and Welling, 2016).",
            "However, one limitation of these GNNs is that the weights are ﬁxed for all neighbors."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Modeling Relational Data with Graph Convolutional Networks",
            "abstract": "Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
            "year": 2017,
            "venue": "Extended Semantic Web Conference",
            "authors": [
              {
                "authorId": "8804828",
                "name": "M. Schlichtkrull"
              },
              {
                "authorId": "41016725",
                "name": "Thomas Kipf"
              },
              {
                "authorId": "2789097",
                "name": "Peter Bloem"
              },
              {
                "authorId": "9965217",
                "name": "Rianne van den Berg"
              },
              {
                "authorId": "144889265",
                "name": "Ivan Titov"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5749336,
          "isinfluential": false,
          "contexts": [
            "Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classiﬁers to distinguish candidate role ﬁllers from texts and achieved better performance."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions",
            "abstract": "",
            "year": 2007,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6644751,
          "isinfluential": true,
          "contexts": [
            "The popular approach is to apply sentential classiﬁcation to ﬁlter the sentences and recognize role ﬁllers from the chosen sentences (Patward-han and Riloff, 2009; Huang and Riloff, 2012).",
            "Notably, among the document-level EE research, some works (Pat-wardhan and Riloff, 2009; Huang and Riloff, 2012; Yang et al., 2018) have noticed the importance of identifying event regions to improve performance.",
            "Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classiﬁers to distinguish candidate role ﬁllers from texts and achieved better performance.",
            "%) compared to previous best in Huang and Riloff (2012)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Modeling Textual Cohesion for Event Extraction",
            "abstract": "\n \n Event extraction systems typically locate the role fillers for an event by analyzing sentences in isolation and identifying each role filler independently of the others. We argue that more accurate event extraction requires a view of the larger context to decide whether an entity is related to a relevant event. We propose a bottom-up approach to event extraction that initially identifies candidate role fillers independently and then uses that information as well as discourse properties to model textual cohesion. The novel component of the architecture is a sequentially structured sentence classifier that identifies event-related story contexts. The sentence classifier uses lexical associations and discourse relations across sentences, as well as domain-specific distributions of candidate role fillers within and across sentences. This approach yields state-of-the-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems.\n \n",
            "year": 2012,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10827006,
          "isinfluential": false,
          "contexts": [
            "Event Extraction (EE), a challenging task in Natural Language Processing, aims to extract key types of information (aka event roles , e.g., perpetrators and victims of an attack event) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer (Yang et al., 2003) and Summarizing (Filatova and Hatzivassiloglou, 2004).",
            "…aims to extract key types of information (aka event roles , e.g., perpetrators and victims of an attack event) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer (Yang et al., 2003) and Summarizing (Filatova and Hatzivassiloglou, 2004)."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event-Based Extractive Summarization",
            "abstract": "",
            "year": 2004,
            "venue": "",
            "authors": [
              {
                "authorId": "144666687",
                "name": "Elena Filatova"
              },
              {
                "authorId": "1799688",
                "name": "V. Hatzivassiloglou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "…methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018).",
            "Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classi-ﬁed into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14493443,
          "isinfluential": false,
          "contexts": [
            "Baselines For comparison, we select the two meth-ods mentioned above as the baselines for CFEED: Boros et al. (2014) and Yang et al. (2018) .",
            "Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classiﬁers to distinguish candidate role ﬁllers from texts and achieved better performance.",
            "The event role ﬁller extractors often use extraction patterns (Riloff, 1996) or classiﬁers (Boros et al., 2014) to identify typical local contexts containing a certain type of event role ﬁllers."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Role Extraction using Domain-Relevant Word Representations",
            "abstract": "The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high. In this article, we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "35101485",
                "name": "Emanuela Boros"
              },
              {
                "authorId": "1740190",
                "name": "Romaric Besançon"
              },
              {
                "authorId": "1679133",
                "name": "Olivier Ferret"
              },
              {
                "authorId": "1704849",
                "name": "Brigitte Grau"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15894892,
          "isinfluential": false,
          "contexts": [
            "Riloff (1996) 2.",
            "Systems like AutoSlog (Riloff et al., 1993) and AutoSlog-TS (Riloff, 1996) directly applied regular patterns to extract role ﬁllers."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automatically Generating Extraction Patterns from Untagged Text",
            "abstract": "Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUG-4 terrorism domain, AutoSlog-TS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input.",
            "year": 1996,
            "venue": "AAAI/IAAI, Vol. 2",
            "authors": [
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18462140,
          "isinfluential": false,
          "contexts": [
            "Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018;…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multi-Event Extraction Guided by Global Constraints",
            "abstract": "",
            "year": 2012,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1762757",
                "name": "Roi Reichart"
              },
              {
                "authorId": "1741283",
                "name": "R. Barzilay"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19220240,
          "isinfluential": false,
          "contexts": [
            "…methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction",
            "abstract": "\n \n Event extraction plays an important role in natural language processing (NLP) applications including question answering and information retrieval. Traditional event extraction relies heavily on lexical and syntactic features, which require intensive human engineering and may not generalize to different datasets. Deep neural networks, on the other hand, are able to automatically learn underlying features, but existing networks do not make full use of syntactic relations. In this paper, we propose a novel dependency bridge recurrent neural network (dbRNN) for event extraction. We build our model upon a recurrent neural network, but enhance it with dependency bridges, which carry syntactically related information when modeling each word.We illustrates that simultaneously applying tree structure and sequence structure in RNN brings much better performance than only uses sequential RNN. In addition, we use a tensor layer to simultaneously capture the various types of latent interaction between candidate arguments as well as identify/classify all arguments of an event. Experiments show that our approach achieves competitive results compared with previous work.\n \n",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "39058310",
                "name": "Lei Sha"
              },
              {
                "authorId": "2053324591",
                "name": "Feng Qian"
              },
              {
                "authorId": "39488576",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19224644,
          "isinfluential": false,
          "contexts": [
            "To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Scale Up Event Extraction Learning via Automatic Training Data Generation",
            "abstract": "\n \n The task of event extraction has long been investigated in a supervised learning paradigm, which is bound by the number and the quality of the training instances. Existing training data must be manually generated through a combination of expert domain knowledge and extensive human involvement. However, due to drastic efforts required in annotating text, the resultant datasets are usually small, which severally affects the quality of the learned model, making it hard to generalize. Our work develops an automatic approach for generating training data for event extraction. Our approach allows us to scale up event extraction training instances from thousands to hundreds of thousands, and it does this at a much lower cost than a manual approach. We achieve this by employing distant supervision to automatically create event annotations from unlabelled text using existing structured knowledge bases or tables.We then develop a neural network model with post inference to transfer the knowledge extracted from structured knowledge bases to automatically annotate typed events with corresponding arguments in text.We evaluate our approach by using the knowledge extracted from Freebase to label texts from Wikipedia articles. Experimental results show that our approach can generate a large number of highquality training instances. We show that this large volume of training data not only leads to a better event extractor, but also allows us to detect multiple typed events.\n \n",
            "year": 2017,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2111108770",
                "name": "Ying Zeng"
              },
              {
                "authorId": "1717629",
                "name": "Yansong Feng"
              },
              {
                "authorId": "2113552697",
                "name": "Rong Ma"
              },
              {
                "authorId": "40072305",
                "name": "Zheng Wang"
              },
              {
                "authorId": "144539156",
                "name": "Rui Yan"
              },
              {
                "authorId": "2973927",
                "name": "Chongde Shi"
              },
              {
                "authorId": "144060462",
                "name": "Dongyan Zhao"
              }
            ]
          }
        }
      ]
    },
    "270619369": {
      "citing_paper_info": {
        "title": "EXCEEDS: Extracting Complex Events as Connecting the Dots to Graphs in Scientific Domain",
        "abstract": "It is crucial to utilize events to understand a specific domain. There are lots of research on event extraction in many domains such as news, finance and biology domain. However, scientific domain still lacks event extraction research, including comprehensive datasets and corresponding methods. Compared to other domains, scientific domain presents two characteristics: denser nuggets and more complex events. To solve the above problem, considering these two characteristics, we first construct SciEvents, a large-scale multi-event document-level dataset with a schema tailored for scientific domain. It has 2,508 documents and 24,381 events under refined annotation and quality control. Then, we propose EXCEEDS, a novel end-to-end scientific event extraction framework by storing dense nuggets in a grid matrix and simplifying complex event extraction into a dot construction and connection task. Experimental results demonstrate state-of-the-art performances of EXCEEDS on SciEvents. Additionally, we release SciEvents and EXCEEDS on GitHub.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2307465524",
            "name": "Yi-Fan Lu"
          },
          {
            "authorId": "134880677",
            "name": "Xian-Ling Mao"
          },
          {
            "authorId": "2217713470",
            "name": "Bo Wang"
          },
          {
            "authorId": "2304978087",
            "name": "Xiao Liu"
          },
          {
            "authorId": "2304465781",
            "name": "Heyan Huang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 1,
        "unique_cited_count": 0,
        "influential_count": 0,
        "detailed_records_count": 1
      },
      "cited_papers": [],
      "citation_details": [
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "OneIE [17]: a joint EE model trained with global features ; BartGen [14]: a document-level event argument extraction (EAE) UIE [25]: a unified text-to-structure generation framework; PAIE [28]: an EAE method that utilizes prompt tuning for ex-tractive"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "269313129": {
      "citing_paper_info": {
        "title": "Controllable Template Generation for Document-level Event Extraction",
        "abstract": "Document-level event extraction task has achieved significant progress based on template generation methods. However, there is no reasonable regulation and restriction in the existing template-based generation methods, which results in the uncontrollability of the generation results. In some scenarios, model generates entities that do not belong to the input text, or generate template content repeatedly. It is determined by the nature of the extraction task and the generation task. To this end, we propose a controllable template generation event extraction model. According to the characteristics of template generation and event extraction tasks, the model devises copy mechanism, inhibition mechanism and rejection mechanism under the appropriately constructed template. Our model achieves state-of-the-art result on MUC-4 dataset, and finally through experimental analysis, it demonstrates the effectiveness of each mechanism we proposed.",
        "year": 2024,
        "venue": "2024 4th International Conference on Neural Networks, Information and Communication (NNICE)",
        "authors": [
          {
            "authorId": "2229484753",
            "name": "Quntian Fang"
          },
          {
            "authorId": "2295618776",
            "name": "Feng Liu"
          },
          {
            "authorId": "2151323750",
            "name": "Zhen Huang"
          },
          {
            "authorId": "2214752982",
            "name": "Zhenliang Guo"
          },
          {
            "authorId": "2297998563",
            "name": "Changjian Wang"
          },
          {
            "authorId": "2117982441",
            "name": "Dongsheng Li"
          },
          {
            "authorId": "8367832",
            "name": "Minghao Hu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 2,
        "unique_cited_count": 2,
        "influential_count": 0,
        "detailed_records_count": 2
      },
      "cited_papers": [
        "247619149",
        "218630327"
      ],
      "citation_details": [
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "Current event extraction studies are mostly end-to-end methods [1], [2], while most of them still stay at sentence-level modeling and struggle to overcome the long-term dependency problem of event argument entities in document-level tasks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247619149,
          "isinfluential": false,
          "contexts": [
            "(4) In order to make the model adapt to the task, we employ the rejection mechanism proposed in UIE [6], which can force the model to complete all event argument extraction (even if these arguments may be empty), thus making the model more suitable for downstream tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unified Structure Generation for Universal Information Extraction",
            "abstract": "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2154976399",
                "name": "Qing Liu"
              },
              {
                "authorId": "40495683",
                "name": "Dai Dai"
              },
              {
                "authorId": "2107521158",
                "name": "Xinyan Xiao"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "2149181702",
                "name": "Hua Wu"
              }
            ]
          }
        }
      ]
    },
    "271855673": {
      "citing_paper_info": {
        "title": "Document-Level Event Extraction with Definition-Driven ICL",
        "abstract": "In the field of Natural Language Processing (NLP), Large Language Models (LLMs) have shown great potential in document-level event extraction tasks, but existing methods face challenges in the design of prompts. To address this issue, we propose an optimization strategy called\"Definition-driven Document-level Event Extraction (DDEE).\"By adjusting the length of the prompt and enhancing the clarity of heuristics, we have significantly improved the event extraction performance of LLMs. We used data balancing techniques to solve the long-tail effect problem, enhancing the model's generalization ability for event types. At the same time, we refined the prompt to ensure it is both concise and comprehensive, adapting to the sensitivity of LLMs to the style of prompts. In addition, the introduction of structured heuristic methods and strict limiting conditions has improved the precision of event and argument role extraction. These strategies not only solve the prompt engineering problems of LLMs in document-level event extraction but also promote the development of event extraction technology, providing new research perspectives for other tasks in the NLP field.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2315949948",
            "name": "Zhuoyuan Liu"
          },
          {
            "authorId": "2315949094",
            "name": "Yilin Luo"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 14,
        "influential_count": 0,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "270209567",
        "248986239",
        "258841149",
        "257985312",
        "269626679",
        "218971783",
        "257353536",
        "249017743",
        "254591242",
        "254823489",
        "258558102",
        "269803960",
        "267750177",
        "233219850"
      ],
      "citation_details": [
        {
          "citedcorpusid": 218971783,
          "isinfluential": false,
          "contexts": [
            "In-Context Learning (ICL) [13] is a strategy that enables pre-trained language models to quickly adapt to different tasks with minimal [14] or zero-shot [15] data."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "The WikiEvents dataset[35] is a resource created to advance research in document-level event extraction, as shown in Table1."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248986239,
          "isinfluential": false,
          "contexts": [
            "Research indicates that explicit prompting methods for LLMs to decompose problems, such as Least-to-Most[28]and zero-shot CoT, improve the reliability of reasoning."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "65855107",
                "name": "Denny Zhou"
              },
              {
                "authorId": "1821614764",
                "name": "Nathanael Scharli"
              },
              {
                "authorId": "2153400663",
                "name": "Le Hou"
              },
              {
                "authorId": "119640649",
                "name": "Jason Wei"
              },
              {
                "authorId": "1471909492",
                "name": "Nathan Scales"
              },
              {
                "authorId": "1524732527",
                "name": "Xuezhi Wang"
              },
              {
                "authorId": "50319359",
                "name": "D. Schuurmans"
              },
              {
                "authorId": "1698617",
                "name": "O. Bousquet"
              },
              {
                "authorId": "1998340269",
                "name": "Quoc Le"
              },
              {
                "authorId": "2226805",
                "name": "Ed H. Chi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249017743,
          "isinfluential": false,
          "contexts": [
            "In-Context Learning (ICL) [13] is a strategy that enables pre-trained language models to quickly adapt to different tasks with minimal [14] or zero-shot [15] data.",
            "Furthermore, the application of various prompting techniques[14, 15, 29]has confirmed the effectiveness of decomposition strategies, enabling models to systematically handle complex issues.",
            "Zero-shot CoT [15], with simple prompts like \"Let’s think step by step,\" enhances the transparency and accuracy of the reasoning process."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Large Language Models are Zero-Shot Reasoners",
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "year": 2022,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "2081836120",
                "name": "Takeshi Kojima"
              },
              {
                "authorId": "2046135",
                "name": "S. Gu"
              },
              {
                "authorId": "1557386977",
                "name": "Machel Reid"
              },
              {
                "authorId": "2153732825",
                "name": "Yutaka Matsuo"
              },
              {
                "authorId": "1715282",
                "name": "Yusuke Iwasawa"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254591242,
          "isinfluential": false,
          "contexts": [
            "However, ICL exhibits high instability in practical applications, where model predictions are influenced by factors such as example order, input length, prompt format, and training data distribution [17]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Diverse Demonstrations Improve In-context Compositional Generalization",
            "abstract": "In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with finetuning.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1422883272",
                "name": "Itay Levy"
              },
              {
                "authorId": "50757607",
                "name": "Ben Bogin"
              },
              {
                "authorId": "1750652",
                "name": "Jonathan Berant"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254823489,
          "isinfluential": false,
          "contexts": [
            "The use of large language models (LLMs) in natural language processing (NLP) tasks has significantly increased, especially with closed models like PaLM [1] , Claude [2] , and GPT-4 [3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Constitutional AI: Harmlessness from AI Feedback",
            "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1486307451",
                "name": "Yuntao Bai"
              },
              {
                "authorId": "148070327",
                "name": "Saurav Kadavath"
              },
              {
                "authorId": "2158813858",
                "name": "Sandipan Kundu"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "1583434563",
                "name": "John Kernion"
              },
              {
                "authorId": "2149890773",
                "name": "Andy Jones"
              },
              {
                "authorId": "2111074159",
                "name": "A. Chen"
              },
              {
                "authorId": "46684455",
                "name": "Anna Goldie"
              },
              {
                "authorId": "1861312",
                "name": "Azalia Mirhoseini"
              },
              {
                "authorId": "2190108315",
                "name": "C. McKinnon"
              },
              {
                "authorId": "2183729976",
                "name": "Carol Chen"
              },
              {
                "authorId": "2061321863",
                "name": "Catherine Olsson"
              },
              {
                "authorId": "2287268442",
                "name": "Chris Olah"
              },
              {
                "authorId": "39182747",
                "name": "Danny Hernandez"
              },
              {
                "authorId": "1943097969",
                "name": "Dawn Drain"
              },
              {
                "authorId": "2081806483",
                "name": "Deep Ganguli"
              },
              {
                "authorId": "2108506462",
                "name": "Dustin Li"
              },
              {
                "authorId": "2175781319",
                "name": "Eli Tran-Johnson"
              },
              {
                "authorId": "47635264",
                "name": "E. Perez"
              },
              {
                "authorId": "2067765208",
                "name": "Jamie Kerr"
              },
              {
                "authorId": "2190111475",
                "name": "J. Mueller"
              },
              {
                "authorId": "70988670",
                "name": "Jeffrey Ladish"
              },
              {
                "authorId": "2068044809",
                "name": "J. Landau"
              },
              {
                "authorId": "1978097132",
                "name": "Kamal Ndousse"
              },
              {
                "authorId": "2105347564",
                "name": "Kamilė Lukošiūtė"
              },
              {
                "authorId": "2154608229",
                "name": "Liane Lovitt"
              },
              {
                "authorId": "2054578129",
                "name": "M. Sellitto"
              },
              {
                "authorId": "2866708",
                "name": "Nelson Elhage"
              },
              {
                "authorId": "2833768",
                "name": "Nicholas Schiefer"
              },
              {
                "authorId": "2190107517",
                "name": "Noem'i Mercado"
              },
              {
                "authorId": "2142833890",
                "name": "Nova Dassarma"
              },
              {
                "authorId": "3112577",
                "name": "R. Lasenby"
              },
              {
                "authorId": "48810415",
                "name": "Robin Larson"
              },
              {
                "authorId": "1380664820",
                "name": "Sam Ringer"
              },
              {
                "authorId": "2154610174",
                "name": "Scott Johnston"
              },
              {
                "authorId": "49604482",
                "name": "Shauna Kravec"
              },
              {
                "authorId": "2154609053",
                "name": "S. E. Showk"
              },
              {
                "authorId": "30176974",
                "name": "Stanislav Fort"
              },
              {
                "authorId": "46239941",
                "name": "Tamera Lanham"
              },
              {
                "authorId": "1419532638",
                "name": "Timothy Telleen-Lawton"
              },
              {
                "authorId": "2154608209",
                "name": "Tom Conerly"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "2162194147",
                "name": "Tristan Hume"
              },
              {
                "authorId": "1799822",
                "name": "Sam Bowman"
              },
              {
                "authorId": "1573482302",
                "name": "Zac Hatfield-Dodds"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              },
              {
                "authorId": "2117706920",
                "name": "Nicholas Joseph"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2053807409",
                "name": "Jared Kaplan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257353536,
          "isinfluential": false,
          "contexts": [
            "Prophet framework proposed by Yu, Zhou et al. [11] integrates answer candidates and answer-aware context examples as heuristic information, markedly boosting performance in knowledge-based Visual Question Answering (VQA) tasks and demonstrating compatibility with various VQA models and LLMs.…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Prophet: Prompting Large Language Models With Complementary Answer Heuristics for Knowledge-Based Visual Question Answering",
            "abstract": "Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet—a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3 (Brown et al. 2020), Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. Prophet is general that can be instantiated with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones). Moreover, Prophet can also be integrated with modern large multimodal models in different stages, which is named Prophet++, to further improve the capabilities on knowledge-based VQA tasks.",
            "year": 2023,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "2210731803",
                "name": "Zhenwei Shao"
              },
              {
                "authorId": "144007938",
                "name": "Zhou Yu"
              },
              {
                "authorId": "50469060",
                "name": "Mei Wang"
              },
              {
                "authorId": "2161356649",
                "name": "Jun Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257985312,
          "isinfluential": false,
          "contexts": [
            "To validate our proposed method, we conducted experimental comparisons with the following event extraction models, used as baselines: OntoGPT[37]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning",
            "abstract": "Abstract Motivation Creating knowledge bases and ontologies is a time consuming task that relies on manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrarily complex nested knowledge schemas. Results Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against an LLM to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for matched elements. We present examples of applying SPIRES in different domains, including extraction of food recipes, multi-species cellular signaling pathways, disease treatments, multi-step drug mechanisms, and chemical to disease relationships. Current SPIRES accuracy is comparable to the mid-range of existing Relation Extraction methods, but greatly surpasses an LLM’s native capability of grounding entities with unique identifiers. SPIRES has the advantage of easy customization, flexibility, and, crucially, the ability to perform new tasks in the absence of any new training data. This method supports a general strategy of leveraging the language interpreting capabilities of LLMs to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the LLM. Availability and implementation SPIRES is available as part of the open source OntoGPT package: https://github.com/monarch-initiative/ontogpt.",
            "year": 2023,
            "venue": "Bioinform.",
            "authors": [
              {
                "authorId": "145710797",
                "name": "J. Caufield"
              },
              {
                "authorId": "2151893083",
                "name": "Harshad B. Hegde"
              },
              {
                "authorId": "2467370",
                "name": "Vincent Emonet"
              },
              {
                "authorId": "40404712",
                "name": "N. Harris"
              },
              {
                "authorId": "2111638",
                "name": "marcin p. joachimiak"
              },
              {
                "authorId": "1404374941",
                "name": "N. Matentzoglu"
              },
              {
                "authorId": "2109893608",
                "name": "Hyeongsik Kim"
              },
              {
                "authorId": "33750513",
                "name": "S. Moxon"
              },
              {
                "authorId": "2065413516",
                "name": "J. Reese"
              },
              {
                "authorId": "1976792",
                "name": "M. Haendel"
              },
              {
                "authorId": "2150260091",
                "name": "Peter N. Robinson"
              },
              {
                "authorId": "52038267",
                "name": "C. Mungall"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258558102,
          "isinfluential": false,
          "contexts": [
            "Wang et al.’s [31] Plan-and-Solve (PS) prompting method guides models to formulate and execute plans to solve complex problems, improving performance in multi-step reasoning tasks."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145131956",
                "name": "Lei Wang"
              },
              {
                "authorId": "2143418409",
                "name": "Wanyu Xu"
              },
              {
                "authorId": "2150277971",
                "name": "Yihuai Lan"
              },
              {
                "authorId": "2203447284",
                "name": "Zhiqiang Hu"
              },
              {
                "authorId": "3458560",
                "name": "Yunshi Lan"
              },
              {
                "authorId": "38656724",
                "name": "Roy Ka-Wei Lee"
              },
              {
                "authorId": "2212836814",
                "name": "Ee-Peng Lim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258841149,
          "isinfluential": false,
          "contexts": [
            "Kim et al.’s [33] fine-tuning dataset COT COLLECTION enhances the generalization ability of small-scale language models on multi-task unseen problems."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
            "abstract": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B&11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2184037220",
                "name": "Seungone Kim"
              },
              {
                "authorId": "102540266",
                "name": "Se June Joo"
              },
              {
                "authorId": "2180527259",
                "name": "Doyoung Kim"
              },
              {
                "authorId": "2000091730",
                "name": "Joel Jang"
              },
              {
                "authorId": "2152111477",
                "name": "Seonghyeon Ye"
              },
              {
                "authorId": "51228826",
                "name": "Jamin Shin"
              },
              {
                "authorId": "4418074",
                "name": "Minjoon Seo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267750177,
          "isinfluential": false,
          "contexts": [
            "Mo et al. [21] [24] effectively improves ICL performance through five stages: expert pool construction, task definition extraction, guided retrieval, expert integration, and continual few-shot learning."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
            "abstract": "There has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identification and correction of potential interface errors. Specifically, our proposed method taps into the inherent contextual information and valuable information in hard negative samples and the nearest positive neighbors to the test and then applies the in-context learning demonstrations based on LLMs. Our experiments on various datasets indicate that c-ICL outperforms previous few-shot in-context learning methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. These improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.",
            "year": 2024,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2199939792",
                "name": "Ying Mo"
              },
              {
                "authorId": "2276103971",
                "name": "Jian Yang"
              },
              {
                "authorId": "2261393008",
                "name": "Jiahao Liu"
              },
              {
                "authorId": "2216176381",
                "name": "Shun Zhang"
              },
              {
                "authorId": "2258759716",
                "name": "Jingang Wang"
              },
              {
                "authorId": "2258837278",
                "name": "Zhoujun Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269626679,
          "isinfluential": false,
          "contexts": [
            "Jiang et al. [19] proposed the P-ICL (Point In-Context Learning) framework, providing critical information about entity types and classifications to LLMs, thereby enhancing named entity recognition tasks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models",
            "abstract": "In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL). However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself. In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type. With such significant information, the LLM can achieve entity classification more precisely. To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering. Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection.",
            "year": 2024,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2295745672",
                "name": "Guochao Jiang"
              },
              {
                "authorId": "2297163713",
                "name": "Zepeng Ding"
              },
              {
                "authorId": "2296782808",
                "name": "Yuchen Shi"
              },
              {
                "authorId": "2300429503",
                "name": "Deqing Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269803960,
          "isinfluential": false,
          "contexts": [
            "In the zero-shot event extraction domain, Zhigang Kan et al. [6] improved argument recognition performance through a multi-turn dialogue approach, demonstrating its potential in event detection."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Emancipating Event Extraction from the Constraints of Long-Tailed Distribution Data Utilizing Large Language Models",
            "abstract": "",
            "year": 2024,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "1382547201",
                "name": "Liwen Peng"
              },
              {
                "authorId": "47300931",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "2129495284",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270209567,
          "isinfluential": false,
          "contexts": [
            "Schema-aware Event Extraction[7].",
            "Fatemeh Shiri et al. [7]optimized the application of LLMs in knowledge graph construction and decision support by integrating advanced prompt techniques such as Chain-of-Thought and Retrieval Augmented Generation, reducing hallucination risks and enhancing accuracy."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs.",
            "abstract": "Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method’s superior performance compared to baseline approaches.",
            "year": 2024,
            "venue": "Fusion",
            "authors": [
              {
                "authorId": "49994056",
                "name": "Fatemeh Shiri"
              },
              {
                "authorId": "2282512109",
                "name": "Van Nguyen"
              },
              {
                "authorId": "2804001",
                "name": "Farhad Moghimifar"
              },
              {
                "authorId": "2304561647",
                "name": "John Yoo"
              },
              {
                "authorId": "2561045",
                "name": "Gholamreza Haffari"
              },
              {
                "authorId": "2256011160",
                "name": "Yuan-Fang Li"
              }
            ]
          }
        }
      ]
    },
    "235253912": {
      "citing_paper_info": {
        "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
        "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
        "year": 2021,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "1748844142",
            "name": "Runxin Xu"
          },
          {
            "authorId": "1500520681",
            "name": "Tianyu Liu"
          },
          {
            "authorId": "143900005",
            "name": "Lei Li"
          },
          {
            "authorId": "7267809",
            "name": "Baobao Chang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 10,
        "influential_count": 3,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "221246218",
        "1671874",
        "202773239",
        "2926851",
        "196178503",
        "13756489",
        "221996144",
        "53081291",
        "19224644",
        "9946972"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1671874,
          "isinfluential": false,
          "contexts": [
            "To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "WordNet: A Lexical Database for English",
            "abstract": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",
            "year": 1995,
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "authors": [
              {
                "authorId": "144096985",
                "name": "G. Miller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2926851,
          "isinfluential": false,
          "contexts": [
            "To take advantage of such interdependency, we propose a novel Tracker module inspired by memory network (Weston et al., 2015)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Memory Networks",
            "abstract": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "145183709",
                "name": "J. Weston"
              },
              {
                "authorId": "3295092",
                "name": "S. Chopra"
              },
              {
                "authorId": "1713934",
                "name": "Antoine Bordes"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9946972,
          "isinfluential": false,
          "contexts": [
            "They conduct experiments on MUC-4 (Sundheim, 1992) dataset with 1, 700 documents and 5 kinds of entity-based arguments, and it is formulated as a table-filling task, coping with single event record of single event",
            "They conduct experiments on MUC-4 (Sundheim, 1992) dataset with 1, 700 documents and 5 kinds of entity-based arguments, and it is formulated as a table-filling task, coping with single event record of single event\ntype."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Overview of the Fourth Message Understanding Evaluation and Conference",
            "abstract": "The Fourth Message Understanding Conference (MUC-4) is the latest in a series of conferences that concern the evaluation of natural language processing (NLP) systems. These conferences have reported on progress being made both in the development of systems capable of analyzing relatively short English texts and in the definition of a rigorous performance evaluation methodology. MUC-4 was preceded by a period of intensive system development by each of the participating organizations and blind testing using materials prepared by NRaD and SAIC that are described in this paper, other papers in this volume, and the MUC-3 proceedings [1].",
            "year": 1992,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": true,
          "contexts": [
            "In our implementation of G IT , we use 8 and 4 layers Transformer (Vaswani et al., 2017) in encoding and decoding module respectively.",
            "Finally we introduce a Tracker module to continuously track all the records with global memory, in which we utilize the global interdependency among records for multi-event extraction (Sec 3.4). former (Vaswani et al., 2017):",
            "Although Zheng et al. (2019) use Transformer to fuse sentences and entities, interdependency among events is neglected.",
            "In our implementation of GIT, we use 8 and 4 layers Transformer (Vaswani et al., 2017) in encoding and decoding module respectively."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19224644,
          "isinfluential": false,
          "contexts": [
            "The task does not require to identify event triggers (Zeng et al., 2018; Liu et al., 2019b), which reduces manual effort of annotation and the application scenarios becomes more extensive."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Scale Up Event Extraction Learning via Automatic Training Data Generation",
            "abstract": "\n \n The task of event extraction has long been investigated in a supervised learning paradigm, which is bound by the number and the quality of the training instances. Existing training data must be manually generated through a combination of expert domain knowledge and extensive human involvement. However, due to drastic efforts required in annotating text, the resultant datasets are usually small, which severally affects the quality of the learned model, making it hard to generalize. Our work develops an automatic approach for generating training data for event extraction. Our approach allows us to scale up event extraction training instances from thousands to hundreds of thousands, and it does this at a much lower cost than a manual approach. We achieve this by employing distant supervision to automatically create event annotations from unlabelled text using existing structured knowledge bases or tables.We then develop a neural network model with post inference to transfer the knowledge extracted from structured knowledge bases to automatically annotate typed events with corresponding arguments in text.We evaluate our approach by using the knowledge extracted from Freebase to label texts from Wikipedia articles. Experimental results show that our approach can generate a large number of highquality training instances. We show that this large volume of training data not only leads to a better event extractor, but also allows us to detect multiple typed events.\n \n",
            "year": 2017,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2111108770",
                "name": "Ying Zeng"
              },
              {
                "authorId": "1717629",
                "name": "Yansong Feng"
              },
              {
                "authorId": "2113552697",
                "name": "Rong Ma"
              },
              {
                "authorId": "40072305",
                "name": "Zheng Wang"
              },
              {
                "authorId": "144539156",
                "name": "Rui Yan"
              },
              {
                "authorId": "2973927",
                "name": "Chongde Shi"
              },
              {
                "authorId": "144060462",
                "name": "Dongyan Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53081291,
          "isinfluential": false,
          "contexts": [
            "To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995).",
            "To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms",
            "abstract": "Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2861442",
                "name": "Yantao Jia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": true,
          "contexts": [
            "To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995).",
            "Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
            "Most of the previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
            ", 2018), pre-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202773239,
          "isinfluential": false,
          "contexts": [
            "Since a document can express events of different types, we formulate the task as a multi-label classification and leverage sentences feature matrix S to\n*Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree.",
            "*Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree.",
            "Some studies also utilize dependency tree information (Liu et al., 2018; Yan et al., 2019)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Detection with Multi-Order Graph Convolution and Aggregated Attention",
            "abstract": "Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use first-order syntactic relations (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2116562619",
                "name": "Haoran Yan"
              },
              {
                "authorId": "2149111400",
                "name": "Xiaolong Jin"
              },
              {
                "authorId": "1500390975",
                "name": "Xiangbin Meng"
              },
              {
                "authorId": "70414094",
                "name": "Jiafeng Guo"
              },
              {
                "authorId": "1717004",
                "name": "Xueqi Cheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221246218,
          "isinfluential": true,
          "contexts": [
            "Hence, extracting events at the document-level is critical and has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020).",
            "Focusing on single event extraction, Du and Cardie (2020a) and Du et al. (2020) concatenate multiple sentences and only consider a single event, which lacks the ability to model multiple events scattered in a long document.",
            "Du and Cardie (2020a) try to encode the sentences in a multi-granularity way and Du et al. (2020) leverage a seq2seq model.",
            "It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Event-based Extraction Using Generative Template-filling Transformers",
            "abstract": "We revisit the classic information extraction problem of document-level template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is \\emph{implicitly} aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions. Our code for the evaluation script and models will be open-sourced at this https URL for reproduction purposes.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221996144,
          "isinfluential": false,
          "contexts": [
            "After heterogeneous graph construction *, we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions inspired by Zeng et al. (2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entity-level graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at this https URL .",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "After heterogeneous graph construction *, we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions.",
            "After heterogeneous graph construction * , we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "These studies usually conduct experiments on sentencelevel event extraction dataset, ACE05 (Walker et al., 2006).",
            "These studies usually conduct experiments on sentence-level event extraction dataset, ACE05 (Walker et al., 2006)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "271496297": {
      "citing_paper_info": {
        "title": "Incorporating Schema-Aware Description into Document-Level Event Extraction",
        "abstract": "Document-level event extraction (DEE) aims to extract the structured event information from a given document, facing two critical challenges: (1) event arguments always scatter across sentences (arguments-scattering); (2) multiple events can co-occur in one document (multi-event). Most recent studies mainly follow two simplified settings to ease the challenges: one simplifies DEE with the no-trigger-words design (NDEE), and the other focuses on event argument extraction (DEAE), a sub-task of DEE. However, the former excludes trigger extraction and suffers from error propagation in the sub-tasks. The latter relies heavily on the gold triggers as prerequisites and struggles to distinguish multiple arguments playing the same role in different events. To address the limitations above, we propose a novel joint trigger and argument extraction paradigm SEELE to enhance the DEE model via incorporating SchEma-awarE descriptions into Document-Level Event extraction. Specifically, the schema-aware descriptions are leveraged from two aspects: (1) guiding the attention mechanism among event-aware tokens across sentences, which relieves arguments-scattering without error propagation; (2) performing the fine-grained contrastive learning to distinguish different events, which mitigates multi-event without gold triggers. Extensive experiments show the superiority of SEELE, achieving notable improvements (2.1% to 9.7% F1) on three NDEE datasets and competitive performance on two DEAE datasets. Our code is available at https://github.com/TheoryRhapsody/SEELE.",
        "year": 2024,
        "venue": "International Joint Conference on Artificial Intelligence",
        "authors": [
          {
            "authorId": "2117883657",
            "name": "Zijie Xu"
          },
          {
            "authorId": "2257246692",
            "name": "Peng Wang"
          },
          {
            "authorId": "1596819256",
            "name": "Wenjun Ke"
          },
          {
            "authorId": "91119265",
            "name": "Guozheng Li"
          },
          {
            "authorId": "2265716887",
            "name": "Jiajun Liu"
          },
          {
            "authorId": "2284872859",
            "name": "Ke Ji"
          },
          {
            "authorId": "2229014483",
            "name": "Xiye Chen"
          },
          {
            "authorId": "2151101491",
            "name": "Chen Wu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "259629787",
        "263829656",
        "211296865",
        "251371417"
      ],
      "citation_details": [
        {
          "citedcorpusid": 211296865,
          "isinfluential": false,
          "contexts": [
            "To optimize these three GPs, we apply Circle loss [Sun et al. , 2020] to alleviate label imbalance of gold arguments and negative spans: where s ( · ) is the score function in GP [Su et al. , 2022], P denotes the set of gold argument spans, N denotes the set of negative spans that are not arguments."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Circle Loss: A Unified Perspective of Pair Similarity Optimization",
            "abstract": "This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity $s_p$ and minimize the between-class similarity $s_n$. We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed $s_n$ and $s_p$ into similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, \\emph {i.e.}, learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.",
            "year": 2020,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "2108935429",
                "name": "Yifan Sun"
              },
              {
                "authorId": "27648874",
                "name": "Changmao Cheng"
              },
              {
                "authorId": "2108544889",
                "name": "Yuhan Zhang"
              },
              {
                "authorId": "2115811693",
                "name": "Chi Zhang"
              },
              {
                "authorId": "144802394",
                "name": "Liang Zheng"
              },
              {
                "authorId": "72682780",
                "name": "Zhongdao Wang"
              },
              {
                "authorId": "1732264",
                "name": "Yichen Wei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251371417,
          "isinfluential": false,
          "contexts": [
            "Specifically, we utilize three Global Pointer Networks (GPs) [Su et al. , 2022] to extract each argument (or trigger) from the document and construct the event complete graph simultaneously, without any error propagation.",
            "To optimize these three GPs, we apply Circle loss [Sun et al. , 2020] to alleviate label imbalance of gold arguments and negative spans: where s ( · ) is the score function in GP [Su et al. , 2022], P denotes the set of gold argument spans, N denotes the set of negative spans that are not arguments."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition",
            "abstract": "Named entity recognition (NER) task aims at identifying entities from a piece of text that belong to predefined semantic types such as person, location, organization, etc. The state-of-the-art solutions for flat entities NER commonly suffer from capturing the fine-grained semantic information in underlying texts. The existing span-based approaches overcome this limitation, but the computation time is still a concern. In this work, we propose a novel span-based NER framework, namely Global Pointer (GP), that leverages the relative positions through a multiplicative attention mechanism. The ultimate goal is to enable a global view that considers the beginning and the end positions to predict the entity. To this end, we design two modules to identify the head and the tail of a given entity to enable the inconsistency between the training and inference processes. Moreover, we introduce a novel classification loss function to address the imbalance label problem. In terms of parameters, we introduce a simple but effective approximate method to reduce the training parameters. We extensively evaluate GP on various benchmark datasets. Our extensive experiments demonstrate that GP can outperform the existing solution. Moreover, the experimental results show the efficacy of the introduced loss function compared to softmax and entropy alternatives.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "51111230",
                "name": "Jianlin Su"
              },
              {
                "authorId": "2159557286",
                "name": "Ahmed Murtadha"
              },
              {
                "authorId": "1382633722",
                "name": "Shengfeng Pan"
              },
              {
                "authorId": "2180707547",
                "name": "Jing Hou"
              },
              {
                "authorId": "2180406438",
                "name": "Jun Sun"
              },
              {
                "authorId": "2180696585",
                "name": "Wanwei Huang"
              },
              {
                "authorId": "2079396269",
                "name": "Bo Wen"
              },
              {
                "authorId": "1807486863",
                "name": "Yunfeng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259629787,
          "isinfluential": false,
          "contexts": [
            "…event triggers and corresponding arguments from natural language texts, facilitating various downstream applications, such as information retrieval [Li et al. , 2023], recommender systems [Lu et al. , 2016], and question answering [Liu et al. , 2023] in finance, healthcare, and law industries."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "IterDE: An Iterative Knowledge Distillation Framework for Knowledge Graph Embeddings",
            "abstract": "Knowledge distillation for knowledge graph embedding (KGE) aims to reduce the KGE model size to address the challenges of storage limitations and knowledge reasoning efficiency. However, current work still suffers from the performance drops when compressing a high-dimensional original KGE model to a low-dimensional distillation KGE model. Moreover, most work focuses on the reduction of inference time but ignores the time-consuming training process of distilling KGE models. In this paper, we propose IterDE, a novel knowledge distillation framework for KGEs. First, IterDE introduces an iterative distillation way and enables a KGE model to alternately be a student model and a teacher model during the iterative distillation process. Consequently, knowledge can be transferred in a smooth manner between high-dimensional teacher models and low-dimensional student models, while preserving good KGE performances. Furthermore, in order to optimize the training process, we consider that different optimization objects between hard label loss and soft label loss can affect the efficiency of training, and then we propose a soft-label weighting dynamic adjustment mechanism that can balance the inconsistency of optimization direction between hard and soft label loss by gradually increasing the weighting of soft label loss. Our experimental results demonstrate that IterDE achieves a new state-of-the-art distillation performance for KGEs compared to strong baselines on the link prediction task. Significantly, IterDE can reduce the training time by 50% on average. Finally, more exploratory experiments show that the soft-label weighting dynamic adjustment mechanism and more fine-grained iterations can improve distillation performance.",
            "year": 2023,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "49721729",
                "name": "Jiajun Liu"
              },
              {
                "authorId": "144282672",
                "name": "Peng Wang"
              },
              {
                "authorId": "2125194341",
                "name": "Ziyu Shang"
              },
              {
                "authorId": "2151101491",
                "name": "Chen Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 263829656,
          "isinfluential": false,
          "contexts": [
            "…event triggers and corresponding arguments from natural language texts, facilitating various downstream applications, such as information retrieval [Li et al. , 2023], recommender systems [Lu et al. , 2016], and question answering [Liu et al. , 2023] in finance, healthcare, and law industries."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
            "abstract": "Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the following findings: (i) \\textsc{SumAsk} consistently and significantly improves LLMs performance on different model sizes, benchmarks and settings; (ii) Zero-shot prompting with ChatGPT achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) LLMs deliver promising performance in extracting overlapping relations; (iv) The performance varies greatly regarding different relations. Different from small language models, LLMs are effective in handling challenge none-of-the-above (NoTA) relation.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "91119265",
                "name": "Guozheng Li"
              },
              {
                "authorId": "2257246692",
                "name": "Peng Wang"
              },
              {
                "authorId": "1596819256",
                "name": "Wenjun Ke"
              }
            ]
          }
        }
      ]
    },
    "273195146": {
      "citing_paper_info": {
        "title": "SIAT: Document-level Event Extraction via Spatiality-Augmented Interaction Model with Adaptive Thresholding",
        "abstract": "Document-level event extraction endeavors to automatically extract structural events from a given document. Many existing approaches focus on modeling entity interactions and decoding these interactions into events, assigning each entity as an event argument. However, these approaches encounter two primary limitations: they exclusively capture semantic dependencies to model entity interactions, overlooking the indication of the spatial distribution features of entities; they decode interactions imprecisely with a hard binary-classification boundary, potentially failing to calibrate micro differences in interactions. To overcome these limitations, we introduce a novel approach termed the Spatiality-augmented Interaction Model with Adaptive Thresholding (SIAT). Our method addresses the first limitation by calculating the relative position encoding of entities to represent spatial interaction features. These features are then integrated with multi-granularity semantic interactions, enhancing the modeling of entity interactions for each entity pair. Furthermore, we introduce an adaptive event decoding mechanism, which establishes a more flexible decision boundary for different entity interactions. Additionally, an adaptive loss function for threshold learning is designed to further refine the model. Experimental results demonstrate that our proposed method achieves competitive performance compared to state-of-the-art methods on two public event extraction datasets while maintaining considerable training efficiency.",
        "year": 2024,
        "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
        "authors": [
          {
            "authorId": "2324872161",
            "name": "Zekun Tao"
          },
          {
            "authorId": "2275820162",
            "name": "Changjian Wang"
          },
          {
            "authorId": "2260832680",
            "name": "Zhiliang Tian"
          },
          {
            "authorId": "2271261537",
            "name": "Kele Xu"
          },
          {
            "authorId": "2115273959",
            "name": "Yong Guo"
          },
          {
            "authorId": "2298565274",
            "name": "Shanshan Li"
          },
          {
            "authorId": "2289504800",
            "name": "Yanru Bai"
          },
          {
            "authorId": "2324859937",
            "name": "Da Xie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 17,
        "influential_count": 1,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "52967399",
        "6628106",
        "119308902",
        "207853145",
        "252519442",
        "248496614",
        "252819226",
        "2778800",
        "12108307",
        "5590763",
        "51871198",
        "6452487",
        "268315840",
        "248721950",
        "14542261",
        "219683473",
        "244119148"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2778800,
          "isinfluential": false,
          "contexts": [
            "They also generated a large-scale corpus called ChiFinAnn through distant supervised alignment [7, 34, 49]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks",
            "abstract": "Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",
            "year": 2015,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5590763,
          "isinfluential": false,
          "contexts": [
            "Nguyen et al. [36] first used GRU [10] to better model the relationship of words within a sentence and predict both event triggers and arguments; Chen et al. [6] adopted LSTM [18] as the encoder and further proposed a dynamic pooling strategy similar to DMCNN. Compared with GRU, LSTM can…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
            "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "3158246",
                "name": "B. V. Merrienboer"
              },
              {
                "authorId": "1854385",
                "name": "Çaglar Gülçehre"
              },
              {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
              },
              {
                "authorId": "2076086",
                "name": "Fethi Bougares"
              },
              {
                "authorId": "144518416",
                "name": "Holger Schwenk"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "Nguyen et al. [36] first used GRU [10] to better model the relationship of words within a sentence and predict both event triggers and arguments; Chen et al. [6] adopted LSTM [18] as the encoder and further proposed a dynamic pooling strategy similar to DMCNN. Compared with GRU, LSTM can…",
            "Several prior studies have concentrated on sentence-level event extraction (SEE) [8, 13, 25, 30, 36, 38], where events are extracted from individual sentences."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "We empirically use the Adam [21] optimizer with a learning rate of 5e-4."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "They also generated a large-scale corpus called ChiFinAnn through distant supervised alignment [7, 34, 49]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14542261,
          "isinfluential": false,
          "contexts": [
            "Chen et al. [8] proposed DMCNN, which uses multiple pooling to improve the CNN model [24]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Gradient-based learning applied to document recognition",
            "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
            "year": 1998,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "1688882",
                "name": "Yann LeCun"
              },
              {
                "authorId": "52184096",
                "name": "L. Bottou"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              },
              {
                "authorId": "1721248",
                "name": "P. Haffner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "Yang et al. [44] proposed the first end-to-end DEE framework with key-event detection."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Aligned with the previous state-of-the-art approach ReDEE [27], the token sequences in the document are first encoded by the BERT [11] encoder.",
            "Numerous approaches leverage pre-trained language models like BERT [11], built upon the Transformer [39], for event extraction."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": true,
          "contexts": [
            "Entity types have been proved effective for downstream sub-modules [41, 50].",
            "To fully evaluate our approach, we compare with several existing baselines as follows: (a) Doc2EDAG [50] first formulates the DEE task as a table filling paradigm.",
            "Zheng et al. [50] proposed Doc2EDAG, which constructs the entity-based directed acyclic graph (EDAG) by expanding paths autoregressively to extract event records.",
            "Each path expansion constitutes a binary classification task utilizing previously modeled entity interactions [27, 41, 50], with the assigned entity becoming an argument.",
            "In this paper, we carried out experiments on two public Chinese datasets: ChiFinAnn [50] and DuEE-Fin [16].",
            "Most existing DEE approaches [27, 41, 46, 50, 51] typically employ pipeline models designed to harness entity interactions across sentences in addressing the aforementioned challenges."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "Ebner et al. [13] regarded the event argument extraction as an argument linking task firstly and proposed a novel dataset named RAMS for evaluation.",
            "Several prior studies have concentrated on sentence-level event extraction (SEE) [8, 13, 25, 30, 36, 38], where events are extracted from individual sentences."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            "A Conditional Random Field (CRF) [23] layer is followed to classify labels of named entities."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244119148,
          "isinfluential": false,
          "contexts": [
            "In addition, Huang and Jia [20] regarded each event record as a sentence community, events were extracted by community detection."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Exploring Sentence Community for Document-Level Event Extraction",
            "abstract": "Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents, and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2131131685",
                "name": "Yusheng Huang"
              },
              {
                "authorId": "1819081375",
                "name": "Weijia Jia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": false,
          "contexts": [
            "Xu et al. [42] encoded the document from the global and local perspectives and employed AMR-guided semantic graphs to capture intra-sentence and inter-sentence semantic features of arguments, where Ren et al. [37] proposed a multi-event head attention network that assigns arguments to roles in…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248721950,
          "isinfluential": false,
          "contexts": [
            "In recent years, large-scale pre-trained language models were employed to generate event records by filling predefined templates [12, 25, 29, 32]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Dynamic Prefix-Tuning for Generative Template-based Event Extraction",
            "abstract": "We consider event extraction in a generative manner with template-based conditional generation.Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information.In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context.Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              },
              {
                "authorId": "2067725506",
                "name": "Ge Shi"
              },
              {
                "authorId": "2217713470",
                "name": "Bo Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252519442,
          "isinfluential": false,
          "contexts": [
            "CRS [40] integrated entity-level and contextual-level representations with time-aware attention for user preference modeling, and utilized pre-trained BART for generation module initialization to enhance context modeling and mitigate data scarcity issues."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Improving Conversational Recommender System Via Contextual and Time-Aware Modeling With Less Domain-Specific Knowledge",
            "abstract": "Conversational Recommender Systems (CRS) has become an emerging research topic seeking to perform recommendations through interactive conversations, which generally consist of generation and recommendation modules. Prior work on CRS tends to incorporate more external and domain-specific knowledge like item reviews to enhance performance. Despite the fact that the collection and annotation of the external domain-specific information needs much human effort and degenerates the generalizability, too much extra knowledge introduces more difficulty to balance among them. Therefore, we propose to fully discover and extract the internal knowledge from the context. We capture both entity-level and contextual-level representations to jointly model user preferences for the recommendation, where a time-aware attention is designed to emphasize the recently appeared items in entity-level representations. We further use the pre-trained BART to initialize the generation module to alleviate the data scarcity and enhance the context modeling. In addition to conducting experiments on a popular dataset (ReDial), we also include a multi-domain dataset (OpenDialKG) to show the effectiveness of our model. Experiments on both datasets show that our model achieves better performance on most evaluation metrics with less external knowledge and generalizes well to other domains. Additional analyses on the recommendation and generation tasks demonstrate the effectiveness of our model in different scenarios.",
            "year": 2022,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "authors": [
              {
                "authorId": "48170367",
                "name": "Lingzhi Wang"
              },
              {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
              },
              {
                "authorId": "145816335",
                "name": "Wei Gao"
              },
              {
                "authorId": "46180553",
                "name": "Xingshan Zeng"
              },
              {
                "authorId": "1784988",
                "name": "Kam-Fai Wong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252819226,
          "isinfluential": false,
          "contexts": [
            "…and local perspectives and employed AMR-guided semantic graphs to capture intra-sentence and inter-sentence semantic features of arguments, where Ren et al. [37] proposed a multi-event head attention network that assigns arguments to roles in event-specific subspace with different attention heads."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "47184362",
                "name": "Yanan Cao"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2075394870",
                "name": "Ping Guo"
              },
              {
                "authorId": "1390641501",
                "name": "Zheng Lin"
              },
              {
                "authorId": "2185915076",
                "name": "Wei Ma"
              },
              {
                "authorId": "2153629743",
                "name": "Yi Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268315840,
          "isinfluential": false,
          "contexts": [
            "TS-AGCMM TS-AGCMM [9] was proposed for sleep stage classification using multichannel brain recordings."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Temporal Self-Attentional and Adaptive Graph Convolutional Mixed Model for Sleep Staging",
            "abstract": "Evaluating sleep quality through reliable sleep staging is of paramount importance. Although many studies reached fair performances in sleep stage classification, effectively leveraging the spatial–temporal characteristics derived from multichannel brain recordings remains challenging. We develop a novel temporal self-attentional and adaptive graph convolutional mixed model (TS-AGCMM), comprising a feature extraction module (FEM), dynamic time warping (DTW)-based attention module, temporal context module (TCM), and adaptive graph convolutional module (AGCM) in this study. First, the FEM enables capturing representative information from raw data. Then, the DTW-based attention module utilizes a dynamic programming algorithm to enhance the spatial information expression ability of extracted features. The TCM includes multihead attention mechanisms that effectively capture temporal dependencies. In particular, we employ an attention module named normalization-based attention module (NAM), which utilizes the contributing factors of weights to suppress less salient information. Meanwhile, the AGCM can obtain optimal spatial functional connections between polysomnography (PSG) channels, which benefit from the adaptive learning property of the adjacency matrix. Finally, we fuse the temporal and spatial features by concat operation to obtain the prediction results. We utilize the Montreal archive of sleep studies (MASS) and ISRUC-S3 to assess TS-AGCMM. The TS-AGCMM exhibits performance comparable to other currently available approaches as per our results, achieving an accuracy of 89.1% and 81.2%, a macroaveraging F1-score of 84.7% and 79.5%, as well as a Cohen’s kappa coefficient of 83.9% and 75.8% on the two databases, respectively.",
            "year": 2024,
            "venue": "IEEE Sensors Journal",
            "authors": [
              {
                "authorId": "2290702854",
                "name": "Ziyang Chen"
              },
              {
                "authorId": "2179635477",
                "name": "Wenbin Shi"
              },
              {
                "authorId": "2221682159",
                "name": "Xianchao Zhang"
              },
              {
                "authorId": "49136477",
                "name": "C. Yeh"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Björne and Salakoski [2] used a CNN to obtain a unified linear sentence representation, including semantic embeddings, positional embeddings, and dependency path embeddings."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "272950526": {
      "citing_paper_info": {
        "title": "Document-level event extraction for fused contextual semantic paragraph segmentation",
        "abstract": "Current approaches to document-level event extraction face challenges such as scattered argument parameters across sentences and multiple event records within a single document. In this study, we propose a solution that combines context paragraph information modeling with argument extraction to tackle these issues. Firstly, we employ advanced techniques for segmenting lengthy documents into smaller parts. Additionally, we utilize pre-trained models to obtain paragraph vector matrices for each segment. By integrating these models, annotated sentence vector matrices and paragraph matrices are combined through a dot product fusion model and concatenated together to generate improved sentence vector embeddings using pooling operations. Next, the entities extracted through named entity recognition are used to select pseudo-trigger words and construct adjacency similarity matrices and composite graphs. Finally, the task of populating event table elements is transformed into a multi-label classification task to address the issue of overlapping roles. Experiments on the chFinAnn dataset demonstrate an overall F1 improvement of 1.5 percentage points, with a significant increase in the model's inference speed.",
        "year": 2024,
        "venue": "4th International Conference on Information Science, Electrical and Automation Engineering",
        "authors": [
          {
            "authorId": "2323228982",
            "name": "JunGuang Zhu"
          },
          {
            "authorId": "2323347506",
            "name": "LiSheng Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "61154509",
        "212648787",
        "247084444",
        "231632837"
      ],
      "citation_details": [
        {
          "citedcorpusid": 61154509,
          "isinfluential": false,
          "contexts": [
            "Subsequent studies utilized joint models [2] to mitigate error propagation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Encyclopedia of Big Data Technologies",
            "abstract": "",
            "year": 2019,
            "venue": "Cambridge International Law Journal",
            "authors": [
              {
                "authorId": "1783693",
                "name": "Sherif Sakr"
              },
              {
                "authorId": "9392149",
                "name": "Albert Y. Zomaya"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212648787,
          "isinfluential": false,
          "contexts": [
            "The last hidden state [4] in the representation direction for each sentence is then concatenated to obtain a semantically enhanced sentence vector representation By performing random queries in graph , this paper transforms the task into a multi-label classification task."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Text Classification Using Single-Layer Multisize Filters Convolutional Neural Network",
            "abstract": "The rapid growth of electronic documents are causing problems like unstructured data that need more time and effort to search a relevant document. Text Document Classification (TDC) has a great significance in information processing and retrieval where unstructured documents are organized into pre-defined classes. Urdu is the most favorite research language in South Asian languages because of its complex morphology, unique features, and lack of linguistic resources like standard datasets. As compared to short text, like sentiment analysis, long text classification needs more time and effort because of large vocabulary, more noise, and redundant information. Machine Learning (ML) and Deep Learning (DL) models have been widely used in text processing. Despite the major limitations of ML models, like learn directed features, these are the favorite methods for Urdu TDC. To the best of our knowledge, it is the first study of Urdu TDC using DL model. In this paper, we design a large multi-purpose and multi-format dataset that contain more than ten thousand documents organize into six classes. We use Single-layer Multisize Filters Convolutional Neural Network (SMFCNN) for classification and compare its performance with sixteen ML baseline models on three imbalanced datasets of various sizes. Further, we analyze the effects of preprocessing methods on SMFCNN performance. SMFCNN outperformed the baseline classifiers and achieved 95.4%, 91.8%, and 93.3% scores of accuracy on medium, large and small size dataset respectively. The designed dataset would be publically and freely available in different formats for future research in Urdu text processing.",
            "year": 2020,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "69936718",
                "name": "Muhammad Pervez Akhter"
              },
              {
                "authorId": "3227620",
                "name": "Jiangbin Zheng"
              },
              {
                "authorId": "41071460",
                "name": "Irfan Raza Naqvi"
              },
              {
                "authorId": "1557620636",
                "name": "Mohammed Abdelmajeed"
              },
              {
                "authorId": "50641421",
                "name": "Atif Mehmood"
              },
              {
                "authorId": "119806457",
                "name": "M. Sadiq"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231632837,
          "isinfluential": false,
          "contexts": [
            "(2) While the Doc2EDAG model [9] integrates information from sentences and entities by incorporating the Transform model, it overlooks the interdependence between events, lacking the ability to model multiple events in dispersed documents."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A survey on extraction of causal relations from natural language text",
            "abstract": "As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.",
            "year": 2021,
            "venue": "Knowledge and Information Systems",
            "authors": [
              {
                "authorId": "2118579490",
                "name": "Jie Yang"
              },
              {
                "authorId": "2046142",
                "name": "S. Han"
              },
              {
                "authorId": "144179461",
                "name": "Josiah Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": false,
          "contexts": [
            "By comparing models at different levels, we conducted the following comparative experiments to validate the effectiveness of the proposed model: (1) The DCFEE model [8] employs a remote supervision approach to address the issue of insufficient document data extraction."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        }
      ]
    },
    "259370740": {
      "citing_paper_info": {
        "title": "Joint Document-Level Event Extraction via Token-Token Bidirectional Event Completed Graph",
        "abstract": "We solve the challenging document-level event extraction problem by proposing a joint exaction methodology that can avoid inefficiency and error propagation issues in classic pipeline methods. Essentially, we address the three crucial limitations in existing studies. First, the autoregressive strategy of path expansion heavily relies on the orders of argument role. Second, the number of events in documents must be specified in advance. Last, unexpected errors usually exist when decoding events based on the entity-entity adjacency matrix. To address these issues, this paper designs a Token-Token Bidirectional Event Completed Graph (TT-BECG) in which the relation eType-Role1-Role2 serves as the edge type, precisely revealing which tokens play argument roles in an event of a specific event type. Exploiting the token-token adjacency matrix of the TT-BECG, we develop an edge-enhanced joint document-level event extraction model. Guided by the target token-token adjacency matrix, the predicted token-token adjacency matrix can be obtained during the model training. Then, extracted events and event records in a document are decoded based on the predicted matrix, including the graph structure and edge type decoding. Extensive experiments are conducted on two public datasets, and the results confirm the effectiveness of our method and its superiority over the state-of-the-art baselines.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2220964416",
            "name": "Qizhi Wan"
          },
          {
            "authorId": "1761110",
            "name": "Changxuan Wan"
          },
          {
            "authorId": "2144212496",
            "name": "Keli Xiao"
          },
          {
            "authorId": "48929153",
            "name": "Dexi Liu"
          },
          {
            "authorId": "2829009",
            "name": "Chenliang Li"
          },
          {
            "authorId": "2790268",
            "name": "Bolong Zheng"
          },
          {
            "authorId": "2110755755",
            "name": "Xiping Liu"
          },
          {
            "authorId": "2160045172",
            "name": "Rong Hu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "249431954",
        "248325339",
        "196178503",
        "52967399",
        "53081291",
        "235458429",
        "6452487",
        "14339673"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "Both sentence-level (Chen et al., 2015; Nguyen et al., 2016) and document-level (Zheng et al., 2019; Zhu et al., 2022) event extraction encoded the entity type feature, verifying its significance."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Both sentence-level (Chen et al., 2015; Nguyen et al., 2016) and document-level (Zheng et al., 2019; Zhu et al., 2022) event extraction encoded the entity type feature, verifying its significance."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "In this paper, the BERT (Devlin et al., 2019) is used to initialize token embedding, and the vector of i -th token w i is denoted as v i ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53081291,
          "isinfluential": false,
          "contexts": [
            "Following previous studies (Chen et al., 2018; Wan et al., 2023a), given that the number of “O” tags is much larger than that of other relation tags, the standard cross-entropy loss with weight is used as our objective function to strengthen the influence of relation tags: where n is the number of…",
            "Consistent with previous work, the Bi-LSTM network can capture the sequence structure information between tokens well, which is conducive to event extraction (Chen et al., 2018; Wan et al., 2023a)."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms",
            "abstract": "Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2861442",
                "name": "Yantao Jia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "Sentence-level event extraction approaches (Sha et al., 2018; Yang et al., 2019; Lu et al., 2021; Wan et al., 2021, 2023a) are difficult to deal with the problem of arguments across sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "Sentence-level event extraction approaches (Sha et al., 2018; Yang et al., 2019; Lu et al., 2021; Wan et al., 2021, 2023a) are difficult to deal with the problem of arguments across sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248325339,
          "isinfluential": false,
          "contexts": [
            "Consistent with previous work, the Bi-LSTM network can capture the sequence structure information between tokens well, which is conducive to event extraction (Chen et al., 2018; Wan et al., 2023a)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Multi-channel Hierarchical Graph Attention Network for Open Event Extraction",
            "abstract": "Event extraction is an essential task in natural language processing. Although extensively studied, existing work shares issues in three aspects, including (1) the limitations of using original syntactic dependency structure, (2) insufficient consideration of the node level and type information in Graph Attention Network (GAT), and (3) insufficient joint exploitation of the node dependency type and part-of-speech (POS) encoding on the graph structure. To address these issues, we propose a novel framework for open event extraction in documents. Specifically, to obtain an enhanced dependency structure with powerful encoding ability, our model is capable of handling an enriched parallel structure with connected ellipsis nodes. Moreover, through a bidirectional dependency parsing graph, it considers the sequence of order structure and associates the ancestor and descendant nodes. Subsequently, we further exploit node information, such as the node level and type, to strengthen the aggregation of node features in our GAT. Finally, based on the coordination of triple-channel features (i.e., semantic, syntactic dependency and POS), the performance of event extraction is significantly improved. Extensive experiments are conducted to validate the effectiveness of our method, and the results confirm its superiority over the state-of-the-art baselines. Furthermore, in-depth analyses are provided to explore the essential factors determining the extraction performance.",
            "year": 2022,
            "venue": "ACM Trans. Inf. Syst.",
            "authors": [
              {
                "authorId": "115377342",
                "name": "Qi-Zhi Wan"
              },
              {
                "authorId": "1761110",
                "name": "Changxuan Wan"
              },
              {
                "authorId": "2144212496",
                "name": "Keli Xiao"
              },
              {
                "authorId": "2160045172",
                "name": "Rong Hu"
              },
              {
                "authorId": "48929153",
                "name": "Dexi Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249431954,
          "isinfluential": false,
          "contexts": [
            "…extraction generally adopted the pipeline pattern (Zheng et al., 2019; Xu et al., 2021; Yang et al., 2021; Huang and Jia, 2021; Zhu et al., 2022; Liang et al., 2022), which decomposes the task into the following sub-tasks: (1) entity extraction (obtaining candidate arguments from a document),…",
            "In terms of implementation strategies, the meth-ods based on graph decoding are mainly divided into entity-based directed acyclic graph (Zheng et al., 2019; Xu et al., 2021; Liang et al., 2022) and pseudo-trigger-aware pruned complete graph (Zhu et al., 2022)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction",
            "abstract": "In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117874913",
                "name": "Yuan Liang"
              },
              {
                "authorId": "3435199",
                "name": "Zhuoxuan Jiang"
              },
              {
                "authorId": "2168284967",
                "name": "Di Yin"
              },
              {
                "authorId": "2064646914",
                "name": "Bo Ren"
              }
            ]
          }
        }
      ]
    },
    "274023993": {
      "citing_paper_info": {
        "title": "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction",
        "abstract": ",",
        "year": 2024,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2330967861",
            "name": "Minghui Liu"
          },
          {
            "authorId": "152439499",
            "name": "Meihan Tong"
          },
          {
            "authorId": "2330765694",
            "name": "Yangda Peng"
          },
          {
            "authorId": "2301943823",
            "name": "Lei Hou"
          },
          {
            "authorId": "2257585852",
            "name": "Juan-Zi Li"
          },
          {
            "authorId": "2330977239",
            "name": "Bin Xu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 1,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "198953378",
        "5421278",
        "12719479",
        "31791545",
        "258865260",
        "119297355",
        "222180086"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5421278,
          "isinfluential": false,
          "contexts": [
            "Following the studies of Artstein and Poesio (2008) and McHugh (2012), we used Cohen’s kappa coefficient to measure the inter-annotator agreement (IAA) for assessing annotation data consistency."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Interrater reliability: the kappa statistic",
            "abstract": "The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen’s kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from −1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen’s suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.",
            "year": 2012,
            "venue": "Biochemia Medica",
            "authors": [
              {
                "authorId": "2068306",
                "name": "M. McHugh"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12719479,
          "isinfluential": false,
          "contexts": [
            "Following the studies of Artstein and Poesio (2008) and McHugh (2012), we used Cohen’s kappa coefficient to measure the inter-annotator agreement (IAA) for assessing annotation data consistency."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Survey Article: Inter-Coder Agreement for Computational Linguistics",
            "abstract": "Abstract This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.",
            "year": 2008,
            "venue": "International Conference on Computational Logic",
            "authors": [
              {
                "authorId": "2038490",
                "name": "Ron Artstein"
              },
              {
                "authorId": "1678591",
                "name": "Massimo Poesio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 31791545,
          "isinfluential": false,
          "contexts": [
            "For instance, it can automatically detect and analyze major events in news reports, providing timely information for decision-makers (Tanev et al., 2008; Piskorski et al., 2007; Atkinson et al., 2013)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Techniques for Multilingual Security-Related Event Extraction from Online News",
            "abstract": "",
            "year": 2013,
            "venue": "Computational Linguistics",
            "authors": [
              {
                "authorId": "29526041",
                "name": "M. Atkinson"
              },
              {
                "authorId": "2942507",
                "name": "Mian Du"
              },
              {
                "authorId": "1678833",
                "name": "J. Piskorski"
              },
              {
                "authorId": "2185304",
                "name": "Hristo Tanev"
              },
              {
                "authorId": "3039512",
                "name": "R. Yangarber"
              },
              {
                "authorId": "2481036",
                "name": "Vanni Zavarella"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119297355,
          "isinfluential": false,
          "contexts": [
            "Evaluation metrics include Precision, Recall, and Macro-F1 score following (Kowsari et al., 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Text Classification Algorithms: A Survey",
            "abstract": "In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.",
            "year": 2019,
            "venue": "Inf.",
            "authors": [
              {
                "authorId": "2060243",
                "name": "Kamran Kowsari"
              },
              {
                "authorId": "26417934",
                "name": "K. Meimandi"
              },
              {
                "authorId": "26408890",
                "name": "Mojtaba Heidarysafa"
              },
              {
                "authorId": "51266150",
                "name": "Sanjana Mendu"
              },
              {
                "authorId": "1771388",
                "name": "Laura E. Barnes"
              },
              {
                "authorId": "2115530197",
                "name": "Donald E. Brown"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": true,
          "contexts": [
            "Precision Recall F1 Overall Performance Table 2 shows experimental results for event classification, highlighting: 1) Transformer-based models (BERT, RoBERTa, ERNIE 3.0) outperform TextCNN, benefiting from pretraining on large-scale unlabeled corpora and possessing extensive background semantic knowledge.",
            "3) RoBERTa (Liu et al., 2019) extends BERT with larger training batches and learning rates."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222180086,
          "isinfluential": false,
          "contexts": [
            "The DuEE dataset (Li et al., 2020) consists of 19,640 events divided into 65 event types and 121 argument roles, offering a rich resource for detailed analysis."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "DuEE: A Large-Scale Dataset for Chinese Event Extraction in Real-World Scenarios",
            "abstract": "",
            "year": 2020,
            "venue": "Natural Language Processing and Chinese Computing",
            "authors": [
              {
                "authorId": "2118335351",
                "name": "Xinyu Li"
              },
              {
                "authorId": "2143520637",
                "name": "Fayuan Li"
              },
              {
                "authorId": "2114093526",
                "name": "Lu Pan"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "143906199",
                "name": "Quan Wang"
              },
              {
                "authorId": "8020700",
                "name": "Yajuan Lyu"
              },
              {
                "authorId": "2116512598",
                "name": "Yong Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258865260,
          "isinfluential": false,
          "contexts": [
            "GENEVA (Parekh et al., 2022) is a large-scale benchmarking dataset designed to evaluate the generalizability of Event Argument Extraction (EAE) models, featuring 115 event types and 220 argument roles."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles",
            "abstract": "Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites aimed at evaluating models’ ability to handle limited data and unseen event type generalization. We benchmark six EAE models from various families. The results show that owing to non-entity argument roles, even the best-performing model can only achieve 39% F1 score, indicating how GENEVA provides new challenges for generalization in EAE. Overall, our large and diverse EAE ontology can aid in creating more comprehensive future resources, while GENEVA is a challenging benchmarking dataset encouraging further research for improving generalizability in EAE. The code and data can be found at https://github.com/PlusLabNLP/GENEVA.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46719088",
                "name": "Tanmay Parekh"
              },
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        }
      ]
    },
    "236460259": {
      "citing_paper_info": {
        "title": "Document-level Event Extraction via Parallel Prediction Networks",
        "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
        "year": 2021,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "1845787839",
            "name": "Hang Yang"
          },
          {
            "authorId": "1381062467",
            "name": "Dianbo Sui"
          },
          {
            "authorId": "152829071",
            "name": "Yubo Chen"
          },
          {
            "authorId": "77397868",
            "name": "Kang Liu"
          },
          {
            "authorId": "11447228",
            "name": "Jun Zhao"
          },
          {
            "authorId": "1799672",
            "name": "Taifeng Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 18,
        "influential_count": 1,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "2213149",
        "3480671",
        "12108307",
        "9776219",
        "9426884",
        "950755",
        "51878680",
        "218630327",
        "221246218",
        "6628106",
        "207853145",
        "2367456",
        "220046861",
        "1240016",
        "43095407",
        "2114517",
        "6452487",
        "14117526"
      ],
      "citation_details": [
        {
          "citedcorpusid": 950755,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role ﬁllers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1240016,
          "isinfluential": false,
          "contexts": [
            "…Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Zero-Shot Transfer Learning for Event Extraction",
            "abstract": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "A great number of previous studies (Ahn, 2006; Ji and Grish-man, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the…",
            "These studies are mainly based on hand-designed features (Li et al., 2013; Kai and Gr-ishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Bj¨orne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020)."
          ],
          "intents": [
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2213149,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role ﬁllers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bootstrapped Training of Event Extraction Classifiers",
            "abstract": "",
            "year": 2012,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "…studies (Ahn, 2006; Ji and Grish-man, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most…",
            "Yang and Mitchell (2016) introduced event structure to jointly extract events and entities within a document."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3480671,
          "isinfluential": false,
          "contexts": [
            "All of these de-coders are based on the non-autoregressive mechanism (Gu et al., 2018), which supports the extraction of all events in parallel.",
            "Both of them are based on the non-autoregressive mechanism (Gu et al., 2018), which supports the extraction of multiple events in parallel."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Non-Autoregressive Neural Machine Translation",
            "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "3016273",
                "name": "Jiatao Gu"
              },
              {
                "authorId": "40518045",
                "name": "James Bradbury"
              },
              {
                "authorId": "2228109",
                "name": "Caiming Xiong"
              },
              {
                "authorId": "2052674293",
                "name": "V. Li"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "…great number of previous studies (Ahn, 2006; Ji and Grish-man, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level…",
            "These studies are mainly based on hand-designed features (Li et al., 2013; Kai and Gr-ishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Bj¨orne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020)."
          ],
          "intents": [
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "During training, we employ the AdamW optimizer (Kingma and Ba, 2014) with the learning rate 1e-5 with batch size 16."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9426884,
          "isinfluential": true,
          "contexts": [
            "Finally, for comparing extracted events to ground truths, we propose a matching loss function inspired by the Hungarian algorithm (Kuhn, 1955; Munkres, 1957).",
            "Inspired by the assigning problem in the operation research (Kuhn, 1955; Munkres, 1957), we propose a matching loss function, which can produce an optimal bipartite matching between predicted and ground-truth events."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Hungarian method for the assignment problem",
            "abstract": "",
            "year": 1955,
            "venue": "50 Years of Integer Programming",
            "authors": [
              {
                "authorId": "2941401",
                "name": "H. Kuhn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9776219,
          "isinfluential": false,
          "contexts": [
            "…Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation",
            "abstract": "",
            "year": 2004,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2862682",
                "name": "G. Doddington"
              },
              {
                "authorId": "2449760",
                "name": "A. Mitchell"
              },
              {
                "authorId": "2282719",
                "name": "Mark A. Przybocki"
              },
              {
                "authorId": "1744313",
                "name": "L. Ramshaw"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              },
              {
                "authorId": "1732071",
                "name": "R. Weischedel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "…and Grish-man, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14117526,
          "isinfluential": false,
          "contexts": [
            "These studies are mainly based on handdesigned features (Li et al., 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al.",
            "These studies are mainly based on hand-designed features (Li et al., 2013; Kai and Gr-ishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Bj¨orne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Detection with Abstract Meaning Representation",
            "abstract": "Event Detection (ED) aims to identify instances of specified types of events in text, which is a crucial component in the overall task of event extraction. The commonly used features consist of lexical, syntactic, and entity information, but the knowledge encoded in the Abstract Meaning Representation (AMR) has not been utilized in this task. AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. In this paper, we demonstrate the effectiveness of AMR to capture and represent the deeper semantic contexts of the trigger words in this task. Experimental results further show that adding AMR features on top of the traditional features can achieve 67.8% (with 2.1% absolute improvement) F-measure (F1), which is comparable to the state-of-the-art approaches.",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2144438930",
                "name": "Xiang Li"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "2065281098",
                "name": "Kai Cao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 43095407,
          "isinfluential": false,
          "contexts": [
            "A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Two-step approach.",
            "abstract": "The authors reference as the basis for their summary the different evidence based guidelines that have been developed over recent years, and a literature search, whose search strategy included, among others, all publications from the past 10 years identified by using the search terms “celiac disease” and “diagnosis” (1). Unfortunately they did not include our article that was published in 2013, even though this article was based on long years of experience and aimed to reach a definitive diagnosis using a minimum number of biopsies (2). \n \nThe best diagnostic test is that which results in the fewest false-positive and false-negative diagnoses; for this reason we’d suggest the following approach: \n \n \nThe first step: simultaneous measuring of IgA and IgG antibodies specific for deamidated gliadin peptides, IgA antibodies specific for human tissue transglutaminase (in addition, total IgA). Most patients will either have a positive reaction to all three tested antigens or will test negative to all three of the specific antibody tests. In both these groups, biopsy is therefore unnecessary, since the positive predictive value (ppv) is 99% and the positive likelihood ratio (lr+) 87, whereas the negative predictive value (npv) is 98% and the negative likelihood ratio (lr-) 0.01. The results become even more meaningful (ppv 99%, lr+ 86; npv 100%, lr- 0.00) (2) if a fourth test is done for IgA endomysial–specific antibodies (2). \n \n \nThe second step is small bowel biopsy. It is necessary only in patients with contradictory antibody results—that is, in patients who were positive in one or two tests only. This “two-step approach” reduces the proportion of patients requiring a biopsy to one-fifth (3, 4).",
            "year": 2014,
            "venue": "Deutsches Ärzteblatt International",
            "authors": [
              {
                "authorId": "1404969664",
                "name": "A. Bürgin-Wolffh"
              },
              {
                "authorId": "7317275",
                "name": "F. Hadžiselimović"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51878680,
          "isinfluential": false,
          "contexts": [
            "These studies are mainly based on hand-designed features (Li et al., 2013; Kai and Gr-ishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Bj¨orne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Biomedical Event Extraction Using Convolutional Neural Networks and Dependency Parsing",
            "abstract": "Event and relation extraction are central tasks in biomedical text mining. Where relation extraction concerns the detection of semantic connections between pairs of entities, event extraction expands this concept with the addition of trigger words, multiple arguments and nested events, in order to more accurately model the diversity of natural language. In this work we develop a convolutional neural network that can be used for both event and relation extraction. We use a linear representation of the input text, where information is encoded with various vector space embeddings. Most notably, we encode the parse graph into this linear space using dependency path embeddings. We integrate our neural network into the open source Turku Event Extraction System (TEES) framework. Using this system, our machine learning model can be easily applied to a large set of corpora from e.g. the BioNLP, DDI Extraction and BioCreative shared tasks. We evaluate our system on 12 different event, relation and NER corpora, showing good generalizability to many tasks and achieving improved performance on several corpora.",
            "year": 2018,
            "venue": "Workshop on Biomedical Natural Language Processing",
            "authors": [
              {
                "authorId": "1761599",
                "name": "Jari Björne"
              },
              {
                "authorId": "1680811",
                "name": "T. Salakoski"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role ﬁllers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": false,
          "contexts": [
            "A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221246218,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role ﬁllers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event-based Extraction Using Generative Template-filling Transformers",
            "abstract": "We revisit the classic information extraction problem of document-level template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is \\emph{implicitly} aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions. Our code for the evaluation script and models will be open-sourced at this https URL for reproduction purposes.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        }
      ]
    },
    "264439234": {
      "citing_paper_info": {
        "title": "Probing Representations for Document-level Event Extraction",
        "abstract": "The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.",
        "year": 2023,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2160611160",
            "name": "Barry Wang"
          },
          {
            "authorId": "13728923",
            "name": "Xinya Du"
          },
          {
            "authorId": "2064285348",
            "name": "Claire Cardie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 2,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "211268325",
        "235097664",
        "204838007",
        "6771196",
        "155092004"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6771196,
          "isinfluential": false,
          "contexts": [
            "Similar to the sentence length task proposed by Adi et al. (2017), we employed a word count ( WordCt ) task and a sentence count ( SentCt ) task, each predicts the number of words and sentences in the text respectively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
            "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.",
            "year": 2016,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2342123860",
                "name": "Yossi Adi"
              },
              {
                "authorId": "2098460",
                "name": "Einat Kermany"
              },
              {
                "authorId": "2083259",
                "name": "Yonatan Belinkov"
              },
              {
                "authorId": "3120346",
                "name": "Ofer Lavi"
              },
              {
                "authorId": "2089067",
                "name": "Yoav Goldberg"
              }
            ]
          }
        },
        {
          "citedcorpusid": 155092004,
          "isinfluential": false,
          "contexts": [
            "Encoding layers Lastly, we experiment to locate the encoding of IE information in different layers of the encoders, a common topic in previous works (Tenney et al., 2019a).",
            "…lookup style embeddings such as GloVe (Pen-nington et al., 2014), these neural contextualized representations are inherently difficult to interpret, leading to ongoing research efforts focused on analyzing their encoded information (Tenney et al., 2019b; Zhou and Srikumar, 2021; Belinkov, 2022)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BERT Rediscovers the Classical NLP Pipeline",
            "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "6117577",
                "name": "Ian Tenney"
              },
              {
                "authorId": "143790066",
                "name": "Dipanjan Das"
              },
              {
                "authorId": "2949185",
                "name": "Ellie Pavlick"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "TANL (Paolini et al., 2021) is a multi-task sequence-to-sequence model that fine-tunes T5 model (Raffel et al., 2020) to translate text input to augmented natural languages, with the in-text augmented parts extracted to be triggers and roles."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 211268325,
          "isinfluential": true,
          "contexts": [
            "This work is inspired by various sentence-level embedding interpretability works, including Conneau et al. (2018) and Alt et al. (2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Probing Linguistic Features of Sentence-Level Representations in Relation Extraction",
            "abstract": "Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3413117",
                "name": "Christoph Alt"
              },
              {
                "authorId": "3449621",
                "name": "Aleksandra Gabryszak"
              },
              {
                "authorId": "36943315",
                "name": "Leonhard Hennig"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235097664,
          "isinfluential": true,
          "contexts": [
            "Additionally, we focus on template-filling-capable IE frameworks as they show more generality in applications (and is supported by more available models like GTT), barring classical relation extraction task dataset like the DocRED(Yao et al., 2019).",
            "In addition to above observations, we see that DyGIE++ and GTT document embeddings capture event information (EvntCt ↑ ) only marginally better than the baseline, whereas the TANL-finetuned encoder often has subpar performance across tasks.",
            "GTT (Du et al., 2021) is a sequence-to-sequence event-extraction model that perform the task end-to-end, without the need of labeled triggers.",
            "Using GTT with the same hyperparameter in its publication, its finetuned encoder shows semantic information encoding mostly (0-indexed) up to layer 7 (IsArg ↑ , ArgTyp ↑ ), meanwhile, event detection capability increases throughout the encoder (CoEvnt ↑ , EvntCt ↑ )."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Template Filling with Generative Transformers",
            "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        }
      ]
    },
    "260386477": {
      "citing_paper_info": {
        "title": "A Trigger-Free Method Enhanced by Coreference Information for Document-Level Event Extraction",
        "abstract": "Document-level event extraction (DEE) aims to extract structured event information from a document. Previous document-level event extraction methods relied on trigger annotation, which was very expensive and time-consuming. In addition, entity representation plays an important role in the overall event extraction task, but we found that the previous work could not effectively use entity abbreviations and coreference information, which limited the representation ability of event-related entities in documents. Based on the above two aspects, we propose a new model named TFECI. In our model, we propose an efficient and effective method to make full use of abbreviations and coreference information to better exert the representational ability of event-related entities. Besides, triggers play an extremely central role in an event, but trigger annotation is often very difficult, so we propose a new strategy to select pseudo-trigger automatically. Experiments show that, compared with the previous system, our system can extract events without trigger annotation and achieve competitive results.",
        "year": 2023,
        "venue": "IEEE International Joint Conference on Neural Network",
        "authors": [
          {
            "authorId": "2215402248",
            "name": "Fumin Chen"
          },
          {
            "authorId": "50141074",
            "name": "Xu Wang"
          },
          {
            "authorId": "2213119745",
            "name": "Xiaohui Liu"
          },
          {
            "authorId": "1800117",
            "name": "Dezhong Peng"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 9,
        "influential_count": 1,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "6628106",
        "235253912",
        "6452487",
        "257019152",
        "12108307",
        "174800619",
        "216562330",
        "51871198",
        "2367456"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "[23] use welldefined features to model and learn event-argument relations between different sentences in a document."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "[2] use two different recurrent neural networks to learn sentence representation and conduct joint event extraction, that is, extract triggers and argument roles simultaneously.",
            "Most of the previous work has focused on event extraction at the sentence level [1], [2], [3], [4], extracting events from individual sentences."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "We use the Adam optimizer [29] to optimize the model while using the learning rate of 5e-4 and set the batch size to 64."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "tion datasets usually use distantly supervised and knowledge base [8], [9] to annotate triggers, which are missing and the"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": true,
          "contexts": [
            "DCFEE-O extracts only one event record from the document and DCFEE-M extracts as many event records as possible.",
            "1) DCFEE [10] has two variants: DCFEEO extracts only one event record from the document and DCFEE-M extracts as many event records as possible.",
            "Among all the models, TFECI is the lightest one and is on the same scale with DCFEE [10], while TFECI surpasses",
            "Among all the models, TFECI is the lightest one and is on the same scale with DCFEE [10], while TFECI surpasses DCFEE-O with about 15.9 absolute gain in F1 score on the ChFinAnn dataset.",
            "[10] first identify a key sentence in the document and then fill the event table by looking for arguments close to the key sentence, but this approach misses the context and global arguments information."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174800619,
          "isinfluential": false,
          "contexts": [
            "Different from the trigger-based methods [24], [25]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Detection without Triggers",
            "abstract": "The goal of event detection (ED) is to detect the occurrences of events and categorize them. Previous work solved this task by recognizing and classifying event triggers, which is defined as the word or phrase that most clearly expresses an event occurrence. As a consequence, existing approaches required both annotated triggers and event types in training data. However, triggers are nonessential to event detection, and it is time-consuming for annotators to pick out the “most clearly” word from a given sentence, especially from a long sentence. The expensive annotation of training corpus limits the application of existing approaches. To reduce manual effort, we explore detecting events without triggers. In this work, we propose a novel framework dubbed as Type-aware Bias Neural Network with Attention Mechanisms (TBNNAM), which encodes the representation of a sentence based on target event types. Experimental results demonstrate the effectiveness. Remarkably, the proposed approach even achieves competitive performances compared with state-of-the-arts that used annotated triggers.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "98177814",
                "name": "Y. Li"
              },
              {
                "authorId": "2182281394",
                "name": "Feng Zhang"
              },
              {
                "authorId": "2122901214",
                "name": "Tao Yang"
              },
              {
                "authorId": "3005633",
                "name": "Xinpeng Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "[21] discuss how to do event types detection without triggers.",
            "[21] are the first to apply the QuestionAnswer method to solve the problem of event extraction."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": false,
          "contexts": [
            "Because mention type features have been shown to improve the performance on downstream tasks [5], [28], we convert mention type features into vectors by looking up the embedding table."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257019152,
          "isinfluential": false,
          "contexts": [
            "A lot of work has obtained good performance by using machine learning algorithm[12], [13], [14], [15], [16], [17] for feature extraction and information mining."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CLSEP: Contrastive learning of sentence embedding with prompt",
            "abstract": "",
            "year": 2023,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "2208546591",
                "name": "Qian Wang"
              },
              {
                "authorId": "2208624997",
                "name": "Weiqi Zhang"
              },
              {
                "authorId": "2156742878",
                "name": "Tianyi Lei"
              },
              {
                "authorId": "144149886",
                "name": "Yu Cao"
              },
              {
                "authorId": "1800117",
                "name": "Dezhong Peng"
              },
              {
                "authorId": "50141074",
                "name": "Xu Wang"
              }
            ]
          }
        }
      ]
    },
    "256385289": {
      "citing_paper_info": {
        "title": "Exploiting event-aware and role-aware with tree pruning for document-level event extraction",
        "abstract": "",
        "year": 2023,
        "venue": "Neural computing & applications (Print)",
        "authors": [
          {
            "authorId": "4595544",
            "name": "Jianwei Lv"
          },
          {
            "authorId": "151473773",
            "name": "Zequn Zhang"
          },
          {
            "authorId": "3287827",
            "name": "Guangluan Xu"
          },
          {
            "authorId": "2112376498",
            "name": "Xian Sun"
          },
          {
            "authorId": "2109104284",
            "name": "Shuchao Li"
          },
          {
            "authorId": "2116620216",
            "name": "Qing Liu"
          },
          {
            "authorId": "2158049043",
            "name": "Pengcheng Dong"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 14,
        "influential_count": 1,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "225160964",
        "209948413",
        "225157900",
        "70346512",
        "9778664",
        "216562330",
        "2213149",
        "2941631",
        "51871198",
        "238590886",
        "226262283",
        "218630327",
        "12108307",
        "2367456"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2213149,
          "isinfluential": false,
          "contexts": [
            "Patwardhan and Riloff [32], Huang and Riloff [33, 34] manually design linguistic features to extract the role ﬁllers with the local and additional context.",
            "Patwardhan and Riloff [32], Huang and Riloff [33, 34] manually design linguistic features to extract the role fillers with the local and additional context."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bootstrapped Training of Event Extraction Classifiers",
            "abstract": "",
            "year": 2012,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "Yang and Mitchell [36] use well-deﬁned features to handle the event-argument relations across sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2941631,
          "isinfluential": false,
          "contexts": [
            "Many previous feature-based methods focus on designing various hand-crafted features to extract events, such as lexical features and syntactic features [10–12]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Dependency Parsing",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2240597",
                "name": "David McClosky"
              },
              {
                "authorId": "1760868",
                "name": "M. Surdeanu"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9778664,
          "isinfluential": false,
          "contexts": [
            "To utilize more knowledge, some studies adopt pre-trained language model [2], document context [22, 24–26], and explicit external knowledge [27, 28] such as WordNet [29]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploiting Document Level Information to Improve Event Detection via Recurrent Neural Networks",
            "abstract": "",
            "year": 2017,
            "venue": "International Joint Conference on Natural Language Processing",
            "authors": [
              {
                "authorId": "31588168",
                "name": "Shaoyang Duan"
              },
              {
                "authorId": "1724097",
                "name": "Ruifang He"
              },
              {
                "authorId": "2118225775",
                "name": "Wenli Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "Previous researches [1–3] mainly focus on sentence-level event extraction (SEE), that is, extracting event information from a sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": true,
          "contexts": [
            "To show the extreme difficulty when arguments-scattering meets multi-events for DEE, we follow the previous works [4, 5, 8, 9] and conduct experiments on two scenarios: single-event (i.",
            "[4], which adopts a key-event detection to guide event table filled with the arguments from key-event mention and surrounding sentences.",
            "[4] extract events from a central sentence and find other arguments from neighboring sentences separately.",
            "[4] first extract events from a central sentence, and then find missing arguments from neighbor sentences as supplements."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 70346512,
          "isinfluential": false,
          "contexts": [
            "To utilize more knowledge, some studies adopt pre-trained language model [2], document context [22, 24–26], and explicit external knowledge [27, 28] such as WordNet [29]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploiting the Ground-Truth: An Adversarial Imitation Based Knowledge Distillation Approach for Event Detection",
            "abstract": "The ambiguity in language expressions poses a great challenge for event detection. To disambiguate event types, current approaches rely on external NLP toolkits to build knowledge representations. Unfortunately, these approaches work in a pipeline paradigm and suffer from error propagation problem. In this paper, we propose an adversarial imitation based knowledge distillation approach, for the first time, to tackle the challenge of acquiring knowledge from rawsentences for event detection. In our approach, a teacher module is first devised to learn the knowledge representations from the ground-truth annotations. Then, we set up a student module that only takes the raw-sentences as the input. The student module is taught to imitate the behavior of the teacher under the guidance of an adversarial discriminator. By this way, the process of knowledge distillation from rawsentence has been implicitly integrated into the feature encoding stage of the student module. To the end, the enhanced student is used for event detection, which processes raw texts and requires no extra toolkits, naturally eliminating the error propagation problem faced by pipeline approaches. We conduct extensive experiments on the ACE 2005 datasets, and the experimental results justify the effectiveness of our approach.",
            "year": 2019,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "48211977",
                "name": "Jian Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209948413,
          "isinfluential": false,
          "contexts": [
            "[13] adopt fine-grained bidirectional long short-term memory and support vector machine to deal with the event trigger identification."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "FBSN: A hybrid fine-grained neural network for biomedical event trigger identification",
            "abstract": "",
            "year": 2020,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "38804628",
                "name": "Yufeng Diao"
              },
              {
                "authorId": "37553559",
                "name": "Hongfei Lin"
              },
              {
                "authorId": "5472921",
                "name": "Liang Yang"
              },
              {
                "authorId": "3061076",
                "name": "Xiaochao Fan"
              },
              {
                "authorId": null,
                "name": "Di Wu"
              },
              {
                "authorId": "151472911",
                "name": "Zhihao Yang"
              },
              {
                "authorId": "49605993",
                "name": "Jian Wang"
              },
              {
                "authorId": "49343380",
                "name": "Kan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie [30] extract events in a Question–Answer way."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie [6] and Du et al. [7] only consider a single event by concatenate multiple sentences, ignoring the modeling of multiple events.",
            "Du and Cardie [30] extract events in a Question–Answer way.",
            "Du and Cardie [6] try to encode the sentences in a multi-granularity way, and [7] leverage a sequence-to-sequence (seq2seq) model."
          ],
          "intents": [
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225157900,
          "isinfluential": false,
          "contexts": [
            "Liu et al. [17], Zhang et al. [18], Hu et al. [19], Yan et al. [20], Lu et al. [21] model graph information to enhance information ﬂow by introducing syntactic shortcut arcs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Syntax grounded graph convolutional network for joint entity and event extraction",
            "abstract": "",
            "year": 2021,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "47538865",
                "name": "Junchi Zhang"
              },
              {
                "authorId": null,
                "name": "Qi He"
              },
              {
                "authorId": "1591125925",
                "name": "Yue Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225160964,
          "isinfluential": false,
          "contexts": [
            "Chen et al. [14], Wang et al. [15] adopt CNN to extract events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Empower Chinese event detection with improved atrous convolution neural networks",
            "abstract": "",
            "year": 2020,
            "venue": "Neural computing & applications (Print)",
            "authors": [
              {
                "authorId": "1390878103",
                "name": "Zhihong Wang"
              },
              {
                "authorId": "2118268124",
                "name": "Yi Guo"
              },
              {
                "authorId": "1716536023",
                "name": "Jiahui Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "Previous researches [1–3] mainly focus on sentence-level event extraction (SEE), that is, extracting event information from a sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238590886,
          "isinfluential": false,
          "contexts": [
            "[21] model graph information to enhance information flow by introducing syntactic shortcut arcs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event detection from text using path-aware graph convolutional network",
            "abstract": "",
            "year": 2021,
            "venue": "Applied intelligence (Boston)",
            "authors": [
              {
                "authorId": "116829010",
                "name": "Shudong Lu"
              },
              {
                "authorId": "2118155668",
                "name": "Si Li"
              },
              {
                "authorId": "2796071",
                "name": "Yajing Xu"
              },
              {
                "authorId": "2148898484",
                "name": "Kai Wang"
              },
              {
                "authorId": "2134722751",
                "name": "Haibo Lan"
              },
              {
                "authorId": "2140051783",
                "name": "Jun Guo"
              }
            ]
          }
        }
      ]
    },
    "266163877": {
      "citing_paper_info": {
        "title": "An Iteratively Parallel Generation Method with the Pre-Filling Strategy for Document-level Event Extraction",
        "abstract": ",",
        "year": 2023,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2185459693",
            "name": "Guanhua Huang"
          },
          {
            "authorId": "2274917073",
            "name": "Runxin Xu"
          },
          {
            "authorId": "2274218927",
            "name": "Ying Zeng"
          },
          {
            "authorId": "2273549520",
            "name": "Jiaze Chen"
          },
          {
            "authorId": "2273577882",
            "name": "Zhouwang Yang"
          },
          {
            "authorId": "2255989628",
            "name": "Weinan E"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 2,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "119308902",
        "235097664",
        "51871198",
        "125977708",
        "234358675",
        "13756489",
        "244119148",
        "8310135",
        "216562330",
        "5692837",
        "236460259",
        "231728756"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5692837,
          "isinfluential": false,
          "contexts": [
            "Next, we leverage a pointer network (Vinyals et al., 2015): where H r ∈ R N r d is event role representation from H t , W r , W a ∈ R d d , v ∈ R d are trainable parameters, + is the broadcasting plus of two matrices, P r ∈ R N r ( N a +1) is the score of arguments corresponding to roles in the…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Pointer Networks",
            "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.",
            "year": 2015,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "39067762",
                "name": "Meire Fortunato"
              },
              {
                "authorId": "3111912",
                "name": "N. Jaitly"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8310135,
          "isinfluential": false,
          "contexts": [
            "…(1) named entity : mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc (Nadeau and Sekine, 2007); (2) entity mention : a text span of entity in the document which refers to a named entity; (3) event argument : an entity…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A survey of named entity recognition and classification",
            "abstract": "This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.",
            "year": 2007,
            "venue": "",
            "authors": [
              {
                "authorId": "40421028",
                "name": "David Nadeau"
              },
              {
                "authorId": "1714612",
                "name": "S. Sekine"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Then these tokens are encoded by a vanilla transformer encoder (Vaswani et al., 2017) to get the hidden representation: Where H w ∈ R N w d , d is the hidden dimension."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "For extracting multiple events without triggers, a trigger-free method is first designed to detect an event in a center sentence and extract the rest of event arguments from surrounding sentences (Yang et al., 2018)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": true,
          "contexts": [
            "The ChFinAnn dataset (Zheng et al., 2019) is a large dataset focuses on five event types from the financial text, which has 25 , 632 / 3 , 204 / 3 , 204 for the train/dev/test set.",
            "Following (Zheng et al., 2019), IPGPF finishes the event extraction by handling three sub-tasks: (1) Named Entity Recognition (NER) : extracting entity mentions as argument candidates from the document; (2) Event Detection (ED) : judging whether there exist predefined event types in the document.",
            "…between sentence representation and argument representation by employing a vanilla transformer encoder: For focusing on the comparison of event record generation module, we keep the same feature H a ∈ R ( N a +1) d and H s ∈ R N s d with (Zheng et al., 2019; Yang et al., 2021) in Equation (17).",
            "We implement IPGPF under Pytorch (Paszke et al., 2017) based on codes released by (Zheng et al., 2019) and (Yang et al., 2021).",
            "Doc2EDAG (Zheng et al., 2019) designs an auto-regressive entity-based directed acyclic graph (EDAG) to generate event records.",
            "Most trigger-free DEE methods (Zheng et al., 2019; Xu et al., 2021a; Liang et al., 2022) build entity-based directed acyclic graph (EDAG) to auto-regressively generate event arguments with their roles under a predefined order.",
            "Then a widely used entity-based directed acyclic graph (EDAG) generation method is proposed to better deal with multiple events extraction (Zheng et al., 2019).",
            "Following (Zheng et al., 2019), we use the micro precision, recall, and F1-score over all arguments.",
            "Due to the space limit, we put the details of NER and ED sub-tasks that are the same with (Zheng et al., 2019) to Appendix A.3 and A.4.",
            "In the model training, following (Zheng et al., 2019), we generate event records for each event type independently and finally sum the loss of all event types."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 125977708,
          "isinfluential": false,
          "contexts": [
            "Regarding the pre-trained encoder, we utilize LERT (Cui et al., 2022) for ChFinAnn and ERNIE (Sun et al., 2019) for DuEE-fin."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ERNIE: Enhanced Representation through Knowledge Integration",
            "abstract": "We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2117103617",
                "name": "Yu Sun"
              },
              {
                "authorId": "104463827",
                "name": "Shuohuan Wang"
              },
              {
                "authorId": "1710861",
                "name": "Yukun Li"
              },
              {
                "authorId": "1718657",
                "name": "Shikun Feng"
              },
              {
                "authorId": "2109214103",
                "name": "Xuyi Chen"
              },
              {
                "authorId": "48213346",
                "name": "Han Zhang"
              },
              {
                "authorId": "2117127187",
                "name": "Xin Tian"
              },
              {
                "authorId": "152867082",
                "name": "Danxiang Zhu"
              },
              {
                "authorId": "50007795",
                "name": "Hao Tian"
              },
              {
                "authorId": "40354707",
                "name": "Hua Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Recently, some works also tried to cast the event extraction task as question answering task (Du and Cardie, 2020b; Zhou et al., 2021), or sequence-to-sequence task (Xi-angyu et al., 2021).",
            "Different from sentence-level event extraction (SEE) (Chen et al., 2015; Nguyen et al., 2016; Du and Cardie, 2020b), event arguments of an event record are usually scattered across multiple sentences, while overlapping arguments contained in several event records appear more often.",
            "Most document-level EE meth-ods are built upon the event triggers, with which they conduct sequence labelling (Du and Cardie, 2020a; Veyseh et al., 2021) or span-based prediction (Ebner et al., 2020; Zhang et al., 2020; Xu et al., 2022) to identify event arguments in the document."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "Sequence-to-sequence (Du et al., 2021a,b; Li et al., 2021; Huang et al., 2021; Huang and Peng, 2021; Hsu et al., 2022), and question answering (Wei et al., 2021; Ma et al., 2022) paradigms are also applied."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 234358675,
          "isinfluential": false,
          "contexts": [
            "Several variant methods based on EDAG generation are presented by utilizing more meticulous feature engineering, such as heterogeneous graph feature (Xu et al., 2021a; Huang et al., 2021) and entity relation feature (Liang et al., 2022).",
            "Sequence-to-sequence (Du et al., 2021a,b; Li et al., 2021; Huang et al., 2021; Huang and Peng, 2021; Hsu et al., 2022), and question answering (Wei et al., 2021; Ma et al., 2022) paradigms are also applied."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction with Efficient End-to-end Learning of Cross-event Dependencies",
            "abstract": "Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency.",
            "year": 2020,
            "venue": "NUSE",
            "authors": [
              {
                "authorId": "1420116116",
                "name": "Kung-Hsiang Huang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235097664,
          "isinfluential": false,
          "contexts": [
            "1 Introduction Document-level event extraction (DEE) aims to extract multiple event records from the entire document (Ebner et al., 2020; Du et al., 2021b; Li et al., 2021; Xu et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Template Filling with Generative Transformers",
            "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": true,
          "contexts": [
            "Different from (Yang et al., 2021) which use a one-stage one-to-one matching to train their parallel model, we design a two-stage many-to-one matching algorithm to first performs multiple iterative refinements on the generated event records, and subsequently filters the best results.",
            "Parallel models (Yang et al., 2021) are presented to generate all event records and roles simultaneously to avoid the error broadcasts in a given event role order.",
            "…between sentence representation and argument representation by employing a vanilla transformer encoder: For focusing on the comparison of event record generation module, we keep the same feature H a ∈ R ( N a +1) d and H s ∈ R N s d with (Zheng et al., 2019; Yang et al., 2021) in Equation (17).",
            "DE-PPN (Yang et al., 2021) uses the same encoders with Doc2EDAG to obtain the features of arguments and sentences, but generates all event roles in parallel.",
            "We implement IPGPF under Pytorch (Paszke et al., 2017) based on codes released by (Zheng et al., 2019) and (Yang et al., 2021).",
            "Additionally, a parallel method is proposed to avoid the error broadcast in EDAG generation (Yang et al., 2021), and an efficient model is designed to lighten the model and accelerates the decoding speed (Zhu et al., 2022).",
            "For the training epochs, we follow (Zheng et al., 2019; Yang et al., 2021; Xu et al., 2021a) to train 100 epochs for all compared models on the ChFi-nAnn dataset."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244119148,
          "isinfluential": false,
          "contexts": [
            "SCDEE (Huang and Jia, 2021) builds an enhanced entity-sentence community graph, then detect event records from the graph and extract entities as arguments corresponding to the event roles."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Sentence Community for Document-Level Event Extraction",
            "abstract": "Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents, and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2131131685",
                "name": "Yusheng Huang"
              },
              {
                "authorId": "1819081375",
                "name": "Weijia Jia"
              }
            ]
          }
        }
      ]
    },
    "265006143": {
      "citing_paper_info": {
        "title": "Advancing document-level event extraction: Integration across texts and reciprocal feedback.",
        "abstract": "The primary objective of document-level event extraction is to extract relevant event information from lengthy texts. However, many existing methods for document-level event extraction fail to fully incorporate the contextual information that spans across sentences. To overcome this limitation, the present study proposes a document-level event extraction model called Integration Across Texts and Reciprocal Feedback (IATRF). The proposed model constructs a heterogeneous graph and employs a graph convolutional network to enhance the connection between document and entity information. This approach facilitates the acquisition of semantic information enriched with document-level context. Additionally, a Transformer classifier is introduced to transform multiple event types into a multi-label classification task. To tackle the challenge of event argument recognition, this paper introduces the Reciprocal Feedback Argument Extraction strategy. Experimental results conducted on both our COSM dataset and the publicly available ChFinAnn dataset demonstrate that the proposed model outperforms previous methods in terms of F1 value, thus confirming its effectiveness. The IATRF model effectively solves the problems of long-distance document context-aware representation and cross-sentence argument dispersion.",
        "year": 2023,
        "venue": "Mathematical biosciences and engineering : MBE",
        "authors": [
          {
            "authorId": "2265267878",
            "name": "Min Zuo"
          },
          {
            "authorId": "2265297340",
            "name": "Jiaqi Li"
          },
          {
            "authorId": "2348603208",
            "name": "Di Wu"
          },
          {
            "authorId": "2217698368",
            "name": "Yingjun Wang"
          },
          {
            "authorId": "2161679811",
            "name": "Wei Dong"
          },
          {
            "authorId": "2265297944",
            "name": "Jianlei Kong"
          },
          {
            "authorId": "2265150486",
            "name": "Kang Hu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 27,
        "unique_cited_count": 27,
        "influential_count": 0,
        "detailed_records_count": 27
      },
      "cited_papers": [
        "1957433",
        "52967399",
        "235363528",
        "1240016",
        "236477583",
        "245335089",
        "244119148",
        "257268255",
        "235421996",
        "235253912",
        "14542261",
        "3626819",
        "252819226",
        "224924246",
        "236460259",
        "18779057",
        "51871198",
        "257126309",
        "247594010",
        "5590763",
        "12740621",
        "252440614",
        "196178503",
        "219683473",
        "210994639",
        "10694510",
        "258801534"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1240016,
          "isinfluential": false,
          "contexts": [
            "These applications span knowledge graph construction [1,2], recommender systems [3,4], intelligent question answering [5,6], as well as other tasks [7–11] for more in-depth and precise analysis."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Zero-Shot Transfer Learning for Event Extraction",
            "abstract": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1957433,
          "isinfluential": false,
          "contexts": [
            "The GloVe model [25] leverages co-occurrence matrices to capture word semantics comprehensively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GloVe: Global Vectors for Word Representation",
            "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3626819,
          "isinfluential": false,
          "contexts": [
            "On the other hand, the ELMo model [26] dynamically adjusts word vectors based on context, effectively addressing the issue of polysemous words; however, it does not fully exploit contextual information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep Contextualized Word Representations",
            "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "50043859",
                "name": "Mark Neumann"
              },
              {
                "authorId": "2136562",
                "name": "Mohit Iyyer"
              },
              {
                "authorId": "40642935",
                "name": "Matt Gardner"
              },
              {
                "authorId": "143997772",
                "name": "Christopher Clark"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5590763,
          "isinfluential": false,
          "contexts": [
            "Nguyen et al. [35] learned sentence representations using a structure based on bidirectional Recurrent Neural Networks (RNN) [36], utilizing memory vectors and memory matrices to store information related to trigger words, arguments, and their dependencies."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
            "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "3158246",
                "name": "B. V. Merrienboer"
              },
              {
                "authorId": "1854385",
                "name": "Çaglar Gülçehre"
              },
              {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
              },
              {
                "authorId": "2076086",
                "name": "Fethi Bougares"
              },
              {
                "authorId": "144518416",
                "name": "Holger Schwenk"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10694510,
          "isinfluential": false,
          "contexts": [
            "Therefore, we will analyze typical models and methods for event extraction that obtain semantic representations at three different levels: words, sentences, and documents [21–23]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Deep learning for sentiment analysis: A survey",
            "abstract": "Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state‐of‐the‐art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.",
            "year": 2018,
            "venue": "WIREs Data Mining Knowl. Discov.",
            "authors": [
              {
                "authorId": "50081327",
                "name": "Lei Zhang"
              },
              {
                "authorId": "1717480",
                "name": "Shuai Wang"
              },
              {
                "authorId": "145321667",
                "name": "B. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12740621,
          "isinfluential": false,
          "contexts": [
            "For an event-argument path consisting of a sequence of entities, the entities in the route are stitched together to obtain a representation of the path 𝑃 (cid:4670)𝐸 (cid:3036) (cid:3117) , … , … , 𝐸 (cid:3036) (cid:3280) (cid:4671) , which is encoded using the Bi-directional Long Short-Term Memory (BiLSTM) [53], adding the embedding transformation of the event type into a vector 𝑇 and a sentence feature vector 𝑆 , which are stored in global storage 𝐺 ,which is shared among the different event types.",
            "…path 𝑃 (cid:4670)𝐸 (cid:3036) (cid:3117) , … , … , 𝐸 (cid:3036) (cid:3280) (cid:4671) , which is encoded using the Bi-directional Long Short-Term Memory (BiLSTM) [53], adding the embedding transformation of the event type into a vector 𝑇 and a sentence feature vector 𝑆 , which are stored in…"
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
            "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
            "year": 2015,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3109481",
                "name": "Zhiheng Huang"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "2150332252",
                "name": "Kai Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14542261,
          "isinfluential": false,
          "contexts": [
            "Chen et al. [33] proposed dynamic multi-pooled Convolutional Neural Networks (CNN) [34] to extract sentence-level clues, employing dynamic pooling layers to preserve more information about event trigger words and arguments."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Gradient-based learning applied to document recognition",
            "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
            "year": 1998,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "1688882",
                "name": "Yann LeCun"
              },
              {
                "authorId": "52184096",
                "name": "L. Bottou"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              },
              {
                "authorId": "1721248",
                "name": "P. Haffner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18779057,
          "isinfluential": false,
          "contexts": [
            "These applications span knowledge graph construction [1,2], recommender systems [3,4], intelligent question answering [5,6], as well as other tasks [7–11] for more in-depth and precise analysis."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Collaborative Social Group Influence for Event Recommendation",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "2148991510",
                "name": "Li Gao"
              },
              {
                "authorId": "153171677",
                "name": "Jia Wu"
              },
              {
                "authorId": "2057901713",
                "name": "Zhi Qiao"
              },
              {
                "authorId": "1857210",
                "name": "Chuan Zhou"
              },
              {
                "authorId": "2118608160",
                "name": "Hong Yang"
              },
              {
                "authorId": "2108954687",
                "name": "Yue Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "Yang et al. [44] proposed the DCFEE model, which extracts trigger words and arguments in a sentence-by-sentence manner.",
            "In order to validate the performance of our model, we performed a comparative analysis of several baseline models of the DEE task: •DCFEE [44] reduces the DEE task to a SEE task by extracting arguments from specific core sentences while looking for missing arguments in neighboring sentences."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "The BERT model [27], which utilizes the Bidirectional Transformer [28] language model, combines contextual semantics with a masking approach for training, resulting in more expressive word vectors."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "To tackle the problem of entities assuming different roles in different events, Yang et al. [38] separated the prediction of arguments and prediction of argument roles tasks, effectively resolving role overlapping."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 210994639,
          "isinfluential": false,
          "contexts": [
            "These applications span knowledge graph construction [1,2], recommender systems [3,4], intelligent question answering [5,6], as well as other tasks [7–11] for more in-depth and precise analysis."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatic Knowledge Graph Construction: A Report on the 2019 ICDM/ICBK Contest",
            "abstract": "Automatic knowledge graph construction seeks to build a knowledge graph from unstructured text in a specific domain or cross multiple domains, without human intervention. IEEE ICDM 2019 and ICBK 2019 invited teams from both degree-granting institutions and industrial labs to compete in the 2019 Knowledge Graph Contest by automatically constructing knowledge graphs in at least two different domains. This article reports the outcomes of the Contest. The participants were expected to build a model to extract knowledge represented as triplets from text data and develop a web application to visualize the triplets. Awards were given to five teams. Their models and key techniques used to construct knowledge graphs are summarized.",
            "year": 2019,
            "venue": "Industrial Conference on Data Mining",
            "authors": [
              {
                "authorId": "1748808",
                "name": "Xindong Wu"
              },
              {
                "authorId": "1576088834",
                "name": "Jia Wu"
              },
              {
                "authorId": "1491233206",
                "name": "Xiaoyi Fu"
              },
              {
                "authorId": "2130145501",
                "name": "Jiachen Li"
              },
              {
                "authorId": "144255235",
                "name": "Peng Zhou"
              },
              {
                "authorId": "2115901508",
                "name": "Xu Jiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            "Sentences of a document are fed into the encoder to obtain a contextual representation, and then the entity information is extracted through the Conditional Random Field (CRF) [52] layer; 2) Heterogeneous Graph for Entity Interactions."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 224924246,
          "isinfluential": false,
          "contexts": [
            "The first challenge is the effective capture of event information that spans multiple sentences [19]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Boundaries and edges rethinking: An end-to-end neural model for overlapping entity relation extraction",
            "abstract": "",
            "year": 2020,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "3350168",
                "name": "Yafeng Ren"
              },
              {
                "authorId": "145628086",
                "name": "Donghong Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": false,
          "contexts": [
            "Xu et al. [46] introduced the Tracker module to model the relationship between events, store decoded event information, and decode information for other event arguments.",
            "•GIT [46]: The model designs a Heterogeneous Graph Interaction Network to describe the global interactions between sentences."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235363528,
          "isinfluential": false,
          "contexts": [
            "Zhou et al. [40] employed a dyadic question and answer approach, enabling the model to comprehend the semantics of roles."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "What the Role is vs. What Plays the Role: Semi-Supervised Event Argument Extraction via Dual Question Answering",
            "abstract": "Event argument extraction is an essential task in event extraction, and become particularly challenging in the case of low-resource scenarios. We solve the issues in existing studies under low-resource situations from two sides. From the perspective of the model, the existing methods always suffer from the concern of insufficient parameter sharing and do not consider the semantics of roles, which is not conducive to dealing with sparse data. And from the perspective of the data, most existing methods focus on data generation and data augmentation. However, these methods rely heavily on external resources, which is more laborious to create than obtain unlabeled data. In this paper, we propose DualQA, a novel framework, which models the event argument extraction task as question answering to alleviate the problem of data sparseness and leverage the duality of event argument recognition which is to ask \"What plays the role\", as well as event role recognition which is to ask \"What the role is\", to mutually improve each other.Experimental results on two datasets prove the effectiveness of our approach, especially in extremely low-resource situations.",
            "year": 2021,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2145499382",
                "name": "Yang Zhou"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2145899648",
                "name": "Yin Wu"
              },
              {
                "authorId": "2111063776",
                "name": "Jiexin Xu"
              },
              {
                "authorId": "2115983287",
                "name": "Jinlong Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235421996,
          "isinfluential": false,
          "contexts": [
            "These applications span knowledge graph construction [1,2], recommender systems [3,4], intelligent question answering [5,6], as well as other tasks [7–11] for more in-depth and precise analysis."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Comprehensive Survey on Graph Anomaly Detection With Deep Learning",
            "abstract": "Anomalies are rare observations (e.g., data records or events) that deviate significantly from the others in the sample. Over the past few decades, research on anomaly mining has received increasing interests due to the implications of these occurrences in a wide range of disciplines - for instance, security, finance, and medicine. For this reason, anomaly detection, which aims to identify these rare observations, has become one of the most vital tasks in the world and has shown its power in preventing detrimental events, such as financial fraud, network intrusions, and social spam. The detection task is typically solved by identifying outlying data points in the feature space, which, inherently, overlooks the relational information in real-world data. At the same time, graphs have been prevalently used to represent the structural/relational information, which raises the graph anomaly detection problem - identifying anomalous graph objects (i.e., nodes, edges and sub-graphs) in a single graph, or anomalous graphs in a set/database of graphs. Conventional anomaly detection techniques cannot tackle this problem well because of the complexity of graph data (e.g., irregular structures, relational dependencies, node/edge types/attributes/directions/multiplicities/weights, large scale, etc.). However, thanks to the advent of deep learning in breaking these limitations, graph anomaly detection with deep learning has received a growing attention recently. In this survey, we aim to provide a systematic and comprehensive review of the contemporary deep learning techniques for graph anomaly detection. Specifically, we provide a taxonomy that follows a task-driven strategy and categorizes existing work according to the anomalous graph objects that they can detect. We especially focus on the challenges in this research area and discuss the key intuitions, technical details as well as relative strengths and weaknesses of various techniques in each category. From the survey results, we highlight 12 future research directions spanning unsolved and emerging problems introduced by graph data, anomaly detection, deep learning and real-world applications. Additionally, to provide a wealth of useful resources for future studies, we have compiled a set of open-source implementations, public datasets, and commonly-used evaluation metrics. With this survey, our goal is to create a “one-stop-shop” that provides a unified understanding of the problem categories and existing approaches, publicly available hands-on resources, and high-impact open challenges for graph anomaly detection using deep learning.",
            "year": 2021,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "authors": [
              {
                "authorId": "2115722071",
                "name": "Xiaoxiao Ma"
              },
              {
                "authorId": "2142734769",
                "name": "Jia Wu"
              },
              {
                "authorId": "2057237074",
                "name": "Shan Xue"
              },
              {
                "authorId": "2118801701",
                "name": "Jian Yang"
              },
              {
                "authorId": "1857210",
                "name": "Chuan Zhou"
              },
              {
                "authorId": "1713128",
                "name": "Quan Z. Sheng"
              },
              {
                "authorId": "2054473562",
                "name": "Hui Xiong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": false,
          "contexts": [
            "Yang et al. [47] proposed a multi-granularity decoder to extract all events in parallel."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236477583,
          "isinfluential": false,
          "contexts": [
            "To address these challenges, DEE requires models that can integrate document-level information while capturing multiple events across multiple sentences [48,49]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction aims to detect the relations within one document, which is challenging since it requires complex reasoning using mentions, entities, local and global contexts. Few previous studies have distinguished local and global reasoning explicitly, which may be problematic because they play different roles in intra-and inter-sentence relations. Moreover, the interactions between local and global contexts should be considered since they could help relation reasoning based on our observation. In this paper, we pro-pose a novel mention-based reasoning (MRN) module based on explicitly and collaboratively local and global reasoning. Based on MRN, we design a co-predictor module to predict entity relations based on local and global entity and relation representations jointly. We evaluate our MRN model on three widely-used benchmark datasets, namely DocRED, CDR, and GDA. Experimental results show that our model outperforms previous state-of-the-art models by a large margin.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2000217741",
                "name": "Jingye Li"
              },
              {
                "authorId": "2113474255",
                "name": "Kang Xu"
              },
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              },
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "3350168",
                "name": "Yafeng Ren"
              },
              {
                "authorId": "1719916",
                "name": "D. Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244119148,
          "isinfluential": false,
          "contexts": [
            "Huang et al. [50] converted each document into an undirected graph based on sentence relationships, dividing the graph into subgraphs representing sentence communities."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Sentence Community for Document-Level Event Extraction",
            "abstract": "Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents, and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2131131685",
                "name": "Yusheng Huang"
              },
              {
                "authorId": "1819081375",
                "name": "Weijia Jia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245335089,
          "isinfluential": false,
          "contexts": [
            "Relying solely on local features such as word-level semantics [32] is inadequate, necessitating the acquisition of contextual semantic representations at the sentence level."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unified Named Entity Recognition as Word-Word Relation Classification",
            "abstract": "So far, named entity recognition (NER) has been involved with three major types, including flat, overlapped (aka. nested), and discontinuous NER, which have mostly been studied individually. Recently, a growing interest has been built for unified NER, tackling the above three jobs concurrently with one single model. Current best-performing methods mainly include span-based and sequence-to-sequence models, where unfortunately the former merely focus on boundary identification and the latter may suffer from exposure bias. In this work, we present a novel alternative by modeling the unified NER as word-word relation classification, namely W^2NER. The architecture resolves the kernel bottleneck of unified NER by effectively modeling the neighboring relations between entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-* (THW-*) relations. Based on the W^2NER scheme we develop a neural framework, in which the unified NER is modeled as a 2D grid of word pairs. We then propose multi-granularity 2D convolutions for better refining the grid representations. Finally, a co-predictor is used to sufficiently reason the word-word relations. We perform extensive experiments on 14 widely-used benchmark datasets for flat, overlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our model beats all the current top-performing baselines, pushing the state-of-the-art performances of unified NER.",
            "year": 2021,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2000217741",
                "name": "Jingye Li"
              },
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "2155403806",
                "name": "Jiang Liu"
              },
              {
                "authorId": "1957924118",
                "name": "Shengqiong Wu"
              },
              {
                "authorId": "2117849151",
                "name": "Meishan Zhang"
              },
              {
                "authorId": "1679617",
                "name": "Chong Teng"
              },
              {
                "authorId": "145628086",
                "name": "Donghong Ji"
              },
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247594010,
          "isinfluential": false,
          "contexts": [
            "Event extraction enables us to swiftly capture the essential elements and crucial information regarding public opinion events in social media data [12], facilitating a comprehensive understanding and analysis of these events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Effective Token Graph Modeling using a Novel Labeling Strategy for Structured Sentiment Analysis",
            "abstract": "The state-of-the-art model for structured sentiment analysis casts the task as a dependency parsing problem, which has some limitations: (1) The label proportions for span prediction and span relation prediction are imbalanced. (2) The span lengths of sentiment tuple components may be very large in this task, which will further exacerbates the imbalance problem. (3) Two nodes in a dependency graph cannot have multiple arcs, therefore some overlapped sentiment tuples cannot be recognized. In this work, we propose nichetargeting solutions for these issues. First, we introduce a novel labeling strategy, which contains two sets of token pair labels, namely essential label set and whole label set. The essential label set consists of the basic labels for this task, which are relatively balanced and applied in the prediction layer. The whole label set includes rich labels to help our model capture various token relations, which are applied in the hidden layer to softly influence our model. Moreover, we also propose an effective model to well collaborate with our labeling strategy, which is equipped with the graph attention networks to iteratively refine token representations, and the adaptive multi-label classifier to dynamically predict multiple relations between token pairs. We perform extensive experiments on 5 benchmark datasets in four languages. Experimental results show that our model outperforms previous SOTA models by a large margin.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3194752",
                "name": "Wenxuan Shi"
              },
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              },
              {
                "authorId": "2000217741",
                "name": "Jingye Li"
              },
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "145628086",
                "name": "Donghong Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252440614,
          "isinfluential": false,
          "contexts": [
            "To broaden the scope of EE’s applicability [13,14], an increasing number of researchers are turning their attention to document-level event extraction (DEE)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "IA-ICGCN: Integrating Prior Knowledge via Intra-event Association and Inter-event Causality for Chinese Causal Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "International Conference on Artificial Neural Networks",
            "authors": [
              {
                "authorId": "2152619150",
                "name": "Zhengming Zhao"
              },
              {
                "authorId": "2178722796",
                "name": "Hang Yu"
              },
              {
                "authorId": "2167614",
                "name": "Xiangfeng Luo"
              },
              {
                "authorId": "49952430",
                "name": "Jianqi Gao"
              },
              {
                "authorId": "2185767423",
                "name": "Xiao Xu"
              },
              {
                "authorId": "2185753370",
                "name": "Shengming Guo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252819226,
          "isinfluential": false,
          "contexts": [
            "Compared to SEE, DEE faces two major challenges: dealing with widely distributed arguments and recognizing multiple events [42]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "47184362",
                "name": "Yanan Cao"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2075394870",
                "name": "Ping Guo"
              },
              {
                "authorId": "1390641501",
                "name": "Zheng Lin"
              },
              {
                "authorId": "2185915076",
                "name": "Wei Ma"
              },
              {
                "authorId": "2153629743",
                "name": "Yi Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257126309,
          "isinfluential": false,
          "contexts": [
            "Recent statistics indicate a growing proportion of public opinion focusing on the food and cosmetic industries [15–17], highlighting the critical role of DEE in analyzing harmful public opinion specifically related to food and cosmetic products."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Analysis of public opinion on food safety in Greater China with big data and machine learning.",
            "abstract": "",
            "year": 2023,
            "venue": "Current Research in Food Science",
            "authors": [
              {
                "authorId": "2135731613",
                "name": "Haoyang Zhang"
              },
              {
                "authorId": "2121621374",
                "name": "Dachuan Zhang"
              },
              {
                "authorId": "5671211",
                "name": "Zhisheng Wei"
              },
              {
                "authorId": "2152890503",
                "name": "Yan Li"
              },
              {
                "authorId": "2209340041",
                "name": "Shaji Wu"
              },
              {
                "authorId": "2051325905",
                "name": "Zhiheng Mao"
              },
              {
                "authorId": "2185039144",
                "name": "Chunmeng He"
              },
              {
                "authorId": "1998838222",
                "name": "Haorui Ma"
              },
              {
                "authorId": "2111552009",
                "name": "Xinping Zeng"
              },
              {
                "authorId": "2162215733",
                "name": "Xiaoling Xie"
              },
              {
                "authorId": "2067130012",
                "name": "Xingran Kou"
              },
              {
                "authorId": "2045279166",
                "name": "Bingwen Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257268255,
          "isinfluential": false,
          "contexts": [
            "Hu et al. [51] aimed to identify role arguments of a specific event type in a document."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Role Knowledge Prompting for Document-Level Event Argument Extraction",
            "abstract": "Document-level event argument extraction (DEAE) aims to identify the arguments corresponding to the roles of a given event type in a document. However, arguments scattering and arguments and roles overlapping make DEAE face great challenges. In this paper, we propose a novel DEAE model called Role Knowledge Prompting for Document-Level Event Argument Extraction (RKDE), which enhances the interaction between templates and roles through a role knowledge guidance mechanism to precisely prompt pretrained language models (PLMs) for argument extraction. Specifically, it not only facilitates PLMs to understand deep semantics but also generates all the arguments simultaneously. The experimental results show that our model achieved decent performance on two public DEAE datasets, with 3.2% and 1.4% F1 improvement on Arg-C, and to some extent, it addressed the overlapping arguments and roles.",
            "year": 2023,
            "venue": "Applied Sciences",
            "authors": [
              {
                "authorId": "40525396",
                "name": "Ruijuan Hu"
              },
              {
                "authorId": "2183241822",
                "name": "Haiyan Liu"
              },
              {
                "authorId": "2210360302",
                "name": "Huijuan Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258801534,
          "isinfluential": false,
          "contexts": [
            "Recent statistics indicate a growing proportion of public opinion focusing on the food and cosmetic industries [15–17], highlighting the critical role of DEE in analyzing harmful public opinion specifically related to food and cosmetic products."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Visual Description Augmented Integration Network for Multimodal Entity and Relation Extraction",
            "abstract": "Multimodal Named Entity Recognition (MNER) and multimodal Relationship Extraction (MRE) play an important role in processing multimodal data and understanding entity relationships across textual and visual domains. However, irrelevant image information may introduce noise that misleads the recognition of information. Additionally, visual and semantic features originate from different modalities, and modal disparity hinders semantic alignment. Therefore, this paper proposes the Visual Description Augmentation Integration Network (VDAIN), which introduces an image description generation technique that allows semantic features generated from image descriptions to be presented in the same modality as the semantic features of textual information. This not only reduces the modal gap but also captures more accurately the high-level semantic information and underlying visual structure in the images. To filter out the modal noise, we use VDAIN to adaptively fuse visual features, semantic features of image descriptions, and textual information, thus eliminating irrelevant modal noise. The F1 score of the proposed model in this paper reaches 75.8% and 87.78% for the MNER task and 82.54% for the MRE task on the three public data sets, respectively, which are significantly better than the baseline model. The experimental results demonstrate the effectiveness of the proposed method in solving the modal noise and modal gap problems.",
            "year": 2023,
            "venue": "Applied Sciences",
            "authors": [
              {
                "authorId": "2074829036",
                "name": "Min Zuo"
              },
              {
                "authorId": "2217698368",
                "name": "Yingjun Wang"
              },
              {
                "authorId": "2161679811",
                "name": "Wei Dong"
              },
              {
                "authorId": "2112265940",
                "name": "Qingchuan Zhang"
              },
              {
                "authorId": "48694031",
                "name": "Yuanyuan Cai"
              },
              {
                "authorId": "2160771121",
                "name": "Jianlei Kong"
              }
            ]
          }
        }
      ]
    },
    "244119148": {
      "citing_paper_info": {
        "title": "Exploring Sentence Community for Document-Level Event Extraction",
        "abstract": "Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents, and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.",
        "year": 2021,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2131131685",
            "name": "Yusheng Huang"
          },
          {
            "authorId": "1819081375",
            "name": "Weijia Jia"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 2,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "19220240",
        "51871927",
        "202770954",
        "222177188",
        "56517517",
        "6628106",
        "88492570",
        "17825977",
        "52967399",
        "51871198",
        "9776219",
        "3292002"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3292002,
          "isinfluential": false,
          "contexts": [
            "To extract multiple events, we employ Graph Attention Networks (GAT) (Velickovic et al., 2018) with the multi-head graph attention to detect overlapping sentence communities (Shchur and Günnemann, 2019), then we classify event types and extract corresponding arguments with an entity-level attention mechanism for each sentence community.",
            "To extract multiple events, we employ Graph Attention Networks (GAT) (Velickovic et al., 2018) with the multi-head graph attention to detect overlapping sentence communities (Shchur and Günnemann, 2019), then we classify event types and extract corresponding arguments with an entity-level attention…"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Graph Attention Networks",
            "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "3444569",
                "name": "Petar Velickovic"
              },
              {
                "authorId": "7153363",
                "name": "Guillem Cucurull"
              },
              {
                "authorId": "8742492",
                "name": "Arantxa Casanova"
              },
              {
                "authorId": "144290131",
                "name": "Adriana Romero"
              },
              {
                "authorId": "144269589",
                "name": "P. Lio’"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": true,
          "contexts": [
            "We employ the Adam (Kingma and Ba, 2015) to optimize the model parameters with the initial learning rate being 0.",
            "We employ the Adam (Kingma and Ba, 2015) to optimize the model parameters with the initial learning rate being 0.001, β1 = 0.9, β2 = 0.999 and = 10−8."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9776219,
          "isinfluential": false,
          "contexts": [
            "Sentence-level Event Extraction mainly follows the requirements of ACE event extraction task (Doddington et al., 2004) that aims to detect the event trigger and arguments from a sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation",
            "abstract": "",
            "year": 2004,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2862682",
                "name": "G. Doddington"
              },
              {
                "authorId": "2449760",
                "name": "A. Mitchell"
              },
              {
                "authorId": "2282719",
                "name": "Mark A. Przybocki"
              },
              {
                "authorId": "1744313",
                "name": "L. Ramshaw"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              },
              {
                "authorId": "1732071",
                "name": "R. Weischedel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17825977,
          "isinfluential": false,
          "contexts": [
            "…document-level events is beneficial for a variety of natural language processing downstream tasks, such as knowledge base construction (Li et al., 2018), article summarization (Lee et al., 2003), and question answering (Srihari and Li, 2000), since it can produce valuable structured information.",
            ", 2018), article summarization (Lee et al., 2003), and question answering (Srihari and Li, 2000), since it can produce valuable structured information."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Ontology-based fuzzy event extraction agent for Chinese e-news summarization",
            "abstract": "",
            "year": 2003,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "2155591290",
                "name": "Chang-Shing Lee"
              },
              {
                "authorId": "3294118",
                "name": "Yea-Juan Chen"
              },
              {
                "authorId": "2117958",
                "name": "Zhi-Wei Jian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19220240,
          "isinfluential": false,
          "contexts": [
            "…document-level events is beneficial for a variety of natural language processing downstream tasks, such as knowledge base construction (Li et al., 2018), article summarization (Lee et al., 2003), and question answering (Srihari and Li, 2000), since it can produce valuable structured information."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction",
            "abstract": "\n \n Event extraction plays an important role in natural language processing (NLP) applications including question answering and information retrieval. Traditional event extraction relies heavily on lexical and syntactic features, which require intensive human engineering and may not generalize to different datasets. Deep neural networks, on the other hand, are able to automatically learn underlying features, but existing networks do not make full use of syntactic relations. In this paper, we propose a novel dependency bridge recurrent neural network (dbRNN) for event extraction. We build our model upon a recurrent neural network, but enhance it with dependency bridges, which carry syntactically related information when modeling each word.We illustrates that simultaneously applying tree structure and sequence structure in RNN brings much better performance than only uses sequential RNN. In addition, we use a tensor layer to simultaneously capture the various types of latent interaction between candidate arguments as well as identify/classify all arguments of an event. Experiments show that our approach achieves competitive results compared with previous work.\n \n",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "39058310",
                "name": "Lei Sha"
              },
              {
                "authorId": "2053324591",
                "name": "Feng Qian"
              },
              {
                "authorId": "39488576",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": true,
          "contexts": [
            "Compared with DCFEE-O and DCFEE-M that predicted missing arguments from surrounding sentences, we believe the improvements of DCFEE should give credit to the graph structure and the GAT layer, which alleviate the long-range dependency issue.",
            "In order to handle multi-event extraction, DCFEE-O and DCFEE-M (Zheng et al., 2019) are proposed by producing one event record and multiple possible argument combinations from one key-event sentence respectively.",
            "Our framework could filter the noise sentences and focus on informative sentences, which is an advantage compared with the baseline DCFEE.",
            "Specifically, compared with DCFEE-O and DCFEE-M, our framework achieves better results both in precision and F1scores on all the five event types.",
            "Recently, a wide variety of deep neural network models (Nguyen et al., 2016; Yang et al., 2018; Sha\n∗Corresponding author.\net al., 2018; Yang et al., 2019; Ahmad et al., 2020; Ma et al., 2020a) have been proposed for event extraction, which could capture the semantic dependencies (mainly sequential…",
            "In order to comprehensively evaluate our framework, we compare it with these following state-of-the-art baselines: • DCFEE (Yang et al., 2018) employs the argument-completion strategy to generate the document-level event record by utilizing the arguments from sentences-level event extraction results.",
            "From the perspective of modeling, Yang et al. (2018) employ a sequence tagging model to extract document-level events by utilizing sentence-level results.",
            "In order to comprehensively evaluate our framework, we compare it with these following state-of-the-art baselines: • DCFEE (Yang et al., 2018) employs the argument-completion strategy to generate the document-level event record by utilizing the arguments from sentences-level event extraction…"
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871927,
          "isinfluential": false,
          "contexts": [
            "…task can be further decomposed into two sub-tasks: Event Detection that aims to identify the event triggers (Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019; Cui et al., 2020; Lai et al., 2020a,b) and Event Argument Role Labeling that aims to predict whether words or…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention",
            "abstract": "Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": null,
                "name": "Yue Zhao"
              },
              {
                "authorId": "2149111400",
                "name": "Xiaolong Jin"
              },
              {
                "authorId": "2219600",
                "name": "Yuanzhuo Wang"
              },
              {
                "authorId": "1717004",
                "name": "Xueqi Cheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Specifically, for each sentence si containing Ni words, we employ BERT representation model on si and obtain the embedding vector of the last layer Bi ∈ RNi×dB , where dB denotes the hidden layer dimensionality of BERT.",
            "Each sentence presents one node considering the entity interaction, and we assign each node with a comprehensively encoded attribute vector based on BERT (Devlin et al., 2019)."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 56517517,
          "isinfluential": false,
          "contexts": [
            "The possible reason for the overall performance dropping may lie in the over-smoothing issue (Zhou et al., 2018) that the node attribute vectors tend to converge to similar values."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Graph Neural Networks: A Review of Methods and Applications",
            "abstract": "",
            "year": 2018,
            "venue": "AI Open",
            "authors": [
              {
                "authorId": "48128428",
                "name": "Jie Zhou"
              },
              {
                "authorId": "52297757",
                "name": "Ganqu Cui"
              },
              {
                "authorId": "2621696",
                "name": "Zhengyan Zhang"
              },
              {
                "authorId": "3443627",
                "name": "Cheng Yang"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 88492570,
          "isinfluential": false,
          "contexts": [
            "…we employ Graph Attention Networks (GAT) (Velickovic et al., 2018) with the multi-head graph attention to detect overlapping sentence communities (Shchur and Günnemann, 2019), then we classify event types and extract corresponding arguments with an entity-level attention mechanism for each…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Overlapping Community Detection with Graph Neural Networks",
            "abstract": "Community detection is a fundamental problem in machine learning. While deep learning has shown great promise in many graphrelated tasks, developing neural models for community detection has received surprisingly little attention. The few existing approaches focus on detecting disjoint communities, even though communities in real graphs are well known to be overlapping. We address this shortcoming and propose a graph neural network (GNN) based model for overlapping community detection. Despite its simplicity, our model outperforms the existing baselines by a large margin in the task of community recovery. We establish through an extensive experimental evaluation that the proposed model is effective, scalable and robust to hyperparameter settings. We also perform an ablation study that confirms that GNN is the key ingredient to the power of the proposed model.",
            "year": 2018,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "32724677",
                "name": "Oleksandr Shchur"
              },
              {
                "authorId": "3075189",
                "name": "Stephan Günnemann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202770954,
          "isinfluential": false,
          "contexts": [
            "…al., 2019; Cui et al., 2020; Lai et al., 2020a,b) and Event Argument Role Labeling that aims to predict whether words or phrases participate in the event argument roles (Wang et al., 2019; Yun et al., 2019; Pouran Ben Veyseh et al., 2020; Ma et al., 2020b; Ahmad et al., 2020; Zhang et al., 2020).",
            ", 2020a,b) and Event Argument Role Labeling that aims to predict whether words or phrases participate in the event argument roles (Wang et al., 2019; Yun et al., 2019; Pouran Ben Veyseh et al., 2020; Ma et al., 2020b; Ahmad et al., 2020; Zhang et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HMEAE: Hierarchical Modular Event Argument Extraction",
            "abstract": "Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              },
              {
                "authorId": "2108485135",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1384550891",
                "name": "Xiang Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222177188,
          "isinfluential": false,
          "contexts": [
            "…a wide variety of deep neural network models (Nguyen et al., 2016; Yang et al., 2018; Sha\n∗Corresponding author.\net al., 2018; Yang et al., 2019; Ahmad et al., 2020; Ma et al., 2020a) have been proposed for event extraction, which could capture the semantic dependencies (mainly sequential…",
            "…al., 2019; Cui et al., 2020; Lai et al., 2020a,b) and Event Argument Role Labeling that aims to predict whether words or phrases participate in the event argument roles (Wang et al., 2019; Yun et al., 2019; Pouran Ben Veyseh et al., 2020; Ma et al., 2020b; Ahmad et al., 2020; Zhang et al., 2020).",
            ", 2020a,b) and Event Argument Role Labeling that aims to predict whether words or phrases participate in the event argument roles (Wang et al., 2019; Yun et al., 2019; Pouran Ben Veyseh et al., 2020; Ma et al., 2020b; Ahmad et al., 2020; Zhang et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction",
            "abstract": "Recent progress in cross-lingual relation and event extraction use graph convolutional networks (GCNs) with universal dependency parses to learn language-agnostic sentence representations such that models trained on one language can be applied to other languages. However, GCNs struggle to model words with long-range dependencies or are not directly connected in the dependency tree. To address these challenges, we propose to utilize the self-attention mechanism where we explicitly fuse structural information to learn the dependencies between words with different syntactic distances. We introduce GATE, a Graph Attention Transformer Encoder, and test its cross-lingual transferability on relation and event extraction tasks. We perform experiments on the ACE05 dataset that includes three typologically different languages: English, Chinese, and Arabic. The evaluation results show that GATE outperforms three recently proposed methods by a large margin. Our detailed analysis reveals that due to the reliance on syntactic dependencies, GATE produces robust representations that facilitate transfer across languages.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "38123220",
                "name": "Wasi Uddin Ahmad"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              }
            ]
          }
        }
      ]
    },
    "252819226": {
      "citing_paper_info": {
        "title": "CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction",
        "abstract": "",
        "year": 2022,
        "venue": "International Conference on Computational Linguistics",
        "authors": [
          {
            "authorId": "2109978994",
            "name": "Yubing Ren"
          },
          {
            "authorId": "47184362",
            "name": "Yanan Cao"
          },
          {
            "authorId": "36595248",
            "name": "Fang Fang"
          },
          {
            "authorId": "2075394870",
            "name": "Ping Guo"
          },
          {
            "authorId": "1390641501",
            "name": "Zheng Lin"
          },
          {
            "authorId": "2185915076",
            "name": "Wei Ma"
          },
          {
            "authorId": "2153629743",
            "name": "Yi Liu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 0,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "140925826",
        "35386653",
        "13756489",
        "245811890",
        "14542261",
        "202539496",
        "14339673",
        "6452487",
        "196178503",
        "10913456",
        "3312944"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3312944,
          "isinfluential": false,
          "contexts": [
            "We trained all models with the AdamW optimizer (Loshchilov and Hutter, 2018).",
            "For strictly consistent comparison, we involve the following strong baselines: • BERT-CRF (Loshchilov and Hutter, 2018), which combines BERT with Condition Random Field (Lafferty et al., 2001), is the most popular method in tagging-based event extraction."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fixing Weight Decay Regularization in Adam",
            "abstract": "We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process.",
            "year": 2017,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2013, 2015) employ various hand-designed features to extract event; (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Liu et al., 2017, 2018) use neural based models such as recurrent neural networks (Zaremba et al., 2014) and convolutional neural network (Le-Cun et…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10913456,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2013, 2015) employ various hand-designed features to extract event; (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Liu et al., 2017, 2018) use neural based models such as recurrent neural networks (Zaremba et al., 2014) and convolutional neural network (Le-Cun et…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Detection and Domain Adaptation with Convolutional Neural Networks",
            "abstract": "We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Analogy to multi-head attention (Vaswani et al., 2017), we propose a role-intera C tive mu L ti-event head attent I on netw O rk (CLIO) for DEE."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2013, 2015) employ various hand-designed features to extract event; (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Liu et al., 2017, 2018) use neural based models such as recurrent neural networks (Zaremba et al., 2014) and convolutional neural network (Le-Cun et…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14542261,
          "isinfluential": false,
          "contexts": [
            ", 2014) and convolutional neural network (LeCun et al., 1998) to extract event."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Gradient-based learning applied to document recognition",
            "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
            "year": 1998,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "1688882",
                "name": "Yann LeCun"
              },
              {
                "authorId": "52184096",
                "name": "L. Bottou"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              },
              {
                "authorId": "1721248",
                "name": "P. Haffner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 35386653,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2013, 2015) employ various hand-designed features to extract event; (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Liu et al., 2017, 2018) use neural based models such as recurrent neural networks (Zaremba et al., 2014) and convolutional neural network (Le-Cun et…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms",
            "abstract": "This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 140925826,
          "isinfluential": false,
          "contexts": [
            "Cognitive scientists believe that humans remember and understand reality primarily in terms of events (Shipley and Zacks, 2008)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Understanding events : from perception to action",
            "abstract": "PART I FOUNDATIONS 1. An Invitation to an Event 2. Event Concepts 3. Events Are What We Make of Them PART II DEVELOPING AN UNDERSTANDING OF EVENTS Overview 4. Perceptual Development in Infancy as the Foundation of Event Perception 5. Pragmatics of Human Action 6. Event Memory in Infancy and Early Childhood 7. Current Events: How Infants Parse the World and Events for Language 8. Speaking of Events: Event Word Learning and Event Representation PART III PERCEIVING AND SEGMENTING EVENTS Overview Section 1: Perceiving Action Events 9. Representations of Voluntary Arm Movements in the Motor Cortex and Their Transformations 10. Events and Actions as Dynamically Molded Spatiotemporal Objects: A Critique of the Motor Theory of Biological Motion Perception 11. Movement Style, Movement Features, and the Recognition of Affect from Human Movement 12. Retrieving Information freom Human Movement Patterns 13. Neurophysiology of Action Recognition 14. Animacy and Intention in the Brain: Neuroscience of Social Event Perception Section 2: Segmenting Events 15. The Role of Segmentation in Perception and Understanding of Events 16. Geometric Information for Event Segmentation 17. The Structure of Experience PART IV REPRESENTING AND REMEMBERING EVENTS Overview Section 1: Representing Events 18. Computational Vision Approaches for Event Modeling 19. Shining Spotlights, Zooming Lenses, Grabbing Hands, and Pecking Chickens: The Ebb and Flow of Attention During Events 20. Dynamics and the Perception of Causal Events Section 2: Remembering Events 21. The Boundaries of Episodic Memories 22. The Human Prefrontal Cortex Stores Structured Event Complexes 23. Neurocognitive Mechanisms of Human Comprehension",
            "year": 2008,
            "venue": "",
            "authors": [
              {
                "authorId": "1773234",
                "name": "T. Shipley"
              },
              {
                "authorId": "2366786",
                "name": "Jeffrey M. Zacks"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "With the recent success of BERT (Devlin et al., 2019), pretrained language models have also been used for SEE (Wang et al., 2019b,c; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020; Wang et al., 2021; Lu et al., 2021; Liu et al., 2022)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "With the recent success of BERT (Devlin et al., 2019), pretrained language models have also been used for SEE (Wang et al., 2019b,c; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020; Wang et al., 2021; Lu et al., 2021; Liu et al., 2022)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245811890,
          "isinfluential": false,
          "contexts": [
            "We apply an approach, proposed by (Hoffmann et al., 2022) based on InfoNCE, to include multiple positives."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives",
            "abstract": "This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a new member in the family of InfoNCE losses that preserves a ranked ordering of positive samples. In contrast to the standard InfoNCE loss, which requires a strict binary separation of the training pairs into similar and dissimilar samples, RINCE can exploit information about a similarity ranking for learning a corresponding embedding space. We show that the proposed loss function learns favorable embeddings compared to the standard InfoNCE whenever at least noisy ranking information can be obtained or when the definition of positives and negatives is blurry. We demonstrate this for a supervised classification task with additional superclass labels and noisy similarity scores. Furthermore, we show that RINCE can also be applied to unsupervised training with experiments on unsupervised representation learning from videos. In particular, the embedding yields higher classification accuracy, retrieval rates and performs better on out-of-distribution detection than the standard InfoNCE loss.",
            "year": 2022,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "151491781",
                "name": "David T. Hoffmann"
              },
              {
                "authorId": "1581475970",
                "name": "Nadine Behrmann"
              },
              {
                "authorId": "145689714",
                "name": "Juergen Gall"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              },
              {
                "authorId": "1811235696",
                "name": "M. Noroozi"
              }
            ]
          }
        }
      ]
    },
    "246634770": {
      "citing_paper_info": {
        "title": "Document-Level Event Extraction via Human-Like Reading Process",
        "abstract": "Document-level Event Extraction (DEE) is particularly tricky due to the two challenges it poses: scattering-arguments and multi-events. The first challenge means that arguments of one event record could reside in different sentences in the document, while the second one reflects that one document may simultaneously contain multiple such event records. Motivated by humans’ reading cognitive to extract information of interests, in this paper, we propose a method called HRE (Human Reading inspired Extractor for Document Events), where DEE is decomposed into these two iterative stages, rough reading and elaborate reading. Specifically, the first stage browses the document to detect the occurrence of events, and the second stage serves to extract specific event arguments. For each concrete event role, elaborate reading hierarchically works from sentences to characters to locate arguments across sentences, thus the scattering-arguments problem is tackled. Meanwhile, rough reading is explored in a multi-round manner to discover undetected events, thus the multi-events problem is handled. Experiment results show the superiority of HRE over prior competitive methods.",
        "year": 2022,
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "authors": [
          {
            "authorId": "9144106",
            "name": "Shiyao Cui"
          },
          {
            "authorId": "2024380701",
            "name": "Xin Cong"
          },
          {
            "authorId": "48613402",
            "name": "Yu Bowen"
          },
          {
            "authorId": "2079682",
            "name": "Tingwen Liu"
          },
          {
            "authorId": "2108899101",
            "name": "Yucheng Wang"
          },
          {
            "authorId": "1700736",
            "name": "Jinqiao Shi"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 0,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "52816033",
        "216562330",
        "69988562",
        "235458429",
        "218630327",
        "207853145",
        "220046861"
      ],
      "citation_details": [
        {
          "citedcorpusid": 52816033,
          "isinfluential": false,
          "contexts": [
            "Despite successful efforts [1, 2, 3, 4, 5, 6, 7] to extract events within a sentence, a."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
            "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2730687",
                "name": "Zhunchen Luo"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 69988562,
          "isinfluential": false,
          "contexts": [
            "Recently, simulating human’s reading cognitive process to address specific natural language processing (NLP) tasks [13, 14] has"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Human-Like Semantic Cognition Network for Aspect-Level Sentiment Classification",
            "abstract": "In this paper, we propose a novel Human-like Semantic Cognition Network (HSCN) for aspect-level sentiment classification, motivated by the principles of human beings’ reading cognitive process (pre-reading, active reading, post-reading). We first design a word-level interactive perception module to capture the correlation between context words and the given target words, which can be regarded as pre-reading. Second, to mimic the process of active reading, we propose a targetaware semantic distillation module to produce the targetspecific context representation for aspect-level sentiment prediction. Third, we further devise a semantic deviation metric module to measure the semantic deviation between the targetspecific context representation and the given target, which evaluates the degree we understand the target-specific context semantics. The measured semantic deviation is then used to fine-tune the above active reading process in a feedback regulation way. To verify the effectiveness of our approach, we conduct extensive experiments on three widely used datasets. The experiments demonstrate that HSCN achieves impressive results compared to other strong competitors.",
            "year": 2019,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40846265",
                "name": "Zeyang Lei"
              },
              {
                "authorId": "3001727",
                "name": "Yujiu Yang"
              },
              {
                "authorId": "144346838",
                "name": "Min Yang"
              },
              {
                "authorId": "47748857",
                "name": "Wei Zhao"
              },
              {
                "authorId": "145505201",
                "name": "J. Guo"
              },
              {
                "authorId": "2153629538",
                "name": "Yi Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "To date, most DEE methods [8, 9, 10, 11] mainly focus on the first challenge but ignores the second one.",
            "ArgSpan [10] extracts scattering arguments by enumerating possible argument spans within a specified scope of sentence window."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Despite successful efforts [1, 2, 3, 4, 5, 6, 7] to extract events within a sentence, a."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "To date, most DEE methods [8, 9, 10, 11] mainly focus on the first challenge but ignores the second one."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": false,
          "contexts": [
            "To date, most DEE methods [8, 9, 10, 11] mainly focus on the first challenge but ignores the second one."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "Despite successful efforts [1, 2, 3, 4, 5, 6, 7] to extract events within a sentence, a."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        }
      ]
    },
    "251892871": {
      "citing_paper_info": {
        "title": "DEERE: Document-Level Event Extraction as Relation Extraction",
        "abstract": "The descriptions of complex events usually span sentences, so we need to extract complete event information from the whole document. To address the challenges of document-level event extraction, we propose a novel framework named Document-level Event Extraction as Relation Extraction (DEERE), which is suitable for document-level event extraction tasks without trigger-word labelling. By well-designed task transformation, DEERE remodels event extraction as single-stage relation extraction, which can mitigate error propagation. A long text supported encoder is adopted in the relation extraction model to aware the global context effectively. A fault-tolerant event integration algorithm is designed to improve the prediction accuracy. Experimental results show that our approach advances the SOTA for the ChFinAnn dataset by an average F1-score of 3.7. The code and data are available at https://github.com/maomaotfntfn/DEERE.",
        "year": 2022,
        "venue": "Mobile Information Systems",
        "authors": [
          {
            "authorId": "2151968956",
            "name": "Jian Li"
          },
          {
            "authorId": "40525396",
            "name": "Ruijuan Hu"
          },
          {
            "authorId": "39046114",
            "name": "Keliang Zhang"
          },
          {
            "authorId": "2183241822",
            "name": "Haiyan Liu"
          },
          {
            "authorId": "2148986836",
            "name": "Yanzhou Ma"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 16,
        "influential_count": 3,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "235458429",
        "2524712",
        "14339673",
        "234790176",
        "5993783",
        "233307138",
        "196178503",
        "52967399",
        "41089825",
        "202539496",
        "950755",
        "231728756",
        "202537635",
        "2114517",
        "6452487",
        "51871198"
      ],
      "citation_details": [
        {
          "citedcorpusid": 950755,
          "isinfluential": false,
          "contexts": [
            "Early classification models [20, 21], divide DEE into two subtasks: recognition of event descriptors and detecting arguments, using SVM as classifiers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "SEE uses only features obtained from intrasentences, and traditional feature engineering-based approaches [12, 13], Table 3: F1-score for all event types on single-event (S."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2524712,
          "isinfluential": false,
          "contexts": [
            "Early classification models [20, 21], divide DEE into two subtasks: recognition of event descriptors and detecting arguments, using SVM as classifiers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
            "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
            "year": 2009,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5993783,
          "isinfluential": false,
          "contexts": [
            "Pipeline methods [14, 15], whether using CNN or RNN, use a pipeline approach to split the extraction process into two separate processes, extracting event trigger words and detecting arguments."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A language-independent neural network for event detection",
            "abstract": "Event detection remains a challenge because of the difficulty of encoding the word semantics in various contexts. Previous approaches have heavily depended on language-specific knowledge and preexisting natural language processing tools. However, not all languages have such resources and tools available compared with English language. A more promising approach is to automatically learn effective features from data, without relying on language-specific resources. In this study, we develop a language-independent neural network to capture both sequence and chunk information from specific contexts and use them to train an event detector for multiple languages without any manually encoded features. Experiments show that our approach can achieve robust, efficient and accurate results for various languages. In the ACE 2005 English event detection task, our approach achieved a 73.4% F-score with an average of 3.0% absolute improvement compared with state-of-the-art. Additionally, our experimental results are competitive for Chinese and Spanish.",
            "year": 2016,
            "venue": "Science China Information Sciences",
            "authors": [
              {
                "authorId": "2674998",
                "name": "Xiaocheng Feng"
              },
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "39483833",
                "name": "Duyu Tang"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "152277111",
                "name": "Bing Qin"
              },
              {
                "authorId": "40282288",
                "name": "Ting Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "To reduce error transfer, joint methods [16, 17], consider simultaneous extraction of trigger words and arguments."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Pipeline methods [14, 15], whether using CNN or RNN, use a pipeline approach to split the extraction process into two separate processes, extracting event trigger words and detecting arguments."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 41089825,
          "isinfluential": false,
          "contexts": [
            "e aim of the Event Extraction (EE) task is to extract structured event information from unstructured text [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of event extraction methods from text for decision support systems",
            "abstract": "",
            "year": 2016,
            "venue": "Decision Support Systems",
            "authors": [
              {
                "authorId": "103368343",
                "name": "Frederik Hogenboom"
              },
              {
                "authorId": "1729599",
                "name": "Flavius Frasincar"
              },
              {
                "authorId": "1678244",
                "name": "U. Kaymak"
              },
              {
                "authorId": "144097974",
                "name": "F. D. Jong"
              },
              {
                "authorId": "34682332",
                "name": "E. Caron"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": true,
          "contexts": [
            "Our proposed method will be evaluated on the ChFinAnn dataset, which is also applied by DCFEE [9], Doc2EDAG [10], and DE-PPN [2].",
            "DCFEE [9] is based on sequence annotation, main event discovery, and arguments complementation strategy to construct the extraction model, which solves the argument-scattering to a certain extent.",
            "S&M DCFEE-O 56.0 46.5 86.7 54.1 48.5 41.2 47.7 45.2 68.4 61.1 61.5 49.6 58.0 — DCFEE-M 48.4 43.1 83.8 53.4 48.1 39.6 47.4 42.0 67.0 60.0 58.9 47.7 55.7 — GreedyDec 75.9 40.8 81.7 49.8 62.2 34.6 65.7 29.4 88.5 42.3 74.8 39.4 60.5 — Doc2EDAG 80.0 61.3 89.4 68.4 77.4 64.6 79.4 69.5 85.5 72.5 82.3 67.3 76.3 — DE-PPN-1 82.4 46.3 78.3 53.9 82.2 45.6 78.1 39.3 82.8 38.5 80.7 44.7 66.2 — DE-PPN 82.1 63.5 89.1 70.5 79.7 66.7 80.6 69.6 88.0 73.2 83.9 68.7 77.9 — DEERE (ours) 84.8 65.4 95.6 71.2 84.5 75.1 83.7 71.1 89.2 77.6 87.6 72.1 81.5 90.8 75.9 83.7\nTable 4: F1-score of ablation tests on DEERE variants.",
            "DCFEE-O and DCFEE-M are the single-event version and multievent version, respectively.",
            "Our framework DEERE is compared with the previous SOTA methods as follows: DCFEE [9] proposed a DEE method based on key-event detection and argument completion."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            ") are mostly based on BERT [7] or other pretrained language models.",
            "In recent years, the relationship extraction SOTA models (Casrel, TPLinker, etc.) are mostly based on BERT [7] or other pretrained language models.)e original BERT uses absolute position encoding and can handle a maximum text length of 512 tokens.",
            ")e head size of the GlobalPointer in the relation extraction model is set to 64.)e text encoder adopts the char-based RoFormer (Chinese_roformer_char_L-12_H-768_A-12), whose parameter scale is comparable to BERTand the vocabulary is reduced to 12000."
          ],
          "intents": [
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "To address the problem of overlapping roles, pretrained language models [18, 19], are used to model intrasentence and intersentence contextual information, improving the accuracy of the task overall."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202537635,
          "isinfluential": false,
          "contexts": [
            ")e training results are shown in Figure 5, where Casrel [5] is a two-stage entity-relationship joint extraction model, and NEZHA [11] C B",
            ")e training results are shown in Figure 5, where Casrel [5] is a two-stage entity-relationship joint extraction model, and NEZHA [11]\nis another pretrained language model using relative position encoding."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "NEZHA: Neural Contextualized Representation for Chinese Language Understanding",
            "abstract": "The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora. In this technical report, we present our practice of pre-training language models named NEZHA (NEural contextualiZed representation for CHinese lAnguage understanding) on Chinese corpora and finetuning for the Chinese NLU tasks. The current version of NEZHA is based on BERT with a collection of proven improvements, which include Functional Relative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy, Mixed Precision Training and the LAMB Optimizer in training the models. The experimental results show that NEZHA achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including named entity recognition (People's Daily NER), sentence matching (LCQMC), Chinese sentiment classification (ChnSenti) and natural language inference (XNLI).",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "153693128",
                "name": "Junqiu Wei"
              },
              {
                "authorId": "153457264",
                "name": "Xiaozhe Ren"
              },
              {
                "authorId": "2108181908",
                "name": "Xiaoguang Li"
              },
              {
                "authorId": "2129867182",
                "name": "WenYong Huang"
              },
              {
                "authorId": "2048004675",
                "name": "Yi Liao"
              },
              {
                "authorId": "2108738457",
                "name": "Yasheng Wang"
              },
              {
                "authorId": "31196965",
                "name": "J. Lin"
              },
              {
                "authorId": "145820291",
                "name": "Xin Jiang"
              },
              {
                "authorId": "2117025507",
                "name": "Xiao Chen"
              },
              {
                "authorId": "1688015",
                "name": "Qun Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "To address the problem of overlapping roles, pretrained language models [18, 19], are used to model intrasentence and intersentence contextual information, improving the accuracy of the task overall."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "In addition, there are methods [26, 27], that transform EE into other tasks such as reading comprehension and intelligent quizzing."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233307138,
          "isinfluential": true,
          "contexts": [
            "RoleSelection EventDivision EF ER EU EO EP Macro-avg Micro-avg + + 74.5 93.0 80.4 77.9 81.9 81.5 83.7 + − 72.7 92.9 80.7 78.4 81.6 81.3 83.5 − + 72.6 92.2 79.8 75.0 79.6 79.8 81.8 − − 70.0 91.9 74.3 68.0 67.6 74.4 74.6\n50\n60\n70\n80\n90\n(% )\n750 1000 1250 1500 1750 2000 2250 2500500 Max-len of RoFormer\nPrecision Recall F1-score\nFigure 4: Effect of maximum encoding length.\ncannot be adapted to tasks that rely on complex semantic relationships.",
            "For this, we try to use RoFormer [8], a Transformer that uses relative position encoding, to encode every document as a whole.",
            ")e performance of EE is basically synchronized with that of relation extraction, and the combination of GPLinker and RoFormer obtains the best score.",
            "Since most of the documents in the dataset are within 2000 tokens, the maximum encoding length (max_len) of RoFormer is set to 2000.",
            ")e head size of the GlobalPointer in the relation extraction model is set to 64.)e text encoder adopts the char-based RoFormer (Chinese_roformer_char_L-12_H-768_A-12), whose parameter scale is comparable to BERTand the vocabulary is reduced to 12000."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
            "abstract": "",
            "year": 2021,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "51111230",
                "name": "Jianlin Su"
              },
              {
                "authorId": "2140045110",
                "name": "Yu Lu"
              },
              {
                "authorId": "1382633722",
                "name": "Shengfeng Pan"
              },
              {
                "authorId": "2079396269",
                "name": "Bo Wen"
              },
              {
                "authorId": "1807486863",
                "name": "Yunfeng Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 234790176,
          "isinfluential": false,
          "contexts": [
            ")e multilayer bidirectional network MLBiNet [25] fuses cross-sentence semantic and associative event information to enhance the discrimination of each event mention."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network",
            "abstract": "We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "104300808",
                "name": "Dongfang Lou"
              },
              {
                "authorId": "10099514",
                "name": "Zhilin Liao"
              },
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2153010067",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "Neural network-based classification models [22, 23], use word embeddings as the input to the decision tree, and then the structured information of the document is obtained through the integration of information."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "Each kind of token-pair is recognized by a specific GlobalPointer, and all GlobalPointer modules share the same text encoder.",
            "However, since the results of relation extraction will directly affect the results of EE, we adopt the recently proposed GPLinker [3], an entity-relationship joint extraction model based on GlobalPointer [4].",
            ")e head size of the GlobalPointer in the relation extraction model is set to 64.)e text encoder adopts the char-based RoFormer (Chinese_roformer_char_L-12_H-768_A-12), whose parameter scale is comparable to BERTand the vocabulary is reduced to 12000.",
            "GlobalPointer is essentially a token-pair recognition model, which can be used in nested and nonnested NER."
          ],
          "intents": [
            "--",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "249579113": {
      "citing_paper_info": {
        "title": "Multi-Turn and Multi-Granularity Reader for Document-Level Event Extraction",
        "abstract": "",
        "year": 2022,
        "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
        "authors": [
          {
            "authorId": "1845787839",
            "name": "Hang Yang"
          },
          {
            "authorId": "1763402",
            "name": "Yubo Chen"
          },
          {
            "authorId": "2200096",
            "name": "Kang Liu"
          },
          {
            "authorId": "11447228",
            "name": "Jun Zhao"
          },
          {
            "authorId": "2169815441",
            "name": "Zuyu Zhao"
          },
          {
            "authorId": "5912673",
            "name": "Weijian Sun"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 17,
        "influential_count": 1,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "226262283",
        "8535316",
        "49393754",
        "51871198",
        "221246218",
        "6300274",
        "2524712",
        "9776219",
        "202770954",
        "950755",
        "6826032",
        "52349712",
        "201307832",
        "13756489",
        "218630327",
        "239085620",
        "8471750"
      ],
      "citation_details": [
        {
          "citedcorpusid": 950755,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features [11, 12, 26] or neural-based contextual representation [4, 8, 10].",
            "TIER [11] extracted role fillers from the secondary context that processes the extraction into three stages: classifying narrative document, recognizing event sentence, and noun phrase analysis."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2524712,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features [11, 12, 26] or neural-based contextual representation [4, 8, 10].",
            "GLACIER [26] used a sentential event recognizer to select sentences and then applied a plausible role filler recognizer to extract role fillers as results."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
            "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
            "year": 2009,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6300274,
          "isinfluential": false,
          "contexts": [
            "The mainstream MRC models extract text spans from passages given the questions and have achieved good results [6, 29, 30, 32, 39].",
            "Previous works [29, 30, 32, 39] have shown that the MRC framework can learn and infer in a document through the question-context pair."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension",
            "abstract": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.",
            "year": 2016,
            "venue": "CoCo@NIPS",
            "authors": [
              {
                "authorId": "1752875",
                "name": "Yelong Shen"
              },
              {
                "authorId": "2421691",
                "name": "Po-Sen Huang"
              },
              {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
              },
              {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6826032,
          "isinfluential": false,
          "contexts": [
            "In recent years, the MRC task has been widely investigated since the release of large-scale corpora [14, 16, 27, 38]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
            "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students’ ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.",
            "year": 2017,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1857734",
                "name": "Guokun Lai"
              },
              {
                "authorId": "1912046",
                "name": "Qizhe Xie"
              },
              {
                "authorId": "2391802",
                "name": "Hanxiao Liu"
              },
              {
                "authorId": "35729970",
                "name": "Yiming Yang"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8471750,
          "isinfluential": false,
          "contexts": [
            "As a fundamental and challenging task in natural language processing (NLP) , EE can produce valuable structured information to facilitate many NLP applications, such as knowledge base construction, question answering, and language understanding [1, 13]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Modeling Biological Processes for Reading Comprehension",
            "abstract": "Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora. In this paper, we focus on a new reading comprehension task that requires complex reasoning over a single document. The input is a paragraph describing a biological process, and the goal is to answer questions that require an understanding of the relations between entities and events in the process. To answer the questions, we first predict a rich structure representing the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1750652",
                "name": "Jonathan Berant"
              },
              {
                "authorId": "3052879",
                "name": "Vivek Srikumar"
              },
              {
                "authorId": "2158502941",
                "name": "Pei-Chun Chen"
              },
              {
                "authorId": "1824195",
                "name": "Abby Vander Linden"
              },
              {
                "authorId": "144521450",
                "name": "Brittany Harding"
              },
              {
                "authorId": "2110381942",
                "name": "B. Huang"
              },
              {
                "authorId": "48323507",
                "name": "Peter Clark"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8535316,
          "isinfluential": false,
          "contexts": [
            "The mainstream MRC models extract text spans from passages given the questions and have achieved good results [6, 29, 30, 32, 39].",
            "Previous works [29, 30, 32, 39] have shown that the MRC framework can learn and infer in a document through the question-context pair."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bidirectional Attention Flow for Machine Comprehension",
            "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
            "year": 2016,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "4418074",
                "name": "Minjoon Seo"
              },
              {
                "authorId": "2684226",
                "name": "Aniruddha Kembhavi"
              },
              {
                "authorId": "143787583",
                "name": "Ali Farhadi"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9776219,
          "isinfluential": false,
          "contexts": [
            "A great number of EE research focuses on SEE, and most are based on the expert-annotated benchmark ACE 2005 [7] dataset."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation",
            "abstract": "",
            "year": 2004,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2862682",
                "name": "G. Doddington"
              },
              {
                "authorId": "2449760",
                "name": "A. Mitchell"
              },
              {
                "authorId": "2282719",
                "name": "Mark A. Przybocki"
              },
              {
                "authorId": "1744313",
                "name": "L. Ramshaw"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              },
              {
                "authorId": "1732071",
                "name": "R. Weischedel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": true,
          "contexts": [
            "The second one is how to model long texts, as most MRC methods are based on the Transformer [31] architecture, which is limited to a fixed-length (e.g., 512) input.",
            "For the basic MRC-based method, we adopt Transformer-base [31], which has 12 layers, 768 hidden units, and 12 attention heads, as the local encoder.",
            "Most of them are based on the Transformer architecture [31] with a multi-layer self-attention mechanism to model long dependencies between tokens with limited sequence length.",
            "In this article, we adopt the Transformer [31], which contains multi-layer self-attention modules, as the local encoder: where h i , j ∈ R d h , d h denotes the hidden size."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49393754,
          "isinfluential": false,
          "contexts": [
            "McCann et al. [23] cast 10 tasks (e.g., machine translation, summarization, relation extraction, etc.) as a question answering paradigm and propose a general model for them with jointly training."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Natural Language Decathlon: Multitask Learning as Question Answering",
            "abstract": "Presented on August 28, 2018 at 12:15 p.m. in the Pettit Microelectronics Research Center, Room 102 A/B.",
            "year": 2018,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "143775536",
                "name": "Bryan McCann"
              },
              {
                "authorId": "2844898",
                "name": "N. Keskar"
              },
              {
                "authorId": "2228109",
                "name": "Caiming Xiong"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "DCFEE [36] proposed a tagging-based model for SEE and a key-event detection model with an arguments-completion strategy for DEE.",
            "To solve the preceding challenges, previous works [36, 40] formulated DEE as a two-step paradigm: from sentence-level candidate argument extraction to document-level event fusion.",
            "For exploring the real challenges (i.e., multi-events and arguments scattering) for DEE, DCFEE [36] proposed a pipeline method that contains a neural-based sequence tagging model for SEE and a key-event detection model with an arguments-completion strategy for DEE."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52349712,
          "isinfluential": false,
          "contexts": [
            "In recent years, as neural networks proved the effectiveness for NLP, many approaches [2, 3, 5, 19, 25, 28, 37, 37] have been proposed to improve performance on this task by employing deep learning models, such as recurrent neural networks and convolutional neural networks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Rapid Customization for Event Extraction",
            "abstract": "Extracting events in the form of who is involved in what at when and where from text, is one of the core information extraction tasks that has many applications such as web search and question answering. We present a system for rapidly customizing event extraction capability to find new event types (what happened) and their arguments (who, when, and where). To enable extracting events of new types, we develop a novel approach to allow a user to find, expand and filter event triggers by exploring an unannotated development corpus. The system will then generate mention level event annotation automatically and train a neural network model for finding the corresponding events. To enable extracting arguments for new event types, the system makes novel use of the ACE annotation dataset to train a generic argument attachment model for extracting Actor, Place, and Time. We demonstrate that with less than 10 minutes of human effort per event type, the system achieves good performance for 67 novel event types. Experiments also show that the generic argument attachment model performs well on the novel event types. Our system (code, UI, documentation, demonstration video) is released as open source.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2988645",
                "name": "Yee Seng Chan"
              },
              {
                "authorId": "2305337",
                "name": "Joshua Fasching"
              },
              {
                "authorId": "1817035",
                "name": "Haoling Qiu"
              },
              {
                "authorId": "1875233",
                "name": "Bonan Min"
              }
            ]
          }
        },
        {
          "citedcorpusid": 201307832,
          "isinfluential": false,
          "contexts": [
            "2 A straightforward solution for modeling long texts is the sliding window [34], but this method sacrifices the possibility that the distant tokens “pay attention” to each other."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering",
            "abstract": "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "40296541",
                "name": "Zhiguo Wang"
              },
              {
                "authorId": "145878390",
                "name": "Patrick Ng"
              },
              {
                "authorId": "47646605",
                "name": "Xiaofei Ma"
              },
              {
                "authorId": "1701451",
                "name": "Ramesh Nallapati"
              },
              {
                "authorId": "144028698",
                "name": "Bing Xiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202770954,
          "isinfluential": false,
          "contexts": [
            "To this end, a great number of previous works [9, 19, 25, 33, 37] focus on sentence-level event extraction (SEE) ,which aims to detect events and extract arguments from one sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HMEAE: Hierarchical Modular Event Argument Extraction",
            "abstract": "Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              },
              {
                "authorId": "2108485135",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1384550891",
                "name": "Xiang Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "MGR [8] proposed a tagging-based model to dynamically incorporate paragraph-and sentence-level representations based on contextualized embeddings produced by the pre-trained language model BERT [6].",
            "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features [11, 12, 26] or neural-based contextual representation [4, 8, 10].",
            "Following prior work [8], we adopt head noun phrase match and exact match accuracy to compare the extractions against gold role fillers for evaluation."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221246218,
          "isinfluential": false,
          "contexts": [
            "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features [11, 12, 26] or neural-based contextual representation [4, 8, 10]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event-based Extraction Using Generative Template-filling Transformers",
            "abstract": "We revisit the classic information extraction problem of document-level template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is \\emph{implicitly} aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions. Our code for the evaluation script and models will be open-sourced at this https URL for reproduction purposes.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie [9], Li et al. [18], and Liu et al. [22] introduce an MRC paradigm for EE in an end-to-end manner."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 239085620,
          "isinfluential": false,
          "contexts": [
            "In recent years, as neural networks proved the efectiveness for NLP, many approaches [2, 3, 5, 19, 25, 28, 37, 37] have been proposed to improve performance on this task by employing deep learning models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNN)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
            "abstract": "",
            "year": 2019,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The first one is the task of document-level event role filler extraction, which is based on the classic MUC-4 dataset [24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "234358675": {
      "citing_paper_info": {
        "title": "Document-level Event Extraction with Efficient End-to-end Learning of Cross-event Dependencies",
        "abstract": "Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency.",
        "year": 2020,
        "venue": "NUSE",
        "authors": [
          {
            "authorId": "1420116116",
            "name": "Kung-Hsiang Huang"
          },
          {
            "authorId": "3157053",
            "name": "Nanyun Peng"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 13,
        "influential_count": 3,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "2114517",
        "4952494",
        "204851964",
        "20744",
        "52816033",
        "221246218",
        "119308902",
        "214673189",
        "44161048",
        "57193015",
        "220048375",
        "218551030",
        "8336242"
      ],
      "citation_details": [
        {
          "citedcorpusid": 20744,
          "isinfluential": false,
          "contexts": [
            "The majority of the previous event extraction works focus on sentence level (Li and Ji, 2014; Huang et al., 2020; Lin et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Incremental Joint Extraction of Entity Mentions and Relations",
            "abstract": "We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search. A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations. Experiments on Automatic Content Extraction (ACE) 1 corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": true,
          "contexts": [
            "Li et al. (2013) 3 Document-level Event Extraction",
            "While some works attempted to capture such dependencies with conditional random field or other structured prediction algorithms on hand-crafted features (Li et al., 2013; Lin et al., 2020), these approaches subject to scalablility issue and require certain level of human efforts.",
            "Initial attempts on event extraction relied on hand-crafted features and a pipeline architecture (Ahn, 2006; Gupta and Ji, 2009; Li et al., 2013).",
            "While some works attempted to capture such dependencies with conditional random ﬁeld or other structured prediction algorithms on hand-crafted features (Li et al., 2013; Lin et al., 2020",
            "(Ahn, 2006; Gupta and Ji, 2009; Li et al., 2013)."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4952494,
          "isinfluential": false,
          "contexts": [
            "Biases have been studied in many information extraction tasks, such as relation extraction (Gaut et al., 2020), named entity recognition (Mehrabi et al., 2020), and coreference resolution (Zhao et al., 2018a).",
            "A structured prediction model that learns cross-event interactions can potentially infer the correct event type for death given the previous S ENTENCE event is often carried out by authorities. the scope of sentence (Yang and Mitchell, 2016; Zhao et al., 2018b; Wadden et al., 2019)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
            "abstract": "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "33524946",
                "name": "Jieyu Zhao"
              },
              {
                "authorId": "1785372925",
                "name": "Tianlu Wang"
              },
              {
                "authorId": "2064210",
                "name": "Mark Yatskar"
              },
              {
                "authorId": "2004053",
                "name": "Vicente Ordonez"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8336242,
          "isinfluential": false,
          "contexts": [
            "Initial attempts on event extraction relied on hand-crafted features and a pipeline architecture (Ahn, 2006; Gupta and Ji, 2009; Li et al., 2013)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Predicting Unknown Time Arguments based on Cross-Event Propagation",
            "abstract": "Many events in news articles don't include time arguments. This paper describes two methods, one based on rules and the other based on statistical learning, to predict the unknown time argument for an event by the propagation from its related events. The results are promising - the rule based approach was able to correctly predict 74% of the unknown event time arguments with 70% precision.",
            "year": 2009,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2119997825",
                "name": "Prashant Gupta"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 44161048,
          "isinfluential": false,
          "contexts": [
            "(Ju et al., 2018; Qin et al., 2018; Stanovsky et al., 2018)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Neural Layered Model for Nested Named Entity Recognition",
            "abstract": "Entity mentions embedded in longer entity mentions are referred to as nested entities. Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts. To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers. Each flat NER layer is based on the state-of-the-art flat NER model that captures sequential context representation with bidirectional Long Short-Term Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer. This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way. Our model dynamically stacks the flat NER layers until no outer entities are extracted. Extensive evaluation shows that our dynamic model outperforms state-of-the-art feature-based systems on nested NER, achieving 74.7% and 72.2% on GENIA and ACE2005 datasets, respectively, in terms of F-score.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3237065",
                "name": "Meizhi Ju"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52816033,
          "isinfluential": false,
          "contexts": [
            "Later studies gained signiﬁcant improvement from neural approaches, especially large pre-trained language models (Wad-den et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020)."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
            "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2730687",
                "name": "Zhunchen Luo"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 57193015,
          "isinfluential": false,
          "contexts": [
            "Structured Prediction on Event Extraction Existing event extraction systems integrating structured prediction typically uses conditional random fields (CRFs) to capture dependencies between predicted events (Xu et al., 2019; Wang et al., 2018).",
            "Existing event extraction systems integrating structured prediction typically uses conditional random ﬁelds (CRFs) to capture dependencies between predicted events (Xu et al., 2019; Wang et al., 2018)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bidirectional long short-term memory with CRF for detecting biomedical event trigger in FastText semantic space",
            "abstract": "In biomedical information extraction, event extraction plays a crucial role. Biological events are used to describe the dynamic effects or relationships between biological entities such as proteins and genes. Event extraction is generally divided into trigger detection and argument recognition. The performance of trigger detection directly affects the results of the event extraction. In general, the traditional method is used to address the trigger detection as a classification task, as well as the use of machine learning or rules method, which construct many features to improve the classification results. Moreover, the classification model only recognizes triggers composed of single words, whereas for multiple words, the result is unsatisfactory. The corpus of our model is MLEE. If we were to only use the biomedical LSTM and CRF model without other features, the F-score would reach about 78.08%. Comparing entity to part of speech (POS), we find the entity features more conducive to the improvement of performance of detection, with the F-score potentially reaching about 80%. Furthermore, we also experiment on the other three corpora (BioNLP 2009, BioNLP 2011, and BioNLP 2013) to verify the generalization of our model. Hence, F-scores can reach more than 60%, which are better than the comparative experiments. The trigger recognition method based on the sequence annotation model does not require initial complex feature engineering, and only requires a simple labeling mechanism to complete the training. Therefore, generalization of our model is better compared to other traditional models. Secondly, this method can identify multi-word triggers, thereby improving the F-scores of trigger recognition. Thirdly, details on the entity have a crucial impact on trigger detection. Finally, the combination of character-level word embedding and word-level word embedding provides increasingly effective information for the model; therefore, it is a key to the success of the experiment.",
            "year": 2018,
            "venue": "BMC Bioinformatics",
            "authors": [
              {
                "authorId": "2293045413",
                "name": "Yan Wang"
              },
              {
                "authorId": "49605993",
                "name": "Jian Wang"
              },
              {
                "authorId": "37553559",
                "name": "Hongfei Lin"
              },
              {
                "authorId": "47273981",
                "name": "Xiwei Tang"
              },
              {
                "authorId": "5015052",
                "name": "Shaowu Zhang"
              },
              {
                "authorId": "2071830",
                "name": "Lishuang Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "Zheng et al. (2019) transforms tabular event data into entity-based directed acyclic graphs to tackle the argument scattering challenge.",
            "Evaluation metrics used by previous sentence-level event extraction studies (Wadden et al., 2019; Zheng et al., 2019; Lin et al., 2020) are not suitable for our task as event coreference and entity coreference are not considered."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204851964,
          "isinfluential": false,
          "contexts": [
            "Biases have been studied in many information extraction tasks, such as relation extraction (Gaut et al., 2020), named entity recognition (Mehrabi et al., 2020), and coreference resolution (Zhao et al., 2018a)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition",
            "abstract": "In this paper, we study the bias in named entity recognition (NER) models---specifically, the difference in the ability to recognize male and female names as PERSON entity types. We evaluate NER models on a dataset containing 139 years of U.S. census baby names and find that relatively more female names, as opposed to male names, are not recognized as PERSON entities. The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. The data and code for the application of this benchmark is publicly available for researchers to use.",
            "year": 2019,
            "venue": "ACM Conference on Hypertext & Social Media",
            "authors": [
              {
                "authorId": "51997673",
                "name": "Ninareh Mehrabi"
              },
              {
                "authorId": "145845766",
                "name": "Thamme Gowda"
              },
              {
                "authorId": "2775559",
                "name": "Fred Morstatter"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "143728483",
                "name": "A. Galstyan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214673189,
          "isinfluential": true,
          "contexts": [
            "Later studies gained significant improvement from neural approaches, especially large pre-trained language models (Wadden et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020).",
            "The trigger classiﬁcation F 1 metric adopted by previous works (Wadden et al., 2019; Lin et al., 2020) is used as the oracle value function v ∗ ( y trig , y trig ∗ ) .",
            "Evaluation metrics used by previous sentencelevel event extraction studies (Wadden et al., 2019; Zheng et al., 2019; Lin et al., 2020) are not suitable for our task as event coreference and entity coreference are not considered.",
            "Experiments are conducted at the document level instead of sentence level as previous works (Wadden et al., 2019; Lin et al., 2020).",
            "…compare D EE D with three baselines: (1) B ASE , the base model described in Section 4.1; (2) BC RF extends B ASE by adding a CRF layer on top of the trigger classiﬁer; (3) OneIE + is a pipeline composed of the joint model presented in Lin et al. (2020) and coreference modules adapted from B ASE .",
            "To understand the capabilities of each module, we show an evaluation breakdown on each component following previous works (Wadden et al., 2019; Lin et al., 2020) in Table 2.",
            "The majority of the previous event extraction works focus on sentence level (Li and Ji, 2014; Huang et al., 2020; Lin et al., 2020).",
            "While some works attempted to capture such dependencies with conditional random field or other structured prediction algorithms on hand-crafted features (Li et al., 2013; Lin et al., 2020), these approaches subject to scalablility issue and require certain level of human efforts.",
            "Later studies gained signiﬁcant improvement from neural approaches, especially large pre-trained language models (Wad-den et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020).",
            "Lin et al. (2020) is the state-of-the-art sentence-level event extraction model that utilizes beam search and CRF with global features to model cross sub-task dependencies.",
            "This task is similar to the sentence-level event extraction task addressed by previous studies (Wadden et al., 2019; Lin et al., 2020).",
            "While some works attempted to capture such dependencies with conditional random ﬁeld or other structured prediction algorithms on hand-crafted features (Li et al., 2013; Lin et al., 2020",
            "Arg-I and Arg-C are much lower than the reported scores by previous studies (Wadden et al., 2019; Lin et al., 2020).",
            "Evaluation metrics used by previous sentence-level event extraction studies (Wadden et al., 2019; Zheng et al., 2019; Lin et al., 2020) are not suitable for our task as event coreference and entity coreference are not considered."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "['result']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['result']",
            "['background']",
            "['result']",
            "['background']",
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploiting sequence labeling framework to extract document-level relations from biomedical texts",
            "abstract": "Both intra- and inter-sentential semantic relations in biomedical texts provide valuable information for biomedical research. However, most existing methods either focus on extracting intra-sentential relations and ignore inter-sentential ones or fail to extract inter-sentential relations accurately and regard the instances containing entity relations as being independent, which neglects the interactions between relations. We propose a novel sequence labeling-based biomedical relation extraction method named Bio-Seq. In the method, sequence labeling framework is extended by multiple specified feature extractors so as to facilitate the feature extractions at different levels, especially at the inter-sentential level. Besides, the sequence labeling framework enables Bio-Seq to take advantage of the interactions between relations, and thus, further improves the precision of document-level relation extraction. Our proposed method obtained an F1-score of 63.5% on BioCreative V chemical disease relation corpus, and an F1-score of 54.4% on inter-sentential relations, which was 10.5% better than the document-level classification baseline. Also, our method achieved an F1-score of 85.1% on n2c2-ADE sub-dataset. Sequence labeling method can be successfully used to extract document-level relations, especially for boosting the performance on inter-sentential relation extraction. Our work can facilitate the research on document-level biomedical text mining.",
            "year": 2020,
            "venue": "BMC Bioinformatics",
            "authors": [
              {
                "authorId": "2116263582",
                "name": "Zhiheng Li"
              },
              {
                "authorId": "151472911",
                "name": "Zhihao Yang"
              },
              {
                "authorId": "2068334010",
                "name": "Yang Xiang"
              },
              {
                "authorId": "145965453",
                "name": "Ling Luo"
              },
              {
                "authorId": "2143552718",
                "name": "Yuanyuan Sun"
              },
              {
                "authorId": "37553559",
                "name": "Hongfei Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218551030,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2020a) performs event mention extraction and the two coreference tasks independently using a pipeline approach."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GAIA: A Fine-grained Multimedia Knowledge Extraction System",
            "abstract": "We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3361240",
                "name": "Manling Li"
              },
              {
                "authorId": "2778637",
                "name": "Alireza Zareian"
              },
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "34741133",
                "name": "Xiaoman Pan"
              },
              {
                "authorId": "153188991",
                "name": "Spencer Whitehead"
              },
              {
                "authorId": "2108342501",
                "name": "Brian Chen"
              },
              {
                "authorId": "1993581583",
                "name": "Bo Wu"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "9546964",
                "name": "Shih-Fu Chang"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              },
              {
                "authorId": "1413386261",
                "name": "Dan Napierski"
              },
              {
                "authorId": "2052513135",
                "name": "Marjorie Freedman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220048375,
          "isinfluential": true,
          "contexts": [
            "The trigger classiﬁcation F 1 metric adopted by previous works (Wadden et al., 2019; Lin et al., 2020) is used as the oracle value function v ∗ ( y trig , y trig ∗ ) .",
            "Experiments are conducted at the document level instead of sentence level as previous works (Wadden et al., 2019; Lin et al., 2020).",
            "…compare D EE D with three baselines: (1) B ASE , the base model described in Section 4.1; (2) BC RF extends B ASE by adding a CRF layer on top of the trigger classiﬁer; (3) OneIE + is a pipeline composed of the joint model presented in Lin et al. (2020) and coreference modules adapted from B ASE .",
            "To understand the capabilities of each module, we show an evaluation breakdown on each component following previous works (Wadden et al., 2019; Lin et al., 2020) in Table 2.",
            "The majority of the previous event extraction works focus on sentence level (Li and Ji, 2014; Huang et al., 2020; Lin et al., 2020).",
            "Later studies gained signiﬁcant improvement from neural approaches, especially large pre-trained language models (Wad-den et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020).",
            "Lin et al. (2020) is the state-of-the-art sentence-level event extraction model that utilizes beam search and CRF with global features to model cross sub-task dependencies.",
            "This task is similar to the sentence-level event extraction task addressed by previous studies (Wadden et al., 2019; Lin et al., 2020).",
            "While some works attempted to capture such dependencies with conditional random ﬁeld or other structured prediction algorithms on hand-crafted features (Li et al., 2013; Lin et al., 2020",
            "Arg-I and Arg-C are much lower than the reported scores by previous studies (Wadden et al., 2019; Lin et al., 2020).",
            "Evaluation metrics used by previous sentence-level event extraction studies (Wadden et al., 2019; Zheng et al., 2019; Lin et al., 2020) are not suitable for our task as event coreference and entity coreference are not considered."
          ],
          "intents": [
            "['methodology']",
            "['result']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['result']",
            "['background']",
            "['result']",
            "['background']",
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Joint Neural Model for Information Extraction with Global Features",
            "abstract": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "143857288",
                "name": "Fei Huang"
              },
              {
                "authorId": "3008832",
                "name": "Lingfei Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221246218,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie (2020) employed a mutli-granularity reader to aggregate representations from different levels of granularity.",
            "Du and Cardie (2020) evaluates entity coreference using bipartite matching.",
            "More recently, Du and Cardie (2020) and Du et al. (2020) treat document-level event extraction as a template-ﬁlling task."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event-based Extraction Using Generative Template-filling Transformers",
            "abstract": "We revisit the classic information extraction problem of document-level template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is \\emph{implicitly} aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions. Our code for the evaluation script and models will be open-sourced at this https URL for reproduction purposes.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2020a) performs event mention extraction and the two coreference tasks independently using a pipeline approach."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "257366694": {
      "citing_paper_info": {
        "title": "Multi-Round Extraction and Dynamic Role Selection Framework For Document-Level Event Extraction",
        "abstract": "Document-level Event Extraction (DEE) aims to extract structured event information from a document, which is an indispensable downstream task for many NLP applications. Argument-scatter and multi-event are its two main challenges. Recent work decomposes this challenging DEE task into multiple steps such as entity recognition, contextual information modeling, and event arguments extraction. Besides, they all extract event arguments in a predefined fixed role order. Though it is effective, its cumbersome steps and fixed extraction order will bring about the problem of error propagation. To address this issue, we propose a Multi-round Extraction and Dynamic Role Selection (MREDRS) Framework for the DEE task. We model the DEE task in an end-to-end manner to avoid these cumbersome steps. Multi-round extraction can deal with multi-event problem. In order to avoid the error propagation problem caused by the fixed role extraction order, we dynamically select the next role according to the current extraction state. We conducted experiments on the commonly used DEE dataset and extensive experimental results demonstrated the effectiveness of our method.",
        "year": 2022,
        "venue": "MLNLP",
        "authors": [
          {
            "authorId": "2210834128",
            "name": "Kaizhou Zhang"
          },
          {
            "authorId": "2108237336",
            "name": "Pengfei Wang"
          },
          {
            "authorId": "1452981772",
            "name": "Lei Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 1,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "9946972",
        "236460259",
        "63777164",
        "233219850",
        "14610045",
        "231728756",
        "235253912",
        "11212020"
      ],
      "citation_details": [
        {
          "citedcorpusid": 9946972,
          "isinfluential": false,
          "contexts": [
            "MUC-4 [18] proposes template-filling task that aims to extract event role fillers from the document."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Fourth Message Understanding Evaluation and Conference",
            "abstract": "The Fourth Message Understanding Conference (MUC-4) is the latest in a series of conferences that concern the evaluation of natural language processing (NLP) systems. These conferences have reported on progress being made both in the development of systems capable of analyzing relatively short English texts and in the definition of a rigorous performance evaluation methodology. MUC-4 was preceded by a period of intensive system development by each of the participating organizations and blind testing using materials prepared by NRaD and SAIC that are described in this paper, other papers in this volume, and the MUC-3 proceedings [1].",
            "year": 1992,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11212020,
          "isinfluential": false,
          "contexts": [
            "Attention mechanism [1] is used to choose the next role with the highest probability."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14610045,
          "isinfluential": false,
          "contexts": [
            "Schema induction means that researchers use unsupervised method [8, 13] to induce event schema via very large corpus and extract corresponding events."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Liberal Event Extraction and Event Schema Induction",
            "abstract": ".",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "1739186",
                "name": "Taylor Cassidy"
              },
              {
                "authorId": "2674998",
                "name": "Xiaocheng Feng"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              },
              {
                "authorId": "145325584",
                "name": "Jiawei Han"
              },
              {
                "authorId": "2707234",
                "name": "Avirup Sil"
              }
            ]
          }
        },
        {
          "citedcorpusid": 63777164,
          "isinfluential": false,
          "contexts": [
            "[16, 17] use bidirectional RNN to simultaneously extract trigger words and arguments of events to avoid the error propagation problem in the pipeline method.",
            "The ACE conference greatly facilitates the progress of event extraction at the sentence level [2, 5, 9, 14, 16, 20, 24]."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Association for Computational Linguistics: Human Language Technologies",
            "abstract": "",
            "year": 2011,
            "venue": "",
            "authors": [
              {
                "authorId": "123090878",
                "name": "H. Chinaei"
              },
              {
                "authorId": "107957150",
                "name": "M. Dreyer"
              },
              {
                "authorId": "1734443",
                "name": "J. Gillenwater"
              },
              {
                "authorId": "52589367",
                "name": "S. Hassan"
              },
              {
                "authorId": "15652489",
                "name": "T. Kwiatkowski"
              },
              {
                "authorId": "2066761753",
                "name": "M. Lehr"
              },
              {
                "authorId": "2041318",
                "name": "A. Levenberg"
              },
              {
                "authorId": "1781337",
                "name": "Ziheng Lin"
              },
              {
                "authorId": "1700614",
                "name": "P. Mannem"
              },
              {
                "authorId": "145272998",
                "name": "E. Morley"
              },
              {
                "authorId": "145254207",
                "name": "Nathan Schneider"
              },
              {
                "authorId": "1683714",
                "name": "S. Bergsma"
              },
              {
                "authorId": "34891091",
                "name": "D. Bernhard"
              },
              {
                "authorId": null,
                "name": "F Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "Recently, [6, 12, 25] follow this direction to extract event role fillers and public corresponding RAMS and WIKIEVENTS datasets."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "Recently, [6, 12, 25] follow this direction to extract event role fillers and public corresponding RAMS and WIKIEVENTS datasets."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": true,
          "contexts": [
            "But our framework still gets the highest result on all event types and improves the overall micro F1-score from 71.7% to 73.6% in the multi-event scenario compared to GIT [21].",
            "Recent works [21, 27] decompose it into multiple steps.",
            "6% in the multi-event scenario compared to GIT [21].",
            "[21] follows the tree expanding and further captures the contextual information and models the relations between events and arguments.",
            "• GIT [21]: GIT based on Doc2EDAG, they model the relationship between events and further modeling the relationship between entities and sentences in serial prediction paradigm."
          ],
          "intents": [
            "--",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": false,
          "contexts": [
            "[23] first proposes a parallel prediction paradigm for DEE."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        }
      ]
    },
    "258216783": {
      "citing_paper_info": {
        "title": "A Document-level Event Extraction Method Based on Mogrifier LSTM",
        "abstract": "Element dispersion is a difficulty for document-level event extraction. While classic LSTM lacks the capability to interact between input and context while collecting long sequence features, previous document-level event extraction utilizes the entire document as input, leaving the sequence features of the document devoid of deeper contextual information. This paper suggests a document-level event extraction strategy based on Mogrifier LSTM to solve this issue. We divide the text into multiple paragraphs and then input each paragraph individually into the Mogrifier LSTM. To increase the context modeling capability of lengthy sequence text, the upgraded LSTM will allow the input of the present moment and the output of the preceding moment to be computed several times initially. Then an attention mechanism is introduced to capture the internal correlation of each paragraph and integrate the contextual semantics of each paragraph. Finally, sequence annotation is used to extract dispersed event elements and match event types. According to the experimental results on Chinese financial dataset, the method in this paper can effectively solve the problems of loss of depth information and scattered theoretical elements of long sequence features of documents, and improve the effectiveness of document-level event extraction.",
        "year": 2022,
        "venue": "International Conference on Robotics, Intelligent Control and Artificial Intelligence",
        "authors": [
          {
            "authorId": "50767313",
            "name": "Junwei Ge"
          },
          {
            "authorId": "2156745375",
            "name": "Zhixian Qin"
          },
          {
            "authorId": "1724440",
            "name": "Yiqiu Fang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "6452487",
        "245123950",
        "233613419",
        "5959482",
        "119308902"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5959482,
          "isinfluential": false,
          "contexts": [
            "The paragraph representation layer utilizes pre-trained word vectors using the skip-gram [10] model in word2vec to map a low-latitude sparse word vector to a high-dimensional dense vector."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
              },
              {
                "authorId": "2118440152",
                "name": "Kai Chen"
              },
              {
                "authorId": "32131713",
                "name": "G. Corrado"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "…of this paper’s model for event extraction, this paper’s model is compared with four benchmark models for event extraction, which are: a) JRNN [1], which uses a bidirectional recurrent neural network for joint extraction, and the joint extraction model effectively avoids the error propagation…",
            "For example, JRNN (joint event extraction via recurrent neural networks) [1], uses bidirectional recurrent neural networks to jointly extract event types, event theoretical elements."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "The experimental dataset in this paper is the Chinese financial dataset from the Interdisciplinary Information Science Research at Tsinghua University (https://github.com/dolphin-zs/Doc2EDAG) [12]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233613419,
          "isinfluential": false,
          "contexts": [
            "A joint extraction model based on relation-aware converters [5] uses a relation-aware converter-based encoder to capture contextual information in multiple sentences and then combines the results of event element extraction for event type classification."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TDJEE: A Document-Level Joint Model for Financial Event Extraction",
            "abstract": "Extracting financial events from numerous financial announcements is very important for investors to make right decisions. However, it is still challenging that event arguments always scatter in multiple sentences in a financial announcement, while most existing event extraction models only work in sentence-level scenarios. To address this problem, this paper proposes a relation-aware Transformer-based Document-level Joint Event Extraction model (TDJEE), which encodes relations between words into the context and leverages modified Transformer to capture document-level information to fill event arguments. Meanwhile, the absence of labeled data in financial domain could lead models be unstable in extraction results, which is known as the cold start problem. Furthermore, a Fonduer-based knowledge base combined with the distant supervision method is proposed to simplify the event labeling and provide high quality labeled training corpus for model training and evaluating. Experimental results on real-world Chinese financial announcement show that, compared with other models, TDJEE achieves competitive results and can effectively extract event arguments across multiple sentences.",
            "year": 2021,
            "venue": "Electronics",
            "authors": [
              {
                "authorId": "2155302012",
                "name": "Peng Wang"
              },
              {
                "authorId": "103283164",
                "name": "Zhen-ke Deng"
              },
              {
                "authorId": "2087801075",
                "name": "Ruilong Cui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245123950,
          "isinfluential": false,
          "contexts": [
            "…network for joint extraction, and the joint extraction model effectively avoids the error propagation problem in the pipeline model. b) PTPCG [13], a document-level event extraction model that is compatible with document-level event extraction with and without triggers through pseudo-trigger…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph",
            "abstract": "Most previous studies of document-level event extraction mainly focus on building argument chains in an autoregressive way, which achieves a certain success but is inefficient in both training and inference.\n\nIn contrast to the previous studies, we propose a fast and lightweight model named as PTPCG.\n\nIn our model, we design a novel strategy for event argument combination together with a non-autoregressive decoding algorithm via pruned complete graphs, which are constructed under the guidance of the automatically selected pseudo triggers.\n\nCompared to the previous systems, our system achieves competitive results with 19.8% of parameters and much lower resource consumption, taking only 3.8% GPU hours for training and up to 8.5 times faster for inference.\n\nBesides, our model shows superior compatibility for the datasets with (or without) triggers and the pseudo triggers can be the supplements for annotated triggers to make further improvements.\n\nCodes are available at https://github.com/Spico197/DocEE .",
            "year": 2021,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1914586128",
                "name": "Tong Zhu"
              },
              {
                "authorId": "51912474",
                "name": "Xiaoye Qu"
              },
              {
                "authorId": "48993675",
                "name": "Wenliang Chen"
              },
              {
                "authorId": "2243403387",
                "name": "Zhefeng Wang"
              },
              {
                "authorId": "2422046",
                "name": "Baoxing Huai"
              },
              {
                "authorId": "1677643972",
                "name": "N. Yuan"
              },
              {
                "authorId": "1390813134",
                "name": "Min Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "…the central sentence of the event based on a heuristic strategy, which solves the problem of ar-gument dispersion to some extent. d) HNN-EE [15], a hybrid neural network event joint extraction model, focuses on the dependencies of entities and events in events, using traditional LSTM to…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "247858415": {
      "citing_paper_info": {
        "title": "A Chinese Document-level Event Extraction Method based on ERNIE",
        "abstract": "Event extraction is a key technology in natural language processing and has strong application prospects for extracting knowledge from unstructured data. The current event extraction technique is mainly based on sentence-based event extraction, which has the disadvantages of incomplete coverage of extracted events and ambiguity in event classification. In this paper, we propose the ERNIE-BiGRU-CRF model for chapter-level event extraction, which encodes the semantic enhancement of paragraph text by ERNIE pretrained language model, inputs a bidirectional gated neural network for feature extraction, and finally obtains the annotated sequence by CRF layer. In this paper, we perform event extraction on the Baidu financial domain documen-level event extraction dataset using the sequence-labeled trigger extraction model and the sequence-labeled event element extraction model, and the results show that the final model's F1 value is 5.45 percentage points higher than the baseline model.",
        "year": 2021,
        "venue": "2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT)",
        "authors": [
          {
            "authorId": "2161719957",
            "name": "Jiahua Zhang"
          },
          {
            "authorId": "3039278",
            "name": "Xiaochuan Jing"
          },
          {
            "authorId": "2159389856",
            "name": "Junkang Zheng"
          },
          {
            "authorId": "2150088998",
            "name": "Boya Shi"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 2,
        "unique_cited_count": 1,
        "influential_count": 0,
        "detailed_records_count": 2
      },
      "cited_papers": [
        "226096901"
      ],
      "citation_details": [
        {
          "citedcorpusid": 226096901,
          "isinfluential": false,
          "contexts": [
            "Transformer's main module is the attention module[6], and its core formula is: network."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "136309159",
                "name": "知秀 柴田"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "\"[7] In contrast to BERT, it employs three masking strategies: word mask, entity mask, and phrase mask, all of which are capable of learning entity attributes and relationships implicitly."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "269004443": {
      "citing_paper_info": {
        "title": "Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction",
        "abstract": "This paper presents multiple question generation strategies for document-level event argument extraction. These strategies do not require human involvement and result in uncontextualized questions as well as contextualized questions grounded on the event and document of interest. Experimental results show that combining uncontextualized and contextualized questions is beneficial,especially when event triggers and arguments appear in different sentences. Our approach does not have corpus-specific components, in particular, the question generation strategies transfer across corpora. We also present a qualitative analysis of the most common errors made by our best model.",
        "year": 2024,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2261083690",
            "name": "Md Nayem Uddin"
          },
          {
            "authorId": "2295669823",
            "name": "Enfa Rose George"
          },
          {
            "authorId": "2295665323",
            "name": "Eduardo Blanco"
          },
          {
            "authorId": "2295665486",
            "name": "Steven Corman"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 16,
        "unique_cited_count": 15,
        "influential_count": 1,
        "detailed_records_count": 16
      },
      "cited_papers": [
        "256461151",
        "6452487",
        "253510351",
        "202786778",
        "2867611",
        "204838007",
        "226283556",
        "243865619",
        "2505531",
        "11986411",
        "13804679",
        "9776219",
        "269362270",
        "6540287",
        "2486369"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2486369,
          "isinfluential": false,
          "contexts": [
            "Initially, datasets focused on extracting arguments within the same sentence than the event (Palmer et al., 2005; Walker et al., 2006).",
            "Most event-argument annotation efforts satisfy this requirement, including PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), FrameNet (Baker et al., 1998), RAMS, ACE (Doddington et al., 2004), and WikiEvents (Li et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles",
            "abstract": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank.",
            "year": 2005,
            "venue": "International Conference on Computational Logic",
            "authors": [
              {
                "authorId": "145755155",
                "name": "Martha Palmer"
              },
              {
                "authorId": "2489901",
                "name": "Paul R. Kingsbury"
              },
              {
                "authorId": "1793218",
                "name": "D. Gildea"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2505531,
          "isinfluential": false,
          "contexts": [
            "Most event-argument annotation efforts satisfy this requirement, including PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), FrameNet (Baker et al., 1998), RAMS, ACE (Doddington et al., 2004), and WikiEvents (Li et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Berkeley FrameNet Project",
            "abstract": "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.",
            "year": 1998,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1749194",
                "name": "Collin F. Baker"
              },
              {
                "authorId": "2912454",
                "name": "C. Fillmore"
              },
              {
                "authorId": "1406118956",
                "name": "John B. Lowe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2867611,
          "isinfluential": false,
          "contexts": [
            "Early models were based on handcrafted features (Li et al., 2013; Liao and Grishman, 2010; Hong et al., 2011)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Using Cross-Entity Inference to Improve Event Extraction",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144873792",
                "name": "Yu Hong"
              },
              {
                "authorId": "2107968177",
                "name": "Jianfeng Zhang"
              },
              {
                "authorId": "2084599989",
                "name": "Bin Ma"
              },
              {
                "authorId": "2973770",
                "name": "Jianmin Yao"
              },
              {
                "authorId": "143740945",
                "name": "Guodong Zhou"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "Like most NLP tasks, models for event argument extraction experienced a transformation building on word embeddings, RNNs, and CNNs (Chen et al., 2015; Nguyen et al., 2016)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6540287,
          "isinfluential": false,
          "contexts": [
            "Event argument extraction (Ahn, 2006) has a long history in the field (Grishman and Sundheim, 1996; Doddington et al., 2004)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The stages of event extraction",
            "abstract": "Event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty. In this paper, we present a simple, modular approach to event extraction that allows us to experiment with a variety of machine learning methods for these sub-tasks, as well as to evaluate the impact on performance these sub-tasks have on the overall task.",
            "year": 2006,
            "venue": "",
            "authors": [
              {
                "authorId": "145079003",
                "name": "David Ahn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9776219,
          "isinfluential": false,
          "contexts": [
            "Event argument extraction (Ahn, 2006) has a long history in the field (Grishman and Sundheim, 1996; Doddington et al., 2004).",
            "Event argument extraction (Doddington et al., 2004; Aguilar et al., 2014) is about identifying entities participating in events and specifying their role (e.g., the giver , recipient , and thing given in a giving event).",
            "Most event-argument annotation efforts satisfy this requirement, including PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), FrameNet (Baker et al., 1998), RAMS, ACE (Doddington et al., 2004), and WikiEvents (Li et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation",
            "abstract": "",
            "year": 2004,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2862682",
                "name": "G. Doddington"
              },
              {
                "authorId": "2449760",
                "name": "A. Mitchell"
              },
              {
                "authorId": "2282719",
                "name": "Mark A. Przybocki"
              },
              {
                "authorId": "1744313",
                "name": "L. Ramshaw"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              },
              {
                "authorId": "1732071",
                "name": "R. Weischedel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11986411,
          "isinfluential": false,
          "contexts": [
            "Event argument extraction (Ahn, 2006) has a long history in the field (Grishman and Sundheim, 1996; Doddington et al., 2004)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Message Understanding Conference- 6: A Brief History",
            "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",
            "year": 1996,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13804679,
          "isinfluential": false,
          "contexts": [
            "Inter-sentential arguments are more challenging and have received less attention (Gerber and Chai, 2010; Ruppenhofer et al., 2010).",
            "There are also corpora focused on inter-sentential arguments (Gerber and Chai, 2010; Rup-penhofer et al., 2010; Ebner et al., 2020; Li et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates",
            "abstract": "",
            "year": 2010,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2059184073",
                "name": "Matthew Gerber"
              },
              {
                "authorId": "1707259",
                "name": "J. Chai"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202786778,
          "isinfluential": false,
          "contexts": [
            "We use Pytorch (Paszke et al., 2019) and HuggingFace transformers (Wolf et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
            "year": 2019,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "3407277",
                "name": "Adam Paszke"
              },
              {
                "authorId": "39793298",
                "name": "Sam Gross"
              },
              {
                "authorId": "1403239967",
                "name": "Francisco Massa"
              },
              {
                "authorId": "1977806",
                "name": "Adam Lerer"
              },
              {
                "authorId": "2065251344",
                "name": "James Bradbury"
              },
              {
                "authorId": "114250963",
                "name": "Gregory Chanan"
              },
              {
                "authorId": "2059271276",
                "name": "Trevor Killeen"
              },
              {
                "authorId": "3370429",
                "name": "Zeming Lin"
              },
              {
                "authorId": "3365851",
                "name": "N. Gimelshein"
              },
              {
                "authorId": "3029482",
                "name": "L. Antiga"
              },
              {
                "authorId": "3050846",
                "name": "Alban Desmaison"
              },
              {
                "authorId": "1473151134",
                "name": "Andreas Köpf"
              },
              {
                "authorId": "2052812305",
                "name": "E. Yang"
              },
              {
                "authorId": "2253681376",
                "name": "Zachary DeVito"
              },
              {
                "authorId": "10707709",
                "name": "Martin Raison"
              },
              {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
              },
              {
                "authorId": "22236100",
                "name": "Sasank Chilamkurthy"
              },
              {
                "authorId": "32163737",
                "name": "Benoit Steiner"
              },
              {
                "authorId": "152599430",
                "name": "Lu Fang"
              },
              {
                "authorId": "2113829116",
                "name": "Junjie Bai"
              },
              {
                "authorId": "2127604",
                "name": "Soumith Chintala"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2021) and Ma et al. (2022); Du et al. (2021) leverage generative language models (Raffel et al., 2020; Lewis et al., 2020).",
            "For contextualized questions, we fine-tune T5 (Raffel et al., 2020) with either SQuAD or the weakly supervised data obtained by prompting GPT-4 (weak supervision from LLMs)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226283556,
          "isinfluential": false,
          "contexts": [
            "Some efforts assume event triggers and argument spans are part of the input and present classifiers to identify the argument role (Ebner et al., 2020; Chen et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Modeling of Arguments for Event Understanding",
            "abstract": "We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.",
            "year": 2020,
            "venue": "CODI",
            "authors": [
              {
                "authorId": "104375103",
                "name": "Yunmo Chen"
              },
              {
                "authorId": "40364920",
                "name": "Tongfei Chen"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865619,
          "isinfluential": false,
          "contexts": [
            "Transforming natural language into structured event knowledge benefits many down-stream tasks such as machine reading comprehension (Han et al., 2021), news summarization (Li et al., 2016), coreference resolution (Huang et al., 2019), and dialogue systems (Su et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations",
            "abstract": "Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines’ ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1%, 63.3% and 83.5% for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0%, 79.6%, 100% respectively), highlighting our dataset as a challenging benchmark.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "51518773",
                "name": "Rujun Han"
              },
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "145478138",
                "name": "Jiao Sun"
              },
              {
                "authorId": "66500474",
                "name": "J. Baylón"
              },
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253510351,
          "isinfluential": true,
          "contexts": [
            "Q1 is generated following a role-specific template, Q2 and Q3 are generated prompting techniques (Du and Ji, 2022)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Retrieval-Augmented Generative Question Answering for Event Argument Extraction",
            "abstract": "Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models’ capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example’s context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performances.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2072975661",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 256461151,
          "isinfluential": false,
          "contexts": [
            "Transfer learning has also been explored, including role overlapping knowledge (Zhang et al., 2023), semantic roles (Zhang et al., 2022), abstract meaning representations (Xu et al., 2022), and frame-aware knowledge distillation (Wei et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Transfer Learning from Semantic Role Labeling to Event Argument Extraction with Template-based Slot Querying",
            "abstract": "In this work, we investigate transfer learning from semantic role labeling (SRL) to event argument extraction (EAE), considering their similar argument structures. We view the extraction task as a role querying problem, unifying various methods into a single framework. There are key discrepancies on role labels and distant arguments between semantic role and event argument annotations. To mitigate these discrepancies, we specify natural language-like queries to tackle the label mismatch problem and devise argument augmentation to recover distant arguments. We show that SRL annotations can serve as a valuable resource for EAE, and a template-based slot querying strategy is especially effective for facilitating the transfer. In extensive evaluations on two English EAE benchmarks, our proposed model obtains impressive zero-shot results by leveraging SRL annotations, reaching nearly 80% of the fullysupervised scores. It further provides benefits in low-resource cases, where few EAE annotations are available. Moreover, we show that our approach generalizes to cross-domain and multilingual scenarios.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "2268272",
                "name": "Emma Strubell"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269362270,
          "isinfluential": false,
          "contexts": [
            "Framing the problem in terms of questions and answers is popular (Du and Cardie, 2020; Liu et al., 2020; Li et al., 2020; Uddin et al., 2024)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Asking and Answering Questions to Extract Event-Argument Structures",
            "abstract": "This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model.",
            "year": 2024,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2261083690",
                "name": "Md Nayem Uddin"
              },
              {
                "authorId": "2295669823",
                "name": "Enfa Rose George"
              },
              {
                "authorId": "2295665323",
                "name": "Eduardo Blanco"
              },
              {
                "authorId": "2295665486",
                "name": "Steven Corman"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Initially, datasets focused on extracting arguments within the same sentence than the event (Palmer et al., 2005; Walker et al., 2006)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "233219850": {
      "citing_paper_info": {
        "title": "Document-Level Event Argument Extraction by Conditional Generation",
        "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
        "year": 2021,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2109154767",
            "name": "Sha Li"
          },
          {
            "authorId": "2113323573",
            "name": "Heng Ji"
          },
          {
            "authorId": "153034701",
            "name": "Jiawei Han"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 2,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "11986411",
        "14416802",
        "204960716",
        "131773936",
        "216562330",
        "225039792",
        "309759",
        "261226458",
        "1320606",
        "215745286",
        "204915992",
        "208547716"
      ],
      "citation_details": [
        {
          "citedcorpusid": 309759,
          "isinfluential": false,
          "contexts": [
            "Both (Lai et al., 2020) and (Deng et al., 2020) extend upon the prototype network model (Snell et al., 2017) for classiﬁcation.",
            "In addition to BERT-QA, we also replace our T AP K EY trigger extraction model with a Prototype Network(Snell et al., 2017) 14 ."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Prototypical Networks for Few-shot Learning",
            "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "39770136",
                "name": "Jake Snell"
              },
              {
                "authorId": "1754860",
                "name": "Kevin Swersky"
              },
              {
                "authorId": "1804104",
                "name": "R. Zemel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "For Coref F1, the model is given full credit if the extracted argument is coreferential with the gold-standard argument as used in (Ji and Grishman, 2008)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11986411,
          "isinfluential": false,
          "contexts": [
            "Early work on event extraction originally posed the task as document level role ﬁlling (Grishman and Sundheim, 1996) on a set of narrow scenarios Prosecutors say he drove the truck to Geary Lake in Kansas, that 4,000 pounds of ammonium nitrate laced with nitromethane were loaded into the truck…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Message Understanding Conference- 6: A Brief History",
            "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",
            "year": 1996,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14416802,
          "isinfluential": false,
          "contexts": [
            "An information seeking session (Mai, 2016) can be divided into 6 stages: task initiation, topic selection, pre-focus exploration, focus information, information collection and search closure (Kuhlthau, 1991)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Inside the search process: Information seeking from the user's perspective",
            "abstract": "The article discusses the users’ perspective of information seeking. A model of the information search process is presented derived from a series of five studies investigating common experiences of users in information seeking situations. The cognitive and affective aspects of the process of information seeking suggest a gap between the users’ natural process of information use and the information system and intermediaries’ traditional patterns of information provision.",
            "year": 1991,
            "venue": "Journal of the American Society for Information Science",
            "authors": [
              {
                "authorId": "2422188",
                "name": "C. Kuhlthau"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131773936,
          "isinfluential": false,
          "contexts": [
            "Since the baseline BERT-CRF model (Shi and Lin, 2019) cannot handle new labels directly, we exclude it from comparison."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "abstract": "We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1884766",
                "name": "Peng Shi"
              },
              {
                "authorId": "145580839",
                "name": "Jimmy J. Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204915992,
          "isinfluential": false,
          "contexts": [
            "Both (Lai et al., 2020) and (Deng et al., 2020) extend upon the prototype network model (Snell et al., 2017) for classiﬁcation.",
            ", 2020) and (Deng et al., 2020) extend upon the prototype network model (Snell et al."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection",
            "abstract": "Event detection (ED), a sub-task of event extraction, involves identifying triggers and categorizing event mentions. Existing methods primarily rely upon supervised learning and require large-scale labeled event datasets which are unfortunately not readily available in many real-life applications. In this paper, we consider and reformulate the ED task with limited labeled data as a Few-Shot Learning problem. We propose a Dynamic-Memory-Based Prototypical Network (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learn better prototypes for event types, but also produce more robust sentence encodings for event mentions. Differing from vanilla prototypical networks simply computing event prototypes by averaging, which only consume event mentions once, our model is more robust and is capable of distilling contextual information from event mentions for multiple times due to the multi-hop mechanism of DMNs. The experiments show that DMB-PN not only deals with sample scarcity better than a series of baseline models but also performs more robustly when the variety of event types is relatively large and the instance quantity is extremely small.",
            "year": 2019,
            "venue": "Web Search and Data Mining",
            "authors": [
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "1388742072",
                "name": "Jiaojian Kang"
              },
              {
                "authorId": "2118158068",
                "name": "Yichi Zhang"
              },
              {
                "authorId": "2155468731",
                "name": "Wei Zhang"
              },
              {
                "authorId": "1729778",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "Our base model is an encoder-decoder language model (BART (Lewis et al., 2020), T5 (Raffel et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208547716,
          "isinfluential": false,
          "contexts": [
            "Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generationbased model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass.",
            "Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generation-based model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning…",
            "Recent work on zero-shot event extraction has posed the problem as question answering (Chen et al., 2020; Du and Cardie, 2020; Feng et al., 2020) with different ways of designing the questions."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Reading the Manual: Event Extraction as Definition Comprehension",
            "abstract": "We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, “Some person was born in some location at some time.” We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.",
            "year": 2019,
            "venue": "SPNLP",
            "authors": [
              {
                "authorId": "104375103",
                "name": "Yunmo Chen"
              },
              {
                "authorId": "40364920",
                "name": "Tongfei Chen"
              },
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215745286,
          "isinfluential": false,
          "contexts": [
            "Inspired by (Shwartz et al., 2020), we use clariﬁcation statements to add back constraints without breaking the end-to-end property of the model."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Unsupervised Commonsense Question Answering with Self-Talk",
            "abstract": "Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on \\emph{self-talk} as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as \"$\\textit{what is the definition of ...}$\" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "3103343",
                "name": "Vered Shwartz"
              },
              {
                "authorId": "119659229",
                "name": "Peter West"
              },
              {
                "authorId": "39227408",
                "name": "Ronan Le Bras"
              },
              {
                "authorId": "1857797",
                "name": "Chandra Bhagavatula"
              },
              {
                "authorId": "1699545",
                "name": "Yejin Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": true,
          "contexts": [
            "ularly used BERT-CRF baseline (Shi and Lin, 2019) that performs trigger extraction on sentencelevel and BERT-QA (Du and Cardie, 2020) ran on sentence-level and document-level.",
            "The performance of BERT-QA is greatly limited by the trigger identiﬁcation step.",
            "In addition to BERT-QA, we also replace our T AP K EY trigger extraction model with a Prototype Network(Snell et al., 2017) 14 .",
            "Recent work on zero-shot event extraction has posed the problem as question answering (Chen et al., 2020; Du and Cardie, 2020; Feng et al., 2020) with different ways of designing the questions.",
            "Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generationbased model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass.",
            "edu/rams (12)Note that our preprocessing procedure is slightly different from (Du and Cardie, 2020) as we kept pronouns as valid event triggers and arguments.",
            "Notably, one template per event type is given in the ontology, and does not require further human curation as opposed to the question designing process in question answering (QA) models (Du and Cardie, 2020; Feng et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039792,
          "isinfluential": true,
          "contexts": [
            "Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generationbased model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass.",
            "Notably, one template per event type is given in the ontology, and does not require further human curation as opposed to the question designing process in question answering (QA) models (Du and Cardie, 2020; Feng et al., 2020).",
            "Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generation-based model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning…",
            "Recent work on zero-shot event extraction has posed the problem as question answering (Chen et al., 2020; Du and Cardie, 2020; Feng et al., 2020) with different ways of designing the questions."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction",
            "abstract": "We study the problem of event extraction from text data, which requires both detecting target event types and their arguments. Typically, both the event detection and argument detection subtasks are formulated as supervised sequence labeling problems. We argue that the event extraction models so trained are inherently label-hungry, and can generalize poorly across domains and text genres.We propose a reading comprehension framework for event extraction.Specifically, we formulate event detection as a textual entailment prediction problem, and argument detection as a question answer-ing problem. By constructing proper query templates, our approach can effectively distill rich knowledge about tasks and label semantics from pretrained reading comprehension models. Moreover, our model can be fine-tuned with a small amount of data to boost its performance. Our experiment results show that our method performs strongly for zero-shot and few-shot event extraction, and it achieves state-of-the-art performance on the ACE 2005 benchmark when trained with full supervision.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2054599518",
                "name": "Rui Feng"
              },
              {
                "authorId": "2118573410",
                "name": "Jie Yuan"
              },
              {
                "authorId": "2152737282",
                "name": "Chao Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261226458,
          "isinfluential": false,
          "contexts": [
            "There have been a few datasets published speciﬁcally for implicit semantic role labeling, such as the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), the Beyond NomBank dataset (Gerber and Chai, 2010) and ON5V (Moor et al., 2013).",
            "There have been a few datasets published specifically for implicit semantic role labeling, such as the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), the Beyond NomBank dataset (Gerber and Chai, 2010) and ON5V (Moor et al."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "273962739": {
      "citing_paper_info": {
        "title": "One Small and One Large for Document-level Event Argument Extraction",
        "abstract": "Document-level Event Argument Extraction (EAE) faces two challenges due to increased input length: 1) difficulty in distinguishing semantic boundaries between events, and 2) interference from redundant information. To address these issues, we propose two methods. The first method introduces the Co and Structure Event Argument Extraction model (CsEAE) based on Small Language Models (SLMs). CsEAE includes a co-occurrences-aware module, which integrates information about all events present in the current input through context labeling and co-occurrences event prompts extraction. Additionally, CsEAE includes a structure-aware module that reduces interference from redundant information by establishing structural relationships between the sentence containing the trigger and other sentences in the document. The second method introduces new prompts to transform the extraction task into a generative task suitable for Large Language Models (LLMs), addressing gaps in EAE performance using LLMs under Supervised Fine-Tuning (SFT) conditions. We also fine-tuned multiple datasets to develop an LLM that performs better across most datasets. Finally, we applied insights from CsEAE to LLMs, achieving further performance improvements. This suggests that reliable insights validated on SLMs are also applicable to LLMs. We tested our models on the Rams, WikiEvents, and MLEE datasets. The CsEAE model achieved improvements of 2.1\\%, 2.3\\%, and 3.2\\% in the Arg-C F1 metric compared to the baseline, PAIE~\\cite{PAIE}. For LLMs, we demonstrated that their performance on document-level datasets is comparable to that of SLMs~\\footnote{All code is available at https://github.com/simon-p-j-r/CsEAE}.",
        "year": 2024,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2289798893",
            "name": "Jiaren Peng"
          },
          {
            "authorId": "2257127695",
            "name": "Hongda Sun"
          },
          {
            "authorId": "2237237599",
            "name": "Wenzhong Yang"
          },
          {
            "authorId": "2147426856",
            "name": "Fuyuan Wei"
          },
          {
            "authorId": "2283969809",
            "name": "Liang He"
          },
          {
            "authorId": "2237387710",
            "name": "Liejun Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 4,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "235458009",
        "204960716",
        "267976050",
        "220524732",
        "265221405",
        "258947053",
        "13981987"
      ],
      "citation_details": [
        {
          "citedcorpusid": 13981987,
          "isinfluential": true,
          "contexts": [
            "Conversely, the co-occurrence-aware module significantly boosts performance on the WikiEvents and MLEE datasets, increasing the Arg-C metric by 1.7% and 2.8%, respectively.",
            "Following TabEAE, we analyzed CsEAE’s ability to capture event semantic boundaries on the WikiEvents and MLEE datasets from two perspectives: inter-event semantics and intra-event semantics.",
            "For instance, in the WikiEvents dataset, over 94% of arguments are in the same sentence as the trigger; in the Rams dataset, over 82%; and in the MLEE dataset, over 99%.",
            "Datasets We used the three most commonly employed datasets for document-level event argument extraction (EAE): Rams (Ebner et al. 2020), WikiEvents (Li, Ji, and Han 2021), and MLEE (Pyysalo et al. 2012).",
            "Table 3: Overall performance of LLMs. Doc represents training using the WikiEvents, Rams, and MLEE; News represents training using the ACE, Rams, and WikiEvents, ALL signifies that all five datasets were used for training.",
            "Similarly, on the MLEE dataset, CsEAE achieves improvements of 3.0% in Arg-I and 3.2% in Arg-C metrics.",
            "In this paper, we utilize prompts proposed in PAIE (Ma et al. 2022) for the Rams and WikiEvents datasets and those in TabEAE (He, Hu, and Tang 2023) for the MLEE dataset.",
            "The notable improvement of the co-occurrences-aware module on the WikiEvents and MLEE datasets is attributed to the higher number of events in instances, where the auxiliary information provided by the co-occurrences-aware module leads to a greater performance boost in complex event scenarios.",
            "In the Table 1, our model outperformed all baselines on the Rams and MLEE datasets."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event extraction across multiple levels of biological organization",
            "abstract": "Motivation: Event extraction using expressive structured representations has been a significant focus of recent efforts in biomedical information extraction. However, event extraction resources and methods have so far focused almost exclusively on molecular-level entities and processes, limiting their applicability. Results: We extend the event extraction approach to biomedical information extraction to encompass all levels of biological organization from the molecular to the whole organism. We present the ontological foundations, target types and guidelines for entity and event annotation and introduce the new multi-level event extraction (MLEE) corpus, manually annotated using a structured representation for event extraction. We further adapt and evaluate named entity and event extraction methods for the new task, demonstrating that both can be achieved with performance broadly comparable with that for established molecular entity and event extraction tasks. Availability: The resources and methods introduced in this study are available from http://nactem.ac.uk/MLEE/. Contact: pyysalos@cs.man.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",
            "year": 2012,
            "venue": "Bioinform.",
            "authors": [
              {
                "authorId": "1708916",
                "name": "S. Pyysalo"
              },
              {
                "authorId": "2095533089",
                "name": "Tomoko Ohta"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "2647018",
                "name": "Han-Cheol Cho"
              },
              {
                "authorId": "1737901",
                "name": "Junichi Tsujii"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "We used BART (Lewis et al. 2020) as the backbone for CsEAE.",
            "BART-Gen (Li, Ji, and Han 2021) utilizes a prompt-based generative approach to generate event arguments end-to-end, and subsequently, PAIE (Ma et al. 2022) introduces more effective manually crafted prompts, using slot prompts to extract arguments by filling slots."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220524732,
          "isinfluential": false,
          "contexts": [
            "... Gottschalk, and Demidova 2020), dialogue systems (Zhang, Chen, and Bui 2020), and recommendation systems (Li et al. 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Diagnostic Prediction with Sequence-of-setsRepresentation Learning for Clinical Events",
            "abstract": "Electronic health records (EHRs) contain both ordered and unordered chronologies of clinical events that occur during a patient encounter. However, during data preprocessing steps, many predictive models impose a predefined order on unordered clinical events sets (e.g., alphabetical, natural order from the chart, etc.), which is potentially incompatible with the temporal nature of the sequence and predictive task. To address this issue, we proposeDPSS, which seeks to capture each patient's clinical event records as sequences of event sets. Foreach clinical event set, we assume that the predictive model should be invariant to the order of concurrent events and thus employ a novel permutation sampling mechanism. This paper evaluates the use of this permuted sampling method given different data-driven models for predicting a heart failure (HF) diagnosis in sub-sequent patient visits. Experimental results using the MIMIC-III dataset show that the permutation sampling mechanism offers improved discriminative power based on the area under the receiver operating curve (AUROC) and precision-recall curve (pr-AUC) metrics as HF diagnosis prediction becomes more robust to different data ordering schemes.",
            "year": 2020,
            "venue": "medRxiv",
            "authors": [
              {
                "authorId": "2146331536",
                "name": "Tianran Zhang"
              },
              {
                "authorId": "1998918",
                "name": "Muhao Chen"
              },
              {
                "authorId": "144217742",
                "name": "Alex A. T. Bui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458009,
          "isinfluential": true,
          "contexts": [
            "We used the methods provided by LLama-Factory 2 for model’s SFT, employing LoRA-based (Hu et al. 2021) fine-tuning with a rank r of 8 and a dropout rate of 0.1."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
            "year": 2021,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2157840220",
                "name": "J. E. Hu"
              },
              {
                "authorId": "1752875",
                "name": "Yelong Shen"
              },
              {
                "authorId": "104100507",
                "name": "Phillip Wallis"
              },
              {
                "authorId": "1388725932",
                "name": "Zeyuan Allen-Zhu"
              },
              {
                "authorId": "2110486765",
                "name": "Yuanzhi Li"
              },
              {
                "authorId": "2135571585",
                "name": "Shean Wang"
              },
              {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258947053,
          "isinfluential": true,
          "contexts": [
            "In all experiments in this paper, Arg-I and Arg-C is equivalent to Arg-I+ and Arg-C+. Baselines For SLMs, we categorized the baseline models into two groups: (1) Classification-based models: EEQA (Du and Cardie 2020b), TSAR (Xu et al. 2022), TagPrime-C and TagPrime-CR (Hsu et al. 2023a); (2) Generation-based models: Bart-Gen (Li, Ji, and Han 2021), X-Gear (Huang et al. 2022), AMPERE (Hsu et al. 2023b), PAIE (Ma et al. 2022), TabEAE (He, Hu, and Tang 2023).",
            "Co-occurrences Prefix After constructing the co-occurrences-aware matrix W C for the current event mention D , we condense W C into prefixes (Li and Liang 2021; Hsu et al. 2023b), which then participate in the model’s generation.",
            "…Classification-based models: EEQA (Du and Cardie 2020b), TSAR (Xu et al. 2022), TagPrime-C and TagPrime-CR (Hsu et al. 2023a); (2) Generation-based models: Bart-Gen (Li, Ji, and Han 2021), X-Gear (Huang et al. 2022), AMPERE (Hsu et al. 2023b), PAIE (Ma et al. 2022), TabEAE (He, Hu, and Tang 2023).",
            "Some studies incorporate abstract meaning representation into the extraction task (Xu et al. 2022; Yang et al. 2023; Hsu et al. 2023b)."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model",
            "abstract": "Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on problem re-formulation and prompt design, without incorporating additional information that has been shown to be effective for classification-based models, such as the abstract meaning representation (AMR) of the input passages. Incorporating such information into generation-based models is challenging due to the heterogeneous nature of the natural language form prevalently used in generation-based models and the structured form of AMRs. In this work, we study strategies to incorporate AMR into generation-based EAE models. We propose AMPERE, which generates AMR-aware prefixes for every layer of the generation model. Thus, the prefix introduces AMR information to the generation-based EAE model and then improves the generation. We also introduce an adjusted copy mechanism to AMPERE to help overcome potential noises brought by the AMR graph. Comprehensive experiments and analyses on ACE2005 and ERE datasets show that AMPERE can get 4% - 10% absolute F1 score improvements with reduced training data and it is in general powerful across different training sizes.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "2099588738",
                "name": "Zhiyu Xie"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "2104644641",
                "name": "Premkumar Natarajan"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265221405,
          "isinfluential": true,
          "contexts": [
            "Its best performance, an Arg-C score of 64.71, falls short compared to best results of SLMs (Huang et al. 2024).",
            "Additionally, to more comprehensively validate the effectiveness of CsEAE, we applied the data processing meth-ods used in TextEE (Huang et al. 2024) to WikiEvents and RAMS."
          ],
          "intents": [
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction",
            "abstract": "Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 16 datasets spanning eight diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how they struggle to achieve satisfactory performance. Inspired by our reevaluation results and findings, we discuss the role of event extraction in the current NLP era, as well as future challenges and insights derived from TextEE. We believe TextEE, the first standardized comprehensive benchmarking tool, will significantly facilitate future event extraction research.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "46719088",
                "name": "Tanmay Parekh"
              },
              {
                "authorId": "2099588738",
                "name": "Zhiyu Xie"
              },
              {
                "authorId": "2116461591",
                "name": "Zixuan Zhang"
              },
              {
                "authorId": "2266842956",
                "name": "Premkumar Natarajan"
              },
              {
                "authorId": "2257127887",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "2256996328",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "2271097936",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 267976050,
          "isinfluential": false,
          "contexts": [
            "Inspired by the use of large-scale high-quality data for continuous pretraining (Yang et al. 2024), we attempted multi-dataset fine-tuning to make the LLMs more familiar with event extraction tasks.",
            "Through training data, the model can effectively leverage the latent knowledge accumulated during pre-training to understand and respond to extraction instructions (Yang et al. 2024)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Prompt for extraction: Multiple templates choice model for event extraction",
            "abstract": "",
            "year": 2024,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "2289798893",
                "name": "Jiaren Peng"
              },
              {
                "authorId": "2237237599",
                "name": "Wenzhong Yang"
              },
              {
                "authorId": "2147426856",
                "name": "Fuyuan Wei"
              },
              {
                "authorId": "2276407013",
                "name": "Liang He"
              }
            ]
          }
        }
      ]
    },
    "268598877": {
      "citing_paper_info": {
        "title": "Sorting, Reasoning, and Extraction: An Easy-to-Hard Reasoning Framework for Document-Level Event Argument Extraction",
        "abstract": "Document-level event argument extraction is a crucial task to help understand event information. Existing methods mostly ignore the different extraction difficulties of arguments, and the lack of task planning significantly affects the extraction and reasoning abilities of the model. In this paper, we innovatively analyze the difficulty of arguments and propose a novel framework for reasoning from easy to hard, aiming to use the information of simple arguments to help the extraction of difficult arguments in a human-like way. Specifically, our framework consists of three core modules: sorting, reasoning, and extraction. The sorting module first sorts the argument roles according to the current context and plans the reasoning path from easy to hard. Then, the reasoning module performs information reasoning based on the reasoning path to help capture the information of difficult arguments. Finally, the extraction module utilizes the reasoning information to complete argument extraction. Experimental results on the RAMS and WikiEvents datasets show the great advantages of our proposed approach. In particular, we obtain new state-of-the-art (SOTA) performance in multiple scenarios.",
        "year": 2024,
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "authors": [
          {
            "authorId": "2274084234",
            "name": "Hao Li"
          },
          {
            "authorId": "2257354181",
            "name": "Yanan Cao"
          },
          {
            "authorId": "2109978994",
            "name": "Yubing Ren"
          },
          {
            "authorId": "2257017651",
            "name": "Fang Fang"
          },
          {
            "authorId": "2273820905",
            "name": "Lanxue Zhang"
          },
          {
            "authorId": "2201603227",
            "name": "Yingjie Li"
          },
          {
            "authorId": "2274051475",
            "name": "Shi Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "234358675",
        "266177132",
        "207853145",
        "243865143",
        "248496614"
      ],
      "citation_details": [
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "We conduct experiments on two commonly used document-level EAE datasets: RAMS [17] and WikiEvents [9]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 234358675,
          "isinfluential": false,
          "contexts": [
            "Typical efforts in document-level EAE can be roughly divided into classification-based models [1, 2, 3, 4, 5, 6], question answering (QA)-based models [7, 8], generation-based models [9, 10], and prompt-based models [11, 12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction with Efficient End-to-end Learning of Cross-event Dependencies",
            "abstract": "Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency.",
            "year": 2020,
            "venue": "NUSE",
            "authors": [
              {
                "authorId": "1420116116",
                "name": "Kung-Hsiang Huang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865143,
          "isinfluential": false,
          "contexts": [
            "Typical efforts in document-level EAE can be roughly divided into classification-based models [1, 2, 3, 4, 5, 6], question answering (QA)-based models [7, 8], generation-based models [9, 10], and prompt-based models [11, 12].",
            "DocMRC [8] is another QA-based method, assisted by two data augmentation regimes: implicit knowledge transfer and explicit data augmentation."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction",
            "abstract": "Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach — it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": false,
          "contexts": [
            "Typical efforts in document-level EAE can be roughly divided into classification-based models [1, 2, 3, 4, 5, 6], question answering (QA)-based models [7, 8], generation-based models [9, 10], and prompt-based models [11, 12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266177132,
          "isinfluential": false,
          "contexts": [
            "Typical efforts in document-level EAE can be roughly divided into classification-based models [1, 2, 3, 4, 5, 6], question answering (QA)-based models [7, 8], generation-based models [9, 10], and prompt-based models [11, 12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction",
            "abstract": ",",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2274084234",
                "name": "Hao Li"
              },
              {
                "authorId": "2257354181",
                "name": "Yanan Cao"
              },
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "2257017651",
                "name": "Fang Fang"
              },
              {
                "authorId": "2273820905",
                "name": "Lanxue Zhang"
              },
              {
                "authorId": "2201603227",
                "name": "Yingjie Li"
              },
              {
                "authorId": "2274051475",
                "name": "Shi Wang"
              }
            ]
          }
        }
      ]
    },
    "272536996": {
      "citing_paper_info": {
        "title": "Generative Dual Representations Fusion Network for Document-level Event Argument Extraction",
        "abstract": "Document-level event argument extraction aims to identify the event arguments and predict their roles in a document. There are many non-argument entities that play an essential role in understanding events in the document, which we call event-linking entities. However, recent work on document-level event argument extraction overlooks these entities and gets suboptimal performance. Moreover, a document usually contains multiple events, and they are interconnected with each other. Most recent work models each event in isolation without considering their interconnection. To tackle these problems, we propose a Generative Dual Representations Fusion network (GDRF) for document-level event argument extraction to introduce the significance of event-linking entities for the first time. GDRF retrieves event-linking entities and tags them in the document using the Event-linking Entity based Document Tagging module. To introduce contextual information from event-linking entities and capture the interconnection between multiple events, we propose a Dual Representations Fusion module to fuse original and tagged context representations with a fusion loss. Empirical results on the WIKIEVENTS dataset demonstrate that our model outperforms previous methods, achieving state-of-the-art performance.",
        "year": 2024,
        "venue": "IEEE International Joint Conference on Neural Network",
        "authors": [
          {
            "authorId": "2291016102",
            "name": "Boyang Liu"
          },
          {
            "authorId": "143661036",
            "name": "Guozheng Rao"
          },
          {
            "authorId": "2152833173",
            "name": "Li Zhang"
          },
          {
            "authorId": "1505582308",
            "name": "Qing Cong"
          },
          {
            "authorId": "2284332581",
            "name": "Xin Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 0,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "52967399",
        "204838007",
        "13756489",
        "268342373",
        "252901047",
        "259858959",
        "216562330",
        "259370721",
        "198953378",
        "231728756",
        "268313449",
        "204960716"
      ],
      "citation_details": [
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Some methods mainly used pre-trained models based on the transformer [9] structure to perform EAE task [1], [10].",
            "BART [25] is a pre-trained language model with an encoder-decoder transformer architecture [9], utilizing a denoising autoencoder architecture with bidirectional and autoregressive training objectives to reconstruct the original input sequence."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "For example, Du et al. [2] reformulated EAE as a Question Answering (QA) task and used pre-trained BERT [11] to encode text.",
            "As shown in the Event-linking Entity based Document Tagging module of Figure 3, we encode the Retrieval Sequence with an bidirectional encoder, such as BERT [11] and RoBERTa [26].",
            "For the text encoder in the EEDT module, we use BERT-base-uncased [11] which has 768 hidden embedding dimensions and 12 attention layers, and each layer has 12 attention heads."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": false,
          "contexts": [
            "As shown in the Event-linking Entity based Document Tagging module of Figure 3, we encode the Retrieval Sequence with an bidirectional encoder, such as BERT [11] and RoBERTa [26]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "Lu et al. [12] proposed a sequence-to-structure generation method with the pre-trained language model T5 [13]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "BART [25] is a pre-trained language model with an encoder-decoder transformer architecture [9], utilizing a denoising autoencoder architecture with bidirectional and autoregressive training objectives to reconstruct the original input sequence."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "For example, Du et al. [2] reformulated EAE as a Question Answering (QA) task and used pre-trained BERT [11] to encode text.",
            "• BERT-QA [2]: uses the pre-trained BERT as the base model and adds a linear layer on top to obtain the start and end offsets of the answer (i.e., argument spans) in the input context for each role.",
            "Prior works on EAE are mainly restricted to sentence-level setting [1], [2]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "Another branch of existing methods is based on the generative model [3], [22]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252901047,
          "isinfluential": false,
          "contexts": [
            "Document-level event extraction has attracted more attention in recent years [5], [16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MRCAug: Data Augmentation via Machine Reading Comprehension for Document-Level Event Argument Extraction",
            "abstract": "Document-level event argument extraction (EAE) is a critical event semantic understanding task that requires a model to identify an event's global arguments beyond the sentence level. Existing approaches to this problem are based on supervised learning, which require a large amount of labeled data for model training. However, due to the complicated structure of an event, human annotation for this task is costly, and the issue of inadequacy of training data has long hampered the study. In this study, we propose a novel approach to mitigating the data sparsity problem faced by document-level EAE, by linking the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including an implicit knowledge transfer method, which enables knowledge transfer from other tasks to the document-level EAE task, and an explicit data generation method, which can explicitly generate new training examples by treating a pre-trained MRC model as an annotator. Furthermore, we propose a self-training based noise reduction strategy that can effectively addresses the out-of-domain noise introduced by the data augmentation methods. The extensive assessments on three benchmarks have validated the effectiveness of our approach — it not only achieves state-of-the-art performance but also demonstrates superior results in the data-low scenario.",
            "year": 2022,
            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259370721,
          "isinfluential": false,
          "contexts": [
            "Classiﬁcation-based methods solve the document-level EAE problem by formulating it as a multi-class classiﬁcation task [17]–[19]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction With a Chain Reasoning Paradigm",
            "abstract": "Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies.Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic rules for reasoning.This paradigm naturally captures long-range interdependence due to the chains’ compositional nature, which also improves interpretability by explicitly modeling the reasoning process.We introduce T-norm fuzzy logic for optimization, which permits end-to-end learning and shows promise for integrating the expressiveness of logical reasoning with the generalization of neural networks.In experiments, we show that our approach outperforms previous methods by a significant margin on two standard benchmarks (over 6 points in F1).Moreover, it is data-efficient in low-resource scenarios and robust enough to defend against adversarial attacks.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "2113437300",
                "name": "Chen Liang"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              },
              {
                "authorId": "103896164",
                "name": "Haoyan Liu"
              },
              {
                "authorId": "2168651021",
                "name": "Zhe Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259858959,
          "isinfluential": false,
          "contexts": [
            "Liu et al. [21] considered non-argument contextual clue information to enhance the candidate argument representations and captured the relevance among argument roles."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance",
            "abstract": "Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate arguments. Both STCP and RLIG introduce no more than 1% new parameters compared with the base model and can be easily applied to other event extraction models, which are compact and transplantable. Experiments on two public datasets show that our SCPRG outperforms previous state-of-the-art methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents respectively. Further analyses illustrate the interpretability of our model.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143606898",
                "name": "Wanlong Liu"
              },
              {
                "authorId": "2153264570",
                "name": "Shaohuan Cheng"
              },
              {
                "authorId": "2054124872",
                "name": "DingYi Zeng"
              },
              {
                "authorId": "2064923494",
                "name": "Hong Qu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268313449,
          "isinfluential": false,
          "contexts": [
            "Document-level NLP tasks are also an important research direction [8], [14], [15]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Fake news detection based on dual-channel graph convolutional attention network",
            "abstract": "",
            "year": 2024,
            "venue": "Journal of Supercomputing",
            "authors": [
              {
                "authorId": "2220796048",
                "name": "Mengfan Zhao"
              },
              {
                "authorId": "2290641370",
                "name": "Yutao Zhang"
              },
              {
                "authorId": "2290555223",
                "name": "Guozheng Rao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 268342373,
          "isinfluential": false,
          "contexts": [
            "Traditional works performed event extraction at sentence-level [1], [6], where the event trigger and its arguments are usually within a single sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DE3TC: Detecting Events with Effective Event Type Information and Context",
            "abstract": "Event Detection (ED) is a crucial information extraction task that aims to identify the event triggers and classify them into predefined event types. However, most existing methods did not perform well when processing events with implicit triggers. And most methods considered ED as a sentence-level task, lacking effective context for event semantics. Moreover, how to maintain good performance under low resource conditions still needs further study. To address these problems, we propose a novel end-to-end ED model called DE3TC, which Detects Events with Effective Event Type Information and Context. We construct an event type-specific Clue to capture the interaction between event type name and trigger words, providing event type information for implicit triggers. For accessing the effective context of event semantics for sentence-level ED, we consider the correlations between types and select similar types’ descriptions as context. With contextualized representation from a contextual encoder, DE3TC learns the event type information for all events including implicit ones. And it performs sentence-level ED efficiently with effective contexts. The empirical results on ACE 2005 and MAVEN datasets show that: (i) DE3TC obtains state-of-the-art performance compared with previous methods. (ii) DE3TC is also excelled under low-resource conditions.",
            "year": 2024,
            "venue": "Neural Processing Letters",
            "authors": [
              {
                "authorId": "2291016102",
                "name": "Boyang Liu"
              },
              {
                "authorId": "143661036",
                "name": "Guozheng Rao"
              },
              {
                "authorId": "2284332581",
                "name": "Xin Wang"
              },
              {
                "authorId": "2284395584",
                "name": "Li Zhang"
              },
              {
                "authorId": "1505582308",
                "name": "Qing Cong"
              }
            ]
          }
        }
      ]
    },
    "276399825": {
      "citing_paper_info": {
        "title": "An AMR-based Model with Role And Relation Information for Document-level Event Argument Extraction",
        "abstract": "Abstract Meaning Representation (AMR) has been widely adopted in recent studies for Document-level Event Argument Extraction (DEAE). Since AMR provides practical and understandable semantic information in terms of graphical structure for enhancing the pre-trained language model’s understanding of documents. However, AMR graph structure does not represent events perfectly and loses the relational information of the arguments during parsing. These methods neglect two problems: a) the relevance between context embedding and span representations; b) the co-reference relation of event-specific arguments. To address these issues, we proposed a RRAG model, which addresses the above challenges through two simple yet effective components: (a) Role Aware Encoding Module(RAEM) captures the role and event information by aggregating them with context based on the attention heads. (b)Relation Enhanced AMR Graph(REAG) adds the co-reference relation among AMR graph nodes, augmenting the semantic logic. We conduct comprehensive experiments on RAMS and WikiEvents datasets. Results demonstrate that our approach has superior performance and outperforms the state-of-the-art models by 1.43pt and 1.27pt F1, respectively. The code is available at https://github.com/Hymuile/RRAG.",
        "year": 2024,
        "venue": "International Conference on Computer Science and Artificial Intelligence",
        "authors": [
          {
            "authorId": "2345425917",
            "name": "Wei Liu"
          },
          {
            "authorId": "2263768580",
            "name": "Jiacheng Xu"
          },
          {
            "authorId": "2265344569",
            "name": "Chengxiang Tan"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 5,
        "influential_count": 1,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "265221038",
        "248496246",
        "52967399",
        "53592270",
        "243865619"
      ],
      "citation_details": [
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Moreover, we use 𝐵𝐸𝑅𝑇 𝑏𝑎𝑠𝑒 [11] and 𝑅𝑜𝐵𝐸𝑅𝑇𝑎 𝑙𝑎𝑟𝑔𝑒 [12] as the pre-trained transformer-based encoder."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53592270,
          "isinfluential": true,
          "contexts": [
            "We adopt the PLMs default dropout rate as 0.1, batch size to 2, and use AdamW optimizer[15] and a linearly decaying scheduler[16] with 3e-5 learning rate to train RRAG."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865619,
          "isinfluential": false,
          "contexts": [
            "It plays a crucial role in extracting event structures for various downstream tasks, including machine reading comprehension[1], dialogue systems[2] and recommendation systems[3]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations",
            "abstract": "Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines’ ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1%, 63.3% and 83.5% for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0%, 79.6%, 100% respectively), highlighting our dataset as a challenging benchmark.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "51518773",
                "name": "Rujun Han"
              },
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "145478138",
                "name": "Jiao Sun"
              },
              {
                "authorId": "66500474",
                "name": "J. Baylón"
              },
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496246,
          "isinfluential": false,
          "contexts": [
            "Lin et al.[18] proposed a approach based on curriculum learning and prompt tuning to resolve the long-distance dependency and cross-sentence scatter issues between arguments and the trigger, and use a encoder-decoder model to resemble related knowledge to enhance model."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction",
            "abstract": "Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.",
            "year": 2022,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2157259012",
                "name": "Jiaju Lin"
              },
              {
                "authorId": "2152518131",
                "name": "Qin Chen"
              },
              {
                "authorId": "145558445",
                "name": "Jie Zhou"
              },
              {
                "authorId": null,
                "name": "Jian Jin"
              },
              {
                "authorId": "145836220",
                "name": "Liangye He"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265221038,
          "isinfluential": false,
          "contexts": [
            "Recently, some work[22] has attempted to explore to utilize large language models (LLMs) for EAE tasks."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models",
            "abstract": "Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models. The project page is https://xufangzhi.github.io/symbol-llm-page/.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143943979",
                "name": "Fangzhi Xu"
              },
              {
                "authorId": "150358371",
                "name": "Zhiyong Wu"
              },
              {
                "authorId": "2112455065",
                "name": "Qiushi Sun"
              },
              {
                "authorId": "2266840897",
                "name": "Siyu Ren"
              },
              {
                "authorId": "2266754192",
                "name": "Fei Yuan"
              },
              {
                "authorId": "2266801908",
                "name": "Shuai Yuan"
              },
              {
                "authorId": "144562160",
                "name": "Qika Lin"
              },
              {
                "authorId": "2266841675",
                "name": "Yu Qiao"
              },
              {
                "authorId": "2267153105",
                "name": "Jun Liu"
              }
            ]
          }
        }
      ]
    },
    "272586489": {
      "citing_paper_info": {
        "title": "Document-Level Event Argument Extraction with Constrained Pooling and Relevance Evaluation",
        "abstract": "Document-level event argument extraction aims to identify argument spans and predict the roles they play in the event from a single document. To enhance argument extraction by better leveraging relevant context information, most existing methods focus solely on context information from the separate perspective of either event or role, without concurrently considering the influence of both event and role. Additionally, existing methods struggle to effectively handle the many-to-many relationship between arguments and roles. Therefore, this paper introduces the CasDEE (Cascade Constrained Pooling and Relevance Evaluation for Document-level Event Argument Extraction) model, which filters context information relevant to the target event and its roles following a constraining pathway of event, role, and argument boundary, and integrates it into the representations of both the candidate argument and the role. To tackle the many-to-many relationship between arguments and roles, CasDEE first utilizes a relevance function to evaluate the relevance of each span-role pair, and then selects all span-role pairs whose relevance are greater than the given threshold as the final argument extraction result. Extensive experiments show that CasDEE achieves state-of-the-art performance on RAMS and WikiEvents datasets. Further experiments reveal the superior performance of CasDEE in addressing long-range dependencies, handling the many-to-many relationship between arguments and roles, and multiple events extraction.",
        "year": 2024,
        "venue": "IEEE International Joint Conference on Neural Network",
        "authors": [
          {
            "authorId": "2302994754",
            "name": "Guanghui Wang"
          },
          {
            "authorId": "2302787518",
            "name": "Dexi Liu"
          },
          {
            "authorId": "2220964416",
            "name": "Qizhi Wan"
          },
          {
            "authorId": "2160045172",
            "name": "Rong Hu"
          },
          {
            "authorId": "2110755755",
            "name": "Xiping Liu"
          },
          {
            "authorId": "1761110",
            "name": "Changxuan Wan"
          },
          {
            "authorId": "2264943988",
            "name": "Jiaming Liu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 15,
        "influential_count": 3,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "264452034",
        "248496614",
        "233219850",
        "6628106",
        "208547716",
        "235732095",
        "207853145",
        "216562330",
        "1320606",
        "14339673",
        "222177108",
        "259370619",
        "198953378",
        "433312",
        "52967399"
      ],
      "citation_details": [
        {
          "citedcorpusid": 433312,
          "isinfluential": false,
          "contexts": [
            "It provides inputs to downstream applications such as Summarization [1], Knowledge Base Population [2], and Recommendation [3]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Cross-media Event Extraction and Recommendation",
            "abstract": "The sheer volume of unstructured multimedia data (e.g., texts, images, videos) posted on the Web during events of general interest is overwhelming and difficult to distill if seeking information relevant to a particular concern. We have developed a comprehensive system that searches, identifies, organizes and summarizes complex events from multiple data modalities. It also recommends events related to the user’s ongoing search based on previously selected attribute values and dimensions of events being viewed. In this paper we briefly present the algorithms of each component and demonstrate the system’s capabilities.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144690460",
                "name": "Di Lu"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              },
              {
                "authorId": "3180064",
                "name": "Fangbo Tao"
              },
              {
                "authorId": "145201124",
                "name": "Xiang Ren"
              },
              {
                "authorId": "2070782479",
                "name": "Rachel Guan"
              },
              {
                "authorId": "2484166",
                "name": "R. Korolov"
              },
              {
                "authorId": "2111626",
                "name": "Tongtao Zhang"
              },
              {
                "authorId": "2704179",
                "name": "Dongang Wang"
              },
              {
                "authorId": "1786871",
                "name": "Hongzhi Li"
              },
              {
                "authorId": "1739186",
                "name": "Taylor Cassidy"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "9546964",
                "name": "Shih-Fu Chang"
              },
              {
                "authorId": "145325584",
                "name": "Jiawei Han"
              },
              {
                "authorId": "144922512",
                "name": "W. Wallace"
              },
              {
                "authorId": "1701341",
                "name": "J. Hendler"
              },
              {
                "authorId": "33432486",
                "name": "Mei Si"
              },
              {
                "authorId": "1795727",
                "name": "Lance M. Kaplan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "The Coref F1 evaluates the coreference between extracted arguments and golden arguments as used by Ji and Grishman [21]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": true,
          "contexts": [
            "We set the dropout rate to 0.1, batch size to 8, and train our CasDEE using Adam [24] as optimizer with 3e-5 learning rate."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Early research primarily focused on Sentence-level event argument extraction (S-EAE) [4]–[6], However, in real-world scenarios, many events and their arguments often span multiple sentences.",
            "Unlike the well-developed sentence-level event extraction [4]–[6], [30], [31], the D-EAE faces more challenges, as it involves complex inference across multiple sentences."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "We use BERT base [22] and RoBERTa large [23] as our back-bone encoder for CasDEE."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": false,
          "contexts": [
            "We use BERT base [22] and RoBERTa large [23] as our back-bone encoder for CasDEE."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": true,
          "contexts": [
            "0 [18] and WikiEvents [7].",
            "We follow the official train/dev/test split for RAMS and WikiEvents datasets and use the evaluation script provided by Ebner et al. [18] to evaluate the performance.",
            "Classification-based methods predict the argument role for candidate text spans which usually have a maximum length limitation [18], [20], [33], [34].",
            "A document from the RMAS dataset [18]."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208547716,
          "isinfluential": false,
          "contexts": [
            "Early research primarily focused on Sentence-level event argument extraction (S-EAE) [4]–[6], However, in real-world scenarios, many events and their arguments often span multiple sentences.",
            "Unlike the well-developed sentence-level event extraction [4]–[6], [30], [31], the D-EAE faces more challenges, as it involves complex inference across multiple sentences."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Reading the Manual: Event Extraction as Definition Comprehension",
            "abstract": "We ask whether text understanding has progressed to where we may extract event information through incremental refinement of bleached statements derived from annotation manuals. Such a capability would allow for the trivial construction and extension of an extraction framework by intended end-users through declarations such as, “Some person was born in some location at some time.” We introduce an example of a model that employs such statements, with experiments illustrating we can extract events under closed ontologies and generalize to unseen event types simply by reading new definitions.",
            "year": 2019,
            "venue": "SPNLP",
            "authors": [
              {
                "authorId": "104375103",
                "name": "Yunmo Chen"
              },
              {
                "authorId": "40364920",
                "name": "Tongfei Chen"
              },
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "…compare our model with several baselines and the following previous state-of-the-art models: (1) generation-based models: BART-Gen [25], EA2E [26], PAIE [17], SPEAE [27]; (2) question answering-based models: FEAE [28], BERT-QA [29]; (3) classification-based models: TSAR [10], SCPRG [11], TARA [13]."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222177108,
          "isinfluential": false,
          "contexts": [
            "Unlike the well-developed sentence-level event extraction [4]–[6], [30], [31], the D-EAE faces more challenges, as it involves complex inference across multiple sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Resource-Enhanced Neural Model for Event Argument Extraction",
            "abstract": "Event argument extraction (EAE) aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges: (1) Data scarcity. (2) Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument. (3) Integrating event trigger information into candidate argument representation. For (1), we explore using unlabeled data. For (2), we use Transformer that uses dependency parses to guide the attention mechanism. For (3), we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE 2005 benchmark show that our approach achieves a new state-of-the-art.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2115889791",
                "name": "Jie Ma"
              },
              {
                "authorId": "1717480",
                "name": "Shuai Wang"
              },
              {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
              },
              {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
              },
              {
                "authorId": "1403907739",
                "name": "Yaser Al-Onaizan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "We compare our model with several baselines and the following previous state-of-the-art models: (1) generation-based models: BART-Gen [25], EA2E [26], PAIE [17], SPEAE [27]; (2) question answering-based models: FEAE [28], BERT-QA [29]; (3) classification-based models: TSAR [10], SCPRG [11], TARA…",
            "Li et al. [25] framed EAE as an end-to-end conditional generation task using templates."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235732095,
          "isinfluential": false,
          "contexts": [
            "The event Stab is triggered by stabbed and the arguments for three different roles are scattered across different sentences. this issue, Sheng et al. [14] created role-specific taggers to detect if a token marks the start or end of a role-specific argument.",
            "…PLM encoder already implicitly considers these constraints through multi-head attention mechanisms, we introduce Condition Information Fusion (CIF) [14] to incorporate event type representations directly into the span representation: where µ ∈ R and σ ∈ R are the mean and standard variance taken…"
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Most existing methods assume that events appear in sentences without overlaps, which are not applicable to the complicated overlapping event extraction. This work systematically studies the realistic event overlapping problem, where a word may serve as triggers with several types or arguments with different roles. To tackle the above problem, we propose a novel joint learning framework with cascade decoding for overlapping event extraction, termed as CasEE. Particularly, CasEE sequentially performs type detection, trigger extraction and argument extraction, where the overlapped targets are extracted separately conditioned on the specific former prediction. All the subtasks are jointly learned in a framework to capture dependencies among the subtasks. The evaluation on a public event extraction benchmark FewFC demonstrates that CasEE achieves significant improvements on overlapping event extraction over previous competitive methods.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2054250919",
                "name": "Jiawei Sheng"
              },
              {
                "authorId": "144019293",
                "name": "Shu Guo"
              },
              {
                "authorId": "48613402",
                "name": "Yu Bowen"
              },
              {
                "authorId": "2117126771",
                "name": "Qian Li"
              },
              {
                "authorId": "1491243898",
                "name": "Yiming Hei"
              },
              {
                "authorId": "2108740151",
                "name": "Lihong Wang"
              },
              {
                "authorId": "2079682",
                "name": "Tingwen Liu"
              },
              {
                "authorId": "46485352",
                "name": "Hongbo Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": true,
          "contexts": [
            "For a candidate argument s i,j ranging from w i to w j , Zhang et al. [20] and Xu et al. [10] used average pooling over the hidden states of all tokens within this span to represent it: c s i,j = 1 j − i +1 = i h wk , where h wk is the k th token embedding from H w .",
            "Most existing classification-based methods assign only one role label for an argument [10], [11], [13], which cannot tackle the scenarios where an argument plays multiple roles of an event.",
            "…compare our model with several baselines and the following previous state-of-the-art models: (1) generation-based models: BART-Gen [25], EA2E [26], PAIE [17], SPEAE [27]; (2) question answering-based models: FEAE [28], BERT-QA [29]; (3) classification-based models: TSAR [10], SCPRG [11], TARA [13].",
            "Following Xu et al. [10], we divide the event arguments in the RAMS dataset into five bins according to sentence distance between arguments and trigger, i.e., l = {− 2 , − 1 , 0 , 1 , 2 } .",
            "Following Xu et al. [10] and Liu et al. [11], we extract event arguments for each event in a document independently.",
            "To capture essential context information relevant to the target event from a document, Xu et al. [10] proposed a two-stream encoding module that encodes the document from both global and local perspectives to better utilize context information.",
            "To enhance argument extraction by better leveraging relevant context information, Xu et al. [10] proposed a two-stream encoding module, considering both global and local perspectives to optimize context utilization.",
            "Previous methods focused on the start and end positions of the candidate argument separately, not as a unified whole [10], [11]."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['result']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259370619,
          "isinfluential": false,
          "contexts": [
            "Additionally, Lu et al. [12] created context-aware questions for each role to assist the model in better utilizing role-relevant context information.",
            "Wei et al. [28] and Lu et al. [12] reformulated EAE as a machine reading comprehension task."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Question Generation and Answering",
            "abstract": "Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model. Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2111830948",
                "name": "Di Lu"
              },
              {
                "authorId": "27035817",
                "name": "Shihao Ran"
              },
              {
                "authorId": "2086973507",
                "name": "Joel R. Tetreault"
              },
              {
                "authorId": "144633617",
                "name": "A. Jaimes"
              }
            ]
          }
        },
        {
          "citedcorpusid": 264452034,
          "isinfluential": false,
          "contexts": [
            "Additionally, existing generation-based methods address the issue by generating all arguments for each role [7], [15], [16]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction",
            "abstract": "Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of\"memory\", where the results of already predicted events are cached and can be retrieved to help the prediction of upcoming events. These methods extract events according to their appearance order in the document, however, the event that appears in the first sentence does not mean that it is the easiest to extract. Existing methods might introduce noise to the extraction of upcoming events if they rely on an incorrect prediction of previous events. In order to provide more reliable memory, we propose a simple-to-complex progressive framework for document-level EAE. Specifically, we first calculate the difficulty of each event and then, we conduct the extraction following a simple-to-complex order. In this way, the memory will store the most certain results, and the model could use these reliable sources to help the prediction of more difficult events. Experiments on WikiEvents show that our model outperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex framework is useful in the EAE task.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2007771781",
                "name": "Quzhe Huang"
              },
              {
                "authorId": "2261678163",
                "name": "Yanxi Zhang"
              },
              {
                "authorId": "2261698348",
                "name": "Dongyan Zhao"
              }
            ]
          }
        }
      ]
    },
    "259370571": {
      "citing_paper_info": {
        "title": "Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation",
        "abstract": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2109978994",
            "name": "Yubing Ren"
          },
          {
            "authorId": "47184362",
            "name": "Yanan Cao"
          },
          {
            "authorId": "2075394870",
            "name": "Ping Guo"
          },
          {
            "authorId": "36595248",
            "name": "Fang Fang"
          },
          {
            "authorId": "2185915076",
            "name": "Wei Ma"
          },
          {
            "authorId": "1390641501",
            "name": "Zheng Lin"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 9,
        "influential_count": 1,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "131773936",
        "247084444",
        "53592270",
        "220045828",
        "4698173",
        "246472929",
        "201646309",
        "204838007",
        "49312395"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4698173,
          "isinfluential": false,
          "contexts": [
            "…successfully applied to many NLP tasks, e.g., dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b), machine translation (Zhang et al., 2018; Xu et al., 2020; He et al., 2021) and information extraction (Lee et al., 2022; Zhang et al., 2022; Chen et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Guiding Neural Machine Translation with Retrieved Translation Pieces",
            "abstract": "One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call “translation pieces”. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "4279134",
                "name": "Jingyi Zhang"
              },
              {
                "authorId": "1802277",
                "name": "M. Utiyama"
              },
              {
                "authorId": "1698363",
                "name": "E. Sumita"
              },
              {
                "authorId": "1700325",
                "name": "Graham Neubig"
              },
              {
                "authorId": "145223960",
                "name": "Satoshi Nakamura"
              }
            ]
          }
        },
        {
          "citedcorpusid": 49312395,
          "isinfluential": false,
          "contexts": [
            "Retrieval-augmented methods have recently been successfully applied to many NLP tasks, e.g., dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b), machine translation (Zhang et al., 2018; Xu et al., 2020; He et al., 2021) and information extraction (Lee et al.,…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Response Generation by Context-aware Prototype Editing",
            "abstract": "Open domain response generation has achieved remarkable progress in recent years, but sometimes yields short and uninformative responses. We propose a new paradigm, prototypethen-edit for response generation, that first retrieves a prototype response from a pre-defined index and then edits the prototype response according to the differences between the prototype context and current context. Our motivation is that the retrieved prototype provides a good start-point for generation because it is grammatical and informative, and the post-editing process further improves the relevance and coherence of the prototype. In practice, we design a contextaware editing model that is built upon an encoder-decoder framework augmented with an editing vector. We first generate an edit vector by considering lexical differences between a prototype context and current context. After that, the edit vector and the prototype response representation are fed to a decoder to generate a new response. Experiment results on a large scale dataset demonstrate that our new paradigm significantly increases the relevance, diversity and originality of generation results, compared to traditional generative models. Furthermore, our model outperforms retrieval-based methods in terms of relevance and originality.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2142240763",
                "name": "Yu Wu"
              },
              {
                "authorId": "49807919",
                "name": "Furu Wei"
              },
              {
                "authorId": "3110003",
                "name": "Shaohan Huang"
              },
              {
                "authorId": "1707275",
                "name": "Zhoujun Li"
              },
              {
                "authorId": "92660691",
                "name": "Ming Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53592270,
          "isinfluential": false,
          "contexts": [
            "We ﬁne-tune the models on each dataset independently using AdamW (Loshchilov and Hutter, 2019) and conducted experiments on 4 NVIDIA-V100-32GB."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131773936,
          "isinfluential": false,
          "contexts": [
            "…consistent comparison, we divide several state-of-the-art models into three categories: (1) Multi-label classiﬁcation-based model: BERT-CRF (Shi and Lin, 2019), PAIE (Ma et al., 2022); (2) QA-based model: EEQA (Du and Cardie, 2020) and DocMRC (Liu et al., 2021); and (3) Generation-based…",
            "For strictly consistent comparison, we divide several state-of-the-art models into three categories: (1) Multi-label classiﬁcation-based model: BERT-CRF (Shi and Lin, 2019), PAIE (Ma et al., 2022); (2) QA-based model: EEQA (Du and Cardie, 2020) and DocMRC (Liu et al., 2021); and (3) Generation-based model: BART-Gen (Li et al., 2021) and T5-baseline.",
            "…strictly consistent comparison, we divide several state-of-the-art models into three categories: (1) Multi-label classiﬁcation-based model: BERT-CRF (Shi and Lin, 2019), PAIE (Ma et al., 2022); (2) QA-based model: EEQA (Du and Cardie, 2020) and DocMRC (Liu et al., 2021); and (3) Generation-based…"
          ],
          "intents": [
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "abstract": "We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1884766",
                "name": "Peng Shi"
              },
              {
                "authorId": "145580839",
                "name": "Jimmy J. Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 201646309,
          "isinfluential": false,
          "contexts": [
            "Du and Ji (2022) applied S-BERT (Reimers and Gurevych, 2019) to retrieve the most relevant example for event extraction.",
            "First sample a noise variable (cid:2) from N (0 , 1) Then transform it to 9 − W Calculate the current sample: retrieve (also via S-BERT) the instance label y r that is the top-k relevant to the input label from the training corpus D train .",
            "For retrieval, we use S-BERT (Reimers and Gurevych, 2019) to retrieve semantically similar documents x r ∈ D train ."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
            "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2959414",
                "name": "Nils Reimers"
              },
              {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "We adopt the T5 model (Raffel et al., 2022), an encoder-decoder pre-trained model, as a backbone."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220045828,
          "isinfluential": false,
          "contexts": [
            "…successfully applied to many NLP tasks, e.g., dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b), machine translation (Zhang et al., 2018; Xu et al., 2020; He et al., 2021) and information extraction (Lee et al., 2022; Zhang et al., 2022; Chen et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Boosting Neural Machine Translation with Similar Translations",
            "abstract": "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with “copy” information while translations based on embedding similarities tend to extend the translation “context”. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1925543",
                "name": "Jitao Xu"
              },
              {
                "authorId": "2209023",
                "name": "J. Crego"
              },
              {
                "authorId": "3053934",
                "name": "Jean Senellart"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246472929,
          "isinfluential": false,
          "contexts": [
            "These retrieval-augmented methods use similarity-based retrieval, which is based on a simple hypothesis (Li et al., 2022): the more x r (retrieved demonstration) resembles x (original input), the more likely y r (demonstration label) resembles y (input label), so it will help the generation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Survey on Retrieval-Augmented Text Generation",
            "abstract": "Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "91956362",
                "name": "Huayang Li"
              },
              {
                "authorId": "50087162",
                "name": "Yixuan Su"
              },
              {
                "authorId": "2053327987",
                "name": "Deng Cai"
              },
              {
                "authorId": "2152546690",
                "name": "Yan Wang"
              },
              {
                "authorId": "2978364",
                "name": "Lemao Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": true,
          "contexts": [
            "(Ma et al., 2022), and ‡ denotes the results from our implemented models for a fairer comparison.",
            "For BART-base model, we report the results from Ma et al. (2022).",
            "For strictly consistent comparison, we divide several state-of-the-art models into three categories: (1) Multi-label classiﬁcation-based model: BERT-CRF (Shi and Lin, 2019), PAIE (Ma et al., 2022); (2) QA-based model: EEQA (Du and Cardie, 2020) and DocMRC (Liu et al., 2021); and (3) Generation-based model: BART-Gen (Li et al., 2021) and T5-baseline.",
            "…we divide several state-of-the-art models into three categories: (1) Multi-label classiﬁcation-based model: BERT-CRF (Shi and Lin, 2019), PAIE (Ma et al., 2022); (2) QA-based model: EEQA (Du and Cardie, 2020) and DocMRC (Liu et al., 2021); and (3) Generation-based model: BART-Gen (Li et al.,…"
          ],
          "intents": [
            "['result']",
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        }
      ]
    },
    "258967387": {
      "citing_paper_info": {
        "title": "An AMR-based Link Prediction Approach for Document-level Event Argument Extraction",
        "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2145435513",
            "name": "Yuqing Yang"
          },
          {
            "authorId": "3187768",
            "name": "Qipeng Guo"
          },
          {
            "authorId": "12040998",
            "name": "Xiangkun Hu"
          },
          {
            "authorId": "39939186",
            "name": "Yue Zhang"
          },
          {
            "authorId": "1767521",
            "name": "Xipeng Qiu"
          },
          {
            "authorId": "1852415",
            "name": "Zheng Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 1,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "216562330",
        "249063162",
        "5458500",
        "250637739",
        "1320606",
        "247450724",
        "236460308"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "For Coref F1, the model is given full credit if the extracted argument is corefer-ential with the reference as used in Ji and Grishman (2008)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5458500,
          "isinfluential": false,
          "contexts": [
            "Wang et al. (2021) pre-trains the EAE model with a contrastive loss built on AMR graphs.",
            "Xu and Huang (2022) and Wang et al. (2021) utilize AMR graphs to provide training signals via self-training and contrastive learning, respectively."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Modeling Relational Data with Graph Convolutional Networks",
            "abstract": "Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
            "year": 2017,
            "venue": "Extended Semantic Web Conference",
            "authors": [
              {
                "authorId": "8804828",
                "name": "M. Schlichtkrull"
              },
              {
                "authorId": "41016725",
                "name": "Thomas Kipf"
              },
              {
                "authorId": "2789097",
                "name": "Peter Bloem"
              },
              {
                "authorId": "9965217",
                "name": "Rianne van den Berg"
              },
              {
                "authorId": "144889265",
                "name": "Ivan Titov"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "(1) QA-based models: EEQA (Du and Cardie, 2020b) and FEAE (Wei et al.",
            "Du and Cardie (2020a) chooses the hierarchical method to aggregate information from different granularity."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460308,
          "isinfluential": false,
          "contexts": [
            "Unlike the well-developed sentence-level event extraction (Xi et al., 2021; Ma et al., 2020), the Doc-level EAE faces more challenges."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "31407139",
                "name": "Kaiwen Wei"
              },
              {
                "authorId": "2946890",
                "name": "Xian Sun"
              },
              {
                "authorId": "151473773",
                "name": "Zequn Zhang"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "2116390646",
                "name": "Zhi Guo"
              },
              {
                "authorId": "2152163772",
                "name": "Li Jin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247450724,
          "isinfluential": true,
          "contexts": [
            "To explore the effect of different AMR parsing performance, we compare test results of TARA using transition-based AMR parser and a latest state-of-the-art parser AMRBART (Bai et al., 2022) in Table 9.",
            "Second, the Smatch score of SOTA AMR parsers is around 85 (Bai et al., 2022), which causes information loss as well.",
            "We implement a simple node-to-text aligner and compress the obtained AMR graph as described in Sec-B for AMRBART.",
            "As shown in the table, though AMRBART brings better AMR parsing performance, it dose not gain more improvements for EAE.",
            "We also show the performance using another state-of-the-art AMR parser, AMRBART (Bai et al., 2022 (3) Span-based models: TSAR (Xu et al., 2022).",
            "TAG can be built on vanilla AMR graphs generated by an off-the-shelf AMR parser (Bai et al., 2022; Astudillo et al., 2020), which also provides the alignment information between nodes and words."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "--",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Graph Pre-training for AMR Parsing and Generation",
            "abstract": "Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure.Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively.However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs.In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training.We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks.Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model.To our knowledge, we are the first to consider pre-training on semantic graphs.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "6713131",
                "name": "Xuefeng Bai"
              },
              {
                "authorId": "2109404730",
                "name": "Yulong Chen"
              },
              {
                "authorId": "39939186",
                "name": "Yue Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249063162,
          "isinfluential": false,
          "contexts": [
            "Xu and Huang (2022) and Wang et al. (2021) utilize AMR graphs to provide training signals via self-training and contrastive learning, respectively.",
            "Lin et al. (2022) and Xu and Huang (2022) introduce AMR path information as training signals to correct argument predictions."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improve Event Extraction via Self-Training with Gradient Guidance",
            "abstract": "Data scarcity and imbalance have been the main factors that hinder the progress of event extraction (EE). In this work, we propose a self-training with gradient guidance (STGG) framework which consists of (1) a base event extraction model which is ﬁrstly trained on existing event annotations and then applied to large-scale unlabeled corpora to predict new event mentions, and (2) a scoring model that takes in each predicted event trigger and ar-gument as well as their path in the Abstract Meaning Representation (AMR) graph to estimate a probability score indicating the correctness of the event prediction. The new event predictions along with their correctness scores are then used as pseudo labeled examples to improve the base event extraction model while the magnitude and direction of its gradients are guided by the correctness scores. Experimental results on three benchmark datasets, including ACE05-E, ACE05-E + , and ERE, demonstrate the effectiveness of the STGG framework on event extraction task with up to 1.9 F-score improvement over the base event extraction models. Our experimental analysis further shows that STGG is a general framework as it can be applied to any base event extraction models and improve their performance by leveraging broad unlabeled data, even when the high-quality AMR graph annotations are not available.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2136442661",
                "name": "Zhiyang Xu"
              },
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250637739,
          "isinfluential": false,
          "contexts": [
            "Fan et al. (2022) trains a learnable module to add nodes and edges to the AMR graph.",
            "Fan et al. (2022) and Xu et al. (2021) construct an entity-based graph to model dependencies among the document."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Interactive Information Extraction by Semantic Information Graph",
            "abstract": "Information extraction (IE) mainly focuses on three highly correlated subtasks, i.e., entity extraction, relation extraction and event extraction. Recently, there are studies using Abstract Meaning Representation (AMR) to utilize the intrinsic correlations among these three subtasks. AMR based models are capable of building the relationship of arguments. However, they are hard to deal with relations. In addition, the noises of AMR (i.e., tags unrelated to IE tasks, nodes with unconcerned conception, and edge types with complicated hierarchical structures) disturb the decoding processing of IE. As a result, the decoding processing limited by the AMR cannot be worked effectively. To overcome the shortages, we propose an Interactive Information Extraction (InterIE) model based on a novel Semantic Information Graph (SIG). SIG can guide our InterIE model to tackle the three subtasks jointly. Furthermore, the well-designed SIG without noise is capable of enriching entity and event trigger representation, and capturing the edge connection between the information types. Experimental results show that our InterIE achieves state-of-the-art performance on all IE subtasks on the benchmark dataset (i.e., ACE05-E+ and ACE05-E). More importantly, the proposed model is not sensitive to the decoding order, which goes beyond the limitations of AMR based methods.",
            "year": 2022,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2153361145",
                "name": "Siqi Fan"
              },
              {
                "authorId": "2119256294",
                "name": "Yequan Wang"
              },
              {
                "authorId": "39682944",
                "name": "J. Li"
              },
              {
                "authorId": "143644345",
                "name": "Zheng Zhang"
              },
              {
                "authorId": "2032836",
                "name": "Shuo Shang"
              },
              {
                "authorId": "144433445",
                "name": "Peng Han"
              }
            ]
          }
        }
      ]
    },
    "271115336": {
      "citing_paper_info": {
        "title": "A Document-Level Trigger-Word-Free Multi-Event Extraction Method for Criminal Documents",
        "abstract": "Document Event Extraction (DEE) aims to extract event records from a given document. The DEE task faces two key challenges: parameter dispersion and the multi-event problem. To address these challenges, this paper proposes a trigger-word-free document-level event extraction method for criminal documents. The method utilizes deep learning technology to extract entities and sentence semantic information using pretrained language models. It then extracts dependent syntactic information in the document synchronously using graph neural networks. Finally, it encodes sentence-level and document-level features and employs the parameter path extension method with a memory mechanism to extract event records. The experimental data and evaluation indexes indicate that the method achieves better performance on the dataset, providing an effective solution for event extraction in legal criminal documents. This, in turn, helps to improve the efficiency and accuracy of judicial trials.",
        "year": 2024,
        "venue": "2024 4th International Conference on Computer, Control and Robotics (ICCCR)",
        "authors": [
          {
            "authorId": "2215100550",
            "name": "Teng Yang"
          },
          {
            "authorId": "2305569770",
            "name": "Yunmei Shi"
          },
          {
            "authorId": "2298515078",
            "name": "Haiying Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 0,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "249579113",
        "2367456",
        "52967399",
        "5993783",
        "249431954",
        "226283556",
        "259383402",
        "220046861",
        "209319719",
        "233219850",
        "236460259"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "Using an external knowledge base and a transformer model, Yang et al. [18] and Chen et al. [19] performed joint extraction of events and entities from documents, respectively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5993783,
          "isinfluential": false,
          "contexts": [
            "Recent studies have focused on exploiting more knowledge, such as document context information [11], [12], dependency tree information [13], [14], and external knowledge integration [15], [16]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A language-independent neural network for event detection",
            "abstract": "Event detection remains a challenge because of the difficulty of encoding the word semantics in various contexts. Previous approaches have heavily depended on language-specific knowledge and preexisting natural language processing tools. However, not all languages have such resources and tools available compared with English language. A more promising approach is to automatically learn effective features from data, without relying on language-specific resources. In this study, we develop a language-independent neural network to capture both sequence and chunk information from specific contexts and use them to train an event detector for multiple languages without any manually encoded features. Experiments show that our approach can achieve robust, efficient and accurate results for various languages. In the ACE 2005 English event detection task, our approach achieved a 73.4% F-score with an average of 3.0% absolute improvement compared with state-of-the-art. Additionally, our experimental results are competitive for Chinese and Spanish.",
            "year": 2016,
            "venue": "Science China Information Sciences",
            "authors": [
              {
                "authorId": "2674998",
                "name": "Xiaocheng Feng"
              },
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "39483833",
                "name": "Duyu Tang"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "152277111",
                "name": "Bing Qin"
              },
              {
                "authorId": "40282288",
                "name": "Ting Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "In this module, sentences are encoded using Bert [30], Bi-LSTM and CRF, where Bert is obtained by pre-training with data from criminal documents."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209319719,
          "isinfluential": false,
          "contexts": [
            "Event Extraction (EE) is a challenging task in Information Extraction (IE) that aims to detect events and extract their parameters from text [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Event Extraction From Text",
            "abstract": "Numerous important events happen everyday and everywhere but are reported in different media sources with different narrative styles. How to detect whether real-world events have been reported in articles and posts is one of the main tasks of event extraction. Other tasks include extracting event arguments and identifying their roles, as well as clustering and tracking similar events from different texts. As one of the most important research themes in natural language processing and understanding, event extraction has a wide range of applications in diverse domains and has been intensively researched for decades. This article provides a comprehensive yet up-to-date survey for event extraction from text. We not only summarize the task definitions, data sources and performance evaluations for event extraction, but also provide a taxonomy for its solution approaches. In each solution group, we provide detailed analysis for the most representative methods, especially their origins, basics, strengths and weaknesses. Last, we also present our envisions about future research directions.",
            "year": 2019,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2052727822",
                "name": "Wei Xiang"
              },
              {
                "authorId": "1719003",
                "name": "Bang Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": false,
          "contexts": [
            "However, in the above model, the distance between event parameters and trigger words within a document increases, making it more difficult for the model to extract event elements [22], [23]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226283556,
          "isinfluential": false,
          "contexts": [
            "Using an external knowledge base and a transformer model, Yang et al. [18] and Chen et al. [19] performed joint extraction of events and entities from documents, respectively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Modeling of Arguments for Event Understanding",
            "abstract": "We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.",
            "year": 2020,
            "venue": "CODI",
            "authors": [
              {
                "authorId": "104375103",
                "name": "Yunmo Chen"
              },
              {
                "authorId": "40364920",
                "name": "Tongfei Chen"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "However, in the above model, the distance between event parameters and trigger words within a document increases, making it more difficult for the model to extract event elements [22], [23]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": false,
          "contexts": [
            "Yang et al. [28] introduced a document-level encoder and a multi-granularity decoder to generate perceptual representations of events simultaneously with the documents, to achieve the perceptual representation of all events are extracted simultaneously."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249431954,
          "isinfluential": false,
          "contexts": [
            "Liang et al. [29] proposed a relationship-enhanced attention converter for relationship modeling in document-level event extraction, which uses association matrices and entity representations to enable it to capture relational dependency information."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction",
            "abstract": "In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117874913",
                "name": "Yuan Liang"
              },
              {
                "authorId": "3435199",
                "name": "Zhuoxuan Jiang"
              },
              {
                "authorId": "2168284967",
                "name": "Di Yin"
              },
              {
                "authorId": "2064646914",
                "name": "Bo Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 249579113,
          "isinfluential": false,
          "contexts": [
            "Yang [27] constructed a multi-round, multi-granularity reader designed to read multiple documents with different granularities, such as words, sentences, and paragraphs, simultaneously."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Turn and Multi-Granularity Reader for Document-Level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2169815441",
                "name": "Zuyu Zhao"
              },
              {
                "authorId": "5912673",
                "name": "Weijian Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259383402,
          "isinfluential": false,
          "contexts": [
            "Tang et al. [25] proposed TCEDCL, a cybersecurity event detection method that does not rely on trigger words."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Trigger-free cybersecurity event detection based on contrastive learning",
            "abstract": "",
            "year": 2023,
            "venue": "Journal of Supercomputing",
            "authors": [
              {
                "authorId": "2221777714",
                "name": "Mengmeng Tang"
              },
              {
                "authorId": "2218859",
                "name": "Yuanbo Guo"
              },
              {
                "authorId": "51160762",
                "name": "Qingchun Bai"
              },
              {
                "authorId": "48213346",
                "name": "Han Zhang"
              }
            ]
          }
        }
      ]
    },
    "259858959": {
      "citing_paper_info": {
        "title": "Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance",
        "abstract": "Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling(STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate arguments. Both STCP and RLIG introduce no more than 1% new parameters compared with the base model and can be easily applied to other event extraction models, which are compact and transplantable. Experiments on two public datasets show that our SCPRG outperforms previous state-of-the-art methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents respectively. Further analyses illustrate the interpretability of our model.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2143606898",
            "name": "Wanlong Liu"
          },
          {
            "authorId": "2153264570",
            "name": "Shaohuan Cheng"
          },
          {
            "authorId": "2054124872",
            "name": "DingYi Zeng"
          },
          {
            "authorId": "2064923494",
            "name": "Hong Qu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 26,
        "unique_cited_count": 26,
        "influential_count": 3,
        "detailed_records_count": 26
      },
      "cited_papers": [
        "235254286",
        "220524732",
        "47252984",
        "10913456",
        "259716313",
        "218630327",
        "204960716",
        "248496614",
        "52816033",
        "35386653",
        "174799895",
        "14339673",
        "220047190",
        "119308902",
        "236460308",
        "207853145",
        "252819226",
        "235253912",
        "248721950",
        "252626402",
        "254043718",
        "202539496",
        "8950084",
        "131773936",
        "231728756",
        "216562330"
      ],
      "citation_details": [
        {
          "citedcorpusid": 8950084,
          "isinfluential": false,
          "contexts": [
            "(Wang et al., 2021; Du and Cardie, 2020a) utilize the sequence labeling model BiLSTM-CRF (Zhang et al., 2015) for DEE.",
            ", 2021; Du and Cardie, 2020a) utilize the sequence labeling model BiLSTM-CRF (Zhang et al., 2015) for DEE."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Bidirectional Long Short-Term Memory Networks for Relation Classification",
            "abstract": "",
            "year": 2015,
            "venue": "Pacific Asia Conference on Language, Information and Computation",
            "authors": [
              {
                "authorId": "2108089005",
                "name": "Shu Zhang"
              },
              {
                "authorId": "37032041",
                "name": "Dequan Zheng"
              },
              {
                "authorId": "2435746",
                "name": "Xinchen Hu"
              },
              {
                "authorId": "2150425875",
                "name": "Ming Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10913456,
          "isinfluential": false,
          "contexts": [
            ", 2015) firstly propose a neural pipeline model for event extraction and (Nguyen et al., 2016; Nguyen and Grishman, 2015; Liu et al., 2017; Zhou et al., 2020) further extend the pipeline model to recurrent neural networks and convolutional neural networks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Detection and Domain Adaptation with Convolutional Neural Networks",
            "abstract": "We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "(Chen et al., 2015) firstly propose a neural pipeline model for event extraction and (Nguyen et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 35386653,
          "isinfluential": false,
          "contexts": [
            ", 2015) firstly propose a neural pipeline model for event extraction and (Nguyen et al., 2016; Nguyen and Grishman, 2015; Liu et al., 2017; Zhou et al., 2020) further extend the pipeline model to recurrent neural networks and convolutional neural networks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms",
            "abstract": "This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 47252984,
          "isinfluential": false,
          "contexts": [
            "Considering most candidate arguments are negative samples and the imbalanced role distribution, we adopt focal loss (Lin et al., 2017) to make the training process focus more on useful positive samples, where α and γ are hyperparameters."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Focal Loss for Dense Object Detection",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
              },
              {
                "authorId": "47316088",
                "name": "Priya Goyal"
              },
              {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
              },
              {
                "authorId": "39353098",
                "name": "Kaiming He"
              },
              {
                "authorId": "3127283",
                "name": "Piotr Dollár"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52816033,
          "isinfluential": false,
          "contexts": [
            "Compared with previous works (Liu et al., 2018; Wadden et al., 2019; Tong et al., 2020) focusing on sentence-level EAE, more and more recent works tend to explore documentlevel EAE (Wang et al.",
            "To model the dependency of words in a sentence, (Liu et al., 2018; Yan et al., 2019; Fernandez Astudillo et al., 2020) leverage dependency trees to model semantic and syntactic relations."
          ],
          "intents": [
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
            "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2730687",
                "name": "Zhunchen Luo"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "(Zheng et al., 2019) propose a transformer-based architecture and model"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131773936,
          "isinfluential": true,
          "contexts": [
            "Compared with previous tagging-based and span-based methods like BERT-CRF and Two-Step, our SCPRG equipped with BERTbase yields an improvement of +8.46/+9.64 ∼ +6.36/+7.14 Span F1 and +7.68/+9.00 ∼ +5.38/+6.40 Head F1 on dev/test set, showing that our SCPRG framework has superiority in excluding impossible candidate spans and solving the imbalance of data distribution problem.",
            "Baselines We compare different categories of document-level EAE models which mainly consist of tagging-based methods such as BERT-CRF (Shi and Lin, 2019), BERT-CRFTCD (Ebner et al., 2020), span-based methods like Two-Step (Zhang et al., 2020b), Two-StepTCD (Ebner et al., 2020), TSAR (Xu et al., 2022), and other generation-based methods such as FEAE (Wei et al., 2021), BERTQA (Du and Cardie, 2020b), BART-Gen (Li et al., 2021), EA2E (Zeng et al., 2022).",
            "Baselines We compare different categories of document-level EAE models which mainly consist of tagging-based methods such as BERT-CRF (Shi and Lin, 2019), BERT-CRFTCD (Ebner et al."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "abstract": "We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1884766",
                "name": "Peng Shi"
              },
              {
                "authorId": "145580839",
                "name": "Jimmy J. Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174799895,
          "isinfluential": false,
          "contexts": [
            "Some methods using transformer-based pre-trained model (Wadden et al., 2019; Wang et al., 2019; Tong et al., 2020; Lu et al., 2021; Liu et al., 2022b) also achieve remarkable performance."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adversarial Training for Weakly Supervised Event Detection",
            "abstract": "Modern weakly supervised methods for event detection (ED) avoid time-consuming human annotation and achieve promising results by learning from auto-labeled data. However, these methods typically rely on sophisticated pre-defined rules as well as existing instances in knowledge bases for automatic annotation and thus suffer from low coverage, topic bias, and data noise. To address these issues, we build a large event-related candidate set with good coverage and then apply an adversarial training mechanism to iteratively identify those informative instances from the candidate set and filter out those noisy ones. The experiments on two real-world datasets show that our candidate selection and adversarial training can cooperate together to obtain more diverse and accurate training data for ED, and significantly outperform the state-of-the-art methods in various weakly supervised scenarios. The datasets and source code can be obtained from https://github.com/thunlp/Adv-ED.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "Compared with previous works (Liu et al., 2018; Wadden et al., 2019; Tong et al., 2020) focusing on sentence-level EAE, more and more recent works tend to explore documentlevel EAE (Wang et al.",
            "Some methods using transformer-based pre-trained model (Wadden et al., 2019; Wang et al., 2019; Tong et al., 2020; Lu et al., 2021; Liu et al., 2022b) also achieve remarkable performance.",
            "(Wadden et al., 2019) enumerates all possible spans and construct span graphs with graph neural netpreve nter trans porte r pass enge r origindesti natio n"
          ],
          "intents": [
            "['result']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "BART-Genlarge is based on BARTlarge (Lewis et al., 2019) which is pre-trained on the same corpus.",
            "First, when we remove span-trigger-based contextual pooling (STCP) module, both Span F1 and\n4BART-Genlarge is based on BARTlarge (Lewis et al., 2019) which is pre-trained on the same corpus."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": true,
          "contexts": [
            "In this section, we show the complete cooccurrence frequency matrix which contains all roles in RAMS test set.",
            "Other span based methods (Ebner et al., 2020; Zhang et al., 2020b) predict the argument roles for candidate text spans with a maximum length limitation.",
            "Baselines We compare different categories of document-level EAE models which mainly consist of tagging-based methods such as BERT-CRF (Shi and Lin, 2019), BERT-CRFTCD (Ebner et al., 2020), span-based methods like Two-Step (Zhang et al.",
            "Additionally, when removing role-based latent information guidance (RLIG) module5, the performance of SCPRGbase/ SCPRGlarge drops sharply by 1.03/1.04 Span F1 and 1.58/1.2 Head F1 on RAMS test set.",
            ", 2022), which needs to solve long-distance dependency (Ebner et al., 2020) and cross-sentence inference (Li et al.",
            "Datasets and Metrics We evaluate the proposed model on two large-scale public document-level\nEAE datasets, RAMSv1.",
            "In the main body of the paper, we conduct ablation study on RAMS dataset for SCPRGbase and SCPRGlarge.",
            "Figure 7: A t-SNE visualization example from RAMS, where embeddings of arguments and roles are from 5 different documents.",
            "Following (Xu et al., 2022), we report the Span F1 and Head F1 on dev and test sets for RAMS dataset.",
            "Moreover, many roles co-occur in multiple events (Ebner et al., 2020; Li et al., 2021), which may have close semantic relevance.",
            "Baselines We compare different categories of document-level EAE models which mainly consist of tagging-based methods such as BERT-CRF (Shi and Lin, 2019), BERT-CRFTCD (Ebner et al., 2020), span-based methods like Two-Step (Zhang et al., 2020b), Two-StepTCD (Ebner et al., 2020), TSAR (Xu et al., 2022), and other generation-based methods such as FEAE (Wei et al., 2021), BERTQA (Du and Cardie, 2020b), BART-Gen (Li et al., 2021), EA2E (Zeng et al., 2022).",
            ", 2020b), Two-StepTCD (Ebner et al., 2020), TSAR (Xu et al.",
            "To better illustrate the capabilities of our components, we conduct ablation study on RAMS dataset as shown in Table 4.",
            "Specifically, we count and visualize the frequency of co-occurrence between 15 most frequent roles in RAMS dataset in Figure 2.",
            "Figure 1: A document from RAMS (Ebner et al., 2020) dataset.",
            "• Extensive experiments show that SCPRG outperforms previous start-of-the-art models, with 1.13 F1 and 2.64 F1 improvements on public RAMS and WikiEvents (Li et al., 2021) datasets.",
            "Table 2 shows the experimental results on both dev and test set in RAMS dataset.",
            "63M 48.33 54.04 47.52 55.61 -ASE 372.90M 49.80 56.31 51.73 58.48\nTable 4: Ablation Study on RAMS for SCPRG.\nlarity between latent role representations from two events in RAMS dataset in Fig 6.",
            "We train SCPRG for 50 epochs for RAMS dataset and 100 epochs for WikiEvents dataset."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "['background']",
            "--",
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            ", 2021), BERTQA (Du and Cardie, 2020b), BART-Gen (Li et al.",
            "Baselines We compare different categories of document-level EAE models which mainly consist of tagging-based methods such as BERT-CRF (Shi and Lin, 2019), BERT-CRFTCD (Ebner et al., 2020), span-based methods like Two-Step (Zhang et al., 2020b), Two-StepTCD (Ebner et al., 2020), TSAR (Xu et al., 2022), and other generation-based methods such as FEAE (Wei et al., 2021), BERTQA (Du and Cardie, 2020b), BART-Gen (Li et al., 2021), EA2E (Zeng et al., 2022)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "(Wang et al., 2021; Du and Cardie, 2020a) utilize the sequence labeling model BiLSTM-CRF (Zhang et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220047190,
          "isinfluential": false,
          "contexts": [
            "Compared with previous works (Liu et al., 2018; Wadden et al., 2019; Tong et al., 2020) focusing on sentence-level EAE, more and more recent works tend to explore documentlevel EAE (Wang et al.",
            "Some methods using transformer-based pre-trained model (Wadden et al., 2019; Wang et al., 2019; Tong et al., 2020; Lu et al., 2021; Liu et al., 2022b) also achieve remarkable performance."
          ],
          "intents": [
            "['result']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Detection via Open-domain Trigger Knowledge",
            "abstract": "Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152439499",
                "name": "Meihan Tong"
              },
              {
                "authorId": "143876183",
                "name": "Bin Xu"
              },
              {
                "authorId": "2118512998",
                "name": "Shuai Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "2109935759",
                "name": "Jun Xie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220524732,
          "isinfluential": false,
          "contexts": [
            "and dialogue systems (Zhang et al., 2020a) for presenting unstructured text containing event information in structured form."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Diagnostic Prediction with Sequence-of-setsRepresentation Learning for Clinical Events",
            "abstract": "Electronic health records (EHRs) contain both ordered and unordered chronologies of clinical events that occur during a patient encounter. However, during data preprocessing steps, many predictive models impose a predefined order on unordered clinical events sets (e.g., alphabetical, natural order from the chart, etc.), which is potentially incompatible with the temporal nature of the sequence and predictive task. To address this issue, we proposeDPSS, which seeks to capture each patient's clinical event records as sequences of event sets. Foreach clinical event set, we assume that the predictive model should be invariant to the order of concurrent events and thus employ a novel permutation sampling mechanism. This paper evaluates the use of this permuted sampling method given different data-driven models for predicting a heart failure (HF) diagnosis in sub-sequent patient visits. Experimental results using the MIMIC-III dataset show that the permutation sampling mechanism offers improved discriminative power based on the area under the receiver operating curve (AUROC) and precision-recall curve (pr-AUC) metrics as HF diagnosis prediction becomes more robust to different data ordering schemes.",
            "year": 2020,
            "venue": "medRxiv",
            "authors": [
              {
                "authorId": "2146331536",
                "name": "Tianran Zhang"
              },
              {
                "authorId": "1998918",
                "name": "Muhao Chen"
              },
              {
                "authorId": "144217742",
                "name": "Alex A. T. Bui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            ", 2021) formulate the problem as conditional generation and (Du et al., 2021) regards the problem as a sequence-to-sequence task."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": false,
          "contexts": [
            ", 2022) try to construct graphs based on heuristic rules (Xu et al., 2021; Liu et al., 2022a) or syntactic structures (Xu et al.",
            "Base on their architecture, (Xu et al., 2021) construct a heterogeneous graph and a tracker module to capture the interdependency among events."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235254286,
          "isinfluential": false,
          "contexts": [
            "(Wang et al., 2021; Du and Cardie, 2020a) utilize the sequence labeling model BiLSTM-CRF (Zhang et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "CLEVE: Contrastive Pre-training for Event Extraction",
            "abstract": "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2149202150",
                "name": "Yankai Lin"
              },
              {
                "authorId": "2055765060",
                "name": "Lei Hou"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460308,
          "isinfluential": false,
          "contexts": [
            ", 2022), and other generation-based methods such as FEAE (Wei et al., 2021), BERTQA (Du and Cardie, 2020b), BART-Gen (Li et al.",
            "(Wei et al., 2021) reformulate the task as reading a comprehension task.",
            "Baselines We compare different categories of document-level EAE models which mainly consist of tagging-based methods such as BERT-CRF (Shi and Lin, 2019), BERT-CRFTCD (Ebner et al., 2020), span-based methods like Two-Step (Zhang et al., 2020b), Two-StepTCD (Ebner et al., 2020), TSAR (Xu et al., 2022), and other generation-based methods such as FEAE (Wei et al., 2021), BERTQA (Du and Cardie, 2020b), BART-Gen (Li et al., 2021), EA2E (Zeng et al., 2022)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "31407139",
                "name": "Kaiwen Wei"
              },
              {
                "authorId": "2946890",
                "name": "Xian Sun"
              },
              {
                "authorId": "151473773",
                "name": "Zequn Zhang"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "2116390646",
                "name": "Zhi Guo"
              },
              {
                "authorId": "2152163772",
                "name": "Li Jin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": true,
          "contexts": [
            ", 2022a) or syntactic structures (Xu et al., 2022) and model logical reasoning with Graph Neural Networks (Kipf and Welling, 2016; Zeng et al.",
            "Moreover, (Xu et al., 2022) propose a twostream encoder with AMR-guided graph to solve long-distance dependency problem.",
            "Following (Xu et al., 2022), we report the Span F1 and Head F1 on dev and test sets for RAMS dataset.",
            "However, many previous works (Li et al., 2021; Xu et al., 2022) only utilize pre-trained transformerar X iv :2 31 0.",
            "Span-Trigger-based Contextual Pooling For a candidate span ranging from wi to wj , most previous span-based methods(Zhang et al., 2020b; Xu et al., 2022) represent it through the average pooling of the hidden state of tokens within this span:",
            ", 2020), TSAR (Xu et al., 2022), and other generation-based methods such as FEAE (Wei et al.",
            ", 2020) focusing on sentence-level EAE, more and more recent works tend to explore documentlevel EAE (Wang et al., 2022b; Yang et al., 2021; Xu et al., 2022), which needs to solve long-distance dependency (Ebner et al.",
            "Finally, the boundary loss is defined to detect the start and end position following (Xu et al., 2022): Lb = − ∑|D| i=1[y s i logP s i + (1− ys i ) log(1− P s i ) +ye i logP e i + (1− ye i ) log(1− P e i )], (7) where ys i and y e i denote golden labels and P s i = sigmoid(W4h start i ) and P e i = sigmoid(W5h end i ) are the probabilities of the word wi predicted to be the first or last word of a golden argument span."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "--",
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248721950,
          "isinfluential": false,
          "contexts": [
            "Some methods using transformer-based pre-trained model (Wadden et al., 2019; Wang et al., 2019; Tong et al., 2020; Lu et al., 2021; Liu et al., 2022b) also achieve remarkable performance."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dynamic Prefix-Tuning for Generative Template-based Event Extraction",
            "abstract": "We consider event extraction in a generative manner with template-based conditional generation.Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information.In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context.Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              },
              {
                "authorId": "2067725506",
                "name": "Ge Shi"
              },
              {
                "authorId": "2217713470",
                "name": "Bo Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252626402,
          "isinfluential": false,
          "contexts": [
            ", 2022) try to construct graphs based on heuristic rules (Xu et al., 2021; Liu et al., 2022a) or syntactic structures (Xu et al."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Structure Enhanced Transformer Encoder",
            "abstract": "Document-level relation extraction aims at discovering relational facts among entity pairs in a document, which has attracted more and more attention in recent years. Most existing methods are mainly summarized as graph-based and transformer-based methods. However, previous transformer-based methods neglect structural information between entities, while graph-based methods are unable to extract structural information effectively on account that they isolate the en-coding stage and structure reasoning stage. In this paper, we propose an effective structure enhanced transformer encoder model (SETE), integrating entity structural information into the transformer encoder. We first define a mention-level graph based on mention dependencies and convert it to a token-level graph. Then we design a dual self-attention mechanism, which enriches the structural and contextual information between entities to increase the vanilla transformer encoder inferential capability. Experiments on three public datasets show that the proposed SETE outperforms previous state-of-the-art methods and further analyses illustrate the interpretability of our model.",
            "year": 2022,
            "venue": "IEEE International Joint Conference on Neural Network",
            "authors": [
              {
                "authorId": "2143606898",
                "name": "Wanlong Liu"
              },
              {
                "authorId": "2116635928",
                "name": "Li Zhou"
              },
              {
                "authorId": "2054124872",
                "name": "DingYi Zeng"
              },
              {
                "authorId": "144956396",
                "name": "Hong Qu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252819226,
          "isinfluential": false,
          "contexts": [
            "Besides, (Ren et al., 2022) integrate argument roles into document encoding to aware tokens of multiple role information for nested arguments problem."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLIO: Role-interactive Multi-event Head Attention Network for Document-level Event Extraction",
            "abstract": "",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "47184362",
                "name": "Yanan Cao"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2075394870",
                "name": "Ping Guo"
              },
              {
                "authorId": "1390641501",
                "name": "Zheng Lin"
              },
              {
                "authorId": "2185915076",
                "name": "Wei Ma"
              },
              {
                "authorId": "2153629743",
                "name": "Yi Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254043718,
          "isinfluential": false,
          "contexts": [
            ", 2022c; Chen and Kou, 2023) and Image Semantic Segmentation (Zhang et al., 2022), its impact on Event Argument Extraction in natural language processing has been relatively limited.",
            "Although deep learning has achieved significant success in many areas of computer vision (Li et al., 2022; Wang et al., 2023, 2022a; Pan et al., 2023; Wang and Chen, 2023) like 3D Scene Graph Generation (Liu et al., 2022c; Chen and Kou, 2023) and Image Semantic Segmentation (Zhang et al., 2022), its impact on Event Argument Extraction in natural language processing has been relatively limited."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation",
            "abstract": "Unsupervised image segmentation aims to match low-level visual features with semantic-level representations without outer supervision. In this paper, we address the critical properties from the view of feature alignments and feature uniformity for UISS models. We also make a comparison between UISS and image-wise representation learning. Based on the analysis, we argue that the existing MI-based methods in UISS suffer from representation collapse. By this, we proposed a robust network called Semantic Attention Network(SAN), in which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise and semantic features dynamically. Experimental results on multiple semantic segmentation benchmarks show that our unsupervised segmentation framework specializes in catching semantic representations, which outperforms all the unpretrained and even several pretrained methods.",
            "year": 2022,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2188765758",
                "name": "Daoan Zhang"
              },
              {
                "authorId": "2109422175",
                "name": "Chenming Li"
              },
              {
                "authorId": "2188740062",
                "name": "Haoquan Li"
              },
              {
                "authorId": "67048823",
                "name": "Wen-Fong Huang"
              },
              {
                "authorId": "2192676292",
                "name": "Lingyun Huang"
              },
              {
                "authorId": "2155240973",
                "name": "Jianguo Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259716313,
          "isinfluential": false,
          "contexts": [
            "Although deep learning has achieved significant success in many areas of computer vision (Li et al., 2022; Wang et al., 2023, 2022a; Pan et al., 2023; Wang and Chen, 2023) like 3D Scene Graph Generation (Liu et al."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Ising-Traffic: Using Ising Machine Learning to Predict Traffic Congestion under Uncertainty",
            "abstract": "This paper addresses the challenges in accurate and real-time traffic congestion prediction under uncertainty by proposing Ising-Traffic, a dual-model Ising-based traffic prediction framework that delivers higher accuracy and lower latency than SOTA solutions. While traditional solutions face the dilemma from the trade-off between algorithm complexity and computational efficiency, our Ising-based method breaks away from the trade-off leveraging the Ising model's strong expressivity and the Ising machine's strong computation power. In particular, Ising-Traffic formulates traffic prediction under uncertainty into two Ising models: Reconstruct-Ising and Predict-Ising. Reconstruct-Ising is mapped onto modern Ising machines and handles uncertainty in traffic accurately with negligible latency and energy consumption, while Predict-Ising is mapped onto traditional processors and predicts future congestion precisely with only at most 1.8% computational demands of existing solutions. Our evaluation shows Ising-Traffic delivers on average 98X speedups and 5% accuracy improvement over SOTA.",
            "year": 2023,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2069545159",
                "name": "Zhenyu Pan"
              },
              {
                "authorId": "2109671557",
                "name": "Anshujit Sharma"
              },
              {
                "authorId": "102764428",
                "name": "Jerry Yao-Chieh Hu"
              },
              {
                "authorId": "2191507302",
                "name": "Zhu Liu"
              },
              {
                "authorId": "2312272530",
                "name": "Ang Li"
              },
              {
                "authorId": "2222685309",
                "name": "Han Liu"
              },
              {
                "authorId": "2205181891",
                "name": "Michael Huang"
              },
              {
                "authorId": "2222671400",
                "name": "Tony Geng"
              }
            ]
          }
        }
      ]
    },
    "259370721": {
      "citing_paper_info": {
        "title": "Document-Level Event Argument Extraction With a Chain Reasoning Paradigm",
        "abstract": "Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies.Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic rules for reasoning.This paradigm naturally captures long-range interdependence due to the chains’ compositional nature, which also improves interpretability by explicitly modeling the reasoning process.We introduce T-norm fuzzy logic for optimization, which permits end-to-end learning and shows promise for integrating the expressiveness of logical reasoning with the generalization of neural networks.In experiments, we show that our approach outperforms previous methods by a significant margin on two standard benchmarks (over 6 points in F1).Moreover, it is data-efficient in low-resource scenarios and robust enough to defend against adversarial attacks.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2150168584",
            "name": "Jian Liu"
          },
          {
            "authorId": "2113437300",
            "name": "Chen Liang"
          },
          {
            "authorId": "2310092",
            "name": "Jinan Xu"
          },
          {
            "authorId": "103896164",
            "name": "Haoyan Liu"
          },
          {
            "authorId": "2168651021",
            "name": "Zhe Zhao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 16,
        "unique_cited_count": 15,
        "influential_count": 6,
        "detailed_records_count": 16
      },
      "cited_papers": [
        "252391326",
        "248496614",
        "1164487",
        "2213149",
        "247084444",
        "226262283",
        "218630327",
        "7228830",
        "216562330",
        "247417818",
        "248780117",
        "61701554",
        "236460308",
        "6628106",
        "233219850"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1164487,
          "isinfluential": false,
          "contexts": [
            "First-order logic (FOL) rules can encode declarative knowledge and play a crucial role in symbolic reasoning (Cresswell and Hughes, 1996).",
            "Our approach formalizes the reasoning chain as first-order logic (FOL) rules (Cresswell and Hughes, 1996)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A New Introduction to Modal Logic",
            "abstract": "This long-awaited book replaces Hughes and Cresswell's two classic studies of modal logic: An Introduction to Modal Logic and A Companion to Modal Logic. A New Introduction to Modal Logic is an entirely new work, completely re-written by the authors. They have incorporated all the new developments that have taken place since 1968 in both modal propositional logic and modal predicate logic, without sacrificing tha clarity of exposition and approachability that were essential features of their earlier works. The book takes readers from the most basic systems of modal propositional logic right up to systems of modal predicate with identity. It covers both technical developments such as completeness and incompleteness, and finite and infinite models, and their philosophical applications, especially in the area of modal predicate logic.",
            "year": 1998,
            "venue": "",
            "authors": [
              {
                "authorId": "20926239",
                "name": "P. Crivelli"
              },
              {
                "authorId": "145756963",
                "name": "T. Williamson"
              },
              {
                "authorId": "38231749",
                "name": "G. Hughes"
              },
              {
                "authorId": "2227511",
                "name": "M. Cresswell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2213149,
          "isinfluential": false,
          "contexts": [
            "Earlier efforts on this problem explore the MUC-4 benchmark (Chinchor, 1991; Huang and Riloff, 2012), also known as “template filling” because the entire document is about one event."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Bootstrapped Training of Event Extraction Classifiers",
            "abstract": "",
            "year": 2012,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with a batch size of 10 from [5, 10, 15, 20] and a learning rate of 1e-4 from [1e-3, 1e-4, 1e-5]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7228830,
          "isinfluential": false,
          "contexts": [
            "We validate this assumption by analyzing its performance in low-resource scenarios and for defending against adversarial attacks (Jia and Liang, 2017)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
            "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
            "year": 2017,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "3422908",
                "name": "Robin Jia"
              },
              {
                "authorId": "145419642",
                "name": "Percy Liang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 61701554,
          "isinfluential": false,
          "contexts": [
            "Inspired by work that augments neural networks with FOLs (Li and Srikumar, 2019; Ahmed et al., 2022), we present T-Norm fuzzy logic for relaxation (Hajek, 1998), which leads to an end-to-end training regime."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Metamathematics of Fuzzy Logic",
            "abstract": "",
            "year": 1998,
            "venue": "Trends in Logic",
            "authors": [
              {
                "authorId": "50374961",
                "name": "P. Hájek"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": true,
          "contexts": [
            "We employ Span-F1 on RAMS and Head-F1 and Coref-F1 on WikiEvents as evaluation metrics, where Head-F1 only examines the head word in an argument and Coref-F1 also takes co-reference linkages between arguments into account (Du and Cardie, 2020a; Li et al., 2021; Wei et al., 2021; Ma et al., 2022).",
            "BIOlabel (Shi and Lin, 2019) 7.5 QAEE (Du and Cardie, 2020b) 28.0 DocMRC (Liu et al., 2021) 55.0 FEAE (Wei et al., 2021) 56.3 BART-Gen (Li et al., 2021) 14.0 PAIE (Ma et al., 2022) 11.1 Our Method 33.5 where the concatenation operator of the vector h T ∈ R d and the matrix H B ∈ R M × d is…",
            "…explosion was a sucicde bomber who detonated his belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al.,…",
            "2) Global encoding methods, such as QAEE (Du and Cardie, 2020b) and DocMRC (Liu et al., 2021), which form the task as a document-based question-answering problem, and MemNet (Du et al., 2022), which uses a memory to store global event information.",
            "BIOlabel (Shi and Lin, 2019) 7.5 QAEE (Du and Cardie, 2020b) 28.0 DocMRC (Liu et al., 2021) 55.0 FEAE (Wei et al., 2021) 56.3 BART-Gen (Li et al., 2021) 14.0 PAIE (Ma et al., 2022) 11.1 Our Method 33.5 where the concatenation operator of the vector h T ∈ R d and the matrix H B ∈ R M × d is performed by ﬁrst broadcasting the vector to the same dimension as the matrix, followed by an element-wise concatenation operation.",
            "…hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al., 2020; Xu et al., 2022; Liu et al., 2022a).",
            "From the results, we can see that our model maintains a comparable time to earlier methods such as QAEE and is faster than many models such as FEAE and DocMRC, where FEAE has two base models for knowledge distillation and DocMRC uses external data to pretrain the model."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al.",
            "We employ Span-F1 on RAMS and Head-F1 and Coref-F1 on WikiEvents as evaluation metrics, where Head-F1 only examines the head word in an argument and Coref-F1 also takes co-reference linkages between arguments into account (Du and Cardie, 2020a; Li et al., 2021; Wei et al., 2021; Ma et al., 2022)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "…hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al., 2020; Xu et al., 2022; Liu et al., 2022a)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": true,
          "contexts": [
            "We employ Span-F1 on RAMS and Head-F1 and Coref-F1 on WikiEvents as evaluation metrics, where Head-F1 only examines the head word in an argument and Coref-F1 also takes co-reference linkages between arguments into account (Du and Cardie, 2020a; Li et al., 2021; Wei et al., 2021; Ma et al., 2022).",
            "In this task, the major challenge is to model long-range dependencies between event triggers and arguments, as an event expression can span multiple sentences (Ebner et al., 2020; Liu et al., 2021; Li et al., 2021).",
            "When ground-truth entities are available (such as in WikiEvents (Li et al., 2021)), we consider the candidate set to be the ground-truth entity set; otherwise, we use external toolkits2 to recognize entities.",
            "(Du et al., 2022) 46.2 47.0 46.6 BART-Gen (Li et al., 2021) 42.1 47.3 44.5 PAIE (Ma et al., 2022) - - 49.5 FEAE (Wei et al., 2021) 53.1 42.7 47.4 TSAR (Xu et al",
            "When ground-truth entities are available (such as in WikiEvents (Li et al., 2021)), we consider the candidate set to be the ground-truth entity set; otherwise, we use external toolkits 2 to recognize entities.",
            "BIOlabel (Shi and Lin, 2019) 7.5 QAEE (Du and Cardie, 2020b) 28.0 DocMRC (Liu et al., 2021) 55.0 FEAE (Wei et al., 2021) 56.3 BART-Gen (Li et al., 2021) 14.0 PAIE (Ma et al., 2022) 11.1 Our Method 33.5 where the concatenation operator of the vector h T ∈ R d and the matrix H B ∈ R M × d is…",
            "In this task, the major challenge is to model longrange dependencies between event triggers and arguments, as an event expression can span multiple sentences (Ebner et al., 2020; Liu et al., 2021; Li et al., 2021).",
            "beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al.",
            "We conduct experiments using two document-level EAE benchmarks: RAMS (Ebner et al., 2020) and WikiEvents (Li et al., 2021).",
            ", participants of an event) is a crucial task for document-level event understanding (Ebner et al., 2020; Li et al., 2021).",
            "We verify the effectiveness of our method on two benchmarks (Ebner et al., 2020; Li et al., 2021).",
            "For capturing the document context effectively, prior studies have explored hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al.",
            "…his belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et…",
            "For capturing the document context effectively, prior studies have explored hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du…",
            "Identifying event arguments (i.e., participants of an event) is a crucial task for document-level event understanding (Ebner et al., 2020; Li et al., 2021).",
            "3) Generative methods, such as BART-Gen (Li et al., 2021), which proposes a sequence-to-sequence paradigm for argument extraction, and PAIE (Ma et al.",
            "3) Generative methods, such as BART-Gen (Li et al., 2021), which proposes a sequence-to-sequence paradigm for argument extraction, and PAIE (Ma et al., 2022), which employs a set generation formulation."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "--",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460308,
          "isinfluential": true,
          "contexts": [
            "We employ Span-F1 on RAMS and Head-F1 and Coref-F1 on WikiEvents as evaluation metrics, where Head-F1 only examines the head word in an argument and Coref-F1 also takes co-reference linkages between arguments into account (Du and Cardie, 2020a; Li et al., 2021; Wei et al., 2021; Ma et al., 2022).",
            "(Du et al., 2022) 46.2 47.0 46.6 BART-Gen (Li et al., 2021) 42.1 47.3 44.5 PAIE (Ma et al., 2022) - - 49.5 FEAE (Wei et al., 2021) 53.1 42.7 47.4 TSAR (Xu et al",
            ", 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et al., 2022b).",
            "BIOlabel (Shi and Lin, 2019) 7.5 QAEE (Du and Cardie, 2020b) 28.0 DocMRC (Liu et al., 2021) 55.0 FEAE (Wei et al., 2021) 56.3 BART-Gen (Li et al., 2021) 14.0 PAIE (Ma et al., 2022) 11.1 Our Method 33.5 where the concatenation operator of the vector h T ∈ R d and the matrix H B ∈ R M × d is…",
            "4) Methods using extra supervisions, for example, FEAE (Wei et al., 2021), which adopts frame-related knowledge, and TSAR (Xu et al.",
            "4) Methods using extra supervisions, for example, FEAE (Wei et al., 2021), which adopts frame-related knowledge, and TSAR (Xu et al., 2022), which utilizes abstract meaning representation (AMR) resources.",
            ", 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al.",
            "…hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al., 2020; Xu et al., 2022; Liu et al.,…",
            "…belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et al., 2022b)."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "31407139",
                "name": "Kaiwen Wei"
              },
              {
                "authorId": "2946890",
                "name": "Xian Sun"
              },
              {
                "authorId": "151473773",
                "name": "Zequn Zhang"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "2116390646",
                "name": "Zhi Guo"
              },
              {
                "authorId": "2152163772",
                "name": "Li Jin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": true,
          "contexts": [
            "We employ Span-F1 on RAMS and Head-F1 and Coref-F1 on WikiEvents as evaluation metrics, where Head-F1 only examines the head word in an argument and Coref-F1 also takes co-reference linkages between arguments into account (Du and Cardie, 2020a; Li et al., 2021; Wei et al., 2021; Ma et al., 2022).",
            "(Du et al., 2022) 46.2 47.0 46.6 BART-Gen (Li et al., 2021) 42.1 47.3 44.5 PAIE (Ma et al., 2022) - - 49.5 FEAE (Wei et al., 2021) 53.1 42.7 47.4 TSAR (Xu et al",
            ", 2021), which proposes a sequence-to-sequence paradigm for argument extraction, and PAIE (Ma et al., 2022), which employs a set generation formulation.",
            "…Lin, 2019) 7.5 QAEE (Du and Cardie, 2020b) 28.0 DocMRC (Liu et al., 2021) 55.0 FEAE (Wei et al., 2021) 56.3 BART-Gen (Li et al., 2021) 14.0 PAIE (Ma et al., 2022) 11.1 Our Method 33.5 where the concatenation operator of the vector h T ∈ R d and the matrix H B ∈ R M × d is performed by ﬁrst…",
            "beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al.",
            "…context effectively, prior studies have explored hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b;…",
            "BIOlabel (Shi and Lin, 2019) 7.5 QAEE (Du and Cardie, 2020b) 28.0 DocMRC (Liu et al., 2021) 55.0 FEAE (Wei et al., 2021) 56.3 BART-Gen (Li et al., 2021) 14.0 PAIE (Ma et al., 2022) 11.1 Our Method 33.5 where the concatenation operator of the vector h T ∈ R d and the matrix H B ∈ R M × d is performed by ﬁrst broadcasting the vector to the same dimension as the matrix, followed by an element-wise concatenation operation.",
            "For capturing the document context effectively, prior studies have explored hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al.",
            "…belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et al., 2022b).",
            "3) Generative methods, such as BART-Gen (Li et al., 2021), which proposes a sequence-to-sequence paradigm for argument extraction, and PAIE (Ma et al., 2022), which employs a set generation formulation."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "--",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247417818,
          "isinfluential": false,
          "contexts": [
            "Currently, it still remains an open problem for effectively capturing such dependencies (Liu et al., 2021, 2022c).",
            "…belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et al., 2022b).",
            "…hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al., 2020; Xu et al., 2022; Liu et al., 2022a)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level event argument linking as machine reading comprehension",
            "abstract": "",
            "year": 2022,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": true,
          "contexts": [
            "(Du et al., 2022) 46.2 47.0 46.6 BART-Gen (Li et al., 2021) 42.1 47.3 44.5 PAIE (Ma et al., 2022) - - 49.5 FEAE (Wei et al., 2021) 53.1 42.7 47.4 TSAR (Xu et al",
            "4) Methods using extra supervisions, for example, FEAE (Wei et al., 2021), which adopts frame-related knowledge, and TSAR (Xu et al., 2022), which utilizes abstract meaning representation (AMR) resources.",
            ", 2021), which adopts frame-related knowledge, and TSAR (Xu et al., 2022), which utilizes abstract meaning representation (AMR) resources.",
            "…hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al., 2020; Xu et al., 2022; Liu et al., 2022a)."
          ],
          "intents": [
            "--",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248780117,
          "isinfluential": true,
          "contexts": [
            ", 2021), which form the task as a document-based question-answering problem, and MemNet (Du et al., 2022), which uses a memory to store global event information.",
            "(Du et al., 2022) 46.2 47.0 46.6 BART-Gen (Li et al., 2021) 42.1 47.3 44.5 PAIE (Ma et al., 2022) - - 49.5 FEAE (Wei et al., 2021) 53.1 42.7 47.4 TSAR (Xu et al",
            "beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al.",
            "2) Global encoding methods, such as QAEE (Du and Cardie, 2020b) and DocMRC (Liu et al., 2021), which form the task as a document-based question-answering problem, and MemNet (Du et al., 2022), which uses a memory to store global event information.",
            "For capturing the document context effectively, prior studies have explored hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al.",
            "…capturing the document context effectively, prior studies have explored hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and…",
            "…belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et al., 2022b)."
          ],
          "intents": [
            "['background']",
            "--",
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Dynamic Global Memory for Document-level Argument Extraction",
            "abstract": "Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252391326,
          "isinfluential": false,
          "contexts": [
            "Currently, it still remains an open problem for effectively capturing such dependencies (Liu et al., 2021, 2022c).",
            "…belt as people rushed … beyond-sentence clues by incorporating hierarchical encoding mechanisms (Du and Cardie, 2020a), generative paradigms (Li et al., 2021; Ma et al., 2022; Du et al., 2022), and document-level inductive bias (Wei et al., 2021; Pouran Ben Veyseh et al., 2022; Liu et al., 2022b).",
            "…hierarchical encoding mechanisms, generative perspectives (Li et al., 2021; Du et al., 2022; Ma et al., 2022), document-level inductive biases (Wei et al., 2021; Pouran Ben Veyseh et al., 2022), and external resources (Du and Cardie, 2020b; Liu et al., 2020; Xu et al., 2022; Liu et al., 2022a)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level event argument extraction with self-augmentation and a cross-domain joint training mechanism",
            "abstract": "",
            "year": 2022,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "2113437300",
                "name": "Chen Liang"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In this case, we do not utilize the broad definition of entity because an event argument is defined to be a noun entity (Walker and Consortium, 2005; Ahn, 2006)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "265149752": {
      "citing_paper_info": {
        "title": "LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction",
        "abstract": "In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in diverse tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2111825433",
            "name": "Hanzhang Zhou"
          },
          {
            "authorId": "2266439699",
            "name": "Junlang Qian"
          },
          {
            "authorId": "2112599636",
            "name": "Zijian Feng"
          },
          {
            "authorId": "2266420870",
            "name": "Hui Lu"
          },
          {
            "authorId": "2166589821",
            "name": "Zixiao Zhu"
          },
          {
            "authorId": "2128504277",
            "name": "Kezhi Mao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 17,
        "influential_count": 3,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "990233",
        "233219850",
        "3446415",
        "218630327",
        "248986239",
        "254043800",
        "259370571",
        "203701085",
        "247084444",
        "218971783",
        "230799347",
        "144823696",
        "16482483",
        "250390478",
        "11392154",
        "250264890",
        "246426909"
      ],
      "citation_details": [
        {
          "citedcorpusid": 990233,
          "isinfluential": true,
          "contexts": [
            "The performance comparison of prompts constructed by the two different strategy on the Strate-gyQA (Geva et al., 2021) and SST-2 (Socher et al., 2013) datasets is illustrated in Figure 4.",
            "In addressing RQ3 , we have extended our HD-LoA prompting method to sentiment analysis (SA) and natural language inference (NLI) tasks, utilizing the SST-2 (Socher et al., 2013) and SNLI (Bow-man et al., 2015) datasets for evaluation.",
            "Additionally, we utilize the SST-2 (Socher et al., 2013) and SNLI (Bowman et al., 2015) datasets to assess the effectiveness of our HD-LoA prompting strategy on other non-reasoning tasks: sentiment analysis and natural language inference.",
            "63% on SST-2 and SNLI datasets, respectively."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
            "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
            "year": 2013,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "24590005",
                "name": "Alex Perelygin"
              },
              {
                "authorId": "2110402830",
                "name": "Jean Wu"
              },
              {
                "authorId": "1964541",
                "name": "Jason Chuang"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              },
              {
                "authorId": "34699434",
                "name": "A. Ng"
              },
              {
                "authorId": "144922861",
                "name": "Christopher Potts"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3446415,
          "isinfluential": false,
          "contexts": [
            "Cognitive science studies reveals that humans perform analogical reasoning through a sequence of retrieval , mapping , and evaluation (Gentner and Forbus, 2011; Gentner and Markman, 1997)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Computational models of analogy.",
            "abstract": "Analogical mapping is a core process in human cognition. A number of valuable computational models of analogy have been created, capturing aspects of how people compare representations, retrieve potential analogs from memory, and learn from the results. In the past 25 years, this area has progressed rapidly, fueled by strong collaboration between psychologists and Artificial Intelligence (AI) scientists, with contributions from linguists and philosophers as well. There is now considerable consensus regarding the constraints governing the mapping process. However, computational models still differ in their focus, with some aimed at capturing the range of analogical phenomena at the cognitive level and others aimed at modeling how analogical processes might be implemented in neural systems. Some recent work has focused on modeling interactions between analogy and other processes, and on modeling analogy as a part of larger cognitive systems. WIREs Cogni Sci 2011 2 266-276 DOI: 10.1002/wcs.105 For further resources related to this article, please visit the WIREs website.",
            "year": 2011,
            "venue": "Wiley Interdisciplinary Reviews: Cognitive Science",
            "authors": [
              {
                "authorId": "1704065",
                "name": "D. Gentner"
              },
              {
                "authorId": "1713121",
                "name": "Kenneth D. Forbus"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11392154,
          "isinfluential": false,
          "contexts": [
            "Similarly, in supervised machine learning (ML) systems, models also learn task-specific patterns through training (Shachaf et al., 2021; Najafabadi et al., 2015)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Deep learning applications and challenges in big data analytics",
            "abstract": "Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.",
            "year": 2015,
            "venue": "Journal of Big Data",
            "authors": [
              {
                "authorId": "1979817",
                "name": "M. M. Najafabadi"
              },
              {
                "authorId": "2142983",
                "name": "Flavio Villanustre"
              },
              {
                "authorId": "1725285",
                "name": "T. Khoshgoftaar"
              },
              {
                "authorId": "1809915",
                "name": "Naeem Seliya"
              },
              {
                "authorId": "145539319",
                "name": "Randall Wald"
              },
              {
                "authorId": "50076239",
                "name": "Edin A. Muharemagic"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16482483,
          "isinfluential": false,
          "contexts": [
            "Cognitive science studies reveals that humans perform analogical reasoning through a sequence of retrieval , mapping , and evaluation (Gentner and Forbus, 2011; Gentner and Markman, 1997)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Structure mapping in analogy and similarity.",
            "abstract": "ions Keil, 1989 ; Rips, 1989) . For example, bats have the perceptual and behavioral characteristics of birds (they are similar to birds in this sense), but they are classified as mammals, because of important (though nonobvious) properties, such as giving birth to live young. On the basis of examples like this, similarity's role in categorization has been challenged ; it has been argued that category membership judgments are theory based rather than similarity based (Keil, 1989 ; Murphy & Medin, 1985) . The process of alignment and mapping points the way to a reconciliation of similarity-based and theorybased accounts (see also Goldstone, 1994a) . If we focus purely on perceptual similarity among objects, we are led to conclude that bats should be categorized with birds . On this view, theory-based knowledge (such as why bats are mammals) must intervene from elsewhere to overrule this assignment . However, if the similarity computation is assumed to be that ofstructural alignment, then the similarity between two instances will be based riot only on object-level commonalities but also on common relations such as common causal relations and common origins . Assuming that our representations include information about theory-based relations, such as that bats bear live young, as well as information about features, then the schism between similarity-based and theory-based categorization may be more apparent than real . Developmentally, if we assume that theoretical knowledge is acquired gradually, this view would account for the characteristic-to-defining shift (Keil & Batterman, 1984) in children's interpretations of word meaning from local object features (e.g ., a taxi is bright yellow and has a checkered sign) to deeper relational commonalities (e.g ., a taxi is a vehicle that may be hired to transport people) . Choice and decision . Structural alignment also sheds light on the processes underlying choice behavior. Medin, Goldstone, and Markman (1995) reviewed paral lels between phenomena in decision processing and phenomena in comparison processing that suggest an important role for structural alignment in decision making . Structural alignment influences which features to pay attention to in choice options . Research suggests that alignable differences are given more weight in choice situations than are nonalignable differences (Lindemann & Markman, 1996 ; Markman & Medin, 1995 ; Slovic & MacPhillamy, 1974) . For example, Markman and Medin (1995) asked participants to choose between video games and to justify their choices. Their justifications were more likely to contain alignable differences than nonalignable differences . As another example, Kahneman and Z'versky (1984) described to participants a hypothetical store in which a jacket could be bought for $125 and a calculator for $15 . They offered participants the opportunity to go to another store and save $5 on the total purchase. Participants who were offered ajacket for $125 and a calculator for $10 were more willing to make the effort to go to another store than those offered a jacket for $120 and a calculator for $15 . Even though the monetary reward for going to the other store was the same for both groups, participants were influenced by the alignable difference .",
            "year": 1997,
            "venue": "",
            "authors": [
              {
                "authorId": "1704065",
                "name": "D. Gentner"
              },
              {
                "authorId": "2331233",
                "name": "A. Markman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 144823696,
          "isinfluential": false,
          "contexts": [
            "For example, students often solve new problems by mapping solutions from known problems (Ross, 1987)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "This is like that: The use of earlier problems and the separation of similarity effects.",
            "abstract": "Novices attempting to solve a problem often are reminded of an earlier problem that illustrated a principle. Two experiments examined how these earlier problems are used and how this use is related to these remindings. Subjects studied four probability principles with related word problems. Test problems varied in their similarity to the study problems on story lines, objects, and correspondence of objects (variable roles). Experiment 1 tested whether remindings cue the principle or serve as the sources of detailed analogies. When the appropriate formula was provided with each test, the similarity of story lines had no effect, but object correspondences had a large effect. These results support an analogical account in which mapping is affected by the similarity of objects between study and test problems. Experiment 2 began to separate the aspects of similarity affecting the access and use of earlier problems by showing that, with confusable principles, similar story lines increased the access, but did not affect the use. The access appears to be sensitive to the relative similarity of examples because with distinctive principles, similar story lines had little effect. Discussion focuses on the further specification of the processes of noticing and analogical use of earlier problems.",
            "year": 1987,
            "venue": "",
            "authors": [
              {
                "authorId": "36568337",
                "name": "B. Ross"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203701085,
          "isinfluential": false,
          "contexts": [
            "Document-level Event Argument Extraction (EAE) aims to transform unstructured event information from documents into structured formats encapsulating event arguments, facilitating their interpretation and application in various domains (Grishman, 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Twenty-five years of information extraction",
            "abstract": "Abstract Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.",
            "year": 2019,
            "venue": "Natural Language Engineering",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": true,
          "contexts": [
            "EEQA (Du and Cardie, 2020b) 19.54 PAIE (Ma et al., 2022) 29.86 TSAR (Xu et al., 2022) - 26.67 - - CRP (Liu et al., 2023a) 30.09 FewDocAE (Yang et al., RQ2 Can HD-LoA prompting effectively mitigate the dependency on extensive labeled data while enhancing accuracy for EAE task?",
            "Additionally, we compare our method with various supervised learning methods in EAE, such as Few-DocAE (Yang et al., 2023), CRP (Liu et al., 2023a), PAIE (Ma et al., 2022), TSAR (Xu et al., 2022), EEQA (Du and Cardie, 2020b), etc.",
            "The prevalent approach for this task relies on the collection of labeled data and the subsequent model training via supervised learning (Ren et al., 2023; Liu et al., 2023a; Pouran Ben Veyseh et al., 2022; Zhou and Mao, 2022; Du and Cardie, 2020a)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218971783,
          "isinfluential": false,
          "contexts": [
            "The experiments are carried out using three large language models: the publicly available GPT-3 (Brown et al., 2020) in its text-davinci-003 and gpt-3.",
            "In this context, in-context learning (ICL) (Brown et al., 2020; Liu et al., 2022; Zhou et al., 2022), an emergent ability of large language models (LLMs), offers a promising alternative to supervised learning."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31035595",
                "name": "Tom B. Brown"
              },
              {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
              },
              {
                "authorId": "39849748",
                "name": "Nick Ryder"
              },
              {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
              },
              {
                "authorId": "152724169",
                "name": "J. Kaplan"
              },
              {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
              },
              {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
              },
              {
                "authorId": "67311962",
                "name": "Pranav Shyam"
              },
              {
                "authorId": "144864359",
                "name": "Girish Sastry"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
              },
              {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
              },
              {
                "authorId": "103143311",
                "name": "T. Henighan"
              },
              {
                "authorId": "48422824",
                "name": "R. Child"
              },
              {
                "authorId": "1992922591",
                "name": "A. Ramesh"
              },
              {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2059411355",
                "name": "Clemens Winter"
              },
              {
                "authorId": "144239765",
                "name": "Christopher Hesse"
              },
              {
                "authorId": "2108828435",
                "name": "Mark Chen"
              },
              {
                "authorId": "2064673055",
                "name": "Eric Sigler"
              },
              {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
              },
              {
                "authorId": "145565184",
                "name": "Scott Gray"
              },
              {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
              },
              {
                "authorId": "2115193883",
                "name": "Jack Clark"
              },
              {
                "authorId": "133740015",
                "name": "Christopher Berner"
              },
              {
                "authorId": "52238703",
                "name": "Sam McCandlish"
              },
              {
                "authorId": "38909097",
                "name": "Alec Radford"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "2698777",
                "name": "Dario Amodei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 230799347,
          "isinfluential": false,
          "contexts": [
            "The performance comparison of prompts constructed by the two different strategy on the Strate-gyQA (Geva et al., 2021) and SST-2 (Socher et al., 2013) datasets is illustrated in Figure 4.",
            "We use 500 test samples from the StrategyQA (Geva et al., 2021) dataset and the prompt from Shum et al. (2023) for evaluation."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
            "abstract": "Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.",
            "year": 2021,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "22245981",
                "name": "Mor Geva"
              },
              {
                "authorId": "1783281",
                "name": "Daniel Khashabi"
              },
              {
                "authorId": "153401294",
                "name": "Elad Segal"
              },
              {
                "authorId": "2236429",
                "name": "Tushar Khot"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              },
              {
                "authorId": "1750652",
                "name": "Jonathan Berant"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "The WIKIEVENTS dataset (Li et al., 2021) is excluded from our study because it relies on preprocessed entity candidates for annotating event arguments the annotation, which diverges from the direct ar-gument identification of LLMs."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246426909,
          "isinfluential": false,
          "contexts": [
            "The ICL performance is highly sensitive to the design of in-context demonstrations, such as the selection of examples and the formatting of reasoning steps (Zhang et al., 2023, 2022; Fu et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Training language models to follow instructions with human feedback",
            "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "year": 2022,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "31793034",
                "name": "Long Ouyang"
              },
              {
                "authorId": "49387725",
                "name": "Jeff Wu"
              },
              {
                "authorId": "2115903168",
                "name": "Xu Jiang"
              },
              {
                "authorId": "2061137049",
                "name": "Diogo Almeida"
              },
              {
                "authorId": "2064084601",
                "name": "Carroll L. Wainwright"
              },
              {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
              },
              {
                "authorId": null,
                "name": "Chong Zhang"
              },
              {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
              },
              {
                "authorId": "2117680841",
                "name": "Katarina Slama"
              },
              {
                "authorId": "2064770039",
                "name": "Alex Ray"
              },
              {
                "authorId": "47971768",
                "name": "John Schulman"
              },
              {
                "authorId": "2052366271",
                "name": "Jacob Hilton"
              },
              {
                "authorId": "2151735262",
                "name": "Fraser Kelton"
              },
              {
                "authorId": "2142365973",
                "name": "Luke E. Miller"
              },
              {
                "authorId": "2151735251",
                "name": "Maddie Simens"
              },
              {
                "authorId": "119609682",
                "name": "Amanda Askell"
              },
              {
                "authorId": "2930640",
                "name": "Peter Welinder"
              },
              {
                "authorId": "145791315",
                "name": "P. Christiano"
              },
              {
                "authorId": "2990741",
                "name": "Jan Leike"
              },
              {
                "authorId": "49407415",
                "name": "Ryan J. Lowe"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": true,
          "contexts": [
            "EEQA (Du and Cardie, 2020b) 19.54 PAIE (Ma et al., 2022) 29.86 TSAR (Xu et al., 2022) - 26.67 - - CRP (Liu et al., 2023a) 30.09 FewDocAE (Yang et al., RQ2 Can HD-LoA prompting effectively mitigate the dependency on extensive labeled data while enhancing accuracy for EAE task?",
            "Additionally, we compare our method with various supervised learning methods in EAE, such as Few-DocAE (Yang et al., 2023), CRP (Liu et al., 2023a), PAIE (Ma et al., 2022), TSAR (Xu et al., 2022), EEQA (Du and Cardie, 2020b), etc.",
            "For evaluation, we follow the metrics in (Ma et al., 2022), namely the argument identification F1 score (Arg-I), and the argument classification F1 score (Arg-C)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248986239,
          "isinfluential": false,
          "contexts": [
            "In this context, in-context learning (ICL) (Brown et al., 2020; Liu et al., 2022; Zhou et al., 2022), an emergent ability of large language models (LLMs), offers a promising alternative to supervised learning.",
            "The example selection process of ICL is often an indiscriminate, manual process (Liu et al., 2023b; Wei et al., 2022; Zhou et al., 2022)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "65855107",
                "name": "Denny Zhou"
              },
              {
                "authorId": "1821614764",
                "name": "Nathanael Scharli"
              },
              {
                "authorId": "2153400663",
                "name": "Le Hou"
              },
              {
                "authorId": "119640649",
                "name": "Jason Wei"
              },
              {
                "authorId": "1471909492",
                "name": "Nathan Scales"
              },
              {
                "authorId": "1524732527",
                "name": "Xuezhi Wang"
              },
              {
                "authorId": "50319359",
                "name": "D. Schuurmans"
              },
              {
                "authorId": "1698617",
                "name": "O. Bousquet"
              },
              {
                "authorId": "1998340269",
                "name": "Quoc Le"
              },
              {
                "authorId": "2226805",
                "name": "Ed H. Chi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250264890,
          "isinfluential": false,
          "contexts": [
            "Neverthe-less, HD-LoA prompting demonstrates competitive performance against supervised methods and even outperform these extensively trained models on the DocEE dataset in the cross-domain setting."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Rationale-Augmented Ensembles in Language Models",
            "abstract": "Recent research has shown that rationales, or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input ->output) prompts are expanded to (input, rationale ->output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a unified framework of rationale-augmented ensembles, where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1524732527",
                "name": "Xuezhi Wang"
              },
              {
                "authorId": "119640649",
                "name": "Jason Wei"
              },
              {
                "authorId": "50319359",
                "name": "D. Schuurmans"
              },
              {
                "authorId": "1998340269",
                "name": "Quoc Le"
              },
              {
                "authorId": "2226805",
                "name": "Ed H. Chi"
              },
              {
                "authorId": "65855107",
                "name": "Denny Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250390478,
          "isinfluential": false,
          "contexts": [
            "The prevalent approach for this task relies on the collection of labeled data and the subsequent model training via supervised learning (Ren et al., 2023; Liu et al., 2023a; Pouran Ben Veyseh et al., 2022; Zhou and Mao, 2022; Du and Cardie, 2020a)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Leveraging Redundant Information and Closed Boundary Loss",
            "abstract": "In document-level event argument extraction, an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition, in event argument extraction, most entities are regarded as class “others”, i.e. Universum class, which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without typical common features. Classifiers trained by cross entropy loss could easily misclassify the Universum class because of their open decision boundary. In this paper, to make use of redundant event information underlying a document, we build an entity coreference graph with the graph2token module to produce a comprehensive and coreference-aware representation for every entity and then build an entity summary graph to merge the multiple extraction results. To better classify Universum class, we propose a new loss function to build classifiers with closed boundaries. Experimental results show that our model outperforms the previous state-of-the-art models by 3.35% in F1-score.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2111825433",
                "name": "Hanzhang Zhou"
              },
              {
                "authorId": "2128504277",
                "name": "Kezhi Mao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254043800,
          "isinfluential": false,
          "contexts": [
            "Therefore, un-derstandings of supervised ML systems (e.g. pattern learning) are not applicable for ICL (Min et al., 2022; Akyürek et al., 2022), which necessitates distinct explorations on the mechanism of ICL."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "What learning algorithm is in-context learning? Investigations with linear models",
            "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at https://github.com/ekinakyurek/google-research/blob/master/incontext.",
            "year": 2022,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1992708068",
                "name": "Ekin Akyürek"
              },
              {
                "authorId": "50319359",
                "name": "D. Schuurmans"
              },
              {
                "authorId": "2112400",
                "name": "Jacob Andreas"
              },
              {
                "authorId": "2114186424",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "65855107",
                "name": "Denny Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259370571,
          "isinfluential": false,
          "contexts": [
            "The prevalent approach for this task relies on the collection of labeled data and the subsequent model training via supervised learning (Ren et al., 2023; Liu et al., 2023a; Pouran Ben Veyseh et al., 2022; Zhou and Mao, 2022; Du and Cardie, 2020a)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation",
            "abstract": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109978994",
                "name": "Yubing Ren"
              },
              {
                "authorId": "47184362",
                "name": "Yanan Cao"
              },
              {
                "authorId": "2075394870",
                "name": "Ping Guo"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2185915076",
                "name": "Wei Ma"
              },
              {
                "authorId": "1390641501",
                "name": "Zheng Lin"
              }
            ]
          }
        }
      ]
    },
    "258967833": {
      "citing_paper_info": {
        "title": "Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization",
        "abstract": "Document-level multi-event extraction aims to extract the structural information from a given document automatically. Most recent approaches usually involve two steps: (1) modeling entity interactions; (2) decoding entity interactions into events. However, such approaches ignore a global view of inter-dependency of multiple events. Moreover, an event is decoded by iteratively merging its related entities as arguments, which might suffer from error propagation and is computationally inefficient. In this paper, we propose an alternative approach for document-level multi-event extraction with event proxy nodes and Hausdorff distance minimization. The event proxy nodes, representing pseudo-events, are able to build connections with other event proxy nodes, essentially capturing global information. The Hausdorff distance makes it possible to compare the similarity between the set of predicted events and the set of ground-truth events. By directly minimizing Hausdorff distance, the model is trained towards the global optimum directly, which improves performance and reduces training time. Experimental results show that our model outperforms previous state-of-the-art method in F1-score on two datasets with only a fraction of training time.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2144706657",
            "name": "Xinyu Wang"
          },
          {
            "authorId": "145096580",
            "name": "Lin Gui"
          },
          {
            "authorId": "1390509967",
            "name": "Yulan He"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 1,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "54437926",
        "51871198",
        "245123950",
        "11246193",
        "244119148",
        "22130590",
        "237499254",
        "216562330",
        "5458500",
        "248371142",
        "247084444"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5458500,
          "isinfluential": false,
          "contexts": [
            "Table 5 shows how different components in ProCNet contribute to performance: − Hypernetwork Hypernetwork is removed by replacing GNN-FiLM with RGCN (Schlichtkrull et al., 2018), where all proxy nodes in RGCN share the same message-passing function."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Modeling Relational Data with Graph Convolutional Networks",
            "abstract": "Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
            "year": 2017,
            "venue": "Extended Semantic Web Conference",
            "authors": [
              {
                "authorId": "8804828",
                "name": "M. Schlichtkrull"
              },
              {
                "authorId": "41016725",
                "name": "Thomas Kipf"
              },
              {
                "authorId": "2789097",
                "name": "Peter Bloem"
              },
              {
                "authorId": "9965217",
                "name": "Rianne van den Berg"
              },
              {
                "authorId": "144889265",
                "name": "Ivan Titov"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11246193,
          "isinfluential": false,
          "contexts": [
            "(13) can be computed efficiently with (Ramakrishnan et al., 1991; Bertsekas, 1981)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Approximate Dual Projective Algorithm for Solving Assignment Problems",
            "abstract": "This paper discusses a new algorithm for solving the assignment problem. This algorithm, called the Approximate Dual Projective algorithm for assignment (ADP/A), is a variant of the Karmarkar interior point algorithm for LPs, specialized to solve the assignment problem. Computational results are reported on various classes of assignment problems. These results indicate that this method, holds promise for solving large assignment problems.",
            "year": 1991,
            "venue": "Network Flows And Matching",
            "authors": [
              {
                "authorId": "73457566",
                "name": "K. G. Ramakrishnan"
              },
              {
                "authorId": "2378746",
                "name": "N. Karmarkar"
              },
              {
                "authorId": "47825671",
                "name": "A. Kamath"
              }
            ]
          }
        },
        {
          "citedcorpusid": 22130590,
          "isinfluential": false,
          "contexts": [
            "As the standard Hausdorff distance is highly sensitive to outliers, we use the average Hausdorff distance (Schütze et al., 2012; Taha and Hanbury, 2015): However, in our task, the average Hausdorff distance could suffer a problem that a predicted event, represented by a proxy node, may be guided to…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Using the Averaged Hausdorff Distance as a Performance Measure in Evolutionary Multiobjective Optimization",
            "abstract": "",
            "year": 2012,
            "venue": "IEEE Transactions on Evolutionary Computation",
            "authors": [
              {
                "authorId": "1698609",
                "name": "O. Schütze"
              },
              {
                "authorId": "2115944",
                "name": "X. Esquivel"
              },
              {
                "authorId": "145189704",
                "name": "A. Lara"
              },
              {
                "authorId": "145485250",
                "name": "C. Coello"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": true,
          "contexts": [
            "Two variants of DCFEE are DCFEE-O for single-event and DCFEE-M for multi-event.",
            "Yang et al. (2018) proposed a key-event detection model.",
            "We can observe that a simple argument completion strategy (DCFEE-O and DCFEE-M) produces the worst results.",
            "More implementation details are in Appendix A.2 Baselines The baselines that we compare with are as follows: DCFEE (Yang et al., 2018) uses an argument-completion strategy in the table-filling task."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54437926,
          "isinfluential": false,
          "contexts": [
            "Finally, Hausdorff Distance Minimization minimizes the distance between the set of predicted events and the set of ground-truth events to perform a global training in the new event-level metric space. on joint-learning of the two sub-tasks (Nguyen and Nguyen, 2019; Lin et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "One for All: Neural Joint Modeling of Entities and Events",
            "abstract": "The previous work for event extraction has mainly focused on the predictions for event triggers and argument roles, treating entity mentions as being provided by human annotators. This is unrealistic as entity mentions are usually predicted by some existing toolkits whose errors might be propagated to the event trigger and argument role recognition. Few of the recent work has addressed this problem by jointly predicting entity mentions, event triggers and arguments. However, such work is limited to using discrete engineering features to represent contextual information for the individual tasks and their interactions. In this work, we propose a novel model to jointly perform predictions for entity mentions, event triggers and arguments based on the shared hidden representations from deep learning. The experiments demonstrate the benefits of the proposed method, leading to the state-of-the-art performance for event extraction.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "49035085",
                "name": "T. Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Much research has been done on sentence-level event extraction (Du and Cardie, 2020; Lin et al., 2020; Lu et al., 2021).",
            "Recently, multi-turn Question-Answer (QA) methods have been investigated for EE with hand-designed or automatically generated questions (Du and Cardie, 2020; Li et al., 2020; Wang et al., 2020; Liu et al., 2020; Lyu et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237499254,
          "isinfluential": false,
          "contexts": [
            "More recently, prompt-based learning has been explored using the knowledge in pre-trained language models (Lin et al., 2021; Hsu et al., 2021; Ma et al., 2022).",
            "To address the problem, conditional generation have been proposed, which are conditioned on pre-specified templates or prompts (Du et al., 2021; Huang et al., 2021; Ma et al., 2022)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DEGREE: A Data-Efﬁcient Generative Event Extraction Model",
            "abstract": "",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "3256207",
                "name": "Elizabeth Boschee"
              },
              {
                "authorId": "123937952",
                "name": "Scott Miller"
              },
              {
                "authorId": "145603129",
                "name": "P. Natarajan"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244119148,
          "isinfluential": false,
          "contexts": [
            "Existing approaches (Zheng et al., 2019; Yang et al., 2021; Huang and Jia, 2021; Xu et al., 2021; Liang et al., 2022) usually involve two steps: (1) first model the entity interactions based on contextual representations; (2) then design a decoding strategy to decode the entity interactions into events and arguments."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Sentence Community for Document-Level Event Extraction",
            "abstract": "Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents, and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2131131685",
                "name": "Yusheng Huang"
              },
              {
                "authorId": "1819081375",
                "name": "Weijia Jia"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245123950,
          "isinfluential": false,
          "contexts": [
            "For ChFinAnn, the baseline results are reported in (Zheng et al., 2019; Yang et al., 2021; Xu et al., 2021; Zhu et al., 2022; Liang et al., 2022).",
            "PTPCG (Zhu et al., 2022) combines event arguments together in a non-autoregressive decoding approach with pruned complete graphs, aiming to consume lower computational resources.",
            "For ChFinAnn, the base-line results are reported in (Zheng et al., 2019; Yang et al., 2021; Xu et al., 2021; Zhu et al., 2022; Liang et al., 2022)."
          ],
          "intents": [
            "['result']",
            "--",
            "['result']"
          ],
          "cited_paper_info": {
            "title": "Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph",
            "abstract": "Most previous studies of document-level event extraction mainly focus on building argument chains in an autoregressive way, which achieves a certain success but is inefficient in both training and inference.\n\nIn contrast to the previous studies, we propose a fast and lightweight model named as PTPCG.\n\nIn our model, we design a novel strategy for event argument combination together with a non-autoregressive decoding algorithm via pruned complete graphs, which are constructed under the guidance of the automatically selected pseudo triggers.\n\nCompared to the previous systems, our system achieves competitive results with 19.8% of parameters and much lower resource consumption, taking only 3.8% GPU hours for training and up to 8.5 times faster for inference.\n\nBesides, our model shows superior compatibility for the datasets with (or without) triggers and the pseudo triggers can be the supplements for annotated triggers to make further improvements.\n\nCodes are available at https://github.com/Spico197/DocEE .",
            "year": 2021,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1914586128",
                "name": "Tong Zhu"
              },
              {
                "authorId": "51912474",
                "name": "Xiaoye Qu"
              },
              {
                "authorId": "48993675",
                "name": "Wenliang Chen"
              },
              {
                "authorId": "2243403387",
                "name": "Zhefeng Wang"
              },
              {
                "authorId": "2422046",
                "name": "Baoxing Huai"
              },
              {
                "authorId": "1677643972",
                "name": "N. Yuan"
              },
              {
                "authorId": "1390813134",
                "name": "Min Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": false,
          "contexts": [
            "More recently, prompt-based learning has been explored using the knowledge in pre-trained language models (Lin et al., 2021; Hsu et al., 2021; Ma et al., 2022).",
            "To address the problem, conditional generation have been proposed, which are conditioned on pre-specified templates or prompts (Du et al., 2021; Huang et al., 2021; Ma et al., 2022)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248371142,
          "isinfluential": false,
          "contexts": [
            "Lu et al. (2022a) captured event clues as a series of intermediate results.",
            "Apart from QA-based approaches, sequence-to-sequence learning has also been explored, where the event annotation is flattened as a sequence (Paolini et al., 2021; Lu et al., 2021; Li et al., 2021; Lu et al., 2022b)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Explainable document-level event extraction via back-tracing to sentence-level event clues",
            "abstract": "",
            "year": 2022,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "116829010",
                "name": "Shudong Lu"
              },
              {
                "authorId": "2111304512",
                "name": "Gang Zhao"
              },
              {
                "authorId": "2118153856",
                "name": "Si Li"
              },
              {
                "authorId": "2140051783",
                "name": "Jun Guo"
              }
            ]
          }
        }
      ]
    },
    "267126385": {
      "citing_paper_info": {
        "title": "Document-Level Event Argument Extraction Based on Bidirectional Span Detection",
        "abstract": "In this paper, we propose a document-level event argument extraction model called BSDM based on bidirectional span detection, mainly to address the problems of insufficient use of known information and overlooked correlations between span boundaries in document-level event argument extraction tasks. Firstly, in order to fully utilize known information, we design event types and event roles as event templates in the data preprocessing stage. Then, we integrate trigger and event type information into event roles in the information fusion layer, and identify argument boundaries under the guidance of role information. Secondly, in order to establish correlations between span boundaries, we design forward/backward decoders for the model, decoding argument spans from starting/ending boundaries respectively. Instead of using independent span boundary recognition methods, our model uses de-coded argument boundaries to recognize pending argument boundaries. Finally, considering the core role of trigger in events, we regard event detection as an auxiliary task for document-level event argument extraction. Through multitask joint training, the model will have better performance. We conducted sufficient experiments on the RAMS dataset. The results show that when using BERT-base as the encoder, BSDM achieved an F1 score 0.7% higher than the current SOTA result, and when using BERT-large as the encoder, it showed performance only second to the current SOTA model.",
        "year": 2023,
        "venue": "BigData Congress [Services Society]",
        "authors": [
          {
            "authorId": "2144288975",
            "name": "Yong Zhang"
          },
          {
            "authorId": "2149930939",
            "name": "Feng Xiong"
          },
          {
            "authorId": "2268366408",
            "name": "Kaiyu Zhang"
          },
          {
            "authorId": "2280857439",
            "name": "Jiaxian Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 13,
        "unique_cited_count": 13,
        "influential_count": 1,
        "detailed_records_count": 13
      },
      "cited_papers": [
        "257050669",
        "235458429",
        "1240016",
        "236460308",
        "14339673",
        "222177108",
        "233219850",
        "220046861",
        "196178503",
        "235254286",
        "12108307",
        "231602921",
        "231728756"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1240016,
          "isinfluential": false,
          "contexts": [
            "In traditional event extraction tasks, researchers focused more on extracting information from individual sentences, such as Nguyen [1], Yang [2], Chen [3], Huang [4], Yang [5], Liu [6] and etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Zero-Shot Transfer Learning for Event Extraction",
            "abstract": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "In traditional event extraction tasks, researchers focused more on extracting information from individual sentences, such as Nguyen [1], Yang [2], Chen [3], Huang [4], Yang [5], Liu [6] and etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "In sentence-level event extraction, Chen et al. [9] used DMCNN to capture sentence-level and word-level features to improve the effect of event extraction."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "In traditional event extraction tasks, researchers focused more on extracting information from individual sentences, such as Nguyen [1], Yang [2], Chen [3], Huang [4], Yang [5], Liu [6] and etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": true,
          "contexts": [
            "Zhang et al. [7] proposed a two-step strategy that first matches the central word of the argument using the trigger, and then extends the central word to the left or right to obtain the complete argument span.",
            "The both methods have played a certain role in document-level event extraction tasks, but Zhang's method utilizes too little effective information, only establishing a connection between the trigger and the argument [7].",
            "Zhang et al. [7] proposed a two-step strategy model, first using the trigger word to find the head word of the argument through bi-affine, and assuming that the head word can represent the complete meaning of the argument."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222177108,
          "isinfluential": false,
          "contexts": [
            "In sentence-level event extraction, Ma et al. [19] capture the relationship between trigger and event arguments by modifying the attention heads of the Transformer."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Resource-Enhanced Neural Model for Event Argument Extraction",
            "abstract": "Event argument extraction (EAE) aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges: (1) Data scarcity. (2) Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument. (3) Integrating event trigger information into candidate argument representation. For (1), we explore using unlabeled data. For (2), we use Transformer that uses dependency parses to guide the attention mechanism. For (3), we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE 2005 benchmark show that our approach achieves a new state-of-the-art.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2115889791",
                "name": "Jie Ma"
              },
              {
                "authorId": "1717480",
                "name": "Shuai Wang"
              },
              {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
              },
              {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
              },
              {
                "authorId": "1403907739",
                "name": "Yaser Al-Onaizan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231602921,
          "isinfluential": false,
          "contexts": [
            "Paolini et al. [15] propose TANL to handle a variety of structured prediction tasks, including EAE, by a unified text-to-text approach and extract all arguments in a single pass."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Structured Prediction as Translation between Augmented Natural Languages",
            "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",
            "year": 2021,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1950846",
                "name": "Giovanni Paolini"
              },
              {
                "authorId": "2095707",
                "name": "Ben Athiwaratkun"
              },
              {
                "authorId": "49186783",
                "name": "Jason Krone"
              },
              {
                "authorId": "2115889791",
                "name": "Jie Ma"
              },
              {
                "authorId": "16163297",
                "name": "A. Achille"
              },
              {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
              },
              {
                "authorId": "1790831",
                "name": "C. D. Santos"
              },
              {
                "authorId": "144028698",
                "name": "Bing Xiang"
              },
              {
                "authorId": "1715959",
                "name": "Stefano Soatto"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "In document-level event extraction, Du et al. [17] and Li et al. [18] transform the extraction task into a generation task by designing specific templates for each event type respectively, using BERT and BART models to handle document-level event extraction."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "Li et al. [18] proposed a BART-based generative model BART-Gen, and for each event type, manually created a corresponding generation template, and blanked out the arguments that needed to be identified and played a certain role in the template , and finally regenerated the entire template to fill…",
            "In document-level event extraction, Du et al. [17] and Li et al. [18] transform the extraction task into a generation task by designing specific templates for each event type respectively, using BERT and BART models to handle document-level event extraction."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235254286,
          "isinfluential": false,
          "contexts": [
            "In sentence-level event extraction, Tong et al. [22] introduce open-domain trigger knowledge, and Wang et al. [23] learn event extraction knowledge from large unsupervised databases and semantic structures."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "CLEVE: Contrastive Pre-training for Event Extraction",
            "abstract": "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1390880371",
                "name": "Ziqi Wang"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2149202150",
                "name": "Yankai Lin"
              },
              {
                "authorId": "2055765060",
                "name": "Lei Hou"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "50492525",
                "name": "Peng Li"
              },
              {
                "authorId": "8549842",
                "name": "Juan-Zi Li"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "Lu et al. [16] follow TANL and convert the event extraction task into an end-to-end event generation task."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460308,
          "isinfluential": false,
          "contexts": [
            "Wei et al. [27] proposed a frame-known event argument extraction learning framework FEAE, by designing event templates, identifying and classifying arguments in the way of boundary detection according to the order of the templates."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "31407139",
                "name": "Kaiwen Wei"
              },
              {
                "authorId": "2946890",
                "name": "Xian Sun"
              },
              {
                "authorId": "151473773",
                "name": "Zequn Zhang"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "2116390646",
                "name": "Zhi Guo"
              },
              {
                "authorId": "2152163772",
                "name": "Li Jin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257050669,
          "isinfluential": false,
          "contexts": [
            "How to further use large scale language models as encoder for event identification and arguments extraction has attracted the attention of researchers [28] [29]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Zero-Shot Information Extraction via Chatting with ChatGPT",
            "abstract": "Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2115494116",
                "name": "Xiang Wei"
              },
              {
                "authorId": "2149528155",
                "name": "Xingyu Cui"
              },
              {
                "authorId": "2208958032",
                "name": "Ning Cheng"
              },
              {
                "authorId": "2108114693",
                "name": "Xiaobin Wang"
              },
              {
                "authorId": "2149173726",
                "name": "Xin Zhang"
              },
              {
                "authorId": "2186268584",
                "name": "Shen Huang"
              },
              {
                "authorId": "35930962",
                "name": "Pengjun Xie"
              },
              {
                "authorId": "2291982150",
                "name": "Jinan Xu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2117849151",
                "name": "Meishan Zhang"
              },
              {
                "authorId": "50262192",
                "name": "Yong Jiang"
              },
              {
                "authorId": "144836032",
                "name": "Wenjuan Han"
              }
            ]
          }
        }
      ]
    },
    "265452030": {
      "citing_paper_info": {
        "title": "A Generative Approach for Comprehensive Financial Event Extraction at the Document Level",
        "abstract": "Financial event extraction enables the extraction of comprehensive and accurate information about financial events from documents. This paper explores the current methods for extracting events at the financial document level, which often involve custom-designed networks and processes. We question whether such extensive efforts are truly necessary for this task. Our research is motivated by recent developments in generative event extraction, which have shown success in sentence-level extraction but have yet to be explored for financial document-level extraction. To fill this gap, we propose a generative solution for document-level event extraction, which is more challenging due to the presence of scattered arguments and multiple events. We introduce an encoding scheme to capture entity-to-document level information and a decoding scheme that makes the generative process aware of all relevant contexts. Our results indicate that using our method, a generative-based solution can perform as well as state-of-the-art methods that use a specialized structure for document event extraction, providing an easy-to-use, strong baseline for future research.",
        "year": 2023,
        "venue": "International Conference on AI in Finance",
        "authors": [
          {
            "authorId": "2170166018",
            "name": "Jinan Zou"
          },
          {
            "authorId": "2268337725",
            "name": "Yanxi Liu"
          },
          {
            "authorId": "2261908445",
            "name": "Yuankai Qi"
          },
          {
            "authorId": "2114856638",
            "name": "Hai Cao"
          },
          {
            "authorId": "2244557512",
            "name": "Lingqiao Liu"
          },
          {
            "authorId": "3177281",
            "name": "Javen Qinfeng Shi"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 11,
        "influential_count": 1,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "119308902",
        "216562330",
        "220046861",
        "6628106",
        "2367456",
        "231728756",
        "196178503",
        "226262283",
        "6452487",
        "231602921",
        "218630327"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "Most of them cast event extraction as a classification problem, using global features to capture dependencies among local classifiers and applying joint inference [12, 14, 17, 20, 21, 24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "Most of them cast event extraction as a classification problem, using global features to capture dependencies among local classifiers and applying joint inference [12, 14, 17, 20, 21, 24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "During training, we used the Adam [10] optimizer with a learning rate of 1 𝑒 -4 for 100 epochs."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": true,
          "contexts": [
            "In Zheng et al. [26] a transformer-based end-to-end model is proposed to solve the DEE problem by filling the event tables with an entity-based directed acyclic graph.",
            "Further, we perform entity recognition as a sequence tagging task with BIO (Begin, Inside, Other) schema using 1 𝑁 𝑤 𝑖 as the input like Zheng et al. [26].",
            "Researchers attempt to conduct DEE in a more realistic trigger-free on the ChFinAnn dataset [26], where event types are directly predicted based on the document semantics.",
            "We evaluate our method on the public dataset ChFinAnn collected by Zheng et al. [26].",
            "(2) [Event Role Embeddings]: As predefined by the event extraction task [26], each record is composed of 𝑁 𝑟𝐸 𝑖 event roles.",
            "For example, the Doc2EDAG model [26] uses a directed acyclic graph to combine arguments, which can be both computationally and memory-consuming.",
            "Doc2EDAG Zheng et al. [26] proposed an end-to-end model for DEE that employs a transformer encoder to obtain sentence and entity embeddings and approaches DEE by directly filling event tables with entity-based path expending.",
            "For fair comparisons, we adopt the same evaluation standard used in Doc2EDAG [26], and DE-PPN [23]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "Most of them cast event extraction as a classification problem, using global features to capture dependencies among local classifiers and applying joint inference [12, 14, 17, 20, 21, 24]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "On the other hand, event extraction is viewed as an extractive machine learning comprehension task [4, 11, 15], where models are trained to identify relevant answers to a variety of framing questions for each event component."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "The first group mainly focuses on extracting scattering event arguments in the document [3, 5, 6, 25]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": false,
          "contexts": [
            "The first group mainly focuses on extracting scattering event arguments in the document [3, 5, 6, 25]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "On the other hand, event extraction is viewed as an extractive machine learning comprehension task [4, 11, 15], where models are trained to identify relevant answers to a variety of framing questions for each event component."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231602921,
          "isinfluential": false,
          "contexts": [
            "Recently, there have been developments in using a text generation process for event extraction, which simplifies the event extraction problem [7, 9, 16, 18].",
            "Most recently, a few generation-based event extraction models [7, 9, 16, 18] have been proposed, which generate all arguments and their roles as a way to convert text into a structured form."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Structured Prediction as Translation between Augmented Natural Languages",
            "abstract": "We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",
            "year": 2021,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1950846",
                "name": "Giovanni Paolini"
              },
              {
                "authorId": "2095707",
                "name": "Ben Athiwaratkun"
              },
              {
                "authorId": "49186783",
                "name": "Jason Krone"
              },
              {
                "authorId": "2115889791",
                "name": "Jie Ma"
              },
              {
                "authorId": "16163297",
                "name": "A. Achille"
              },
              {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
              },
              {
                "authorId": "1790831",
                "name": "C. D. Santos"
              },
              {
                "authorId": "144028698",
                "name": "Bing Xiang"
              },
              {
                "authorId": "1715959",
                "name": "Stefano Soatto"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "The first group mainly focuses on extracting scattering event arguments in the document [3, 5, 6, 25]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "The first group mainly focuses on extracting scattering event arguments in the document [3, 5, 6, 25]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "267703783": {
      "citing_paper_info": {
        "title": "A Review of Document-Level Multi-Event Extraction Methods",
        "abstract": "This paper presents a comprehensive review of the prominent methods and techniques employed in the field of document-level multi-event extraction, while also providing a critical evaluation of the associated research. The initial section of this paper introduces the fundamental concepts of document-level multi-event event extraction, including the definition and challenges of event extraction, as well as the specific obstacles encountered in document-level multi-event event extraction. Subsequently, the paper conducts a thorough review of the principal methods used for event extraction, along with an analysis of their applicability to different scenarios and the problems they addressed. Furthermore, the paper offers a detailed exposition and summary of key issues such as thesis element scattering in document-level event extraction, in addition to discussing the prevailing research paradigms in this domain. Lastly, the paper concludes by summarizing the strengths and weaknesses of current research efforts by highlighting the future directions and potential of document-level multi-event extraction research. To summarize, the objective of this paper is to furnish researchers with a thorough and methodical review of document-level multi-event extraction methods, facilitating readers in attaining a more profound comprehension of the most recent advancements and methodologies in this domain.",
        "year": 2023,
        "venue": "2023 9th International Conference on Big Data and Information Analytics (BigDIA)",
        "authors": [
          {
            "authorId": "2284346100",
            "name": "Guangshuai Ding"
          },
          {
            "authorId": "2038523884",
            "name": "Jingjing Tao"
          },
          {
            "authorId": "2284653593",
            "name": "Xiaomin Zhu"
          },
          {
            "authorId": "2184492879",
            "name": "Bin Lin"
          },
          {
            "authorId": "2155676150",
            "name": "W. Zhang"
          },
          {
            "authorId": "2284597647",
            "name": "Yanqing Ye"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 22,
        "unique_cited_count": 22,
        "influential_count": 0,
        "detailed_records_count": 22
      },
      "cited_papers": [
        "13241382",
        "10016456",
        "258011950",
        "259004542",
        "13807460",
        "246273659",
        "238856763",
        "189928589",
        "2367456",
        "226262283",
        "135473179",
        "15865939",
        "235458429",
        "26791976",
        "247619149",
        "258820351",
        "227230416",
        "204915992",
        "11187670",
        "2257053",
        "11751039",
        "202773239"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2257053,
          "isinfluential": false,
          "contexts": [
            "In 1993, Ellen et al. [5] used trigger dictionary and 13 event matching patterns to identify and extract events."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatically Constructing a Dictionary for Information Extraction Tasks",
            "abstract": "Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary.",
            "year": 1993,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "These models can be divided into feature-based manual methods [18,19] and neural network-based methods [20,21]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10016456,
          "isinfluential": false,
          "contexts": [
            "In 1995, Ellen et al. [6] further enhanced their approach with the development of the AutoSlog-ST system."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Acquisition of Linguistic Patterns for Knowledge-based Information Extraction",
            "abstract": "",
            "year": 2000,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "1713428",
                "name": "S. Harabagiu"
              },
              {
                "authorId": "2248349",
                "name": "Steven J. Maiorano"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11187670,
          "isinfluential": false,
          "contexts": [
            "In 2010, Liao et al. [32] presented a statistical model for event triggering and thesis element role classification at the document level, with the objective of achieving intra-and cross-event consistency within documents."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Using Document Level Cross-Event Inference to Improve Event Extraction",
            "abstract": "",
            "year": 2010,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39524110",
                "name": "Shasha Liao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11751039,
          "isinfluential": false,
          "contexts": [
            "For instance, several studies [16,17] have focused on co-extracting entities and capturing inter-entity relationships."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
            "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "37423160",
                "name": "Suncong Zheng"
              },
              {
                "authorId": "2145756722",
                "name": "Feng Wang"
              },
              {
                "authorId": "2682574",
                "name": "Hongyun Bao"
              },
              {
                "authorId": "8361912",
                "name": "Yuexing Hao"
              },
              {
                "authorId": "144032121",
                "name": "P. Zhou"
              },
              {
                "authorId": "2109511511",
                "name": "Bo Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13241382,
          "isinfluential": false,
          "contexts": [
            "Guda et al. [9] introduced a rule-based approach for EE in 2016, aimed at extracting events from natural language text by implementing frame rules suitable for open domain EE."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Rules based event extraction from natural language text",
            "abstract": "",
            "year": 2016,
            "venue": "IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology",
            "authors": [
              {
                "authorId": "9157027",
                "name": "Vanitha Guda"
              },
              {
                "authorId": "9121106",
                "name": "Suresh Kumar Sanampudi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13807460,
          "isinfluential": false,
          "contexts": [
            "News reporting is one of the significant areas where EE plays a crucial role [42,43]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An event-extraction approach for business analysis from online Chinese news",
            "abstract": "",
            "year": 2018,
            "venue": "Electronic Commerce Research and Applications",
            "authors": [
              {
                "authorId": "2004577591",
                "name": "Songqiao Han"
              },
              {
                "authorId": "48507065",
                "name": "Xiaoling Hao"
              },
              {
                "authorId": "117894122",
                "name": "Hailiang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15865939,
          "isinfluential": false,
          "contexts": [
            "In 2011, Liao et al. [37] introduced an unsupervised EE model that utilized topic feature clustering."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Acquiring Topic Features to improve Event Extraction: in Pre-selected and Balanced Collections",
            "abstract": "",
            "year": 2011,
            "venue": "Recent Advances in Natural Language Processing",
            "authors": [
              {
                "authorId": "39524110",
                "name": "Shasha Liao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 26791976,
          "isinfluential": false,
          "contexts": [
            "In 2014, Pham et al. [8] devised a hybrid approach that synergistically combined rule-based and machine learning techniques."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Combination of Rule-based and Machine Learning for Biomedical Event Extraction",
            "abstract": "paper describes the method for biomedical event extraction. The biomedical events occurs in relative to biomedical concepts (objects) as proteins, genes. In this work, we try a hybrid method to identify given event types relative to a given set of proteins in biomedical text. The approach combines rule-based and machine learning. A Set of rules is built based on event triggers, and a set of features is selected to use for machine learning algorithm. Our system consists of four main phases: preprocessing, trigger detection, event detection and post-processing. These phases are developed based on UIMA 1 framework. This work is continuous of our work for BioNLP2013 Shared Task 2 . The final result obtains 36.60 f-score.",
            "year": 2014,
            "venue": "International Conference on Information Resources Management",
            "authors": [
              {
                "authorId": "27711933",
                "name": "Xuan-Quang Pham"
              },
              {
                "authorId": "37751314",
                "name": "Bao-Quoc Ho"
              }
            ]
          }
        },
        {
          "citedcorpusid": 135473179,
          "isinfluential": false,
          "contexts": [
            "These models can be divided into feature-based manual methods [18,19] and neural network-based methods [20,21]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Entity and Event Extraction with Generative Adversarial Imitation Learning",
            "abstract": "We propose a new framework for entity and event extraction based on generative adversarial imitation learning—an inverse reinforcement learning method using a generative adversarial network (GAN). We assume that instances and labels yield to various extents of difficulty and the gains and penalties (rewards) are expected to be diverse. We utilize discriminators to estimate proper rewards according to the difference between the labels committed by the ground-truth (expert) and the extractor (agent). Our experiments demonstrate that the proposed framework outperforms state-of-the-art methods.",
            "year": 2019,
            "venue": "Data Intelligence",
            "authors": [
              {
                "authorId": "2111626",
                "name": "Tongtao Zhang"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "2707234",
                "name": "Avirup Sil"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189928589,
          "isinfluential": false,
          "contexts": [
            "In 2019, Liu et al.[38] introduced an open-domain EE method on the basis of a neural latent variable model."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Open Domain Event Extraction Using Neural Latent Variable Models",
            "abstract": "We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              },
              {
                "authorId": "2145912727",
                "name": "Yue Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202773239,
          "isinfluential": false,
          "contexts": [
            "Furthermore, diverse approaches utilizing various neural network structures have surfaced, including MOGANED [2]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Detection with Multi-Order Graph Convolution and Aggregated Attention",
            "abstract": "Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use first-order syntactic relations (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2116562619",
                "name": "Haoran Yan"
              },
              {
                "authorId": "2149111400",
                "name": "Xiaolong Jin"
              },
              {
                "authorId": "1500390975",
                "name": "Xiangbin Meng"
              },
              {
                "authorId": "70414094",
                "name": "Jiafeng Guo"
              },
              {
                "authorId": "1717004",
                "name": "Xueqi Cheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204915992,
          "isinfluential": false,
          "contexts": [
            "In EE, the meta-learning algorithm can learn new domain tasks quickly and efficiently by continuously iterating and updating the prior knowledge of the existing domain [41]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection",
            "abstract": "Event detection (ED), a sub-task of event extraction, involves identifying triggers and categorizing event mentions. Existing methods primarily rely upon supervised learning and require large-scale labeled event datasets which are unfortunately not readily available in many real-life applications. In this paper, we consider and reformulate the ED task with limited labeled data as a Few-Shot Learning problem. We propose a Dynamic-Memory-Based Prototypical Network (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learn better prototypes for event types, but also produce more robust sentence encodings for event mentions. Differing from vanilla prototypical networks simply computing event prototypes by averaging, which only consume event mentions once, our model is more robust and is capable of distilling contextual information from event mentions for multiple times due to the multi-hop mechanism of DMNs. The experiments show that DMB-PN not only deals with sample scarcity better than a series of baseline models but also performs more robustly when the variety of event types is relatively large and the instance quantity is extremely small.",
            "year": 2019,
            "venue": "Web Search and Data Mining",
            "authors": [
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "1388742072",
                "name": "Jiaojian Kang"
              },
              {
                "authorId": "2118158068",
                "name": "Yichi Zhang"
              },
              {
                "authorId": "2155468731",
                "name": "Wei Zhang"
              },
              {
                "authorId": "1729778",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262283,
          "isinfluential": false,
          "contexts": [
            "Liu et al. [30] proposed an unsupervised question generation method that streamlines the process by transforming an event pattern into a series of natural questions."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction as Machine Reading Comprehension",
            "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\\% in F1 for event argument extraction with only 1\\% data, compared with 2.2\\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\\%$ and $16\\%$ in F1 on two datasets without using any EE training data.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1844673750",
                "name": "Wei Bi"
              },
              {
                "authorId": "3028405",
                "name": "Xiaojiang Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 227230416,
          "isinfluential": false,
          "contexts": [
            "With the advancement of research in graph neural networks, multiple studies [33-35] have illustrated the efficacy of graph structures in encoding event information at the document level."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Relation Extraction with Dual-tier Heterogeneous Graph",
            "abstract": "Document-level relation extraction (RE) poses new challenges over its sentence-level counterpart since it requires an adequate comprehension of the whole document and the multi-hop reasoning ability across multiple sentences to reach the final result. In this paper, we propose a novel graph-based model with Dual-tier Heterogeneous Graph (DHG) for document-level RE. In particular, DHG is composed of a structure modeling layer followed by a relation reasoning layer. The major advantage is that it is capable of not only capturing both the sequential and structural information of documents but also mixing them together to benefit for multi-hop reasoning and final decision-making. Furthermore, we employ Graph Neural Networks (GNNs) based message propagation strategy to accumulate information on DHG. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on two widely used datasets, and further analyses suggest that all the modules in our model are indispensable for document-level RE.",
            "year": 2020,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "122542861",
                "name": "Zhenyu Zhang"
              },
              {
                "authorId": "48613402",
                "name": "Yu Bowen"
              },
              {
                "authorId": "2269366",
                "name": "Xiaobo Shu"
              },
              {
                "authorId": "2079682",
                "name": "Tingwen Liu"
              },
              {
                "authorId": "1598319620",
                "name": "Hengzhu Tang"
              },
              {
                "authorId": "9309853",
                "name": "Wang Yubin"
              },
              {
                "authorId": "48358041",
                "name": "Li Guo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235458429,
          "isinfluential": false,
          "contexts": [
            "Lu et al. [25] introduced a novel EE method named TEXT2EVENT."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
            "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2116315442",
                "name": "Jin Xu"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "120932246",
                "name": "Jialong Tang"
              },
              {
                "authorId": "2112838560",
                "name": "Annan Li"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "145865588",
                "name": "M. Liao"
              },
              {
                "authorId": "2118435689",
                "name": "Shaoyi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238856763,
          "isinfluential": false,
          "contexts": [
            "Subsequently, in 2021, Wang et al. [39] adopted a query-extraction paradigm to enhance event extraction."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding",
            "abstract": "Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and arguments from the input text. With the rich semantics in the queries, our framework benefits from the attention mechanisms to better capture the semantic correlation between the event types or argument roles and the input text. Furthermore, the query-and-extract formulation allows our approach to leverage all available event annotations from various ontologies as a unified model. Experiments on ACE and ERE demonstrate that our approach achieves state-of-the-art performance on each dataset and significantly outperforms existing methods on zero-shot event extraction.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2116423012",
                "name": "Sijia Wang"
              },
              {
                "authorId": "2115621395",
                "name": "Mo Yu"
              },
              {
                "authorId": "3307026",
                "name": "Shiyu Chang"
              },
              {
                "authorId": "49755259",
                "name": "Lichao Sun"
              },
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246273659,
          "isinfluential": false,
          "contexts": [
            "It enables the extraction of pertinent information regarding events related to the stock market, exchange rates, company performance, and more [44,45]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Automatic Information Extraction for Financial Events by Integrating BiGRU and Attention Mechanism",
            "abstract": "In this paper, an information extraction method for financial events written in Chinese is proposed. The core entities of the causes and results, as well as the verbs and conditions are extracted from the financial events reported by websites. The method takes the original words and the part-of-speech of words as two inputs. BERT encoder is utilized to transform the original sentences to word-embedding vectors, which then are send to BiGRU to extract the sematic features. And a full-connected network is overlapped on BiGRU to reduce the impact of “covariate shift”. For the second input, the original sentences are cut by Chinese cut-word tool ‘jieba’ to get the part-of-speech of words, which are then transformed by self-attention mechanism to get global dependencies. The two outputs for word-embedding vectors and part-of-speech of words are combined and then decoded by CRF. Finally, the Viterbi algorithm is utilized to get the best sequences. The experiment results validate the effectiveness of the proposed method.",
            "year": 2022,
            "venue": "Journal of Physics: Conference Series",
            "authors": [
              {
                "authorId": "1680607",
                "name": "Jiaheng Lu"
              },
              {
                "authorId": "2121527390",
                "name": "Weirong Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247619149,
          "isinfluential": false,
          "contexts": [
            "Hsu et al.[27] introduced a unified text-to-structure generation framework (UIE), which demonstrates the ability to universally model various kinds of information extraction tasks, adaptively generate target structures, and collaboratively acquire generic extraction capabilities from diverse…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unified Structure Generation for Universal Information Extraction",
            "abstract": "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2154976399",
                "name": "Qing Liu"
              },
              {
                "authorId": "40495683",
                "name": "Dai Dai"
              },
              {
                "authorId": "2107521158",
                "name": "Xinyan Xiao"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "2149181702",
                "name": "Hua Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258011950,
          "isinfluential": false,
          "contexts": [
            "It allows for the extraction of event information related to diseases, drugs, treatments, and more [46,47]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Contextualized medication event extraction with levitated markers",
            "abstract": "",
            "year": 2023,
            "venue": "Journal of Biomedical Informatics",
            "authors": [
              {
                "authorId": "18150530",
                "name": "J. Vasilakes"
              },
              {
                "authorId": "2200648710",
                "name": "Panagiotis Georgiadis"
              },
              {
                "authorId": "48182862",
                "name": "Nhung T. H. Nguyen"
              },
              {
                "authorId": "2060015680",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258820351,
          "isinfluential": false,
          "contexts": [
            "It allows for the extraction of event information related to diseases, drugs, treatments, and more [46,47]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Medication event extraction in clinical notes: Contribution of the WisPerMed team to the n2c2 2022 challenge",
            "abstract": "",
            "year": 2023,
            "venue": "Journal of Biomedical Informatics",
            "authors": [
              {
                "authorId": "2054386596",
                "name": "Henning Schäfer"
              },
              {
                "authorId": "2078836234",
                "name": "Ahmad Idrissi-Yaghir"
              },
              {
                "authorId": "2218253116",
                "name": "Jeanette Bewersdorff"
              },
              {
                "authorId": "2160925909",
                "name": "Sameh Frihat"
              },
              {
                "authorId": "2955567",
                "name": "C. Friedrich"
              },
              {
                "authorId": "1780779",
                "name": "Torsten Zesch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259004542,
          "isinfluential": false,
          "contexts": [
            "It enables the extraction of event information related to case types, offenses, judgment results, and more [48,49]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Parallel Corpus-Based Approach to the Crime Event Extraction for Low-Resource Languages",
            "abstract": "These days, a lot of crime-related events take place all over the world. Most of them are reported in news portals and social media. Crime-related event extraction from the published texts can allow monitoring, analysis, and comparison of police or criminal activities in different countries or regions. Existing approaches to event extraction mainly suggest processing texts in English, French, Chinese, and some other resource-rich and well-annotated languages. This paper presents a parallel corpus-based approach that follows a closed-domain event extraction methodology to event extraction from web news articles in low-resource languages. To identify the event, its arguments, and the arguments’ roles in the source-language part of the corpus we utilize an enhanced pattern-based method that involves the multilingual synonyms dictionary with knowledge about crime-related concepts and logic-linguistic equations. The event extraction from the target-language part of the corpus uses a cross-lingual crime-related event extraction transfer technique that is based on supplementary knowledge about the semantic similarity patterns of the considered pair of languages. The presented approach does not require a preliminarily annotated corpus for training making it more attractive to low-resource languages and allows extracting TRANSFER, CRIME, and POLICE types of events and their seven subtypes from various topics of news articles simultaneously. Implementation of our approach for the Russian-Kazakh parallel corpus of news portals articles allowed obtaining the F1-measure of crime-related event extraction of over 82% for the source language and 63% for the target language.",
            "year": 2023,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "145863351",
                "name": "N. Khairova"
              },
              {
                "authorId": "145470529",
                "name": "O. Mamyrbayev"
              },
              {
                "authorId": "2699023",
                "name": "N. Rizun"
              },
              {
                "authorId": "2309418803",
                "name": "Mariia Razno"
              },
              {
                "authorId": "2219795947",
                "name": "Ybytayeva Galiya"
              }
            ]
          }
        }
      ]
    },
    "268253036": {
      "citing_paper_info": {
        "title": "Large Language Models for Document-Level Event-Argument Data Augmentation for Challenging Role Types",
        "abstract": "Event Argument Extraction (EAE) is an extremely difficult information extraction problem -- with significant limitations in few-shot cross-domain (FSCD) settings. A common solution to FSCD modeling is data augmentation. Unfortunately, existing augmentation methods are not well-suited to a variety of real-world EAE contexts including (i) The need to model long documents (10+ sentences) (ii) The need to model zero and few-shot roles (i.e. event roles with little to no training representation). In this work, we introduce two novel LLM-powered data augmentation frameworks for synthesizing extractive document-level EAE samples using zero in-domain training data. Our highest performing methods provide a 16-pt increase in F1 score on extraction of zero shot role types. To better facilitate analysis of cross-domain EAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain. Our experiments show that LLM-based augmentation can boost RDF1 performance by up to 11 F1 points compared to baseline methods.",
        "year": 2024,
        "venue": "",
        "authors": [
          {
            "authorId": "2337095",
            "name": "Joseph Gatto"
          },
          {
            "authorId": "2181370903",
            "name": "Parker Seegmiller"
          },
          {
            "authorId": "2264340968",
            "name": "Omar Sharif"
          },
          {
            "authorId": "2243193868",
            "name": "S. Preum"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 9,
        "influential_count": 2,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "259137390",
        "252391326",
        "243865143",
        "201646309",
        "252090194",
        "258967387",
        "258865260",
        "255522592",
        "204838007"
      ],
      "citation_details": [
        {
          "citedcorpusid": 201646309,
          "isinfluential": true,
          "contexts": [
            "…role not found in the event schema when generating the Mad Lib, 7 allenai/longformer-base-4096 we may handle this in two ways: (i) we can use SBERT (Reimers and Gurevych, 2019), a popular textual embedding method, to encode the hal-lucinated category name and compute the cosine similarity between…",
            "Our matching algorithm compares the argument in the synthetic event structure with every possible n-gram (up to n=20) in the document and returns the n-gram with the highest cosine similarity using SBERT 4 .",
            "For example, if the LLM invents a new role not found in the event schema when generating the Mad Lib, 7 allenai/longformer-base-4096 we may handle this in two ways: (i) we can use SBERT (Reimers and Gurevych, 2019), a popular textual embedding method, to encode the hal-lucinated category name and compute the cosine similarity between it and all possible role names for a given sample.",
            "We embed each role in each set using S-BERT (Reimers and Gurevych, 2019) to get a set of source role embeddings S and a set of target role embed-dings T ."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
            "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2959414",
                "name": "Nils Reimers"
              },
              {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "MTF augments DocEE documents by masking a single contiguous span of non-annotated text in each document and infilling the mask using a T5 (Raffel et al., 2020) model fine-tuned on the Gi-gaword corpus (Graff et al., 2003)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865143,
          "isinfluential": false,
          "contexts": [
            "The few prior works in DocEAE augmentation do not generate new data samples but rather augment existing samples (Liu et al., 2022) or use a pre-trained model to weakly label unannotated corpora (Liu et al., 2021).",
            "In (Liu et al., 2021), they use pre-trained EAE models to silver-label (i.e. use a pre-trained model to annotate) unannotated documents as additional data augmentation."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction",
            "abstract": "Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach — it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252090194,
          "isinfluential": false,
          "contexts": [
            "A key issue in the FSCD DocEAE problem is the need to model 0-shot roles (Yang et al., 2023a), i.e. roles which exist in an event’s schema and thus might appear in the test set, but have no representation in the training data.",
            "For this reason, other recent works have chosen to focus their attention on DocEE (Yang et al., 2023a).",
            "Recent works such as (Tong et al., 2022; Yang et al., 2023a) have shown that extracting event arguments from long documents given limited training data in a new domain remains extremely challenging."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Few-Shot Document-Level Event Argument Extraction",
            "abstract": "Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is limited in production.To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing document-level event extraction dataset. We first define the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling instead of the traditional N-Way-K-Shot strategy. Then we adjust the current document-level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find this novel task to be very challenging with substantively low performance. Considering FewDocAE is closely related to practical use under low-resource regimes, we hope this benchmark encourages more research in this direction. Our data and codes will be available online.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2145170944",
                "name": "Xianjun Yang"
              },
              {
                "authorId": "2140021277",
                "name": "Yujie Lu"
              },
              {
                "authorId": "21038849",
                "name": "Linda Petzold"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252391326,
          "isinfluential": false,
          "contexts": [
            "The few prior works in DocEAE augmentation do not generate new data samples but rather augment existing samples (Liu et al., 2022) or use a pre-trained model to weakly label unannotated corpora (Liu et al., 2021).",
            "In (Liu et al., 2022), they use pre-trained language models to augment existing samples by masking annotated argument spans and then generating alternate ones."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level event argument extraction with self-augmentation and a cross-domain joint training mechanism",
            "abstract": "",
            "year": 2022,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "2113437300",
                "name": "Chen Liang"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 255522592,
          "isinfluential": true,
          "contexts": [
            "We note that the use of two baseline methods is in-line with prior works on data augmentation for Event Extraction (Gao et al., 2022).",
            "Event Argument Extraction (EAE) is a challenging subtask of the Event Extraction (EE) problem, where the goal is to (i) identify which event(s) occur in the text and (ii) extract event arguments from the text as a structure.",
            "Gao et al. (2022) do the opposite by first masking unannotated spans, then using a pre-trained T5 model to replace such spans as data augmentation.",
            "For example, many related works in EAE data augmentation such as (Gao et al., 2022; Ma et al., 2023; Wang et al., 2023) only focus on sentence-level tasks."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction",
            "abstract": "We present Mask-then-Fill, a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first randomly masks out an adjunct sentence fragment and then infills a variable-length text span with a fine-tuned infilling model. The main advantage lies in that it can replace a fragment of arbitrary length in the text with another fragment of variable length, compared to the existing methods which can only replace a single word or a fixed-length fragment. On trigger and argument extraction tasks, the proposed framework is more effective than baseline methods and it demonstrates particularly strong results in the low-resource setting. Our further analysis shows that it achieves a good balance between diversity and distributional similarity.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1741104",
                "name": "Jun Gao"
              },
              {
                "authorId": "49778308",
                "name": "Changlong Yu"
              },
              {
                "authorId": "2158630134",
                "name": "Wei Wang"
              },
              {
                "authorId": "46430770",
                "name": "Huan Zhao"
              },
              {
                "authorId": "2115804042",
                "name": "Ruifeng Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258865260,
          "isinfluential": false,
          "contexts": [
            "This restricts our capacity to evaluate on trigger-dependent EAE baselines (Parekh et al., 2023; Huang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles",
            "abstract": "Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites aimed at evaluating models’ ability to handle limited data and unseen event type generalization. We benchmark six EAE models from various families. The results show that owing to non-entity argument roles, even the best-performing model can only achieve 39% F1 score, indicating how GENEVA provides new challenges for generalization in EAE. Overall, our large and diverse EAE ontology can aid in creating more comprehensive future resources, while GENEVA is a challenging benchmarking dataset encouraging further research for improving generalizability in EAE. The code and data can be found at https://github.com/PlusLabNLP/GENEVA.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46719088",
                "name": "Tanmay Parekh"
              },
              {
                "authorId": "34809425",
                "name": "I-Hung Hsu"
              },
              {
                "authorId": "3137324",
                "name": "Kuan-Hao Huang"
              },
              {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258967387,
          "isinfluential": false,
          "contexts": [
            "In recent years, there has been considerable progress in the domain of Event Argument Extraction (EAE), as advances in question answering (Du and Cardie, 2020), prompt tuning (Ma et al., 2022), and semantic graph modeling (Yang et al., 2023b) have led to state-of-the-art results on EAE benchmarks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An AMR-based Link Prediction Approach for Document-level Event Argument Extraction",
            "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2145435513",
                "name": "Yuqing Yang"
              },
              {
                "authorId": "3187768",
                "name": "Qipeng Guo"
              },
              {
                "authorId": "12040998",
                "name": "Xiangkun Hu"
              },
              {
                "authorId": "39939186",
                "name": "Yue Zhang"
              },
              {
                "authorId": "1767521",
                "name": "Xipeng Qiu"
              },
              {
                "authorId": "1852415",
                "name": "Zheng Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259137390,
          "isinfluential": false,
          "contexts": [
            "We use an implementation of F1 score for EAE from the OmniEvent library (Peng et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation",
            "abstract": "Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OMNIEVENT, which can be obtained from https://github.com/THU-KEG/OmniEvent.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "47837854",
                "name": "Hao Peng"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "2218965360",
                "name": "Feng Yao"
              },
              {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
              },
              {
                "authorId": "2055765060",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              },
              {
                "authorId": null,
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2211946754",
                "name": "Weixing Shen"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "RAMS only uses documents with only 5-sentences and WikiEvents contains annotation for only a limited number of documents.",
            "However, RAMS and WikiEvents have significant limitations, as discussed in (Tong et al., 2022).",
            "More recently, some work has focused on document-level datasets such as RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021), and DocEE (Tong et al., 2022)."
          ],
          "intents": [
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "This restricts our capacity to evaluate on trigger-dependent EAE baselines (Parekh et al., 2023; Huang et al., 2023).",
            "For example, (Huang et al., 2023) proposed a new metric (ARG-I+) for trigger-based event extraction to better capture whether a role is associated with the correct trigger."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "248496614": {
      "citing_paper_info": {
        "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
        "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
        "year": 2022,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "1748844142",
            "name": "Runxin Xu"
          },
          {
            "authorId": "144202874",
            "name": "Peiyi Wang"
          },
          {
            "authorId": "1701889",
            "name": "Tianyu Liu"
          },
          {
            "authorId": "48486877",
            "name": "Shuang Zeng"
          },
          {
            "authorId": "7267809",
            "name": "Baobao Chang"
          },
          {
            "authorId": "3335836",
            "name": "Zhifang Sui"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 16,
        "unique_cited_count": 15,
        "influential_count": 3,
        "detailed_records_count": 16
      },
      "cited_papers": [
        "6452487",
        "220046861",
        "218630327",
        "216562330",
        "196178503",
        "202542357",
        "202539732",
        "220524732",
        "15552794",
        "236460308",
        "119308902",
        "231728756",
        "269498086",
        "52967399",
        "204960716"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "Chen et al. (2015) ﬁrstly propose a neural pipeline model to extract events, while Nguyen et al. (2016) utilize a joint model to mitigate error propagation."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15552794,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2014) and Judea and Strube (2016) use handcrafted features to extract events from the sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Constructing Information Networks Using One Single Model",
            "abstract": "In this paper, we propose a new framework that unifies the output of three information extraction (IE) tasks - entity mentions, relations and events as an information network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many relations can now be considered as the resultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and significantly advances state-of-the-art end-toend event argument extraction.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144873792",
                "name": "Yu Hong"
              },
              {
                "authorId": "1695451",
                "name": "Sujian Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "In our implementation, we use BERT base (Devlin et al., 2019) and RoBERTa large (Liu et al., 2019) as our backbone encoder for T SAR , with global and local encoders sharing parameters.",
            "In our implementation, we use BERTbase (Devlin et al., 2019) and RoBERTalarge (Liu et al."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "Some studies first identify entities in the document, followed by assigning these entities as specific argument roles (Yang et al., 2018; Zheng et al., 2019; Xu et al., 2021).",
            "Some studies ﬁrst identify entities in the document, followed by assigning these entities as speciﬁc argument roles (Yang et al., 2018; Zheng et al., 2019; Xu et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196178503,
          "isinfluential": false,
          "contexts": [
            "Data augmentation is also considered (Yang et al., 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
            "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2812772",
                "name": "Sen Yang"
              },
              {
                "authorId": "49732389",
                "name": "Dawei Feng"
              },
              {
                "authorId": "2570205",
                "name": "Linbo Qiao"
              },
              {
                "authorId": "150356963",
                "name": "Zhigang Kan"
              },
              {
                "authorId": "144032853",
                "name": "Dongsheng Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539732,
          "isinfluential": false,
          "contexts": [
            "Some studies ﬁrst identify entities in the document, followed by assigning these entities as speciﬁc argument roles (Yang et al., 2018; Zheng et al., 2019; Xu et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs",
            "abstract": "Accelerating research in the emerging field of deep graph learning requires new tools. Such systems should support graph as the core abstraction and take care to maintain both forward (i.e. supporting new research ideas) and backward (i.e. integration with existing components) compatibility. In this paper, we present Deep Graph Library (DGL). DGL enables arbitrary message handling and mutation operators, flexible propagation rules, and is framework agnostic so as to leverage high-performance tensor, autograd operations, and other feature extraction modules already available in existing frameworks. DGL carefully handles the sparse and irregular graph structure, deals with graphs big and small which may change dynamically, fuses operations, and performs auto-batching, all to take advantages of modern hardware. DGL has been tested on a variety of models, including but not limited to the popular Graph Neural Networks (GNN) and its variants, with promising speed, memory footprint and scalability.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1508337194",
                "name": "Minjie Wang"
              },
              {
                "authorId": "2490788",
                "name": "Lingfan Yu"
              },
              {
                "authorId": "122579067",
                "name": "Da Zheng"
              },
              {
                "authorId": "47594426",
                "name": "Quan Gan"
              },
              {
                "authorId": "2851123",
                "name": "Yujie Gai"
              },
              {
                "authorId": "3060913",
                "name": "Zihao Ye"
              },
              {
                "authorId": "2112144150",
                "name": "Mufei Li"
              },
              {
                "authorId": "9695889",
                "name": "Jinjing Zhou"
              },
              {
                "authorId": "2111041646",
                "name": "Qi Huang"
              },
              {
                "authorId": "2112662910",
                "name": "Chao Ma"
              },
              {
                "authorId": "2116357414",
                "name": "Ziyue Huang"
              },
              {
                "authorId": "3187768",
                "name": "Qipeng Guo"
              },
              {
                "authorId": "145140331",
                "name": "Haotong Zhang"
              },
              {
                "authorId": "49955730",
                "name": "Haibin Lin"
              },
              {
                "authorId": "7818229",
                "name": "J. Zhao"
              },
              {
                "authorId": "46276184",
                "name": "Jinyang Li"
              },
              {
                "authorId": "46234526",
                "name": "Alex Smola"
              },
              {
                "authorId": "38448016",
                "name": "Zheng Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202542357,
          "isinfluential": false,
          "contexts": [
            "From the global perspective, we first construct the global AMR graphs by fully connecting the root nodes of AMR graphs of different sentences, since the root nodes contain the core semantics according to the AMR core-semantic principle (Cai and Lam, 2019) 1."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Core Semantic First: A Top-down Approach for AMR Parsing",
            "abstract": "We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The core semantic first principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "46689291",
                "name": "Deng Cai"
              },
              {
                "authorId": "1380007189",
                "name": "W. Lam"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "436 These results suggest that encoding the document 437 in a two-stream way, and introducing AMR graphs 438 to facilitate interactions, is beneficial to capturing 439\n2We use TSARlarge based on RoBERTalarge to compare with BART-Gen based on BARTlarge, as they are pre-trained on the same corpus with the same batch size and training steps.\nintra-sentential and inter-sentential features, and440 thus improves the performance.441 Moreover, we follow Li et al. (2021) to evaluate442 both argument identification and argument classifi-443 cation, and report the Head F1 and Coref F1.",
            "5) BART-Gen (Li et al., 2021) for- 423 mulate the task as a sequence-to-sequence task and 424 uses BARTlarge (Lewis et al., 2020) to generate 425 corresponding arguments in a predefined format."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "BERT-QA and BERT-QA-Doc extract run on sentence-level and document-level, respectively.",
            "4) BERT-QA (Du and Cardie, 2020c) a QA-based model.",
            "4) BERT-QA (Du and Cardie, 2020c) is also a QA-based model."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": false,
          "contexts": [
            "with BIO-schema (Du and Cardie, 2020a; Veyseh et al., 2021), while span-based methods predict the argument role for candidate text spans which usually have a maximum length limitation (Ebner et al.",
            "Tagging-based methods directly conduct se- 162 quence labeling for each token in the document 163 with BIO-schema (Du and Cardie, 2020a; Veyseh 164 et al., 2021), while span-based methods predict the 165 argument role for candidate text spans which usu- 166 ally have a maximum length limitation (Ebner et al., 167\n2020; Zhang et al., 2020b)."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": true,
          "contexts": [
            "2) Two-Step (Zhang et al., 2020b) is a span-based method, which ﬁrst identiﬁes the head word of possible argument span, and then extends to the full span.",
            "It helps to transform the unstructured text into structured event knowledge that can be further utilized in recommendation systems (Li et al., 2020), dialogue systems (Zhang et al., 2020a), and so on.",
            "2) Two-Step (Zhang et al., 2020b) is a span-based method, which first identifies the head word of possible argument span, and then extends to the full span.",
            "Instead, span-based methods predict argu-ment roles for candidate spans (Ebner et al., 2020; Zhang et al., 2020b).",
            "Instead, span-based methods predict argument roles for candidate spans (Ebner et al., 2020; Zhang et al., 2020b).",
            "Although Two-Step and BART-Gen wrongly predict the place as Iraq and Syria , and Two-Step even fails to extract the Attacker , T SAR manage to extract the cross-sentence arguments.",
            "Following Zhang et al. (2020b), we report the Span F1 and Head F1 for RAMS dataset.",
            "…directly conduct sequence labeling for each token in the document End: with BIO-schema (Du and Cardie, 2020a; Veyseh et al., 2021), while span-based methods predict the argument role for candidate text spans which usually have a maximum length limitation (Ebner et al., 2020; Zhang et al., 2020b).",
            ", 2021), while span-based methods predict the argument role for candidate text spans which usually have a maximum length limitation (Ebner et al., 2020; Zhang et al., 2020b).",
            "BERT-CRF TCD and Two-Step TCD refers to adopting Type-Constraint Decoding mechanism as used in (Ebner et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "--",
            "['methodology']",
            "['methodology']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220524732,
          "isinfluential": true,
          "contexts": [
            "2) Two-Step (Zhang et al., 2020b) is a span-based method, which ﬁrst identiﬁes the head word of possible argument span, and then extends to the full span.",
            "It helps to transform the unstructured text into structured event knowledge that can be further utilized in recommendation systems (Li et al., 2020), dialogue systems (Zhang et al., 2020a), and so on.",
            "Instead, span-based methods predict argu-ment roles for candidate spans (Ebner et al., 2020; Zhang et al., 2020b).",
            ", 2020), dialogue systems (Zhang et al., 2020a), and so on.",
            "Although Two-Step and BART-Gen wrongly predict the place as Iraq and Syria , and Two-Step even fails to extract the Attacker , T SAR manage to extract the cross-sentence arguments.",
            "Following Zhang et al. (2020b), we report the Span F1 and Head F1 for RAMS dataset.",
            "…directly conduct sequence labeling for each token in the document End: with BIO-schema (Du and Cardie, 2020a; Veyseh et al., 2021), while span-based methods predict the argument role for candidate text spans which usually have a maximum length limitation (Ebner et al., 2020; Zhang et al., 2020b).",
            "BERT-CRF TCD and Two-Step TCD refers to adopting Type-Constraint Decoding mechanism as used in (Ebner et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Diagnostic Prediction with Sequence-of-setsRepresentation Learning for Clinical Events",
            "abstract": "Electronic health records (EHRs) contain both ordered and unordered chronologies of clinical events that occur during a patient encounter. However, during data preprocessing steps, many predictive models impose a predefined order on unordered clinical events sets (e.g., alphabetical, natural order from the chart, etc.), which is potentially incompatible with the temporal nature of the sequence and predictive task. To address this issue, we proposeDPSS, which seeks to capture each patient's clinical event records as sequences of event sets. Foreach clinical event set, we assume that the predictive model should be invariant to the order of concurrent events and thus employ a novel permutation sampling mechanism. This paper evaluates the use of this permuted sampling method given different data-driven models for predicting a heart failure (HF) diagnosis in sub-sequent patient visits. Experimental results using the MIMIC-III dataset show that the permutation sampling mechanism offers improved discriminative power based on the area under the receiver operating curve (AUROC) and precision-recall curve (pr-AUC) metrics as HF diagnosis prediction becomes more robust to different data ordering schemes.",
            "year": 2020,
            "venue": "medRxiv",
            "authors": [
              {
                "authorId": "2146331536",
                "name": "Tianran Zhang"
              },
              {
                "authorId": "1998918",
                "name": "Muhao Chen"
              },
              {
                "authorId": "144217742",
                "name": "Alex A. T. Bui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "Another line of studies reformulate the task as a sequence-to-sequence task (Du et al., 2021a,b; Li et al., 2021), or machine reading comprehension task (Wei et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460308,
          "isinfluential": false,
          "contexts": [
            "Another line of studies reformulate the task as a sequence-to-sequence task (Du et al., 2021a,b; Li et al., 2021), or machine reading comprehension task (Wei et al., 2021).",
            ", 2021), or machine reading comprehension task (Wei et al., 2021).",
            "3) FEAE (Wei et al., 2021), Frame-aware Event Argument Extraction, is a concurrent work based on question answering."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "31407139",
                "name": "Kaiwen Wei"
              },
              {
                "authorId": "2946890",
                "name": "Xian Sun"
              },
              {
                "authorId": "151473773",
                "name": "Zequn Zhang"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "2116390646",
                "name": "Zhi Guo"
              },
              {
                "authorId": "2152163772",
                "name": "Li Jin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269498086,
          "isinfluential": false,
          "contexts": [
            "To better model the interactions among words, Liu et al. (2018); Yan et al. (2019); Ma et al. (2020) make use of the dependency tree, and Wadden et al. (2019) enumer-ates all possible spans and propagate information in the span graph."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "As791 the number of AMR relation types is large, which792 results in too many demanded parameters, we fol-793 low Zhang and Ji (2021) to cluster the relation types794 into main categories as shown in Table 5.795\nB Statistics of Datasets796\nThe detailed data statistics of RAMS and797 WikiEvents…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "278287001": {
      "citing_paper_info": {
        "title": "Efficient Document-level Event Relation Extraction",
        "abstract": "Event Relation Extraction (ERE) predicts temporal and causal relationships between events, playing a crucial role in constructing comprehensive event knowledge graphs. However, existing approaches based on pairwise comparisons often suffer from computational inefficiency, particularly at the document level, due to the quadratic operations required. Additionally, the predominance of unrelated events also leads to largely skewed data distributions. In this paper, we propose an innovative two-stage framework to tackle the challenges, consisting of a retriever to identify the related event pairs and a cross-encoder to classify the relationships between the retrieved pairs. Evaluations across representative benchmarks demonstrate our approach achieves better efficiency and significantly better performance. We also investigate leveraging event coreference chains for ERE and demonstrate their effectiveness.",
        "year": 2025,
        "venue": "Workshop on Representation Learning for NLP",
        "authors": [
          {
            "authorId": "2316953868",
            "name": "Ruochen Li"
          },
          {
            "authorId": "2358983718",
            "name": "Zimu Wang"
          },
          {
            "authorId": "2319193188",
            "name": "Xinya Du"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 7,
        "influential_count": 2,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "32821791",
        "198953378",
        "236460024",
        "109933467",
        "220484653",
        "265212914",
        "273185680"
      ],
      "citation_details": [
        {
          "citedcorpusid": 32821791,
          "isinfluential": false,
          "contexts": [
            "We compare our method against various baselines: BiLSTM (Cheng and Miyao, 2017) captures the dependency paths between events."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths",
            "abstract": "Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a “common root” assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.).",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49412583",
                "name": "Fei Cheng"
              },
              {
                "authorId": "1768065",
                "name": "Yusuke Miyao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 109933467,
          "isinfluential": false,
          "contexts": [
            "LIP (Gao et al., 2019) combines document structure with textual content, identifying nuanced event relations using structural patterns.",
            "Experimental results are depicted in Tables 1 and 2, from which we have the following observations: Firstly, the introduce of retriever significantly enhances cross-encoder performance, with BERT and RoBERTa outperforming more complex models like LIP and RichGCN."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Modeling Document-level Causal Structures for Event Causal Relation Identification",
            "abstract": "We aim to comprehensively identify all the event causal relations in a document, both within a sentence and across sentences, which is important for reconstructing pivotal event structures. The challenges we identified are two: 1) event causal relations are sparse among all possible event pairs in a document, in addition, 2) few causal relations are explicitly stated. Both challenges are especially true for identifying causal relations between events across sentences. To address these challenges, we model rich aspects of document-level causal structures for achieving comprehensive causal relation identification. The causal structures include heavy involvements of document-level main events in causal relations as well as several types of fine-grained constraints that capture implications from certain sentential syntactic relations and discourse relations as well as interactions between event causal relations and event coreference relations. Our experimental results show that modeling the global and fine-grained aspects of causal structures using Integer Linear Programming (ILP) greatly improves the performance of causal relation identification, especially in identifying cross-sentence causal relations.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "48204509",
                "name": "Lei Gao"
              },
              {
                "authorId": "3466801",
                "name": "Prafulla Kumar Choubey"
              },
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": true,
          "contexts": [
            "We employ diverse retrievers (RoBERTa-Large, Liu et al., 2019) and S-BERT (Reimers and Gurevych, 2019)) and classifiers (RoBERTa-Large and T5-Large (Raffel et al., 2020)).",
            "Furthermore, the performance with the RoBERTa retriever gains more improvements, even outperforming S-BERT, possibly because RoBERTa are more proficient to leverage deep contextual insights from coreference chains.",
            "BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are transformer-based discriminative models, and T5 (Raffel et al., 2020) is a transformer-based generative model.",
            "( 2) For the second stage we employ the representative RoBERTa and T5 cross-encoders.",
            "Experimental results are depicted in Tables 1 and 2, from which we have the following observations: Firstly, the introduce of retriever significantly enhances cross-encoder performance, with BERT and RoBERTa outperforming more complex models like LIP and RichGCN."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220484653,
          "isinfluential": false,
          "contexts": [
            "Recent progress of ERE has been made based on pre-trained language models (PLMs), utilizing semantic structures (Tran Phu and Nguyen, 2021; Hu et al., 2023), temporal clues (Wen and Ji, 2021), and external knowledge (Liu et al., 2020a; Cao et al., 2021) to enrich the event representations."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Knowledge Enhanced Event Causality Identification with Mention Masking Generalizations",
            "abstract": "Identifying causal relations of events is a crucial language understanding task. Despite many efforts for this task, existing methods lack the ability to adopt background knowledge, and they typically generalize poorly to new, previously unseen data. In this paper, we present a new method for event causality identification, aiming to address limitations of previous methods. On the one hand, our model can leverage external knowledge for reasoning, which can greatly enrich the representation of events; On the other hand, our model can mine event-agnostic, context-specific patterns, via a mechanism called event mention masking generalization, which can greatly enhance the ability of our model to handle new, previously unseen cases. In experiments, we evaluate our model on three benchmark datasets and show our model outperforms previous methods by a significant margin. Moreover, we perform 1) cross-topic adaptation, 2) exploiting unseen predicates, and 3) cross-task adaptation to evaluate the generalization ability of our model. Experimental results show that our model demonstrates a definite advantage over previous methods.",
            "year": 2020,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "48211977",
                "name": "Jian Liu"
              },
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460024,
          "isinfluential": false,
          "contexts": [
            "Recent progress of ERE has been made based on pre-trained language models (PLMs), utilizing semantic structures (Tran Phu and Nguyen, 2021; Hu et al., 2023), temporal clues (Wen and Ji, 2021), and external knowledge (Liu et al., 2020a; Cao et al., 2021) to enrich the event representations."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks",
            "abstract": "Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 265212914,
          "isinfluential": false,
          "contexts": [
            "Previous research has mainly focused on enriching event semantics (Wen and Ji, 2021; and Nguyen, 2021), or exploiting large language models (LLMs) (Peng et al., 2023a)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
            "abstract": "In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.",
            "year": 2023,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "47837854",
                "name": "Hao Peng"
              },
              {
                "authorId": "48631777",
                "name": "Xiaozhi Wang"
              },
              {
                "authorId": "2220188202",
                "name": "Jianhui Chen"
              },
              {
                "authorId": "2266802240",
                "name": "Weikai Li"
              },
              {
                "authorId": "121817444",
                "name": "Y. Qi"
              },
              {
                "authorId": "2200687506",
                "name": "Zimu Wang"
              },
              {
                "authorId": "2220655963",
                "name": "Zhili Wu"
              },
              {
                "authorId": "10673612",
                "name": "Kaisheng Zeng"
              },
              {
                "authorId": "2258944802",
                "name": "Bin Xu"
              },
              {
                "authorId": "2055765060",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 273185680,
          "isinfluential": false,
          "contexts": [
            "…current research faces a unique challenge in inefficient learning and inference because the determination of relationships requires pairwise classification after iterating through all event pairs (Hu et al., 2023; Wang et al., 2024), which inherently exhibits quadratic time complexity."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering",
            "abstract": "As an essential task in information extraction (IE), Event-Event Causal Relation Extraction (ECRE) aims to identify and classify the causal relationships between event mentions in natural language texts. However, existing research on ECRE has highlighted two critical challenges, including the lack of document-level modeling and causal hallucinations. In this paper, we propose a Knowledge-guided binary Question Answering (KnowQA) method with event structures for ECRE, consisting of two stages: Event Structure Construction and Binary Question Answering. We conduct extensive experiments under both zero-shot and fine-tuning settings with large language models (LLMs) on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the usefulness of event structures on document-level ECRE and the effectiveness of KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only the effectiveness but also the high generalizability and low inconsistency of our method, particularly when with complete event structures after fine-tuning the models.",
            "year": 2024,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2200687506",
                "name": "Zimu Wang"
              },
              {
                "authorId": "2324921937",
                "name": "Lei Xia"
              },
              {
                "authorId": "2284640298",
                "name": "Wei Wang"
              },
              {
                "authorId": "2325116028",
                "name": "Xinya Du"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "The T5 model with the S-BERT retriever achieves the best performance on all datasets, demonstrating their superior capability in event relation classification and candidate event pair identification.",
            "We employ diverse retrievers (RoBERTa-Large, Liu et al., 2019) and S-BERT (Reimers and Gurevych, 2019)) and classifiers (RoBERTa-Large and T5-Large (Raffel et al., 2020)).",
            "We further investigate the impact of the number of candidates retrieved per event ( k ), where S-BERT re-triever and T5 classifier are used on the ESC dataset.",
            "Furthermore, the performance with the RoBERTa retriever gains more improvements, even outperforming S-BERT, possibly because RoBERTa are more proficient to leverage deep contextual insights from coreference chains.",
            "Finally, after fine-tuning the retriever model, particularly S-BERT, the DERE performance can be further improved.",
            "BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are transformer-based discriminative models, and T5 (Raffel et al., 2020) is a transformer-based generative model.",
            "( 2) For the second stage we employ the representative RoBERTa and T5 cross-encoders.",
            "Experimental results are depicted in Tables 1 and 2, from which we have the following observations: Firstly, the introduce of retriever significantly enhances cross-encoder performance, with BERT and RoBERTa outperforming more complex models like LIP and RichGCN."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "252090194": {
      "citing_paper_info": {
        "title": "Few-Shot Document-Level Event Argument Extraction",
        "abstract": "Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is limited in production.To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing document-level event extraction dataset. We first define the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling instead of the traditional N-Way-K-Shot strategy. Then we adjust the current document-level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find this novel task to be very challenging with substantively low performance. Considering FewDocAE is closely related to practical use under low-resource regimes, we hope this benchmark encourages more research in this direction. Our data and codes will be available online.",
        "year": 2022,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2145170944",
            "name": "Xianjun Yang"
          },
          {
            "authorId": "2140021277",
            "name": "Yujie Lu"
          },
          {
            "authorId": "21038849",
            "name": "Linda Petzold"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 18,
        "influential_count": 5,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "6953475",
        "202771953",
        "131774949",
        "11986411",
        "53080736",
        "233296646",
        "225039792",
        "14515377",
        "236486186",
        "250390839",
        "234742165",
        "248496614",
        "9776219",
        "50771731",
        "309759",
        "60441361",
        "2367456",
        "248512966"
      ],
      "citation_details": [
        {
          "citedcorpusid": 309759,
          "isinfluential": false,
          "contexts": [
            "Moreover, prototypical networks (ProtoNet) (Snell et al., 2017) have been proven to be very powerful for solving few-shot problems by representing each category as a prototype in both Vision (Pan et al., 2019; Dong and Xing, 2018) and NLP (Sun et al., 2019; Gao et al., 2019) domains.",
            "In order to adapt to our few-shot setting, inspired by the successful applications of the prototypical network (ProtoNet) (Snell et al., 2017) for meta-learning, we assume there exists one prototypical representation for each argument type.",
            "The ProtoNet approach (Snell et al., 2017) assumes there exists one prototypical representation for each argument class and learns a metric space where categorization is performed by labeling each query term with the value calculated from the distance between prototype representations that are…"
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Prototypical Networks for Few-shot Learning",
            "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "39770136",
                "name": "Jake Snell"
              },
              {
                "authorId": "1754860",
                "name": "Kevin Swersky"
              },
              {
                "authorId": "1804104",
                "name": "R. Zemel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2367456,
          "isinfluential": false,
          "contexts": [
            "This is opposite to joint extraction where the task is to jointly extract all events and their associated arguments all at once (Sha et al., 2018; Yang and Mitchell, 2016)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Events and Entities within a Document Context",
            "abstract": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7324641",
                "name": "Bishan Yang"
              },
              {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6953475,
          "isinfluential": false,
          "contexts": [
            "Few-shot learning (FSL) (Fei-Fei et al., 2006) is proposed to tackle",
            "Different from FSL for single sentences by traditional N -way- K - Shot sampling, a novel N -Way- D -Doc sampling strategy is proposed for our document-level task, as can be seen from the example in Fig.",
            "Few-shot learning (FSL) (Fei-Fei et al., 2006) is proposed to tackle such limitations to make machine learning models more applicable given limited annotated examples and has been used a lot in the IE area (Han et al., 2018; Ding et al., 2021; Lai et al., 2021)."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "One-shot learning of object categories",
            "abstract": "",
            "year": 2006,
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "authors": [
              {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
              },
              {
                "authorId": "2276554",
                "name": "R. Fergus"
              },
              {
                "authorId": "1690922",
                "name": "P. Perona"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9776219,
          "isinfluential": false,
          "contexts": [
            "2020) mainly focuses on sentence-level event extraction, such as the popular ACE2005 (Doddington et al. 2004) dataset.",
            "Previous research (Yang et al., 2019; Tong et al., 2020) mainly focuses on sentence-level event extraction, such as the popular ACE2005 (Dodding-ton et al., 2004) dataset."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation",
            "abstract": "",
            "year": 2004,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2862682",
                "name": "G. Doddington"
              },
              {
                "authorId": "2449760",
                "name": "A. Mitchell"
              },
              {
                "authorId": "2282719",
                "name": "Mark A. Przybocki"
              },
              {
                "authorId": "1744313",
                "name": "L. Ramshaw"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              },
              {
                "authorId": "1732071",
                "name": "R. Weischedel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11986411,
          "isinfluential": false,
          "contexts": [
            "While other datasets such as WikiEvents (Li et al., 2021) and MUC-4 (Grishman and Sundheim, 1996) only contain extremely limited types and documents, thus not suitable for our settings."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Message Understanding Conference- 6: A Brief History",
            "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",
            "year": 1996,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14515377,
          "isinfluential": false,
          "contexts": [
            "0 (Pradhan et al., 2013) named entity recognition corpora, all focus on single sentence-level semantics."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards Robust Linguistic Analysis using OntoNotes",
            "abstract": "",
            "year": 2013,
            "venue": "Conference on Computational Natural Language Learning",
            "authors": [
              {
                "authorId": "1735131",
                "name": "Sameer Pradhan"
              },
              {
                "authorId": "1719404",
                "name": "Alessandro Moschitti"
              },
              {
                "authorId": "1702849",
                "name": "Nianwen Xue"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              },
              {
                "authorId": "1737207",
                "name": "Anders Björkelund"
              },
              {
                "authorId": "143835479",
                "name": "Olga Uryupina"
              },
              {
                "authorId": null,
                "name": "Yuchen Zhang"
              },
              {
                "authorId": "2069512660",
                "name": "Z. Zhong"
              }
            ]
          }
        },
        {
          "citedcorpusid": 50771731,
          "isinfluential": false,
          "contexts": [
            "Many approaches and datasets (Petroni et al., 2018; Hürriyeto˘glu et al., 2021; Giorgi et al., 2021; Zavarella et al., 2022) across diverse domains have been proposed for document-level argument extraction to go beyond single-sentence inference."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "An Extensible Event Extraction System With Cross-Media Event Resolution",
            "abstract": "The automatic extraction of breaking news events from natural language text is a valuable capability for decision support systems. Traditional systems tend to focus on extracting events from a single media source and often ignore cross-media references. Here, we describe a large-scale automated system for extracting natural disasters and critical events from both newswire text and social media. We outline a comprehensive architecture that can identify, categorize and summarize seven different event types - namely floods, storms, fires, armed conflict, terrorism, infrastructure breakdown, and labour unavailability. The system comprises fourteen modules and is equipped with a novel coreference mechanism, capable of linking events extracted from the two complementary data sources. Additionally, the system is easily extensible to accommodate new event types. Our experimental evaluation demonstrates the effectiveness of the system.",
            "year": 2018,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "40052301",
                "name": "F. Petroni"
              },
              {
                "authorId": "32319713",
                "name": "Natraj Raman"
              },
              {
                "authorId": "2066157683",
                "name": "Timothy Nugent"
              },
              {
                "authorId": "2144369",
                "name": "Armineh Nourbakhsh"
              },
              {
                "authorId": "51132947",
                "name": "Zarko Panic"
              },
              {
                "authorId": "36532736",
                "name": "Sameena Shah"
              },
              {
                "authorId": "2227049",
                "name": "Jochen L. Leidner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53080736,
          "isinfluential": false,
          "contexts": [
            "There have been growing interests under few-shot settings for named entity recognition (Ding et al., 2021; Das et al., 2022), and relation extraction (Han et al., 2018; Popovic and Färber, 2022) under single-sentence and document-level scenarios.",
            "Few-shot learning (FSL) (Fei-Fei et al., 2006) is proposed to tackle such limitations to make machine learning models more applicable given limited annotated examples and has been used a lot in the IE area (Han et al., 2018; Ding et al., 2021; Lai et al., 2021)."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation",
            "abstract": "We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "144459859",
                "name": "Hao Zhu"
              },
              {
                "authorId": "92720938",
                "name": "Pengfei Yu"
              },
              {
                "authorId": "2124823889",
                "name": "Ziyun Wang"
              },
              {
                "authorId": "144779803",
                "name": "Y. Yao"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 60441361,
          "isinfluential": false,
          "contexts": [
            "This way we ideally reduce the risk of only representing many NOTA token by one vector as also pointed by (Allen et al., 2019), since there might be multiple NOTA Prototypes."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Infinite Mixture Prototypes for Few-Shot Learning",
            "abstract": "We propose infinite mixture prototypes to adaptively represent both simple and complex data distributions for few-shot learning. Our infinite mixture prototypes represent each class by a set of clusters, unlike existing prototypical methods that represent each class by a single cluster. By inferring the number of clusters, infinite mixture prototypes interpolate between nearest neighbor and prototypical representations, which improves accuracy and robustness in the few-shot regime. We show the importance of adaptive capacity for capturing complex data distributions such as alphabets, with 25% absolute accuracy improvements over prototypical networks, while still maintaining or improving accuracy on the standard Omniglot and mini-ImageNet benchmarks. In clustering labeled and unlabeled data by the same clustering rule, infinite mixture prototypes achieves state-of-the-art semi-supervised accuracy. As a further capability, we show that infinite mixture prototypes can perform purely unsupervised clustering, unlike existing prototypical methods.",
            "year": 2019,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "145254624",
                "name": "Kelsey R. Allen"
              },
              {
                "authorId": "1782282",
                "name": "Evan Shelhamer"
              },
              {
                "authorId": "13574658",
                "name": "Hanul Shin"
              },
              {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131774949,
          "isinfluential": false,
          "contexts": [
            "Moreover, prototypical networks (ProtoNet) (Snell et al., 2017) have been proven to be very powerful for solving few-shot problems by representing each category as a prototype in both Vision (Pan et al., 2019; Dong and Xing, 2018) and NLP (Sun et al., 2019; Gao et al., 2019) domains.",
            "Moreover, prototypical networks(ProtoNet) (Snell, Swersky, and Zemel 2017) have been proven to be very powerful for solving few-shot problems by representing each category as a prototype in both CV (Pan et al. 2019; Dong and Xing 2018) and NLP (Sun et al."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Transferrable Prototypical Networks for Unsupervised Domain Adaptation",
            "abstract": "In this paper, we introduce a new idea for unsupervised domain adaptation via a remold of Prototypical Networks, which learn an embedding space and perform classification via a remold of the distances to the prototype of each class. Specifically, we present Transferrable Prototypical Networks (TPN) for adaptation such that the prototypes for each class in source and target domains are close in the embedding space and the score distributions predicted by prototypes separately on source and target data are similar. Technically, TPN initially matches each target example to the nearest prototype in the source domain and assigns an example a ``pseudo\" label. The prototype of each class could then be computed on source-only, target-only and source-target data, respectively. The optimization of TPN is end-to-end trained by jointly minimizing the distance across the prototypes on three types of data and KL-divergence of score distributions output by each pair of the prototypes. Extensive experiments are conducted on the transfers across MNIST, USPS and SVHN datasets, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain an accuracy of 80.4% of single model on VisDA 2017 dataset.",
            "year": 2019,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "3202968",
                "name": "Yingwei Pan"
              },
              {
                "authorId": "145690248",
                "name": "Ting Yao"
              },
              {
                "authorId": "3431141",
                "name": "Yehao Li"
              },
              {
                "authorId": "40457242",
                "name": "Yu Wang"
              },
              {
                "authorId": "143977389",
                "name": "C. Ngo"
              },
              {
                "authorId": "144025741",
                "name": "Tao Mei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202771953,
          "isinfluential": false,
          "contexts": [
            "Event argument extraction (EAE), a sub-task of event extraction, is a fundamental task for many downstream NLP applications in the IE community.",
            "Moreover, prototypical networks (ProtoNet) (Snell et al., 2017) have been proven to be very powerful for solving few-shot problems by representing each category as a prototype in both Vision (Pan et al., 2019; Dong and Xing, 2018) and NLP (Sun et al., 2019; Gao et al., 2019) domains.",
            "2019; Dong and Xing 2018) and NLP (Sun et al. 2019; Gao et al. 2019) domains."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Attention Prototypical Networks for Few-Shot Text Classification",
            "abstract": "Most of the current effective methods for text classification tasks are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available. In this work, we propose a hierarchical attention prototypical networks (HAPN) for few-shot text classification. We design the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of semantic space, so it can highlight or weaken the importance of the features, words, and instances separately. We verify the effectiveness of our model on two standard benchmark few-shot text classification datasets—FewRel and CSID, and achieve the state-of-the-art performance. The visualization of hierarchical attention layers illustrates that our model can capture more important features, words, and instances. In addition, our attention mechanism increases support set augmentability and accelerates convergence speed in the training stage.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2156300",
                "name": "Shengli Sun"
              },
              {
                "authorId": "2112549330",
                "name": "Qingfeng Sun"
              },
              {
                "authorId": "2075360696",
                "name": "Kevin Zhou"
              },
              {
                "authorId": "1379581011",
                "name": "Tengchao Lv"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039792,
          "isinfluential": false,
          "contexts": [
            "There has also been research (Deng et al., 2020; Lai et al., 2021; Feng et al., 2020; Lai et al., 2020) for few-shot event extraction within single-sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction",
            "abstract": "We study the problem of event extraction from text data, which requires both detecting target event types and their arguments. Typically, both the event detection and argument detection subtasks are formulated as supervised sequence labeling problems. We argue that the event extraction models so trained are inherently label-hungry, and can generalize poorly across domains and text genres.We propose a reading comprehension framework for event extraction.Specifically, we formulate event detection as a textual entailment prediction problem, and argument detection as a question answer-ing problem. By constructing proper query templates, our approach can effectively distill rich knowledge about tasks and label semantics from pretrained reading comprehension models. Moreover, our model can be fine-tuned with a small amount of data to boost its performance. Our experiment results show that our method performs strongly for zero-shot and few-shot event extraction, and it achieves state-of-the-art performance on the ACE 2005 benchmark when trained with full supervision.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2054599518",
                "name": "Rui Feng"
              },
              {
                "authorId": "2118573410",
                "name": "Jie Yuan"
              },
              {
                "authorId": "2152737282",
                "name": "Chao Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233296646,
          "isinfluential": true,
          "contexts": [
            "Instead of initializing the NONA vectors by randomly as by (Sabo et al., 2021) or from sampled support sets as in (Popovic and Färber, 2022) and then gradually update them, we adopt a K-means MNAV strategy.",
            "We believe that this is a big challenge for few-shot document-level tasks, which is not only a issue for small language models (Sabo et al., 2021) but also true for large models like GPT-3 (Brown et al., 2020) and leave more exploration for future work.",
            "This extremely unbalanced setting is a good testbed for validating the model ability due to its similar distribution of many real-world few-shot problems, as also pointed by (Sabo et al., 2021).",
            "Here we adjust the Multiple NOTA(None-Of-the-Above) Vectors(MNAV) proposed by (Sabo et al., 2021) for few-shot relation extraction to our Few-DocAE since they both face the same issue that the majority labels belong to NOTA.",
            "Instead of building a new dataset from scratch, we aim at leveraging the existing supervised dataset for reconstructing the instances by a novel N -Way- D -Doc sampling strategy, inspired by similar work (Sabo et al., 2021; Popovic and Färber, 2022).",
            "Similar memory issue has also been reported by (Sabo et al., 2021) when handling few-shot learning for relation extraction."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes",
            "abstract": "We explore few-shot learning (FSL) for relation classification (RC). Focusing on the realistic scenario of FSL, in which a test instance might not belong to any of the target categories (none-of-the-above, [NOTA]), we first revisit the recent popular dataset structure for FSL, pointing out its unrealistic data distribution. To remedy this, we propose a novel methodology for deriving more realistic few-shot test data from available datasets for supervised RC, and apply it to the TACRED dataset. This yields a new challenging benchmark for FSL-RC, on which state of the art models show poor performance. Next, we analyze classification schemes within the popular embedding-based nearest-neighbor approach for FSL, with respect to constraints they impose on the embedding space. Triggered by this analysis, we propose a novel classification scheme in which the NOTA category is represented as learned vectors, shown empirically to be an appealing option for FSL.",
            "year": 2021,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "51032559",
                "name": "O. Sabo"
              },
              {
                "authorId": "51131518",
                "name": "Yanai Elazar"
              },
              {
                "authorId": "79775260",
                "name": "Yoav Goldberg"
              },
              {
                "authorId": "7465342",
                "name": "Ido Dagan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 234742165,
          "isinfluential": true,
          "contexts": [
            "Soft sampling methods like N -Way- K ∼ 2 K Shots in (Ding et al., 2021) still do not work for our long documents since K ∼ 2 K Shots are still hard to be satisfied.",
            "The first one masks all arguments in the training set with O if they also appear in the val and test sets as used in (Ding et al. 2021), while in the second strategy, we can mask all the arguments in the val and test sets if they are shared by the training set.",
            "Soft sampling methods like N -Way-K∼2K Shots in (Ding et al. 2021) still do not work for our long documents since K∼2K Shots are still hard to be satisfied.",
            "Previous research (Yang and Katiyar, 2020) tries to use greedy sampling to guarantee the strict K shots requirements for sentence-level few-shot NER task, but this is not applicable due to the sparse density of arguments in the document as also been observed by (Ding et al., 2021).",
            "The latest Few-NERD (Ding et al. 2021) named entity recognition dataset also includes information across sentences.",
            "Previous research (Yang and Katiyar 2020) tries to use greedy sampling to guarantee the strict K shots requirements for sentencelevel few-shot NER task, but this is not applicable due to the sparse density of arguments in the document as also been observed by (Ding et al. 2021).",
            "Few-shot learning (FSL) (Fei-Fei et al., 2006) is proposed to tackle such limitations to make machine learning models more applicable given limited annotated examples and has been used a lot in the IE area (Han et al., 2018; Ding et al., 2021; Lai et al., 2021).",
            "There have been growing interests under few-shot settings for named entity recognition (Ding et al., 2021; Das et al., 2022), and relation extraction (Han et al., 2018; Popovic and Färber, 2022) under single-sentence and document-level scenarios.",
            "There have been growing interests under few-shot settings for named entity recognition (Ding et al. 2021; Das et al. 2022), and relation extraction (Han et al.",
            "And (Ding et al. 2021) lose the K shots requirement to K∼2K shots.",
            "The first one masks all arguments in the training set with O if they also appear in the val and test sets as used in (Ding et al., 2021), while in the second strategy, we can mask all the arguments in the val and test sets if they are shared by the training set."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Few-NERD: A Few-shot Named Entity Recognition Dataset",
            "abstract": "Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46649145",
                "name": "Ning Ding"
              },
              {
                "authorId": "2149131512",
                "name": "Guangwei Xu"
              },
              {
                "authorId": "2135835258",
                "name": "Yulin Chen"
              },
              {
                "authorId": "2108114693",
                "name": "Xiaobin Wang"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "35930962",
                "name": "Pengjun Xie"
              },
              {
                "authorId": "16215052",
                "name": "Haitao Zheng"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236486186,
          "isinfluential": false,
          "contexts": [
            "Many approaches and datasets (Petroni et al., 2018; Hürriyetoğlu et al., 2021; Giorgi et al., 2021; Zavarella et al., 2022) across diverse domains have been proposed for document-level argument extraction to go beyond single-sentence inference.",
            "Many approaches and datasets (Petroni et al., 2018; Hürriyeto˘glu et al., 2021; Giorgi et al., 2021; Zavarella et al., 2022) across diverse domains have been proposed for document-level argument extraction to go beyond single-sentence inference."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multilingual Protest News Detection - Shared Task 1, CASE 2021",
            "abstract": "Benchmarking state-of-the-art text classification and information extraction systems in multilingual, cross-lingual, few-shot, and zero-shot settings for socio-political event information collection is achieved in the scope of the shared task Socio-political and Crisis Events Detection at the workshop CASE @ ACL-IJCNLP 2021. Socio-political event data is utilized for national and international policy- and decision-making. Therefore, the reliability and validity of these datasets are of the utmost importance. We split the shared task into three parts to address the three aspects of data collection (Task 1), fine-grained semantic classification (Task 2), and evaluation (Task 3). Task 1, which is the focus of this report, is on multilingual protest news detection and comprises four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence coreference identification (subtask 3), and event extraction (subtask 4). All subtasks had English, Portuguese, and Spanish for both training and evaluation data. Data in Hindi language was available only for the evaluation of subtask 1. The majority of the submissions, which are 238 in total, are created using multi- and cross-lingual approaches. Best scores are above 77.27 F1-macro for subtask 1, above 85.32 F1-macro for subtask 2, above 84.23 CoNLL 2012 average score for subtask 3, and above 66.20 F1-macro for subtask 4 in all evaluation settings. The performance of the best system for subtask 4 is above 66.20 F1 for all available languages. Although there is still a significant room for improvement in cross-lingual and zero-shot settings, the best submissions for each evaluation scenario yield remarkable results. Monolingual models outperformed the multilingual models in a few evaluation scenarios.",
            "year": 2021,
            "venue": "CASE",
            "authors": [
              {
                "authorId": "79828215",
                "name": "Ali Hürriyetoǧlu"
              },
              {
                "authorId": "1905627896",
                "name": "Osman Mutlu"
              },
              {
                "authorId": "80471629",
                "name": "E. Yörük"
              },
              {
                "authorId": "3445542",
                "name": "Farhana Ferdousi Liza"
              },
              {
                "authorId": "2107939988",
                "name": "Ritesh Kumar"
              },
              {
                "authorId": "2121380947",
                "name": "Shyam Ratan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496614,
          "isinfluential": true,
          "contexts": [
            "Following these datasets, many novel methods for solving such new challenges brought by the longer context have also been investigated and witness significant progress (Du and Cardie, 2020; Li et al., 2021; Xu et al., 2022)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction",
            "abstract": "Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "144202874",
                "name": "Peiyi Wang"
              },
              {
                "authorId": "1701889",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248512966,
          "isinfluential": true,
          "contexts": [
            "Note that (Popovic and Färber, 2022) adopts D -doc sampling for document-level relation extraction.",
            "Instead of initializing the NONA vectors by randomly as by (Sabo et al., 2021) or from sampled support sets as in (Popovic and Färber, 2022) and then gradually update them, we adopt a K-means MNAV strategy.",
            "It is notable that (Popovic and Färber, 2022) tackle this problem for few-shot document-level relation extraction by D -Doc setting where both N and K are variables between documents and individual episodes.",
            "There have been growing interests under few-shot settings for named entity recognition (Ding et al., 2021; Das et al., 2022), and relation extraction (Han et al., 2018; Popovic and Färber, 2022) under single-sentence and document-level scenarios.",
            "Instead of building a new dataset from scratch, we aim at leveraging the existing supervised dataset for reconstructing the instances by a novel N -Way- D -Doc sampling strategy, inspired by similar work (Sabo et al., 2021; Popovic and Färber, 2022)."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Few-Shot Document-Level Relation Extraction",
            "abstract": "We present FREDo, a few-shot document-level relation extraction (FSDLRE) benchmark. As opposed to existing benchmarks which are built on sentence-level relation extraction corpora, we argue that document-level corpora provide more realism, particularly regarding none-of-the-above (NOTA) distributions. Therefore, we propose a set of FSDLRE tasks and construct a benchmark based on two existing supervised learning data sets, DocRED and sciERC. We adapt the state-of-the-art sentence-level method MNAV to the document-level and develop it further for improved domain adaptation. We find FSDLRE to be a challenging setting with interesting new characteristics such as the ability to sample NOTA instances from the support set. The data, code, and trained models are available online (https://github.com/nicpopovic/FREDo).",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "51063305",
                "name": "Nicholas Popovic"
              },
              {
                "authorId": "151112226",
                "name": "Michael Färber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250390839,
          "isinfluential": true,
          "contexts": [
            "The recent RAMS (Ebner et al., 2020) and DocEE (Tong et al., 2022) corpora focus on multi-sentence event extraction.",
            "On the other hand, the results convince our motivation by extending to document-level argument extraction as a large portion of arguments can only be extracted across sentences, as also confirmed under supervised condition by (Tong et al., 2022).",
            "For Cross domain, we follow the original event splits in DocEE (Tong et al. 2022) where the authors choose the natural disasters events as the target domain, including Floods, Droughts, Earthquakes, Insect Disaster, Famine, Tsunamis, Mudslides, Hurricanes, Fire, and Volcano Eruption, and leave the remaining 49 event types as source domains.",
            "On the other hand, the results convince our motivation by extending to document-level argument extraction as a large portion of arguments can only be extracted across sentences, as also confirmed under supervised condition by (Tong et al. 2022).",
            "traction dataset is the DocEE (Tong et al. 2022), which consists of 27, 000+ events, 180, 000+ arguments over 27, 485 Wikipedia articles.",
            "Longformer (Beltagy, Peters, and Cohan 2020)) has also been proven to improve the argument extraction task (Tong et al. 2022).",
            "EE datasets include RAMS (Ebner et al., 2020) and DocEE (Tong et al., 2022), and their statistics is shown in Table 1.",
            "Previous work on document-level EE using BERT Seq(Du and Cardie 2020; Tong et al. 2022) demonstrate the success of using a pre-trained BERT model to sequentially label words in the article.",
            "In addition, DocEE (Tong et al. 2022) includes 21, 450 documentlevel events with 109, 395 arguments, making it the largest document-level event extraction dataset.",
            "Previous work on document-level EE using BERT_Seq (Du and Cardie, 2020; Tong et al., 2022) demonstrate the success of using a pre-trained BERT model to sequentially label words in the article.",
            "Since the DocEE dataset (Tong et al. 2022) follows the main event extraction (Hamborg et al.",
            "The largest document-level event extraction dataset is the DocEE (Tong et al., 2022), which consists of 27 , 000+ events, 180 , 000+ arguments over 27 , 485 Wikipedia articles.",
            "The authors (Tong et al. 2022) provide Cross domain scenario, where the training and test labels are entirely disjoint, sharing no mutual domain information.",
            "In the released DocEE (Tong et al. 2022) corpus, there are 31 hard news event types and 28 soft news event types with their corresponding arguments.",
            "The authors (Tong et al., 2022) provide Cross domain scenario, where the training and test labels are entirely disjoint, sharing no mutual domain information.",
            "And the superior performance of the long document transformer (e.g. Longformer (Beltagy et al., 2020)) has also been proven to improve the argument extraction task (Tong et al., 2022).",
            "2020) and DocEE (Tong et al. 2022) corpora focus on multisentence event extraction.",
            "Since the DocEE dataset (Tong et al., 2022) follows the main event extraction (Hamborg et al., 2018) setting where no trigger words exist and the article title t and the article a itself together determine the event type, we follow their setting and assume the event type e is given, then aim at…"
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DocEE: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction",
            "abstract": "Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "152439499",
                "name": "Meihan Tong"
              },
              {
                "authorId": "2113744169",
                "name": "Bin Xu"
              },
              {
                "authorId": "2118512998",
                "name": "Shuai Wang"
              },
              {
                "authorId": "2175603382",
                "name": "Meihuan Han"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "1737159260",
                "name": "Jiangqi Zhu"
              },
              {
                "authorId": "2175598253",
                "name": "Siyu Chen"
              },
              {
                "authorId": "145779862",
                "name": "Lei Hou"
              },
              {
                "authorId": "2133353675",
                "name": "Juanzi Li"
              }
            ]
          }
        }
      ]
    },
    "248780133": {
      "citing_paper_info": {
        "title": "Document-Level Event Argument Extraction via Optimal Transport",
        "abstract": "Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents. Hence, in this work, we study the importance of syntactic structures in document-level EAE. Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.",
        "year": 2022,
        "venue": "Findings",
        "authors": [
          {
            "authorId": "1738693802",
            "name": "Amir Pouran Ben Veyseh"
          },
          {
            "authorId": "1789308",
            "name": "Minh Le Nguyen"
          },
          {
            "authorId": "2462276",
            "name": "Franck Dernoncourt"
          },
          {
            "authorId": "1875233",
            "name": "Bonan Min"
          },
          {
            "authorId": "1811211",
            "name": "Thien Huu Nguyen"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 15,
        "influential_count": 4,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "19235598",
        "235792444",
        "215816738",
        "226283556",
        "220048375",
        "226283579",
        "52967399",
        "202541610",
        "184487889",
        "2524712",
        "14339673",
        "54437926",
        "10743051",
        "203610361",
        "2114517"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "Motivated by the effectiveness of dependency paths for sentence-level EAE in prior work (Li et al., 2013), we employ the dependency path (DP) between the event trigger w t and the argument candidate w a in T as the anchor to prune the unrelated words.",
            "Early models for this task employed feature-based methods (Patwardhan and Riloff, 2009; Riedel and McCallum, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2524712,
          "isinfluential": false,
          "contexts": [
            "Early models for this task employed feature-based methods (Patwardhan and Riloff, 2009; Riedel and McCallum, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
            "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
            "year": 2009,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10743051,
          "isinfluential": false,
          "contexts": [
            "Early models for this task employed feature-based methods (Patwardhan and Riloff, 2009; Riedel and McCallum, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Robust Biomedical Event Extraction with Dual Decomposition and Minimal Domain Adaptation",
            "abstract": "",
            "year": 2011,
            "venue": "BioNLP@ACL",
            "authors": [
              {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Later, deep learning emerged as the state-of-the-art approach for sentence-level EE (Chen et al., 2015; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b; Lai et al., 2020; Nguyen et al., 2021; Veyseh et al., 2021) or speciﬁcally, EAE (Wang et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 19235598,
          "isinfluential": false,
          "contexts": [
            "In the next step, we feed T (cid:48) into a Graph Con-volution Network (GCN) (Kipf and Welling, 2017; Nguyen and Grishman, 2018) to learn more abstract representation vectors for the words in T (cid:48) , leveraging BiLSTM-induced vectors in H as the inputs."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Graph Convolutional Networks With Argument-Aware Pooling for Event Detection",
            "abstract": "\n \n The current neural network models for event detection have only considered the sequential representation of sentences. Syntactic representations have not been explored in this area although they provide an effective mechanism to directly link words to their informative context for event detection in the sentences. In this work, we investigate a convolutional neural network based on dependency trees to perform event detection. We propose a novel pooling method that relies on entity mentions to aggregate the convolution vectors. The extensive experiments demonstrate the benefits of the dependency-based convolutional neural networks and the entity mention-based pooling method for event detection. We achieve the state-of-the-art performance on widely used datasets with both perfect and predicted entity mentions.\n \n",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "The vector x i is constructed by concatenating the following vectors: A) Contextualized Word Embedding: We feed the input text [ CLS ] w 1 w 2 . . . w n [ SEP ] into the BERT base model (Devlin et al., 2019); we use the hidden state of w i in the ﬁnal layer as the contextualized word embedding."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54437926,
          "isinfluential": false,
          "contexts": [
            "Later, deep learning emerged as the state-of-the-art approach for sentence-level EE (Chen et al., 2015; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b; Lai et al., 2020; Nguyen et al., 2021; Veyseh et al., 2021) or speciﬁcally, EAE (Wang et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "One for All: Neural Joint Modeling of Entities and Events",
            "abstract": "The previous work for event extraction has mainly focused on the predictions for event triggers and argument roles, treating entity mentions as being provided by human annotators. This is unrealistic as entity mentions are usually predicted by some existing toolkits whose errors might be propagated to the event trigger and argument role recognition. Few of the recent work has addressed this problem by jointly predicting entity mentions, event triggers and arguments. However, such work is limited to using discrete engineering features to represent contextual information for the individual tasks and their interactions. In this work, we propose a novel model to jointly perform predictions for entity mentions, event triggers and arguments based on the shared hidden representations from deep learning. The experiments demonstrate the benefits of the proposed method, leading to the state-of-the-art performance for event extraction.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "49035085",
                "name": "T. Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 184487889,
          "isinfluential": true,
          "contexts": [
            "…that cannot ﬂexibly prune unrelated words e.g., iDepNN (Gupta et al., 2019) to prune syntactic structures along dependency paths, and EoG (Christopoulou et al., 2019) and GCNN (Sahu et al., 2019) to employ heuristic discourse information (coreference links), thus leading to inferior performance.",
            "Concretely, we compare our model with: (i) the iDepNN model in (Gupta et al., 2019) that employs the syntactic structure of the document with pruning along the dependency path; (ii) the GCNN model in (Sahu et al., 2019) that leverages both syntactic and discourse-level (i.e., co-reference links) structures to encode the document; (iii) the LSR model in (Nan et al., 2020) that infers document structures by a deep reasoning module; and (iv) the EoG model in (Christopoulou et al., 2019) that encodes syntactic and discourse structures using high dimensional vectors to represent the edges of the structure graphs.",
            "Event Extraction (EE) is one of the important sub-tasks of Information Extraction (IE).",
            "…tasks, e.g., relation extraction (RE), document structures (i.e., graphs to capture interactions between different words/sentences) have been showed to enhance representation learning (Thayaparan et al., 2019; Gupta et al., 2019; Sahu et al., 2019; Christopoulou et al., 2019; Nan et al., 2020).",
            "In particular, most baseline models employ human-designed rules to compute document structures that cannot ﬂexibly prune unrelated words e.g., iDepNN (Gupta et al., 2019) to prune syntactic structures along dependency paths, and EoG (Christopoulou et al., 2019) and GCNN (Sahu et al., 2019) to employ heuristic discourse information (coreference links), thus leading to inferior performance.",
            "Ecologists say the ﬁres pose a direct threat tothe role ofSiberianpristineBorealinabsorbingclimate-warmingemissions. ment structure-aware baselines fail (i.e., iDepNN, EoG, GCNN, and LSR).",
            "…model in (Gupta et al., 2019) that employs the syntactic structure of the document with pruning along the dependency path; (ii) the GCNN model in (Sahu et al., 2019) that leverages both syntactic and discourse-level (i.e., co-reference links) structures to encode the document; (iii) the LSR…",
            "Document structures has been also exploited for other Information Extraction (Sahu et al., 2019; Gupta et al., 2019; Nan et al., 2020; Tran et al., 2020) and NLP tasks (Pan et al., 2020; Bal-achandran et al., 2020; Zhang et al., 2020a; Lu et al., 2021)."
          ],
          "intents": [
            "['background']",
            "--",
            "--",
            "['background']",
            "--",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network",
            "abstract": "Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422905",
                "name": "Sunil Kumar Sahu"
              },
              {
                "authorId": "48810605",
                "name": "Fenia Christopoulou"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202541610,
          "isinfluential": true,
          "contexts": [
            "Concretely, we compare our model with: (i) the iDepNN model in (Gupta et al., 2019) that employs the syntactic structure of the document with pruning along the dependency path; (ii) the GCNN model in (Sahu et al., 2019) that leverages both syntactic and discourse-level (i.e., co-reference links) structures to encode the document; (iii) the LSR model in (Nan et al., 2020) that infers document structures by a deep reasoning module; and (iv) the EoG model in (Christopoulou et al., 2019) that encodes syntactic and discourse structures using high dimensional vectors to represent the edges of the structure graphs.",
            "…structures that cannot ﬂexibly prune unrelated words e.g., iDepNN (Gupta et al., 2019) to prune syntactic structures along dependency paths, and EoG (Christopoulou et al., 2019) and GCNN (Sahu et al., 2019) to employ heuristic discourse information (coreference links), thus leading to inferior…",
            "…the document; (iii) the LSR model in (Nan et al., 2020) that infers document structures by a deep reasoning module; and (iv) the EoG model in (Christopoulou et al., 2019) that encodes syntactic and discourse structures using high dimensional vectors to represent the edges of the structure…",
            "…tasks, e.g., relation extraction (RE), document structures (i.e., graphs to capture interactions between different words/sentences) have been showed to enhance representation learning (Thayaparan et al., 2019; Gupta et al., 2019; Sahu et al., 2019; Christopoulou et al., 2019; Nan et al., 2020).",
            "In particular, most baseline models employ human-designed rules to compute document structures that cannot ﬂexibly prune unrelated words e.g., iDepNN (Gupta et al., 2019) to prune syntactic structures along dependency paths, and EoG (Christopoulou et al., 2019) and GCNN (Sahu et al., 2019) to employ heuristic discourse information (coreference links), thus leading to inferior performance.",
            "Ecologists say the ﬁres pose a direct threat tothe role ofSiberianpristineBorealinabsorbingclimate-warmingemissions. ment structure-aware baselines fail (i.e., iDepNN, EoG, GCNN, and LSR).",
            "…that cannot ﬂexibly prune unrelated words e.g., iDepNN (Gupta et al., 2019) to prune syntactic structures along dependency paths, and EoG (Christopoulou et al., 2019) and GCNN (Sahu et al., 2019) to employ heuristic discourse information (coreference links), thus leading to inferior…"
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs",
            "abstract": "Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48810605",
                "name": "Fenia Christopoulou"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203610361,
          "isinfluential": false,
          "contexts": [
            "…tasks, e.g., relation extraction (RE), document structures (i.e., graphs to capture interactions between different words/sentences) have been showed to enhance representation learning (Thayaparan et al., 2019; Gupta et al., 2019; Sahu et al., 2019; Christopoulou et al., 2019; Nan et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks",
            "abstract": "Recent advances in reading comprehension have resulted in models that surpass human performance when the answer is contained in a single, continuous passage of text. However, complex Question Answering (QA) typically requires multi-hop reasoning - i.e. the integration of supporting facts from different sources, to infer the correct answer. This paper proposes Document Graph Network (DGN), a message passing architecture for the identification of supporting facts over a graph-structured representation of text. The evaluation on HotpotQA shows that DGN obtains competitive results when compared to a reading comprehension baseline operating on raw text, confirming the relevance of structured representations for supporting multi-hop reasoning.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "102669988",
                "name": "Mokanarangan Thayaparan"
              },
              {
                "authorId": "34102057",
                "name": "Marco Valentino"
              },
              {
                "authorId": "71034258",
                "name": "Viktor Schlegel"
              },
              {
                "authorId": "145528474",
                "name": "A. Freitas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215816738,
          "isinfluential": false,
          "contexts": [
            "Later, deep learning emerged as the state-of-the-art approach for sentence-level EE (Chen et al., 2015; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b; Lai et al., 2020; Nguyen et al., 2021; Veyseh et al., 2021) or speciﬁcally, EAE (Wang et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Question Answering-Based Framework for One-Step Event Argument Extraction",
            "abstract": "Event argument extraction, which aims to identify arguments of specific events and label their roles, is a challenging subtask of event extraction. Previous approaches solve this problem in a two-stage manner that first extracts named entities as argument candidates and then determines their roles. However, many nested entities may be missed or wrongly predicted during the argument candidate extraction procedure, which substantially affects the performance of the downstream classifier. In this paper, we propose a novel one-step question answering based framework, which performs argument candidate extraction and argument role classification simultaneously to mitigate the error propagation problem in conventional two-stage methods. Since the conventional question answering based framework cannot be applied directly to this task, we design a Question Answering based Sequence Labeling (QA-SL) model to tackle inexistent argument roles and multiple argument token spans. Moreover, considering the overwhelming number of parameters in question answering based neural network models and the relatively small size of event extraction corpus, we fine-tune the pre-trained model from BERT to mitigate the data scarcity problem. Extensive experiments demonstrate the benefits of the proposed method, leading to a competitive performance compared with state-of-the-art methods. To the best of our knowledge, this is the first work to cast event argument extraction as a question answering task.",
            "year": 2020,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2108289105",
                "name": "Yunyan Zhang"
              },
              {
                "authorId": "3287827",
                "name": "Guangluan Xu"
              },
              {
                "authorId": "2153675622",
                "name": "Yang Wang"
              },
              {
                "authorId": "7413451",
                "name": "Daoyu Lin"
              },
              {
                "authorId": "2146313684",
                "name": "Feng Li"
              },
              {
                "authorId": "2151103098",
                "name": "Chenglong Wu"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "143848928",
                "name": "Tinglei Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220048375,
          "isinfluential": false,
          "contexts": [
            "We employ the same data split and pre-processing scripts as prior works (Lin et al., 2020; Chen et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Joint Neural Model for Information Extraction with Global Features",
            "abstract": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2117032681",
                "name": "Ying Lin"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "143857288",
                "name": "Fei Huang"
              },
              {
                "authorId": "3008832",
                "name": "Lingfei Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226283556,
          "isinfluential": true,
          "contexts": [
            "Speciﬁcally, we compare our model with the RAMS model model in (Ebner et al., 2020), the Head-based model in (Zhang et al., 2020c) and the Joint model in (Chen et al., 2020).",
            "We compare our model with the state-of-the-art models for document-level EAE, i.e., the RAMS model (Ebner et al., 2020) and Joint (Chen et al., 2020) models in this analysis.",
            "Here in addition to OTEAE and Joint (Chen et al., 2020), we show the performance of two other models: (i) the BERT-based model that directly uses the BiLSTM vectors in H to form the overall representation vector V = [ h t , h a , MAX _ P OOL ( h 1 , . . . , h n )] for predictions (i.e., the OT-based pruning and regularization are not applied here); and (ii) the GTM model in (Veyseh et al., 2020) that currently has the current state-of-the-art model for sentence-level EAE on ACE 2005.",
            "The most similar models to our document-level EAE model involve RAMS model (Ebner et al., 2020), Head-based (Zhang et al., 2020c) and the Joint model (Chen et al., 2020).",
            "To be directly comparable with (Chen et al., 2020) (the current state-of-the-art model for document-level EAE), we assume golden event trigger and argument spans in this experiment.",
            "We employ the same data split and pre-processing scripts as prior works (Lin et al., 2020; Chen et al., 2020).",
            "To provide more insight into the performance of the proposed model OTEAE, following (Chen et al., 2020), we further examine the models’ performance on the well-known ACE 2005 dataset for the sentence-level EAE task.",
            "To the best of our knowledge, there are two prior works on document-level EAE (Ebner et al., 2020; Chen et al., 2020).",
            "Compared to the sequence-based baselines (i.e., Head-based, Joint), we attribute the success of our model to the ability to capture long-distance dependencies between words in multiple sentences (via syntactic structures) that can encode documents with richer information.",
            "Here in addition to OTEAE and Joint (Chen et al., 2020), we show the performance of two other models: (i) the BERT-based model that directly uses the BiLSTM vectors in H to form the overall representation vector V = [ h t , h a , MAX _ P OOL ( h 1 , . . . , h n )] for predictions (i.e., the…",
            "As such, the Joint model in (Chen et al., 2020) currently has the best reported performance on RAMS."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "['methodology']",
            "['result']",
            "['methodology']",
            "['methodology']",
            "['result']",
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Modeling of Arguments for Event Understanding",
            "abstract": "We recognize the task of event argument linking in documents as similar to that of intent slot resolution in dialogue, providing a Transformer-based model that extends from a recently proposed solution to resolve references to slots. The approach allows for joint consideration of argument candidates given a detected event, which we illustrate leads to state-of-the-art performance in multi-sentence argument linking.",
            "year": 2020,
            "venue": "CODI",
            "authors": [
              {
                "authorId": "104375103",
                "name": "Yunmo Chen"
              },
              {
                "authorId": "40364920",
                "name": "Tongfei Chen"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226283579,
          "isinfluential": false,
          "contexts": [
            "Document structures has been also exploited for other Information Extraction (Sahu et al., 2019; Gupta et al., 2019; Nan et al., 2020; Tran et al., 2020) and NLP tasks (Pan et al., 2020; Bal-achandran et al., 2020; Zhang et al., 2020a; Lu et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction",
            "abstract": "The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for representation learning. However, this model does not capture the representations for the nodes in the graphs, thus preventing it from effectively encoding the specific and relevant information of the nodes for DRE. To address this issue, we propose to explicitly compute the representations for the nodes in the graph-based edge-oriented model for DRE. These node representations allow us to introduce two novel representation regularization mechanisms to improve the representation vectors for DRE. The experiments show that our model achieves state-of-the-art performance on two benchmark datasets.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "144356780",
                "name": "H. Tran"
              },
              {
                "authorId": "49035085",
                "name": "T. Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235792444,
          "isinfluential": false,
          "contexts": [
            "Document structures has been also exploited for other Information Extraction (Sahu et al., 2019; Gupta et al., 2019; Nan et al., 2020; Tran et al., 2020) and NLP tasks (Pan et al., 2020; Bal-achandran et al., 2020; Zhang et al., 2020a; Lu et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Predicting Patient Readmission Risk from Medical Text via Knowledge Graph Enhanced Multiview Graph Convolution",
            "abstract": "Unplanned intensive care unit (ICU) readmission rate is an important metric for evaluating the quality of hospital care. Efficient and accurate prediction of ICU readmission risk can not only help prevent patients from inappropriate discharge and potential dangers, but also reduce associated costs of healthcare. In this paper, we propose a new method that uses medical text of Electronic Health Records (EHRs) for prediction, which provides an alternative perspective to previous studies that heavily depend on numerical and time-series features of patients. More specifically, we extract discharge summaries of patients from their EHRs, and represent them with multiview graphs enhanced by an external knowledge graph. Graph convolutional networks are then used for representation learning. Experimental results prove the effectiveness of our method, yielding state-of-the-art performance for this task.",
            "year": 2021,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "35539899",
                "name": "Qiuhao Lu"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1721158",
                "name": "D. Dou"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Note that as solving the OT problem in Equation 1 is intractable, we employ the entropy-based approximation of OT and solve it with the Sinkhorn algorithm (Peyre and Cuturi, 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "In the next step, we feed T (cid:48) into a Graph Con-volution Network (GCN) (Kipf and Welling, 2017; Nguyen and Grishman, 2018) to learn more abstract representation vectors for the words in T (cid:48) , leveraging BiLSTM-induced vectors in H as the inputs.",
            "Concretely, we compare our model with: (i) the iDepNN model in (Gupta et al., 2019) that employs the syntactic structure of the document with pruning along the dependency path; (ii) the GCNN model in (Sahu et al., 2019) that leverages both syntactic and discourse-level (i.e., co-reference links) structures to encode the document; (iii) the LSR model in (Nan et al., 2020) that infers document structures by a deep reasoning module; and (iv) the EoG model in (Christopoulou et al., 2019) that encodes syntactic and discourse structures using high dimensional vectors to represent the edges of the structure graphs.",
            "We denote the hidden vectors produced in the last layer of the GCN model GCN by: H (cid:48) = h i 1 , . . . , h i m = GCN ( H, T (cid:48) ) where m is the number of words in T (cid:48) ( m < n ) and h i k is the vector for the word w i k (i.e., the k -word in T (cid:48) ).",
            "However, due to the contextualization in the input encoder with BERT, the noisy information of unrelated words might still be included in the representations H for the selected words in the pruned tree T (cid:48) , thus being propagated by the GCN into the representations H (cid:48) .",
            "In particular, we evaluate the performance of the following ablated models: (1) Reg − : This model excludes the regularization loss, i.e., L reg , from the overall loss function L ; (2) OT − : This baseline eliminates the OT-based component for tree pruning, instead, it prunes dependency structures along dependency paths; (3) Prune − : This model employs the full dependency tree as the structure to be consumed by the GCN model.",
            "In particular, most baseline models employ human-designed rules to compute document structures that cannot ﬂexibly prune unrelated words e.g., iDepNN (Gupta et al., 2019) to prune syntactic structures along dependency paths, and EoG (Christopoulou et al., 2019) and GCNN (Sahu et al., 2019) to employ heuristic discourse information (coreference links), thus leading to inferior performance.",
            "Ecologists say the ﬁres pose a direct threat tothe role ofSiberianpristineBorealinabsorbingclimate-warmingemissions. ment structure-aware baselines fail (i.e., iDepNN, EoG, GCNN, and LSR).",
            "As the output vectors from the GCN model will be used by the role prediction, we implement this regularization technique based on the representation vectors induced from GCN. Formally, we ﬁrst feed the H and the full dependency tree T for D into the same GCN model, i.e., H (cid:48)(cid:48) = GCN ( H, T ) .",
            "Here, we still retain the OT-based pruning and regularization components; however, instead of using GCN-based representations, the vectors for ﬁnal predictions and regularization need to be computed over the BiLSTM-induced vectors in H .",
            "Based on our experiments, the following hyper parameters are chosen: 50 dimensions for position embeddings; 1 layer for BiLSTM and 2 layers for GCN; 150 dimensions for the hidden states of the BiLSTM, GCN and feed-forward networks; 64 for the batch size; 0.2 for the learning rate with the Adam optimizer, and 0.1 for the trade-off parameter β .",
            "As such, in addition to the pruned structure, we apply the GCN model over the original dependency structure to obtain another set of representations vectors for the words.",
            "As such, the regularization component which requires a pruned tree is also excluded from the ﬁnal loss function; (4) GCN − : This model excludes the GCN model from OTEAE.",
            "The pruned document structure will be leveraged to learn representation vectors for input documents to perform argument role predictions using Graph Convolution Networks (GCN) (Kipf and Welling, 2017).",
            "volution Network (GCN) (Kipf and Welling, 2017; Nguyen and Grishman, 2018) to learn more abstract representation vectors for the words in T ′, leveraging BiLSTM-induced vectors in H as the inputs."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "248218685": {
      "citing_paper_info": {
        "title": "ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification",
        "abstract": "Document-level Event Causality Identification (DECI) aims to identify event-event causal relations in a document. Existing works usually build an event graph for global reasoning across multiple sentences. However, the edges between events have to be carefully designed through heuristic rules or external tools. In this paper, we propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI, to ease the graph construction and improve it over the noisy edge issue. Different from conventional event graphs, we define a pair of events as a node and build a complete event relational graph without any prior knowledge or tools. This naturally formulates DECI as a node classification problem, and thus we capture the causation transitivity among event pairs via a graph transformer. Furthermore, we design a criss-cross constraint and an adaptive focal loss for the imbalanced classification, to alleviate the issues of false positives and false negatives. Extensive experiments on two benchmark datasets show that ERGO greatly outperforms previous state-of-the-art (SOTA) methods (12.8% F1 gains on average).",
        "year": 2022,
        "venue": "International Conference on Computational Linguistics",
        "authors": [
          {
            "authorId": "2108612706",
            "name": "Meiqi Chen"
          },
          {
            "authorId": "145014675",
            "name": "Yixin Cao"
          },
          {
            "authorId": "2162737824",
            "name": "Kunquan Deng"
          },
          {
            "authorId": "2027599235",
            "name": "Mukai Li"
          },
          {
            "authorId": "2119044303",
            "name": "Kunpeng Wang"
          },
          {
            "authorId": "2156121678",
            "name": "Jing Shao"
          },
          {
            "authorId": null,
            "name": "Yan Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 19,
        "unique_cited_count": 19,
        "influential_count": 5,
        "detailed_records_count": 19
      },
      "cited_papers": [
        "9137624",
        "202771243",
        "235313618",
        "109933467",
        "31562985",
        "17227879",
        "47252984",
        "1162419",
        "198953378",
        "189898081",
        "220484653",
        "235313431",
        "53592270",
        "38687082",
        "51878335",
        "9130816",
        "174801632",
        "215737171",
        "247594480"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1162419,
          "isinfluential": false,
          "contexts": [
            "Then, researchers resort to a large amount of labeled data to mitigate the efforts of feature engineering and to learn diverse causal expressions (Hu et al., 2017; Hashimoto, 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Inference of Fine-Grained Event Causality from Blogs and Films",
            "abstract": "Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire.",
            "year": 2017,
            "venue": "NEWS@ACL",
            "authors": [
              {
                "authorId": "2111296747",
                "name": "Zhichao Hu"
              },
              {
                "authorId": "1928044",
                "name": "Elahe Rahimtoroghi"
              },
              {
                "authorId": "2234088162",
                "name": "M. Walker"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9130816,
          "isinfluential": false,
          "contexts": [
            "In the ﬁrst research line, early methods usually design various features tailored for causal expressions, such as lexical and syntactic patterns (Riaz and Girju, 2013, 2014a,b), causality cues or markers (Riaz and Girju, 2010; Do et al., 2011; Hidey and McKeown, 2016), statistical information…",
            "…as lexical and syntactic patterns (Riaz and Girju, 2013, 2014a,b), causality cues or markers (Riaz and Girju, 2010; Do et al., 2011; Hidey and McKeown, 2016), statistical information (Beamer and Girju, 2009; Hashimoto et al., 2014), and temporal patterns (Riaz and Girju, 2014a; Ning et al., 2018)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "In-depth Exploitation of Noun and Verb Semantics to Identify Causation in Verb-Noun Pairs",
            "abstract": "Recognition of causality is important to achieve natural language discourse understanding. Previous approaches rely on shallow linguistic features. In this work, we propose to identify causality in verbnoun pairs by exploiting deeper semantics of nouns and verbs. Particularly, we acquire and employ three novel types of knowledge: (1) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies, (2) data-driven semantic classes of verbal events with the least tendency to encode causality, and (3) tendencies of verb frames to encode causality. Using these knowledge sources, we achieve around 15% improvement in Fscore over a supervised classifier trained using linguistic features.",
            "year": 2014,
            "venue": "SIGDIAL Conference",
            "authors": [
              {
                "authorId": "2852149",
                "name": "M. Riaz"
              },
              {
                "authorId": "2469966",
                "name": "Roxana Girju"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9137624,
          "isinfluential": false,
          "contexts": [
            "…such as lexical and syntactic patterns (Riaz and Girju, 2013, 2014a,b), causality cues or markers (Riaz and Girju, 2010; Do et al., 2011; Hidey and McKeown, 2016), statistical information (Beamer and Girju, 2009; Hashimoto et al., 2014), and temporal patterns (Riaz and Girju, 2014a;…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Identifying Causal Relations Using Parallel Wikipedia Articles",
            "abstract": "The automatic detection of causal relationships in text is important for natural language understanding. This task has proven to be difﬁcult, however, due to the need for world knowledge and inference. We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality. Unlike the explicit markers, a closed class, these markers vary signiﬁ-cantly in their linguistic forms. We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases, creating a training set via distant supervision. We also train a causal classiﬁer using features from the open class markers and semantic features providing contextual information. The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text.",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3443981",
                "name": "Christopher Hidey"
              },
              {
                "authorId": "145590324",
                "name": "K. McKeown"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17227879,
          "isinfluential": false,
          "contexts": [
            "…as lexical and syntactic patterns (Riaz and Girju, 2013, 2014a,b), causality cues or markers (Riaz and Girju, 2010; Do et al., 2011; Hidey and McKeown, 2016), statistical information (Beamer and Girju, 2009; Hashimoto et al., 2014), and temporal patterns (Riaz and Girju, 2014a; Ning et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features",
            "abstract": "We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture! exacerbate desertification from the web using semantic relation (between nouns), context, and association features. Experiments show that our method outperforms baselines that are based on state-of-the-art methods. We also propose methods of generating future scenarios like conduct slash-and-burn agriculture! exacerbate desertification! increase Asian dust (from China)! asthma gets worse. Experiments show that we can generate 50,000 scenarios with 68% precision. We also generated a scenario deforestation continues! global warming worsens! sea temperatures rise! vibrio parahaemolyticus fouls (water), which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we “predicted” the future event sequence in a sense.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1760686",
                "name": "Chikara Hashimoto"
              },
              {
                "authorId": "1768754",
                "name": "Kentaro Torisawa"
              },
              {
                "authorId": "1922763",
                "name": "Julien Kloetzer"
              },
              {
                "authorId": "3151314",
                "name": "Motoki Sano"
              },
              {
                "authorId": "2060429542",
                "name": "István Varga"
              },
              {
                "authorId": "1694341",
                "name": "Jong-Hoon Oh"
              },
              {
                "authorId": "2101431",
                "name": "Y. Kidawara"
              }
            ]
          }
        },
        {
          "citedcorpusid": 31562985,
          "isinfluential": false,
          "contexts": [
            "…as lexical and syntactic patterns (Riaz and Girju, 2013, 2014a,b), causality cues or markers (Riaz and Girju, 2010; Do et al., 2011; Hidey and McKeown, 2016), statistical information (Beamer and Girju, 2009; Hashimoto et al., 2014), and temporal patterns (Riaz and Girju, 2014a; Ning et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Using a Bigram Event Model to Predict Causal Potential",
            "abstract": "",
            "year": 2009,
            "venue": "Conference on Intelligent Text Processing and Computational Linguistics",
            "authors": [
              {
                "authorId": "145021570",
                "name": "Brandon Beamer"
              },
              {
                "authorId": "2469966",
                "name": "Roxana Girju"
              }
            ]
          }
        },
        {
          "citedcorpusid": 38687082,
          "isinfluential": false,
          "contexts": [
            "Causality can reveal reliable structures of texts, which is beneﬁ-cial to widespread applications, such as machine reading comprehension (Berant et al., 2014), question answering (Oh et al., 2016), and future event forecasting (Hashimoto, 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Semi-Supervised Learning Approach to Why-Question Answering",
            "abstract": "\n \n We propose a semi-supervised learning method for improving why-question answering (why-QA). The key of our method is to generate training data (question-answer pairs) from causal relations in texts such as \"[Tsunamis are generated](effect) because [the ocean's water mass is displaced by an earthquake](cause).\" A naive method for the generation would be to make a question-answer pair by simply converting the effect part of the causal relations into a why-question, like \"Why are tsunamis generated?\" from the above example, and using the source text of the causal relations as an answer. However, in our preliminary experiments, this naive method actually failed to improve the why-QA performance. The main reason was that the machine-generated questions were often incomprehensible like \"Why does (it) happen?\", and that the system suffered from overfitting to the results of our automatic causality recognizer. Hence, we developed a novel method that effectively filters out incomprehensible questions and retrieves from texts answers that are likely to be paraphrases of a given causal relation. Through a series of experiments, we showed that our approach significantly improved the precision of the top answer by 8% over the current state-of-the-art system for Japanese why-QA.\n \n",
            "year": 2016,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1694341",
                "name": "Jong-Hoon Oh"
              },
              {
                "authorId": "1768754",
                "name": "Kentaro Torisawa"
              },
              {
                "authorId": "1760686",
                "name": "Chikara Hashimoto"
              },
              {
                "authorId": "34611748",
                "name": "R. Iida"
              },
              {
                "authorId": "2110304080",
                "name": "Masahiro Tanaka"
              },
              {
                "authorId": "1922763",
                "name": "Julien Kloetzer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 47252984,
          "isinfluential": false,
          "contexts": [
            "To address this problem, we leverage an adaptive loss function for training, following focal loss (Lin et al., 2017).",
            "This leads to an imbalanced classiﬁcation problem (Lin et al., 2017)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Focal Loss for Dense Object Detection",
            "abstract": "",
            "year": 2017,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
              },
              {
                "authorId": "47316088",
                "name": "Priya Goyal"
              },
              {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
              },
              {
                "authorId": "39353098",
                "name": "Kaiming He"
              },
              {
                "authorId": "3127283",
                "name": "Piotr Dollár"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51878335,
          "isinfluential": false,
          "contexts": [
            "…as lexical and syntactic patterns (Riaz and Girju, 2013, 2014a,b), causality cues or markers (Riaz and Girju, 2010; Do et al., 2011; Hidey and McKeown, 2016), statistical information (Beamer and Girju, 2009; Hashimoto et al., 2014), and temporal patterns (Riaz and Girju, 2014a; Ning et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Reasoning for Temporal and Causal Relations",
            "abstract": "Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints that are inherent in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "117662877",
                "name": "Zhili Feng"
              },
              {
                "authorId": "2119796958",
                "name": "Hao Wu"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53592270,
          "isinfluential": false,
          "contexts": [
            "We optimize our model with AdamW (Loshchilov and Hutter, 2019) with a linear warm-up."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 109933467,
          "isinfluential": true,
          "contexts": [
            "Gao et al. (2019) use Integer Linear Programming (ILP) to model the global causal structures; RichGCN (Tran Phu and Nguyen, 2021) constructs document-level interaction graphs and uses Graph Convolutional Network (GCN, Kipf and Welling (2017)) to capture relevant connections.",
            "However, it performs much worse than LIP, RichGCN, and ERGO on inter-sentence settings, which indicates that a document-level structure or graph is helpful to capture the global interactions for prediction.",
            "An event graph is typically constructed to assist the global inference, where edges are carefully designed via heuristic rules or NLP tools, such as adjacent sentences and dependency parser (Gao et al., 2019; Tran Phu and Nguyen, 2021).",
            "Following Gao et al. (2019), we group documents according to their topics.",
            "(2) LR+ and LIP (Gao et al., 2019), feature-based methods that construct document-level structures and use various types of resources.",
            "Evaluation Metrics For evaluation, we adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics, same as previous methods (Gao et al., 2019; Tran Phu and Nguyen, 2021) to ensure comparability."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Modeling Document-level Causal Structures for Event Causal Relation Identification",
            "abstract": "We aim to comprehensively identify all the event causal relations in a document, both within a sentence and across sentences, which is important for reconstructing pivotal event structures. The challenges we identified are two: 1) event causal relations are sparse among all possible event pairs in a document, in addition, 2) few causal relations are explicitly stated. Both challenges are especially true for identifying causal relations between events across sentences. To address these challenges, we model rich aspects of document-level causal structures for achieving comprehensive causal relation identification. The causal structures include heavy involvements of document-level main events in causal relations as well as several types of fine-grained constraints that capture implications from certain sentential syntactic relations and discourse relations as well as interactions between event causal relations and event coreference relations. Our experimental results show that modeling the global and fine-grained aspects of causal structures using Integer Linear Programming (ILP) greatly improves the performance of causal relation identification, especially in identifying cross-sentence causal relations.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "48204509",
                "name": "Lei Gao"
              },
              {
                "authorId": "3466801",
                "name": "Prafulla Kumar Choubey"
              },
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174801632,
          "isinfluential": false,
          "contexts": [
            "Following Baldini Soares et al. (2019), we use the embedding of token “<t>” for event representation."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Matching the Blanks: Distributional Similarity for Relation Learning",
            "abstract": "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris’ distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task’s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "7353832",
                "name": "Livio Baldini Soares"
              },
              {
                "authorId": "143883142",
                "name": "Nicholas FitzGerald"
              },
              {
                "authorId": "2065574525",
                "name": "Jeffrey Ling"
              },
              {
                "authorId": "15652489",
                "name": "T. Kwiatkowski"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": false,
          "contexts": [
            "Along with the success of sentence-level natural language understanding, many tasks are extended to the entire document, such as relation extraction (Yao et al., 2019), natural language inference (Yin et al., 2021), and event argument extraction (Li et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198953378,
          "isinfluential": false,
          "contexts": [
            "The reason may be: 1) Longformer continues pre-training from Roberta (Liu et al., 2019), which has been found to outperform BERT on many tasks; 2) Longformer leverages an efﬁcient local and global attention pattern, beneﬁcial to capture longer contextual information for inference."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "40511414",
                "name": "Myle Ott"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "3048577",
                "name": "Jingfei Du"
              },
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202771243,
          "isinfluential": false,
          "contexts": [
            "Then, researchers resort to a large amount of labeled data to mitigate the efforts of feature engineering and to learn diverse causal expressions (Hu et al., 2017; Hashimoto, 2019).",
            "Causality can reveal reliable structures of texts, which is beneﬁ-cial to widespread applications, such as machine reading comprehension (Berant et al., 2014), question answering (Oh et al., 2016), and future event forecasting (Hashimoto, 2019)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Weakly Supervised Multilingual Causality Extraction from Wikipedia",
            "abstract": "We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -> Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1760686",
                "name": "Chikara Hashimoto"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215737171,
          "isinfluential": true,
          "contexts": [
            "Specifically, we use global attention on the “<s>” token (Longformer uses “<s>” and “</s>” as the special start and end tokens, corresponding to BERT’s “[CLS]” and “[SEP]”), and local attention on other tokens, which could build full sequence representations.",
            "In our implementation, we apply the efﬁcient local and global attention pattern of Longformer.",
            "In this paper, we choose pre-trained BERT (De-vlin et al., 2019) and Longformer (Beltagy et al., 2020) as encoders for comparison.",
            "Using Longformer BASE as the encoder, ERGO achieves better results than ERGO-BERT BASE , which also achieves new SOTA.",
            "Longformer for Document Encoder Long-former (Beltagy et al., 2020) introduces a localized sliding window based attention (the default window size is 512) with little global attention to reduce computation and extend BERT for long documents.",
            "The maximum document length allowed by Longformer is 4096, which is suitable for most documents.",
            "The reason may be: 1) Longformer continues pre-training from Roberta (Liu et al., 2019), which has been found to outperform BERT on many tasks; 2) Longformer leverages an efﬁcient local and global attention pattern, beneﬁcial to capture longer contextual information for inference.",
            "We use uncased BERT-base (Devlin et al., 2019) or Longformser-base (Beltagy et al., 2020) as the document encoder."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "['background']",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Longformer: The Long-Document Transformer",
            "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              },
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "2527954",
                "name": "Arman Cohan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220484653,
          "isinfluential": true,
          "contexts": [
            "Existing methods mostly focus on sentence-level ECI (SECI) (Liu et al., 2020; Zuo et al., 2021a; Figure 1: Example of DECI.",
            "Following (Liu et al., 2020) and (Tran Phu and Nguyen, 2021), we employ a 10-fold cross-validation evaluation.",
            "SECI Baselines (1) KMMG (Liu et al., 2020), which proposes a mention masking generalization method and use extenal knowledge databases.",
            "To alleviate the annotation cost, recent methods leverage Pre-trained Language Models (PLMs, e.g., BERT (Devlin et al., 2019)) for the ECI task and have achieved SOTA performance (Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020).",
            "In terms of text corpus, there are mainly two types of methods: SECI and DECI.",
            "We compare our proposed ERGO with various state-of-the-art SECI and DECI methods."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Knowledge Enhanced Event Causality Identification with Mention Masking Generalizations",
            "abstract": "Identifying causal relations of events is a crucial language understanding task. Despite many efforts for this task, existing methods lack the ability to adopt background knowledge, and they typically generalize poorly to new, previously unseen data. In this paper, we present a new method for event causality identification, aiming to address limitations of previous methods. On the one hand, our model can leverage external knowledge for reasoning, which can greatly enrich the representation of events; On the other hand, our model can mine event-agnostic, context-specific patterns, via a mechanism called event mention masking generalization, which can greatly enhance the ability of our model to handle new, previously unseen cases. In experiments, we evaluate our model on three benchmark datasets and show our model outperforms previous methods by a significant margin. Moreover, we perform 1) cross-topic adaptation, 2) exploiting unseen predicates, and 3) cross-task adaptation to evaluate the generalization ability of our model. Experimental results show that our model demonstrates a definite advantage over previous methods.",
            "year": 2020,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "48211977",
                "name": "Jian Liu"
              },
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313431,
          "isinfluential": true,
          "contexts": [
            "Zuo et al. (2021b) propose a data augmentation method to further solve the data lacking problem.",
            "Existing methods mostly focus on sentence-level ECI (SECI) (Liu et al., 2020; Zuo et al., 2021a; Figure 1: Example of DECI.",
            "(5) CauSeRL (Zuo et al., 2021a), which learns context-speciﬁc causal patterns from external causal statements for ECI.",
            "Zuo et al. (2021a) learn context-speciﬁc causal patterns from external causal statements and incorporate them into a target ECI model.",
            "(4) LearnDA (Zuo et al., 2021b), which uses knowledge bases to augment training data."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification",
            "abstract": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313618,
          "isinfluential": false,
          "contexts": [
            "Existing methods mostly focus on sentence-level ECI (SECI) (Liu et al., 2020; Zuo et al., 2021a; Figure 1: Example of DECI.",
            "Zuo et al. (2021a) learn context-speciﬁc causal patterns from external causal statements and incorporate them into a target ECI model.",
            "(5) CauSeRL (Zuo et al., 2021a), which learns context-speciﬁc causal patterns from external causal statements for ECI."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement",
            "abstract": "Current models for event causality identification (ECI) mainly adopt a supervised framework, which heavily rely on labeled data for training. Unfortunately, the scale of current annotated datasets is relatively limited, which cannot provide sufficient support for models to capture useful indicators from causal statements, especially for handing those new, unseen cases. To alleviate this problem, we propose a novel approach, shortly named CauSeRL, which leverages external causal statements for event causality identification. First of all, we design a self-supervised framework to learn context-specific causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into the target ECI model. Experimental results show that our method significantly outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively).",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247594480,
          "isinfluential": true,
          "contexts": [
            "2120 weighting factor in the focal loss to balance two classes’ training, which is not considered in (Tan et al., 2022).",
            "The difference is that the focal loss in (Tan et al., 2022) is used to make long-tail (positive) classes contribute more to the overall loss, while the focal loss in our ERGO tackles the imbalance issue of DECI task by focusing more on difficult samples.",
            "A concurrent and relevant work is (Tan et al., 2022), which also leverages focal loss for entity relation extraction."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation",
            "abstract": "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.",
            "year": 2022,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "118358816",
                "name": "Qingyu Tan"
              },
              {
                "authorId": "22272507",
                "name": "Ruidan He"
              },
              {
                "authorId": "1996394",
                "name": "Lidong Bing"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              }
            ]
          }
        }
      ]
    },
    "279284305": {
      "citing_paper_info": {
        "title": "CURE: Contextual Understanding and Reasoning for Document-level Event Causality Identification",
        "abstract": "Document-level Event Causality Identification (DECI) aims to determine whether a causal relationship holds between events that may be in intra-or inter-sentences. Existing methods typically rely on uniform graph-based representations, often overlooking that di ff erent events demand distinct contextual cues. We propose CURE , a DECI framework that integrates comprehensive document-level understanding with event-specific reasoning. Specifically, CURE first leverages large language models (LLMs) to derive unique contextual representations for each event from the same long document, ensuring that each event’s embedding focuses on its most relevant information. Next, it constructs a sparse event dependency graph that selectively links each event to a limited set of preceding events, inspired by human reading and forgetting patterns. CURE also adapts its inference strategy for intra-and inter-sentence event pairs. Experiments on two public datasets show that CURE outperforms state-of-the-art baselines.",
        "year": 2025,
        "venue": "Data Intelligence",
        "authors": [
          {
            "authorId": "9308532",
            "name": "Changsen Yuan"
          },
          {
            "authorId": "2367656006",
            "name": "Rui Lin"
          },
          {
            "authorId": "2259998448",
            "name": "Cunhan Guo"
          },
          {
            "authorId": "2366481375",
            "name": "Ge Shi"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 1,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "13756489",
        "235097277",
        "252819228",
        "270199894",
        "202782267",
        "248377560",
        "53592270",
        "220484653",
        "202537879",
        "8387007",
        "250291480",
        "235313431"
      ],
      "citation_details": [
        {
          "citedcorpusid": 8387007,
          "isinfluential": false,
          "contexts": [
            "To demonstrate the performance of our model, we evaluate the model on two domains two datasets, which are EventStoryLine 1 [18] and the English dataset in Multilingual Event Causality Identification Corpus (English-MECI) 2 [12]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures",
            "abstract": "Learning commonsense causal and temporal relation between events is one of the major steps towards deeper language understanding. This is even more crucial for understanding stories and script learning. A prerequisite for learning scripts is a semantic framework which enables capturing rich event structures. In this paper we introduce a novel semantic annotation framework, called Causal and Temporal Relation Scheme (CaTeRS), which is unique in simultaneously capturing a comprehensive set of temporal and causal relations between events. By annotating a total of 1,600 sentences in the context of 320 five-sentence short stories sampled from ROCStories corpus, we demonstrate that these stories are indeed full of causal and temporal relations. Furthermore, we show that the CaTeRS annotation scheme enables high inter-annotator agreement for broad-coverage event entity annotation and moderate agreement on semantic link annotation.",
            "year": 2016,
            "venue": "EVENTS@HLT-NAACL",
            "authors": [
              {
                "authorId": "2400138",
                "name": "N. Mostafazadeh"
              },
              {
                "authorId": "26322400",
                "name": "Alyson Grealish"
              },
              {
                "authorId": "1729918",
                "name": "Nathanael Chambers"
              },
              {
                "authorId": "145844737",
                "name": "James F. Allen"
              },
              {
                "authorId": "1909300",
                "name": "Lucy Vanderwende"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Although the multi-head attention [24] can generate unique contextual hidden layer representations for di ff erent events, in practice, it tends to focus more on the words adjacent to the target event.",
            "We use multi-head attention [24] to capture the dependency information of the target event."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53592270,
          "isinfluential": false,
          "contexts": [
            "We optimize our model with AdamW [15]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202537879,
          "isinfluential": false,
          "contexts": [
            "Such approaches often leverage syntactic cues and lexical triggers to detect causal links, but their scope is limited to local contexts [25, 19, 21, 8, 9, 22]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "An Improved Neural Baseline for Temporal Relation Extraction",
            "abstract": "Determining temporal relations (e.g., before or after) between events has been a challenging natural language understanding task, partly due to the difficulty to generate large amounts of high-quality training data. Consequently, neural approaches have not been widely used on it, or showed only moderate improvements. This paper proposes a new neural system that achieves about 10% absolute improvement in accuracy over the previous best system (25% error reduction) on two benchmark datasets. The proposed system is trained on the state-of-the-art MATRES dataset and applies contextualized word embeddings, a Siamese encoder of a temporal common sense knowledge base, and global inference via integer linear programming (ILP). We suggest that the new approach could serve as a strong baseline for future research in this area.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "17097887",
                "name": "Sanjay Subramanian"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202782267,
          "isinfluential": false,
          "contexts": [
            "As shown in Figure 1, event pairs, such as ( grateful and treatment in intra-sentence) and ( left and move in inter-sentence), exist the causal relation, and they either occur within the same sentence [10, 14, 11] or span across an entire document [20, 17, 26]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Causality Recognition Exploiting Multiple Annotators’ Judgments and Background Knowledge",
            "abstract": "We propose new BERT-based methods for recognizing event causality such as “smoke cigarettes” –> “die of lung cancer” written in web texts. In our methods, we grasp each annotator’s policy by training multiple classifiers, each of which predicts the labels given by a single annotator, and combine the resulting classifiers’ outputs to predict the final labels determined by majority vote. Furthermore, we investigate the effect of supplying background knowledge to our classifiers. Since BERT models are pre-trained with a large corpus, some sort of background knowledge for event causality may be learned during pre-training. Our experiments with a Japanese dataset suggest that this is actually the case: Performance improved when we pre-trained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2315457",
                "name": "Kazuma Kadowaki"
              },
              {
                "authorId": "34611748",
                "name": "R. Iida"
              },
              {
                "authorId": "1768754",
                "name": "Kentaro Torisawa"
              },
              {
                "authorId": "1694341",
                "name": "Jong-Hoon Oh"
              },
              {
                "authorId": "1922763",
                "name": "Julien Kloetzer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220484653,
          "isinfluential": false,
          "contexts": [
            "As shown in Figure 1, event pairs, such as ( grateful and treatment in intra-sentence) and ( left and move in inter-sentence), exist the causal relation, and they either occur within the same sentence [10, 14, 11] or span across an entire document [20, 17, 26]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Knowledge Enhanced Event Causality Identification with Mention Masking Generalizations",
            "abstract": "Identifying causal relations of events is a crucial language understanding task. Despite many efforts for this task, existing methods lack the ability to adopt background knowledge, and they typically generalize poorly to new, previously unseen data. In this paper, we present a new method for event causality identification, aiming to address limitations of previous methods. On the one hand, our model can leverage external knowledge for reasoning, which can greatly enrich the representation of events; On the other hand, our model can mine event-agnostic, context-specific patterns, via a mechanism called event mention masking generalization, which can greatly enhance the ability of our model to handle new, previously unseen cases. In experiments, we evaluate our model on three benchmark datasets and show our model outperforms previous methods by a significant margin. Moreover, we perform 1) cross-topic adaptation, 2) exploiting unseen predicates, and 3) cross-task adaptation to evaluate the generalization ability of our model. Experimental results show that our model demonstrates a definite advantage over previous methods.",
            "year": 2020,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "48211977",
                "name": "Jian Liu"
              },
              {
                "authorId": "2150168776",
                "name": "Jian Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235097277,
          "isinfluential": false,
          "contexts": [
            "DECI studies have introduced methods to integrate global context, employing graph-based models, hierarchical encoders, and attention mechanisms to capture long-distance dependencies among events and entities [6, 20].",
            "As shown in Figure 1, event pairs, such as ( grateful and treatment in intra-sentence) and ( left and move in inter-sentence), exist the causal relation, and they either occur within the same sentence [10, 14, 11] or span across an entire document [20, 17, 26].",
            "…data to solve the data lacking; • LR + and LIP [6] that models rich causal structures via designing constraints and objection function; • RichGCN [20] that builds multi-level graphs to capture structure-preserved features; • ERGO [2] that designs an event relational graph and converts the event…"
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures",
            "abstract": "We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept inter-sentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2107017319",
                "name": "Minh Tran Phu"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313431,
          "isinfluential": false,
          "contexts": [
            "• LearnDA [28] that augments data to solve the data lacking; • LR + and LIP [6] that models rich causal structures via designing constraints and objection function; • RichGCN [20] that builds multi-level graphs to capture structure-preserved features; • ERGO [2] that designs an event relational…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification",
            "abstract": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248377560,
          "isinfluential": false,
          "contexts": [
            "Such approaches often leverage syntactic cues and lexical triggers to detect causal links, but their scope is limited to local contexts [25, 19, 21, 8, 9, 22]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "The Causal News Corpus: Annotating Causal Relations in Event Sentences from News",
            "abstract": "Despite the importance of understanding causality, corpora addressing causal relations are limited. There is a discrepancy between existing annotation guidelines of event causality and conventional causality corpora that focus more on linguistics. Many guidelines restrict themselves to include only explicit relations or clause-based arguments. Therefore, we propose an annotation schema for event causality that addresses these concerns. We annotated 3,559 event sentences from protest event news with labels on whether it contains causal relations or not. Our corpus is known as the Causal News Corpus (CNC). A neural network built upon a state-of-the-art pre-trained language model performed well with 81.20% F1 score on test set, and 83.46% in 5-folds cross-validation. CNC is transferable across two external corpora: CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of these external datasets for training, we achieved up to approximately 64% F1 on the CNC test set without additional fine-tuning. CNC also served as an effective training and pre-training dataset for the two external corpora. Lastly, we demonstrate the difficulty of our task to the layman in a crowd-sourced annotation exercise. Our annotated corpus is publicly available, providing a valuable resource for causal text mining researchers.",
            "year": 2022,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2121387601",
                "name": "Fiona Anting Tan"
              },
              {
                "authorId": "1389950549",
                "name": "Ali Hurriyetouglu"
              },
              {
                "authorId": "1864635",
                "name": "Tommaso Caselli"
              },
              {
                "authorId": "1996659",
                "name": "Nelleke Oostdijk"
              },
              {
                "authorId": "2842296",
                "name": "Tadashi Nomoto"
              },
              {
                "authorId": "1453653937",
                "name": "Hansi Hettiarachchi"
              },
              {
                "authorId": "2353069354",
                "name": "Iqra Ameer"
              },
              {
                "authorId": "138537239",
                "name": "Onur Uca"
              },
              {
                "authorId": "3445542",
                "name": "Farhana Ferdousi Liza"
              },
              {
                "authorId": "2046818614",
                "name": "Tiancheng Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 250291480,
          "isinfluential": false,
          "contexts": [
            "As shown in Figure 1, event pairs, such as ( grateful and treatment in intra-sentence) and ( left and move in inter-sentence), exist the causal relation, and they either occur within the same sentence [10, 14, 11] or span across an entire document [20, 17, 26].",
            "Man et al. [17] proposes to model the important context sentences to identify relations."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Selecting Optimal Context Sentences for Event-Event Relation Extraction",
            "abstract": "Understanding events entails recognizing the structural and temporal orders between event mentions to build event structures/graphs for input documents. To achieve this goal, our work addresses the problems of subevent relation extraction (SRE) and temporal event relation extraction (TRE) that aim to predict subevent and temporal relations between two given event mentions/triggers in texts. Recent state-of-the-art methods for such problems have employed transformer-based language models (e.g., BERT) to induce effective contextual representations for input event mention pairs. However, a major limitation of existing transformer-based models for SRE and TRE is that they can only encode input texts of limited length (i.e., up to 512 sub-tokens in BERT), thus unable to effectively capture important context sentences that are farther away in the documents. In this work, we introduce a novel method to better model document-level context with important context sentences for event-event relation extraction. Our method seeks to identify the most important context sentences for a given entity mention pair in a document and pack them into shorter documents to be consume entirely by transformer-based language models for representation learning. The REINFORCE algorithm is employed to train models where novel reward functions are presented to capture model performance, and context-based and knowledge-based similarity between sentences for our problem. Extensive experiments demonstrate the effectiveness of the proposed method with state-of-the-art performance on benchmark datasets.",
            "year": 2022,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2027979466",
                "name": "Hieu Man"
              },
              {
                "authorId": "1692755523",
                "name": "Nghia Trung Ngo"
              },
              {
                "authorId": "15802036",
                "name": "L. Van"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252819228,
          "isinfluential": false,
          "contexts": [
            "Following [12], we use the 263 documents as the Train dataset, 88 documents as the Dev dataset, and 87 documents as the Test dataset.",
            "To demonstrate the performance of our model, we evaluate the model on two domains two datasets, which are EventStoryLine 1 [18] and the English dataset in Multilingual Event Causality Identification Corpus (English-MECI) 2 [12]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "MECI: A Multilingual Dataset for Event Causality Identification",
            "abstract": "",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1405279380",
                "name": "Viet Dac Lai"
              },
              {
                "authorId": "3460489",
                "name": "Amir Pouran Ben Veyseh"
              },
              {
                "authorId": "1789308",
                "name": "Minh Le Nguyen"
              },
              {
                "authorId": "2462276",
                "name": "Franck Dernoncourt"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 270199894,
          "isinfluential": true,
          "contexts": [
            "5-turbo and gpt-4; • iLIF [13] that designs an iterative learning and identifying framework.",
            "Liu et al. [13] proposes to design an iterative learning and identifying framework.",
            "In addition, we follow previous work (iLIF) [13], using the multi-iteration to update the d ( i , j ) o to reason the causal relation for event pairs.",
            "We strictly follow the settings in [13] for other parameters."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Identifying while Learning for Document Event Causality Identification",
            "abstract": "Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of identifying after learning paradigm, where events' representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignoring causal direction. In this paper, we take care of the causal direction and propose a new identifying while learning mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events' representations for boosting next round of causality identification. To this end, this paper designs an *iterative learning and identifying framework*: In each iteration, we construct an event causality graph, on which events' causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.",
            "year": 2024,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2304744193",
                "name": "Cheng Liu"
              },
              {
                "authorId": "2052727822",
                "name": "Wei Xiang"
              },
              {
                "authorId": "2156645305",
                "name": "Bang Wang"
              }
            ]
          }
        }
      ]
    },
    "250390478": {
      "citing_paper_info": {
        "title": "Document-Level Event Argument Extraction by Leveraging Redundant Information and Closed Boundary Loss",
        "abstract": "In document-level event argument extraction, an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition, in event argument extraction, most entities are regarded as class “others”, i.e. Universum class, which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without typical common features. Classifiers trained by cross entropy loss could easily misclassify the Universum class because of their open decision boundary. In this paper, to make use of redundant event information underlying a document, we build an entity coreference graph with the graph2token module to produce a comprehensive and coreference-aware representation for every entity and then build an entity summary graph to merge the multiple extraction results. To better classify Universum class, we propose a new loss function to build classifiers with closed boundaries. Experimental results show that our model outperforms the previous state-of-the-art models by 3.35% in F1-score.",
        "year": 2022,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2111825433",
            "name": "Hanzhang Zhou"
          },
          {
            "authorId": "2128504277",
            "name": "Kezhi Mao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 10,
        "influential_count": 1,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "8291528",
        "53109320",
        "226976039",
        "234790176",
        "54475483",
        "198229624",
        "233219850",
        "236460259",
        "235253912",
        "216562330"
      ],
      "citation_details": [
        {
          "citedcorpusid": 8291528,
          "isinfluential": false,
          "contexts": [
            "However, the SVM-based methods above are developed for structured data and are hardly to integrate with deep neural network-based representation learning to form an end-to-end train-189 ing procedure for natural language processing tasks. where D is the output of the Bi-LSTM encoding layer, ent start",
            "We found the root cause Research works in universum usually employ ad-180 ditional unlabeled universum data to provide prior knowledge for the task, such as universum support vector machine (SVM) (Weston et al., 2006; Qi et al., 2012; Richhariya and Tanveer, 2020), and semi-supervised learning (Liu et al., 2015; Xiao et al., 2021).",
            "…in universum usually employ ad-180 ditional unlabeled universum data to provide prior knowledge for the task, such as universum support vector machine (SVM) (Weston et al., 2006; Qi et al., 2012; Richhariya and Tanveer, 2020), and semi-supervised learning (Liu et al., 2015; Xiao et al., 2021)."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Inference with the Universum",
            "abstract": "",
            "year": 2006,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "145183709",
                "name": "J. Weston"
              },
              {
                "authorId": "2939803",
                "name": "R. Collobert"
              },
              {
                "authorId": "50095217",
                "name": "Fabian H Sinz"
              },
              {
                "authorId": "52184096",
                "name": "L. Bottou"
              },
              {
                "authorId": "50560492",
                "name": "V. Vapnik"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53109320,
          "isinfluential": false,
          "contexts": [
            "Graph structure is used in EAE to produce com-511 prehensive representation for coreference entities (Luan et al., 2019; Qian et al., 2019; Xu et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GraphIE: A Graph-Based Framework for Information Extraction",
            "abstract": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks — namely textual, social media and visual information extraction — shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.",
            "year": 2018,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "5606742",
                "name": "Yujie Qian"
              },
              {
                "authorId": "2628786",
                "name": "Enrico Santus"
              },
              {
                "authorId": "8752221",
                "name": "Zhijing Jin"
              },
              {
                "authorId": "144084849",
                "name": "Jiang Guo"
              },
              {
                "authorId": "1741283",
                "name": "R. Barzilay"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54475483,
          "isinfluential": false,
          "contexts": [
            ", 2021), auto-encoder based anomaly detection (Ionescu et al., 2019), OpenMax layer for open set recognition (Bendale and Boult, 2016)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection in Video",
            "abstract": "Abnormal event detection in video is a challenging vision problem. Most existing approaches formulate abnormal event detection as an outlier detection task, due to the scarcity of anomalous data during training. Because of the lack of prior information regarding abnormal events, these methods are not fully-equipped to differentiate between normal and abnormal events. In this work, we formalize abnormal event detection as a one-versus-rest binary classification problem. Our contribution is two-fold. First, we introduce an unsupervised feature learning framework based on object-centric convolutional auto-encoders to encode both motion and appearance information. Second, we propose a supervised classification approach based on clustering the training samples into normality clusters. A one-versus-rest abnormal event classifier is then employed to separate each normality cluster from the rest. For the purpose of training the classifier, the other clusters act as dummy anomalies. During inference, an object is labeled as abnormal if the highest classification score assigned by the one-versus-rest classifiers is negative. Comprehensive experiments are performed on four benchmarks: Avenue, ShanghaiTech, UCSD and UMN. Our approach provides superior results on all four data sets. On the large-scale ShanghaiTech data set, our method provides an absolute gain of 8.4% in terms of frame-level AUC compared to the state-of-the-art method.",
            "year": 2018,
            "venue": "Computer Vision and Pattern Recognition",
            "authors": [
              {
                "authorId": "1817759",
                "name": "Radu Tudor Ionescu"
              },
              {
                "authorId": "2358803",
                "name": "F. Khan"
              },
              {
                "authorId": "41021255",
                "name": "Mariana-Iuliana Georgescu"
              },
              {
                "authorId": "144082425",
                "name": "Ling Shao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 198229624,
          "isinfluential": false,
          "contexts": [
            "We use SpanBERT (Joshi et al., 2020) to implement coreference resolution on documents during preprocessing."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
            "abstract": "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1",
            "year": 2019,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "1780531",
                "name": "Daniel S. Weld"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Most previous event argument extraction models make predictions at sentence-level (Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019b; Du and Cardie, 2020b; Wei et al., 2021; Wang et al., 2021; Dutta et al., 2021",
            "Research on document-level event extraction has been focused on tackling challenges such as arguments-scattering and multiple-events (Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2021; Lou et al., 2021; Li et al., 2021; Huang and Peng,",
            "Previous works are mostly focused on sentence level EE (Liao and Grishman, 2010; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019b; Du and Cardie, 2020b; Wei et al., 2021; Wang et al., 2021; Lyu et al., 2021)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226976039,
          "isinfluential": false,
          "contexts": [
            "Closed boundary classification methods are also developed in anomaly detection and open set recognition, such as deep one-class learning (Ruff et al., 2018; Defard et al., 2021), auto-encoder based anomaly detection (Ionescu et al."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization",
            "abstract": "We present a new framework for Patch Distribution Modeling, PaDiM, to concurrently detect and localize anomalies in images in a one-class learning setting. PaDiM makes use of a pretrained convolutional neural network (CNN) for patch embedding, and of multivariate Gaussian distributions to get a probabilistic representation of the normal class. It also exploits correlations between the different semantic levels of CNN to better localize anomalies. PaDiM outperforms current state-of-the-art approaches for both anomaly detection and localization on the MVTec AD and STC datasets. To match real-world visual industrial inspection, we extend the evaluation protocol to assess performance of anomaly localization algorithms on non-aligned dataset. The state-of-the-art performance and low complexity of PaDiM make it a good candidate for many industrial applications.",
            "year": 2020,
            "venue": "ICPR Workshops",
            "authors": [
              {
                "authorId": "2331312081",
                "name": "Thomas Defard"
              },
              {
                "authorId": "2592210",
                "name": "Aleksandr Setkov"
              },
              {
                "authorId": "37995240",
                "name": "Angélique Loesch"
              },
              {
                "authorId": "2641858",
                "name": "Romaric Audigier"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "Research on document-level event extraction has been focused on tackling challenges such as arguments-scattering and multiple-events (Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2021; Lou et al., 2021; Li et al., 2021; Huang and Peng,"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 234790176,
          "isinfluential": false,
          "contexts": [
            "Research on document-level event extraction has been focused on tackling challenges such as arguments-scattering and multiple-events (Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2021; Lou et al., 2021; Li et al., 2021; Huang and Peng,"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network",
            "abstract": "We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "104300808",
                "name": "Dongfang Lou"
              },
              {
                "authorId": "10099514",
                "name": "Zhilin Liao"
              },
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2153010067",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": true,
          "contexts": [
            "Classification-based event argument extraction models (Huang and Peng, 2021; Xu et al., 2021; Yang et al., 2021) all employ cross entropy loss for classifier training, without considering the characteristics of Universum class: scattered dis-",
            "Graph structure is used in EAE to produce com-511 prehensive representation for coreference entities (Luan et al., 2019; Qian et al., 2019; Xu et al., 2021).",
            "3) An entity may appear multiple times in the document, directly averaging them as the entity’s feature representation (Xu et al., 2021) may introduce noise.",
            "Research works in event extraction (Xu et al., 2021; Luan et al., 2019; Qian et al., 2019) consider the first observation but neglect the second one.",
            "Graph structure is used in EAE to produce a comprehensive representation of coreference entities (Luan et al., 2019; Qian et al., 2019; Xu et al., 2021).",
            "Cross entropy loss is usually employed in classifier training (Zheng et al., 2019; Huang and Peng, 2021; Xu et al., 2021; Yang et al., 2021)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": false,
          "contexts": [
            "Classification-based event argument extraction models (Huang and Peng, 2021; Xu et al., 2021; Yang et al., 2021) all employ cross entropy loss for classifier training, without considering the characteristics of Universum class: scattered dis-",
            "Cross entropy loss is usually employed in classifier training (Zheng et al., 2019; Huang and Peng, 2021; Xu et al., 2021; Yang et al., 2021)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "However, the SVM-based methods above are developed for structured data and are hardly to integrate with deep neural network-based representation learning to form an end-to-end train-189 ing procedure for natural language processing tasks. where D is the output of the Bi-LSTM encoding layer, ent start",
            "We found the root cause Research works in universum usually employ ad-180 ditional unlabeled universum data to provide prior knowledge for the task, such as universum support vector machine (SVM) (Weston et al., 2006; Qi et al., 2012; Richhariya and Tanveer, 2020), and semi-supervised learning (Liu et al., 2015; Xiao et al., 2021).",
            "…in universum usually employ ad-180 ditional unlabeled universum data to provide prior knowledge for the task, such as universum support vector machine (SVM) (Weston et al., 2006; Qi et al., 2012; Richhariya and Tanveer, 2020), and semi-supervised learning (Liu et al., 2015; Xiao et al., 2021)."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            ", 136 2016; Liu et al., 2018; Yang et al., 2019b; Du 137 and Cardie, 2020b; Wei et al., 2021; Wang et al., 138 2021; Dutta et al., 2021). Considering that real 139 world events are often distributed across sentences, 140 document-level event extraction has attracted more 141 attentions recently. Zheng et al. (2019) propose 142 the Doc2EDAG model to overcome the argument 143 scattering problem."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "252901047": {
      "citing_paper_info": {
        "title": "MRCAug: Data Augmentation via Machine Reading Comprehension for Document-Level Event Argument Extraction",
        "abstract": "Document-level event argument extraction (EAE) is a critical event semantic understanding task that requires a model to identify an event's global arguments beyond the sentence level. Existing approaches to this problem are based on supervised learning, which require a large amount of labeled data for model training. However, due to the complicated structure of an event, human annotation for this task is costly, and the issue of inadequacy of training data has long hampered the study. In this study, we propose a novel approach to mitigating the data sparsity problem faced by document-level EAE, by linking the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including an implicit knowledge transfer method, which enables knowledge transfer from other tasks to the document-level EAE task, and an explicit data generation method, which can explicitly generate new training examples by treating a pre-trained MRC model as an annotator. Furthermore, we propose a self-training based noise reduction strategy that can effectively addresses the out-of-domain noise introduced by the data augmentation methods. The extensive assessments on three benchmarks have validated the effectiveness of our approach — it not only achieves state-of-the-art performance but also demonstrates superior results in the data-low scenario.",
        "year": 2022,
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "authors": [
          {
            "authorId": "2150168584",
            "name": "Jian Liu"
          },
          {
            "authorId": "47559028",
            "name": "Yufeng Chen"
          },
          {
            "authorId": "2310092",
            "name": "Jinan Xu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 13,
        "influential_count": 5,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "236460308",
        "207853145",
        "216562330",
        "153312535",
        "6546734",
        "222208551",
        "11986411",
        "6644751",
        "236087324",
        "4321928",
        "12108307",
        "220046861",
        "52967399"
      ],
      "citation_details": [
        {
          "citedcorpusid": 4321928,
          "isinfluential": false,
          "contexts": [
            ") [40], which modifies the term logP (â|D̂, q̂) to −(1− P (â|D̂, q̂)) in (3) for model training, and the method based on example re-weighting (Re-Weighting) [41], which uses a model trained with in-domain data to give out-of-domain and",
            "In Discussion (Section VI-D), we compare our method to methods based on loss function modification [40] and example re-weighting [41] to assess its effectiveness."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning to Reweight Examples for Robust Deep Learning",
            "abstract": "Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2540599",
                "name": "Mengye Ren"
              },
              {
                "authorId": "3468942",
                "name": "Wenyuan Zeng"
              },
              {
                "authorId": "49188662",
                "name": "Binh Yang"
              },
              {
                "authorId": "2422559",
                "name": "R. Urtasun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6546734,
          "isinfluential": false,
          "contexts": [
            "In the ﬁne-tuning state, we employ the RAMS development set for hyper-parameter tuning, and ﬁnally, the batch size is set to 20 (selected from [1, 5, 10, 20, 30, 40]) and the learning rate is set to 2e-5 (picked from [1e-5, 2e-5, 3e-5, 4e-5]).",
            "[40], which modiﬁes the term log P (ˆ a | ˆ D , ˆ q ) to − (1 − P (ˆ a | ˆ D , ˆ q )) in (3) for model training, and the method based on example re-weighting (Re-Weighting) [41], which uses a model trained with in-domain data to give out-of-domain and automatically generated data a weight for…",
            "In Discussion (Section VI-D), we compare our method to methods based on loss function modiﬁcation [40] and example re-weighting [41] to assess its effectiveness."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Robust Loss Functions under Label Noise for Deep Neural Networks",
            "abstract": "\n \n In many applications of classifier learning, training data suffers from label noise. Deep networks are learned using huge training data where the problem of noisy labels is particularly relevant. The current techniques proposed for learning deep networks under label noise focus on modifying the network architecture and on algorithms for estimating true labels from noisy labels. An alternate approach would be to look for loss functions that are inherently noise-tolerant. For binary classification there exist theoretical results on loss functions that are robust to label noise. In this paper, we provide some sufficient conditions on a loss function so that risk minimization under that loss function would be inherently tolerant to label noise for multiclass classification problems. These results generalize the existing results on noise-tolerant loss functions for binary classification. We study some of the widely used loss functions in deep networks and show that the loss function based on mean absolute value of error is inherently robust to label noise. Thus standard back propagation is enough to learn the true classifier even under label noise. Through experiments, we illustrate the robustness of risk minimization with such loss functions for learning neural networks.\n \n",
            "year": 2017,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "3201314",
                "name": "Aritra Ghosh"
              },
              {
                "authorId": "2065156735",
                "name": "Himanshu Kumar"
              },
              {
                "authorId": "143637056",
                "name": "P. Sastry"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6644751,
          "isinfluential": false,
          "contexts": [
            "For this MUC-4 task formulation, traditional approaches have proposed feature-based methods [18], [22], [23], while modern approacheshavestudiedusingneuralnetworksbasedmethodsto obtain document-level cues for learning [19], [24], [25]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Modeling Textual Cohesion for Event Extraction",
            "abstract": "\n \n Event extraction systems typically locate the role fillers for an event by analyzing sentences in isolation and identifying each role filler independently of the others. We argue that more accurate event extraction requires a view of the larger context to decide whether an entity is related to a relevant event. We propose a bottom-up approach to event extraction that initially identifies candidate role fillers independently and then uses that information as well as discourse properties to model textual cohesion. The novel component of the architecture is a sequentially structured sentence classifier that identifies event-related story contexts. The sentence classifier uses lexical associations and discourse relations across sentences, as well as domain-specific distributions of candidate role fillers within and across sentences. This approach yields state-of-the-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems.\n \n",
            "year": 2012,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11986411,
          "isinfluential": false,
          "contexts": [
            ", VICTIM) regarding an event template [17], [18], [19], [20], [21]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Message Understanding Conference- 6: A Brief History",
            "abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",
            "year": 1996,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "For the sentence-level event argument extraction task, the existing methods [30], [31] have used distant supervision for data augmentation, leveraging external knowledge bases to generate new training data.",
            "In the fine-tuning state, we employ the RAMS development set for hyper-parameter tuning, and finally, the batch size is set to 20 (selected from [1, 5, 10, 20, 30, 40])"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": true,
          "contexts": [
            "In our MRC model, we use a BERT-base-uncased architecture [12] to maintain consistency with previous studies [2], [5].",
            "Furthermore, this collaborative architecture allows us to directly leverage the advanced models in MRC to handle document-level EAE, which have been proved to excel in capturing document-level clues [12].",
            "In particular, we create a BERT based encoder [12] to jointly encode q r and D , by ﬁrst generating an extended input sequence to concatenate q r and D : where [CLS] and [SEP] are special tokens used in BERT [12].",
            "We prefer the BERT-based MRC model [12], and in addition to the question answering dataset, i.e., SQuAD 2.0 [36], we also use corpora in FrameNet semantic role labeling (SRL) [37] and ACE 2005 event extraction (EE) [38] for pre-training 2 , by framing them in an MRC formulation in a similar fashion.",
            "In the implicit knowledge transfer method, after pre-training, the MRC model scores 83.5%, 72.1%, and 70.1% in F1 on SQuAD 2.0, FrameNet SRL, and ACE 2005 EE datasets, respectively, matching the state-of-the-art performance [11], [12], [13]."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 153312535,
          "isinfluential": false,
          "contexts": [
            "To name a few, authors in [9], [33] cast relation"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Entity-Relation Extraction as Multi-Turn Question Answering",
            "abstract": "In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2845020",
                "name": "Xiaoya Li"
              },
              {
                "authorId": "2065089223",
                "name": "Fan Yin"
              },
              {
                "authorId": "1879521408",
                "name": "Zijun Sun"
              },
              {
                "authorId": "2321046587",
                "name": "Xiayu Li"
              },
              {
                "authorId": "2061016896",
                "name": "Arianna Yuan"
              },
              {
                "authorId": "2064208431",
                "name": "Duo Chai"
              },
              {
                "authorId": "2152177227",
                "name": "Mingxin Zhou"
              },
              {
                "authorId": "49298465",
                "name": "Jiwei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": true,
          "contexts": [
            "1 (bottom) gives the data statistics of the currently largest document-level benchmark (i.e., RAMS [2]), where we can see that over 63% of roles have less than 100 instances.",
            "…can be viewed as eliciting knowledge from a pre-trained MRC model to generate new examples, and as the training set is explicitly expanded, the method has the potential to beneﬁt any model proposed for document-level EAE (e.g., that based on sequence labeling [13] or span prediction [2]).",
            "T EXTUAL event descriptions may span over multiple sentences; therefore to fully understand the semantics of an event we need to gather information from a document context for reasoning [1], [2], [3].",
            "Despite its effectiveness, one disadvantage of implicit knowl-edgetransferisthatitcannotcreateexplicittrainingdata,henceit canonlybeneﬁtamodelinanMRCformulationbutnotinother formulations[2],[4].",
            "For example, the state-of-the-art model attains less than 10% in F1 when the argument and trigger are in different sentences [2], [4], [6].",
            "To validate the effectiveness of our approach, we have conducted extensive tests on three datasets, i.e., RAMS [2], WikiEvents [5], and MUC-4 [14].",
            "To address the data sparsity issue, we take a perspective from MRC and aim to use resources in the domain of MRC for learning, as opposed to previous methods viewing the task as a sequence labeling [13] or span ranking [2].",
            "[2], W IKI E VENTS (W IKI E) [5] AND MUC-4 [14] 0.9) may be too easy for a model to learn new information, while examples with very low predictive probabilities (e.g., < 0.6) may be too noisy; therefore we choose examples with medium difﬁculty and noisy extent and force the model to train on them.",
            "Document-level event argument extraction (EAE) is such a task requiring a model to extract arguments (i.e., participants) of an event at the document level [2], [4].",
            "Unlike theMUC-4formulation,arecentwork[2]proposesanewbench-markthatismoreﬁne-grainedandannotatestriggersintexts."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": true,
          "contexts": [
            "T EXTUAL event descriptions may span over multiple sentences; therefore to fully understand the semantics of an event we need to gather information from a document context for reasoning [1], [2], [3].",
            "…in the implicit knowledge transfer method, and in the explicit data generation method, we apply our method to a BERT-CRF model and a QAEE model [3], which uses a ﬁne-grained query generation strategy (we directly use the trigger prediction result of QAEE andonot…",
            "Particularly, we note document-level EAE is strongly related to a machine reading comprehension (MRC) problem [7], [8], if the argument extraction process is viewed as an answer retrieving procedure [3], [9], [10], [11].",
            "To name a few, authors in [9], [33] cast relation extraction into a question answering problem; authors in [10] address named entity recognition via an MRC formulation; authors in [3], [11] frame sentence-level event extraction as an MRC problem.",
            "Followingearlierstudies ondocument-levelEAE[2],[3],[4],[25],weuseprecision (P), recall (R), and F1 score (F1) as evaluation metrics and we adopt the exact match (EM) criterion: only when the projected argument span matches exactly a gold one do we consider it a correct prediction."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220046861,
          "isinfluential": true,
          "contexts": [
            "Despite its effectiveness, one disadvantage of implicit knowl-edgetransferisthatitcannotcreateexplicittrainingdata,henceit canonlybeneﬁtamodelinanMRCformulationbutnotinother formulations[2],[4].",
            "…BART-Gen employs the BART-large architecture, which has many more parameters than the BERT-base architecture used in previous meth-ods [2], [4], to make the parameter set comparable, we change the conﬁguration to BART-based. roles deﬁned by the gold event types and “ w/o Type Constraint”…",
            "On RAMS, for example, when the event argument is two sentences ahead of 5 The results are based on the development set following [4], and we only use the conﬁguration of “ w/ Type Constraint” to simplify discussion.",
            "For example, the state-of-the-art model attains less than 10% in F1 when the argument and trigger are in different sentences [2], [4], [6].",
            "Despitemanyadvancesonthistask,theproblemofinadequate training data still limits a model’s performance [29] — for example, even on the largest dataset RAMS, over 63% of the roles have fewer than 100 examples, limiting the performance of the currently best model to less than 50% in F1 [4].",
            "Document-level event argument extraction (EAE) is such a task requiring a model to extract arguments (i.e., participants) of an event at the document level [2], [4].",
            "Followingthework,awork[4]constructs a head-to-region approach yielding promising results; authors in [26] study the extent to which the pre-trained language model can assist learning; authors in [27] suggests that knowledge from FrameNet [28] be used to aid the reasoning process; authors in [5]…",
            "Followingearlierstudies ondocument-levelEAE[2],[3],[4],[25],weuseprecision (P), recall (R), and F1 score (F1) as evaluation metrics and we adopt the exact match (EM) criterion: only when the projected argument span matches exactly a gold one do we consider it a correct prediction.",
            "We conduct an error analysis following [4], by randomly selecting 100 error cases from the RAMS development set and determining their commonality.",
            "(cid:2) Head-Expand [4]."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Two-Step Approach for Implicit Event Argument Detection",
            "abstract": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1929423",
                "name": "Zhisong Zhang"
              },
              {
                "authorId": "145771502",
                "name": "X. Kong"
              },
              {
                "authorId": "100468503",
                "name": "Zhengzhong Liu"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 222208551,
          "isinfluential": false,
          "contexts": [
            "in [26] study the extent to which the pre-trained language model can assist learning; authors in [27] suggests that knowledge from"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset",
            "abstract": "Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT’s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, varying from a low of 17.77% for Place to a high of 51.61% for Artifact. Next, we find that linear combinations of these heads, estimated with approx. 11% of available total event argument detection supervision, can push performance well higher for some roles — highest two being Victim (68.29% Accuracy) and Artifact (58.82% Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate “best heads” for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in another sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated “nonce” words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.",
            "year": 2020,
            "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
            "authors": [
              {
                "authorId": "3375999",
                "name": "Varun Prashant Gangal"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236087324,
          "isinfluential": false,
          "contexts": [
            "Despitemanyadvancesonthistask,theproblemofinadequate training data still limits a model’s performance [29] — for example, even on the largest dataset RAMS, over 63% of the roles have fewer than 100 examples, limiting the performance of the currently best model to less than 50% in F1 [4]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Argument Linking: A Survey and Forecast",
            "abstract": "Semantic role labeling (SRL) -- identifying the semantic relationships between a predicate and other constituents in the same sentence -- is a well-studied task in natural language understanding (NLU). However, many of these relationships are evident only at the level of the document, as a role for a predicate in one sentence may often be filled by an argument in a different one. This more general task, known as implicit semantic role labeling or argument linking, has received increased attention in recent years, as researchers have recognized its centrality to information extraction and NLU. This paper surveys the literature on argument linking and identifies several notable shortcomings of existing approaches that indicate the paths along which future research effort could most profitably be spent.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1447311948",
                "name": "William Gantt Walden"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460308,
          "isinfluential": false,
          "contexts": [
            "…approach yielding promising results; authors in [26] study the extent to which the pre-trained language model can assist learning; authors in [27] suggests that knowledge from FrameNet [28] be used to aid the reasoning process; authors in [5] investigate a generative perspective on the task,…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
            "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "31407139",
                "name": "Kaiwen Wei"
              },
              {
                "authorId": "2946890",
                "name": "Xian Sun"
              },
              {
                "authorId": "151473773",
                "name": "Zequn Zhang"
              },
              {
                "authorId": "2108123471",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "2116390646",
                "name": "Zhi Guo"
              },
              {
                "authorId": "2152163772",
                "name": "Li Jin"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": true,
          "contexts": [
            "We prefer the BERT-based MRC model [12], and in addition to the question answering dataset, i.e., SQuAD 2.0 [36], we also use corpora in FrameNet semantic role labeling (SRL) [37] and ACE 2005 event extraction (EE) [38] for pre-training 2 , by framing them in an MRC formulation in a similar fashion."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "252625384": {
      "citing_paper_info": {
        "title": "Document-Level Event Temporal Relation Extraction on Global and Local Cues",
        "abstract": "Most previous work focused on extracting event temporal relations that the events appear in the same sentence or in two adjacent sentences, failing to address those nonadjacent-sentence event relations, which limits the development of tem-poral relation extraction at document-level and its real-world application. In this paper, we propose a novel Document-level event Temporal Relation Extraction (DTRE) model which can incorporate effective global cues with local cues. In particular, we select both the contextual sentences strongly related to the events and the temporal words in the context as global cues, which can provide additional semantic cues to extract those nonadjacent-sentence event temporal relations. Moreover, we further encode the events and their neighbor words as local cues to extract those intra-sentence relations and enhance the event representation. Experimental results on the English dataset show that our proposed DTRE outperforms several state-of-the-art baselines, especially for handling those nonadjacent-sentence temporal relations.",
        "year": 2022,
        "venue": "IEEE International Joint Conference on Neural Network",
        "authors": [
          {
            "authorId": "2153375930",
            "name": "Jing Li"
          },
          {
            "authorId": "145846496",
            "name": "Sheng Xu"
          },
          {
            "authorId": "47470867",
            "name": "Peifeng Li"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 13,
        "influential_count": 1,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "10503028",
        "215548441",
        "233307040",
        "52967399",
        "1957433",
        "202565622",
        "7294125",
        "232062161",
        "32821791",
        "218581125",
        "5165854",
        "237100961",
        "15139323"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1957433,
          "isinfluential": true,
          "contexts": [
            "These three local words are embedded using the external dictionary GloVE [26] and then concatenated as two local feature vectors e loc 1 and e loc 2 of two events e 1 and e 2 , respectively.",
            "The dimension of the word embedding in local information and TSDP is 300, using GloVe [26] as the external word to vector dictionary.",
            "Specifically, at the input end of attention, each word on TSDP is embedded by GLoVe [26] and the embedded sequences are denoted as e T = { v 1 , v 2 , ...v k − 1 , v k } , where v i is the embedded representation of each word in TSDP and k is its maximum length."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GloVe: Global Vectors for Word Representation",
            "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5165854,
          "isinfluential": false,
          "contexts": [
            "Low prevalence of global document-level temporal ordering annotation in existing corpora allows previous models to achieve moderate performance simply using local syntactic cues, such as SDP (Shortest Dependency Path) [5].",
            "Previous studies on event relation extraction often used SDP [5] as input, which simply connected the two shortest paths from the two events to the root node in a dependency tree."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Shortest Path Dependency Kernel for Relation Extraction",
            "abstract": "We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.",
            "year": 2005,
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "authors": [
              {
                "authorId": "3139133",
                "name": "Razvan C. Bunescu"
              },
              {
                "authorId": "1797655",
                "name": "R. Mooney"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7294125,
          "isinfluential": false,
          "contexts": [
            "Currently, the most popular temporal relation corpora, Time-Bank [3] and TimeBank-Dense [4], are annotated on sentence-level.",
            "Almost all temporal relation corpora are sentence-level and annotated on English, such as TimeBank [3], TimeBank-Dense [4], RED [7] and MATRES [8], which only annotated temporal relations between two events in same sentence or between adjacent sentences.",
            "Besides, we also report the results on the sentence-level event temporal relation dataset TB-Dense [4]."
          ],
          "intents": [
            "['background']",
            "--",
            "['result']"
          ],
          "cited_paper_info": {
            "title": "An Annotation Framework for Dense Event Ordering",
            "abstract": "Today’s event ordering research is heavily dependent on annotated corpora. Current corpora influence shared evaluations and drive algorithm development. Partly due to this dependence, most research focuses on partial orderings of a document’s events. For instance, the TempEval competitions and the TimeBank only annotate small portions of the event graph, focusing on the most salient events or on specific types of event pairs (e.g., only events in the same sentence). Deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented. This paper proposes a new annotation process with a mechanism to force annotators to label connected graphs. It generates 10 times more relations per document than the TimeBank, and our TimeBank-Dense corpus is larger than all current corpora. We hope this process and its dense corpus encourages research on new global models with deeper reasoning.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1739186",
                "name": "Taylor Cassidy"
              },
              {
                "authorId": "2064356436",
                "name": "Bill McDowell"
              },
              {
                "authorId": "1729918",
                "name": "Nathanael Chambers"
              },
              {
                "authorId": "2105138",
                "name": "Steven Bethard"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10503028,
          "isinfluential": false,
          "contexts": [
            "Inspired by Krause et. al [25]who found that incorporating nearby words of the events can provide additional information to improve event coreference task, we choose events themselves and their neighbors as local information which need to be further emphasized and apply them to the local…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Linking with Sentential Features from Convolutional Neural Networks",
            "abstract": "Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexicallevel and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research.",
            "year": 2016,
            "venue": "Conference on Computational Natural Language Learning",
            "authors": [
              {
                "authorId": "32632038",
                "name": "Sebastian Krause"
              },
              {
                "authorId": "2724114",
                "name": "Feiyu Xu"
              },
              {
                "authorId": "1781790",
                "name": "H. Uszkoreit"
              },
              {
                "authorId": "3319373",
                "name": "Dirk Weissenborn"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15139323,
          "isinfluential": false,
          "contexts": [
            "Almost all temporal relation corpora are sentence-level and annotated on English, such as TimeBank [3], TimeBank-Dense [4], RED [7] and MATRES [8], which only annotated temporal relations between two events in same sentence or between adjacent sentences."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation",
            "abstract": "There have been a wide range of recent annotated corpora concerning events, either regarding event coreference, the temporal order of events, hierarchical “subevent” structure of events, or causal relationships between events. However, although some believe that these different phenomena will display rich interactions, relatively few corpora annotate all of those layers of annotation in a uniﬁed fashion. This paper describes the annotation methodology for the Richer Event Descriptions corpus, which annotates entities, events, times, their coreference and partial coreference relations, and the temporal, causal and subevent relationships between the events. It suggests that such rich annotations of within-document event phenomena can be built with high quality through a multi-stage annotation pipeline, and that the resultant corpus could be useful for systems hoping to transition from the detection of isolated mentions of events toward a richer understanding of events grounded in the temporal, causal, referential and bridging relations that deﬁne them.",
            "year": 2016,
            "venue": "",
            "authors": [
              {
                "authorId": "1388957618",
                "name": "Timothy J. O'Gorman"
              },
              {
                "authorId": "1410067271",
                "name": "Kristin Wright-Bettner"
              },
              {
                "authorId": "145755155",
                "name": "Martha Palmer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 32821791,
          "isinfluential": false,
          "contexts": [
            "Motivated by Xu et. al [15], Cheng and Miyao [16]first used SDP as input of BiLSTM to recognize event temporal relations in same sentence or in adjacent sentences.",
            "CHeng et. al [16] first used SDP in temporal relation classification and proposed the hypothesis of common root, assuming two SDPs distributed in adjacent sentences can be connected through a virtual root.",
            "…method assigns a majority-class label to each event pair; 2) CAEVO [28]: A cascading event ordering architecture which based on sieves; 3) BiLSTM [16]: A BiLSTM model using SDP for the first time; 4) SP+ILP [29]: A method using structured perceptron and integer linear programming to obtain…"
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths",
            "abstract": "Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a “common root” assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.).",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49412583",
                "name": "Fei Cheng"
              },
              {
                "authorId": "1768065",
                "name": "Yusuke Miyao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "…Vector Machine) to improve event representation and extract temporal relation with shared representations and structured prediction; 6) BERT [31]: A BERT-base model which takes the event sentences with the beginning and end tags and then uses the output vector corresponding to the [ CLS ]…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202565622,
          "isinfluential": false,
          "contexts": [
            "…using structured perceptron and integer linear programming to obtain dependencies between events and infer the final temporal relation; 5) MT [30]: A joint model combining BiLSTM and structured SVM(Support Vector Machine) to improve event representation and extract temporal relation with…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction",
            "abstract": "We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10% and 6.8% on two benchmark datasets respectively.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "51518773",
                "name": "Rujun Han"
              },
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215548441,
          "isinfluential": false,
          "contexts": [
            "Joint learning and multi-task learning methods were also applied to obtain better event representation, which contains richer features [6], [19]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Severing the Edge between before and after: Neural Architectures for Temporal Ordering of Events",
            "abstract": "In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques. Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
              },
              {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
              },
              {
                "authorId": "1717480",
                "name": "Shuai Wang"
              },
              {
                "authorId": "3158273",
                "name": "Nima Pourdamghani"
              },
              {
                "authorId": "2065005",
                "name": "Yogarshi Vyas"
              },
              {
                "authorId": "2115889791",
                "name": "Jie Ma"
              },
              {
                "authorId": "50339091",
                "name": "Parminder Bhatia"
              },
              {
                "authorId": "145590324",
                "name": "K. McKeown"
              },
              {
                "authorId": "1403907739",
                "name": "Yaser Al-Onaizan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218581125,
          "isinfluential": false,
          "contexts": [
            "To remedy this, a few studies have explored the approaches of using external knowledge [20]–[22] and event causal relation patterns [23]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Temporal Common Sense Acquisition with Minimal Supervision",
            "abstract": "Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145360756",
                "name": "Ben Zhou"
              },
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              },
              {
                "authorId": "1783281",
                "name": "Daniel Khashabi"
              },
              {
                "authorId": "144590225",
                "name": "D. Roth"
              }
            ]
          }
        },
        {
          "citedcorpusid": 232062161,
          "isinfluential": false,
          "contexts": [
            "Obviously, temporal relation contains the temporal clues of document organization and extracting event temporal relation can help many downstream NLP (Natural Languange Processing) applications, such as question answering [1], automatic summarization [2]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Combining Temporal Event Relations and Pre-Trained Language Models for Text Summarization",
            "abstract": "In this paper, we introduce an innovative high performing deep learning architecture for text summarization using pre-trained language models. Language model (LM) pre-training has provided an impressive performance on many of language understanding tasks. However, it remains much for experimentation to efficiently utilize these pre-trained models for specific tasks. We propose a novel architecture(ENEMAbst) effectively utilizing pre-trained LM MASS for summarization. We also propose that the use of temporal relations across event representations can significantly improve language generation and inference. We demonstrate experimentally that using events provides a more in-depth, comprehensive understanding of the text. We showcase how MASS, along with Event representations, can be used for summarization. We couple our Event-Network-Encoders(ENE) with MASS based document level encoders(MASSEnc) by attention augmenting MASSEnc with ENE. We devise an innovative fine-tuning schedule to train our Pretrained-Encoders and Pretrained-Decoders with ENE efficiently. We show empirically that our model performs best compared to state-of-the-art baseline methods for abstractive and extractive summarization. Our model provides 0.85 point improvement from 43.331 to 44.179 for abstractive summarization and 0.63 point improvement from 43.85 to 44.48 for extractive summarization on ROUGE-1 over the existing best performing models.",
            "year": 2020,
            "venue": "International Conference on Machine Learning and Applications",
            "authors": [
              {
                "authorId": "46205943",
                "name": "Divyanshu Daiya"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233307040,
          "isinfluential": false,
          "contexts": [
            "Based on this method, Dai et. al [17]used Graph Convolution Network (GCN) to obtain better interaction between words on the shortest dependency path to enhance the performance and Zhang et. al [18]constructed syntactic-guided temporal graph transformer to find deep connections between events."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Extracting Temporal Event Relation with Syntactic-Guided Temporal Graph Transformer",
            "abstract": "Extracting temporal relations (e.g., before, after, concurrent) among events is crucial to nat-ural language understanding. Previous studies mainly rely on neural networks to learn effective features or manual-crafted linguistic features for temporal relation extraction, which usually fail when the context between two events is complex or wide. Inspired by the examination of available temporal relation annotations and human-like cognitive procedures, we propose a new Temporal Graph Transformer network to (1) explicitly ﬁnd the connection between two events from a syntactic graph constructed from one or two continuous sentences, and (2) automatically locate the most indicative temporal cues from the path of the two event mentions as well as their surrounding concepts in the syntactic graph with a new temporal-oriented attention mechanism. Experiments on MATRES and TB-Dense datasets show that our approach significantly outperforms previous state-of-the-art methods on both end-to-end temporal relation extraction and temporal relation classiﬁcation.",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2108433254",
                "name": "Shuaicheng Zhang"
              },
              {
                "authorId": "34170717",
                "name": "Lifu Huang"
              },
              {
                "authorId": "3333257",
                "name": "Qiang Ning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237100961,
          "isinfluential": false,
          "contexts": [
            "We use the same data division following Naik et. al [9] and Liu et. al [24].",
            "Liu et. al [24]regarded each document as an uncertainty-guided temporal graph, in which the nodes and edges represented events and event-event relations, respectively.",
            "…model which takes the event sentences with the beginning and end tags and then uses the output vector corresponding to the [ CLS ] of the last layer to extract temporal relations directly; 7) UCGraph [24]: A Graph model using pre-training mechanism to learn inter-dependencies of temporal relations."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Discourse-Level Event Temporal Ordering with Uncertainty-Guided Graph Completion",
            "abstract": "Learning to order events at discourse-level is a crucial text understanding task.\n\nDespite many efforts for this task, the current state-of-the-art methods rely heavily on manually designed features, which are costly to produce and are often specific to tasks/domains/datasets.\n\nIn this paper, we propose a new graph perspective on the task, which does not require complex feature engineering but can assimilate global features and learn inter-dependencies effectively.\n\nSpecifically, in our approach, each document is considered as a temporal graph, in which the nodes and edges represent events and event-event relations respectively.\n\nIn this sense, the temporal ordering task corresponds to constructing edges for an empty graph.\n\nTo train our model, we design a graph mask pre-training mechanism, which can learn inter-dependencies of temporal relations by learning to recover a masked edge following graph topology.\n\nIn the testing stage, we design an certain-first strategy based on model uncertainty, which can decide the prediction orders and reduce the risk of error propagation.\n\nThe experimental results demonstrate that our approach outperforms previous methods consistently and can meanwhile maintain good global consistency.",
            "year": 2021,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2150168584",
                "name": "Jian Liu"
              },
              {
                "authorId": "2310092",
                "name": "Jinan Xu"
              },
              {
                "authorId": "47559028",
                "name": "Yufeng Chen"
              },
              {
                "authorId": "2108257896",
                "name": "Yujie Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "MaxS ( W j , W i ) represents the maximum semantic similarity between the words W i in sen c and the words W j in sen e following Xu [27]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "256315539": {
      "citing_paper_info": {
        "title": "Ripl: Document-Level Event Argument Extraction via Role-aware Interactive Pointer Labeling Network",
        "abstract": "Document-level event argument extraction (DEAE) aims to recognize event arguments spreading across multiple sentences and their corresponding roles. DEAE is a challenging but indispensable subtask in general document-level event extraction. Existing methods are not effective due to two challenges of this task: (a) traditional extraction methods cannot assign multiple labels to a token, so these methods are unable to identify two nested arguments belonging to different roles; (b) previous methods are weak in encoding the document-level sequence and they have never modeled the relevance between different roles. In this paper, we propose a Role-aware Interactive Pointer Labeling Network (Ripl) to solve these two challenges. For challenge (a), we construct specific document representations for each role and propose a novel interactive pointer labeling strategy to extract nested arguments. For challenge (b), we introduce role prior knowledge into the original document, make the representation of each word in document captures the semantic information of the role and model the semantic relevance among the roles. Experiments on the MUC-4 event extraction dataset show that Ripl outperforms the previous state-of-the-art model by 5.9 F1 on the head noun match metric. Then we demonstrate the effectiveness and interpretability of our model with extensive experiments.",
        "year": 2022,
        "venue": "2022 IEEE International Conference on Big Data (Big Data)",
        "authors": [
          {
            "authorId": "2340566480",
            "name": "Yuanzhuo Li"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 2,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "6644751",
        "17719760",
        "218630327",
        "14117526",
        "6452487",
        "8950084",
        "3312944",
        "950755",
        "219683473",
        "204902024",
        "14542261"
      ],
      "citation_details": [
        {
          "citedcorpusid": 950755,
          "isinfluential": false,
          "contexts": [
            "TIER [5] propose a model in pipeline way.",
            "[5], [6], [18] employ traditional classification paradigm to identify the trigger and determine the event type, then they identify the arguments and classify the roles they play in an event; [3], [22] use the sequence labeling model BiLSTM [25]-CRF [7] to automatically extract events."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3312944,
          "isinfluential": true,
          "contexts": [
            "…32 and the max training epoch as 20; Furthermore, we regularize our network using dropout, the dropout ratio of linear and attention layers are both 0.3; The initial learning rate is 2 e − 5 for BERT parameters and 2 e − 3 for other parameters; We trained all models with the AdamW optimizer [15]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Fixing Weight Decay Regularization in Adam",
            "abstract": "We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process.",
            "year": 2017,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6452487,
          "isinfluential": false,
          "contexts": [
            "As a similar task, most of sentence-level event extraction works formalize the event argument extraction task as a sequence tagging problem [14], [17], [20], i.",
            "[9], [11] employ various hand-designed features to extract event; [1], [13], [17] use deep learning based models such as recurrent neural networks (RNNs) [24] and convolutional neural network (CNN) [8] to extract event."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Recurrent Neural Networks",
            "abstract": "Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6644751,
          "isinfluential": false,
          "contexts": [
            "[5], [6], [18] employ traditional classification paradigm to identify the trigger and determine the event type, then they identify the arguments and classify the roles they play in an event; [3], [22] use the sequence labeling model BiLSTM [25]-CRF [7] to automatically extract events.",
            "Cohesion Extract [6] adopt a bottom-up approach, which first aggressively identifies candidate role arguments in the document and then refines the candidate set with cohesion sentence classifier."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Modeling Textual Cohesion for Event Extraction",
            "abstract": "\n \n Event extraction systems typically locate the role fillers for an event by analyzing sentences in isolation and identifying each role filler independently of the others. We argue that more accurate event extraction requires a view of the larger context to decide whether an entity is related to a relevant event. We propose a bottom-up approach to event extraction that initially identifies candidate role fillers independently and then uses that information as well as discourse properties to model textual cohesion. The novel component of the architecture is a sequentially structured sentence classifier that identifies event-related story contexts. The sentence classifier uses lexical associations and discourse relations across sentences, as well as domain-specific distributions of candidate role fillers within and across sentences. This approach yields state-of-the-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems.\n \n",
            "year": 2012,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8950084,
          "isinfluential": false,
          "contexts": [
            "[5], [6], [18] employ traditional classification paradigm to identify the trigger and determine the event type, then they identify the arguments and classify the roles they play in an event; [3], [22] use the sequence labeling model BiLSTM [25]-CRF [7] to automatically extract events.",
            "Specifically, [3] partitioned one document into a set of contiguous sentences in the document, and used BiLSTM-CRF [7], [25] to extract event arguments and their corresponding roles."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Bidirectional Long Short-Term Memory Networks for Relation Classification",
            "abstract": "",
            "year": 2015,
            "venue": "Pacific Asia Conference on Language, Information and Computation",
            "authors": [
              {
                "authorId": "2108089005",
                "name": "Shu Zhang"
              },
              {
                "authorId": "37032041",
                "name": "Dequan Zheng"
              },
              {
                "authorId": "2435746",
                "name": "Xinchen Hu"
              },
              {
                "authorId": "2150425875",
                "name": "Ming Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14117526,
          "isinfluential": false,
          "contexts": [
            "[9], [11] employ various hand-designed features to extract event; [1], [13], [17] use deep learning based models such as recurrent neural networks (RNNs) [24] and convolutional neural network (CNN) [8] to extract event."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Detection with Abstract Meaning Representation",
            "abstract": "Event Detection (ED) aims to identify instances of specified types of events in text, which is a crucial component in the overall task of event extraction. The commonly used features consist of lexical, syntactic, and entity information, but the knowledge encoded in the Abstract Meaning Representation (AMR) has not been utilized in this task. AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. In this paper, we demonstrate the effectiveness of AMR to capture and represent the deeper semantic contexts of the trigger words in this task. Experimental results further show that adding AMR features on top of the traditional features can achieve 67.8% (with 2.1% absolute improvement) F-measure (F1), which is comparable to the state-of-the-art approaches.",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2144438930",
                "name": "Xiang Li"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "2065281098",
                "name": "Kai Cao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14542261,
          "isinfluential": false,
          "contexts": [
            "[9], [11] employ various hand-designed features to extract event; [1], [13], [17] use deep learning based models such as recurrent neural networks (RNNs) [24] and convolutional neural network (CNN) [8] to extract event."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Gradient-based learning applied to document recognition",
            "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
            "year": 1998,
            "venue": "Proceedings of the IEEE",
            "authors": [
              {
                "authorId": "1688882",
                "name": "Yann LeCun"
              },
              {
                "authorId": "52184096",
                "name": "L. Bottou"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              },
              {
                "authorId": "1721248",
                "name": "P. Haffner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17719760,
          "isinfluential": false,
          "contexts": [
            "[9], [11] employ various hand-designed features to extract event; [1], [13], [17] use deep learning based models such as recurrent neural networks (RNNs) [24] and convolutional neural network (CNN) [8] to extract event."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Recurrent Neural Network Regularization",
            "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.",
            "year": 2014,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204902024,
          "isinfluential": false,
          "contexts": [
            "In order to solve the first problem, we apply the pointer labeling strategy [12] to extract nested arguments."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified MRC Framework for Named Entity Recognition",
            "abstract": "The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the per label is formalized as extracting answer spans to the question “which person is mentioned in the text\".This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions. Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER. We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation. We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2845020",
                "name": "Xiaoya Li"
              },
              {
                "authorId": "2108687075",
                "name": "Jingrong Feng"
              },
              {
                "authorId": "65844131",
                "name": "Yuxian Meng"
              },
              {
                "authorId": "5439717",
                "name": "Qinghong Han"
              },
              {
                "authorId": "144894849",
                "name": "Fei Wu"
              },
              {
                "authorId": "49298465",
                "name": "Jiwei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218630327,
          "isinfluential": true,
          "contexts": [
            "[5], [6], [18] employ traditional classification paradigm to identify the trigger and determine the event type, then they identify the arguments and classify the roles they play in an event; [3], [22] use the sequence labeling model BiLSTM [25]-CRF [7] to automatically extract events.",
            "Multi-Granularity Reader (MG-Reader) [3] propose a multi-granularity reader based on the LSTM network to dynamically incorporate paragraph-and sentence-level contextualized representations to assist the extraction of event arguments.",
            "We choose MG-Reader [3] as a comparison, the results of MUC-4 are listed in Table 4.",
            "Specifically, [3] partitioned one document into a set of contiguous sentences in the document, and used BiLSTM-CRF [7], [25] to extract event arguments and their corresponding roles.",
            "For fair comparison, we adopt the same data preprocessing method as the previous work [3]."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['result']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
            "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            "[5], [6], [18] employ traditional classification paradigm to identify the trigger and determine the event type, then they identify the arguments and classify the roles they play in an event; [3], [22] use the sequence labeling model BiLSTM [25]-CRF [7] to automatically extract events.",
            "Specifically, [3] partitioned one document into a set of contiguous sentences in the document, and used BiLSTM-CRF [7], [25] to extract event arguments and their corresponding roles."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        }
      ]
    },
    "231728756": {
      "citing_paper_info": {
        "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
        "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
        "year": 2021,
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "13728923",
            "name": "Xinya Du"
          },
          {
            "authorId": "2531268",
            "name": "Alexander M. Rush"
          },
          {
            "authorId": "1748501",
            "name": "Claire Cardie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 15,
        "influential_count": 3,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "5073927",
        "51871927",
        "7961699",
        "950755",
        "14365335",
        "2114517",
        "6644751",
        "14339673",
        "54088698",
        "11239061",
        "52816033",
        "189898081",
        "5188467",
        "278288",
        "6341459"
      ],
      "citation_details": [
        {
          "citedcorpusid": 278288,
          "isinfluential": false,
          "contexts": [
            "Unsupervised event schema induction based approaches (Chambers and Jurafsky, 2011; Chambers, 2013; Cheung et al., 2013) are also able",
            "Unsupervised event schema induction based approaches (Chambers and Jurafsky, 2011; Cham-bers, 2013; Cheung et al., 2013) to model the coreference relations and entities at document-level, but have been proved to perform substantially worse than supervised models (Pat-wardhan and Riloff, 2009; Huang…"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Probabilistic Frame Induction",
            "abstract": "In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.",
            "year": 2013,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3159752",
                "name": "J. Cheung"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              },
              {
                "authorId": "1909300",
                "name": "Lucy Vanderwende"
              }
            ]
          }
        },
        {
          "citedcorpusid": 950755,
          "isinfluential": false,
          "contexts": [
            "Evaluation Metric The metric for past work on document-level role-filler mentions extraction (Patwardhan and Riloff, 2009; Huang and Riloff, 2011; Du and Cardie, 2020) calculates mention-level precision across all alternative mentions for each rolefiller entity."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5073927,
          "isinfluential": false,
          "contexts": [
            "Document-level template filling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014).",
            "Document-level template ﬁlling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014)."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2nd Edition",
            "abstract": "is one of the most recognizablecharacters in 20th century cinema. HAL is an artiﬁcial agent capable of such advancedlanguage behavior as speaking and understanding English, and at a crucial moment inthe plot, even reading lips. It is now clear that HAL’s creator, Arthur C. Clarke, wasa little optimistic in predicting when an artiﬁcial agent such as HAL would be avail-able. But just how far off was he? What would it take to create at least the language-relatedpartsofHAL?WecallprogramslikeHALthatconversewithhumansinnatural",
            "year": 2000,
            "venue": "Prentice Hall series in artificial intelligence",
            "authors": [
              {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
              },
              {
                "authorId": "10796472",
                "name": "James H. Martin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5188467,
          "isinfluential": false,
          "contexts": [
            "Document-level template filling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Design of the MUC-6 Evaluation",
            "abstract": "The sixth in a series of \"Message Understanding Conferences\", which are designed to promote and evaluate research in information extraction, was held last fall. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe the development of the \"message understanding\" task over the course of the prior MUCs, some of the motivations for the new format, and the steps which led up to the formal evaluation.",
            "year": 1995,
            "venue": "NIST's TIPSTER Text Program",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6341459,
          "isinfluential": false,
          "contexts": [
            "Unsupervised event schema induction based approaches (Chambers and Jurafsky, 2011; Chambers, 2013; Cheung et al., 2013) are also able"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Schema Induction with a Probabilistic Entity-Driven Model",
            "abstract": "Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).",
            "year": 2013,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1729918",
                "name": "Nathanael Chambers"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6644751,
          "isinfluential": true,
          "contexts": [
            "We base the REE task on the original MUC3 formulation (Sundheim, 1991), but simplify it as done in prior research (Huang and Riloff, 2012; Du and Cardie, 2020).",
            "Similar to (Huang and Riloff, 2012; Du and Cardie, 2020), we use the 1300 documents for training, 200 documents (TST1+TST2) as the development set and 200 documents (TST3+TST4) as the test set.",
            "CohesionExtract (Huang and Riloff, 2012) is a bottom-up approach for event extraction that first aggressively identifies candidate role-fillers, and prune the candidates located in event-irrelevant sentences.",
            "to model the coreference relations and entities at document-level, but have been proved to perform substantially worse than supervised models (Patwardhan and Riloff, 2009; Huang and Riloff, 2012).",
            "These event roles represent the agents, patients, and instruments associated with terrorism events (Huang and Riloff, 2012)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Modeling Textual Cohesion for Event Extraction",
            "abstract": "\n \n Event extraction systems typically locate the role fillers for an event by analyzing sentences in isolation and identifying each role filler independently of the others. We argue that more accurate event extraction requires a view of the larger context to decide whether an entity is related to a relevant event. We propose a bottom-up approach to event extraction that initially identifies candidate role fillers independently and then uses that information as well as discourse properties to model textual cohesion. The novel component of the architecture is a sequentially structured sentence classifier that identifies event-related story contexts. The sentence classifier uses lexical associations and discourse relations across sentences, as well as domain-specific distributions of candidate role fillers within and across sentences. This approach yields state-of-the-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems.\n \n",
            "year": 2012,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7961699,
          "isinfluential": true,
          "contexts": [
            "We treat document-level REE as a sequence-tosequence task (Sutskever et al., 2014) in order to better model the cross-role dependencies and crosssentence noun phrase coreference structure.",
            "We treat document-level REE as a sequence-to-sequence task (Sutskever et al., 2014) in order to better model the cross-role dependencies and cross-sentence noun phrase coreference structure.",
            "Instead of using a sequence-to-sequence learning architecture with separate modules (Sutskever et al., 2014; Bahdanau et al., 2015), we use a single pretrained transformer model (Devlin et al., 2019) for both parts, and introduce no additional ﬁne-tuned parameters.",
            "Instead of using a sequence-to-sequence learning architecture with separate modules (Sutskever et al., 2014; Bahdanau et al., 2015), we use a single pretrained transformer model (Devlin et al."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
            "year": 2014,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11239061,
          "isinfluential": true,
          "contexts": [
            "We use CEAF-REE which is covered in Section 3 as the evaluation metric.",
            "Finally, we calculate the precision, recall and F-measure for CEAF-REE as follows: We list several cases (Figure 6) and their CEAF-REE scores (Table 8) to facilitate understanding.",
            "Drawing insights from the entity-based CEAF metric (Luo, 2005) from the coreference resolution literature, we design a metric (CEAF-REE) for measuring models’ performance on this documentlevel role-filler entity extraction task.",
            "Drawing insights from the entity-based CEAF metric (Luo, 2005) from the coreference resolution literature, we design a metric ( CEAF-REE ) for measuring models’ performance on this document-level role-ﬁller entity extraction task.",
            "We include more details for our CEAF-TF metric in the appendix.",
            "• To measure the model’s ability to both extract entities for each role, and implicitly recognize coreferent relations between entity mentions, we design a metric (CEAF-REE) based on a maximum bipartite matching algorithm, drawing insights from the CEAF (Luo, 2005) coreference resolution measure."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "On Coreference Resolution Performance Metrics",
            "abstract": "The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widely-known MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.",
            "year": 2005,
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "authors": [
              {
                "authorId": "34501578",
                "name": "Xiaoqiang Luo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grish-man, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance.",
            ", 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al.",
            "As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14365335,
          "isinfluential": false,
          "contexts": [
            "We base the REE task on the original MUC3 formulation (Sundheim, 1991), but simplify it as done in prior research (Huang and Riloff, 2012; Du and Cardie, 2020).",
            "It is of great importance for automating many real-world tasks, such as event extraction from newswire (Sundheim, 1991)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Third Message Understanding Evaluation and Conference",
            "abstract": "The Naval Ocean Systems Center (NOSC) has conducted the third in a series of evaluations of English text analysis systems. These evaluations are intended to advance our understanding of the merits of current text analysis techniques, as applied to the performance of a realistic information extraction task. The latest one is also intended to provide insight into information retrieval technology (document retrieval and categorization) used instead of or in concert with language understanding technology. The inputs to the analysis/extraction process consist of naturally-occurring texts that were obtained in the form of electronic messages. The outputs of the process are a set of templates or semantic frames resembling the contents of a partially formatted database.",
            "year": 1991,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871927,
          "isinfluential": false,
          "contexts": [
            "Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention",
            "abstract": "Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": null,
                "name": "Yue Zhao"
              },
              {
                "authorId": "2149111400",
                "name": "Xiaolong Jin"
              },
              {
                "authorId": "2219600",
                "name": "Yuanzhuo Wang"
              },
              {
                "authorId": "1717004",
                "name": "Xueqi Cheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52816033,
          "isinfluential": false,
          "contexts": [
            "More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grish-man, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
            "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2730687",
                "name": "Zhunchen Luo"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54088698,
          "isinfluential": false,
          "contexts": [
            "For the machine translation task, He et al. (2018) propose a model which shares the parameters of each layer between the encoder and decoder to regularize and coordinate the learning."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation",
            "abstract": "Neural Machine Translation (NMT) has achieved remarkable progress with the quick evolvement of model structures. In this paper, we propose the concept of layer-wise coordination for NMT, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level. Specifically, we design a layer-wise attention and mixed attention mechanism, and further share the parameters of each layer between the encoder and decoder to regularize and coordinate the learning. Experiments show that combined with the state-of-the-art Transformer model, layer-wise coordination achieves improvements on three IWSLT and two WMT translation tasks. More specifically, our method achieves 34.43 and 29.01 BLEU score on WMT16 English-Romanian and WMT14 English-German tasks, outperforming the Transformer baseline.",
            "year": 2018,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "47237216",
                "name": "Tianyu He"
              },
              {
                "authorId": "2112780268",
                "name": "Xu Tan"
              },
              {
                "authorId": "2794096",
                "name": "Yingce Xia"
              },
              {
                "authorId": "1391126980",
                "name": "Di He"
              },
              {
                "authorId": "143826491",
                "name": "Tao Qin"
              },
              {
                "authorId": "31482866",
                "name": "Zhibo Chen"
              },
              {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": false,
          "contexts": [
            "Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        }
      ]
    },
    "258718175": {
      "citing_paper_info": {
        "title": "Integrating Hybrid Features for Document-Level Event Role Extraction Method",
        "abstract": "Event extraction is a sub-task of information extraction and is an important part of natural language processing. Depending on the range of features used, event extraction methods are classified as sentence-level or document-level. However, document-level event extraction is more practical for practical tasks. Document-level event extraction is a difficult task, as it requires features to be extracted from a larger amount of text to determine which span of text is the desired event element. However, most methods do not utilize both sentence-level and document-level features. In order to utilize hybrid feature information and fuse it, this paper proposes a document-level event extraction method that integrating hybrid features. The event extraction method is based on Dynamic Multi-Pooling Convolutional Neural Network (DMCNN) and Bi-directional Long Short-Term Memory (BiLSTM), combined with self-attention mechanisms and Conditional Random Field (CRF). We evaluate the model proposed in this paper on the MUC-4 dataset and the experimental results show that our proposed model outperforms previous work.",
        "year": 2022,
        "venue": "International Conference on Artificial Intelligence and Pattern Recognition",
        "authors": [
          {
            "authorId": "2108122649",
            "name": "Jingyao Zhang"
          },
          {
            "authorId": "145017758",
            "name": "Tao Xu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 10,
        "unique_cited_count": 10,
        "influential_count": 1,
        "detailed_records_count": 10
      },
      "cited_papers": [
        "231728756",
        "209319719",
        "9946972",
        "4760632",
        "238660430",
        "202539496",
        "226096901",
        "76666127",
        "2524712",
        "6644751"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2524712,
          "isinfluential": false,
          "contexts": [
            "Patwardhan et al. [16] jointly considered evidence across sentences and noun phrases in a probabilistic framework to extract role fillers.",
            "In order to verify the superiority and effectiveness of the proposed model, we compared the proposed model with other representative document-level event extraction methods, including GLACIER [16], Cohesion Extract [17] and GRIT [19]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
            "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
            "year": 2009,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4760632,
          "isinfluential": false,
          "contexts": [
            "Most researchers have captured long-range dependencies in long sequences by using recurrent neural networks [12]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses",
            "abstract": "Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\\,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.",
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "40895509",
                "name": "Trieu H. Trinh"
              },
              {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
              },
              {
                "authorId": "1821711",
                "name": "Thang Luong"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6644751,
          "isinfluential": false,
          "contexts": [
            "However, researchers typically use pipelined architectures for event extraction tasks, with separate classifiers for each type of actor and associated context detection [9]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Modeling Textual Cohesion for Event Extraction",
            "abstract": "\n \n Event extraction systems typically locate the role fillers for an event by analyzing sentences in isolation and identifying each role filler independently of the others. We argue that more accurate event extraction requires a view of the larger context to decide whether an entity is related to a relevant event. We propose a bottom-up approach to event extraction that initially identifies candidate role fillers independently and then uses that information as well as discourse properties to model textual cohesion. The novel component of the architecture is a sequentially structured sentence classifier that identifies event-related story contexts. The sentence classifier uses lexical associations and discourse relations across sentences, as well as domain-specific distributions of candidate role fillers within and across sentences. This approach yields state-of-the-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems.\n \n",
            "year": 2012,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9946972,
          "isinfluential": false,
          "contexts": [
            "Extraction of document-level events is essential to facilitate downstream applications such as information retrieval and article summarization [3], as well as real-life applications such as trend analysis of world events [4]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Fourth Message Understanding Evaluation and Conference",
            "abstract": "The Fourth Message Understanding Conference (MUC-4) is the latest in a series of conferences that concern the evaluation of natural language processing (NLP) systems. These conferences have reported on progress being made both in the development of systems capable of analyzing relatively short English texts and in the definition of a rigorous performance evaluation methodology. MUC-4 was preceded by a period of intensive system development by each of the participating organizations and blind testing using materials prepared by NRaD and SAIC that are described in this paper, other papers in this volume, and the MUC-3 proceedings [1].",
            "year": 1992,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 76666127,
          "isinfluential": false,
          "contexts": [
            "The computational formula is as follows: For contextualized embedding, we use the average of the contextual features generated by the 12 layers in the pre-trained BERT-base language model [21] and freeze the weights during training."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
            "abstract": "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.",
            "year": 2019,
            "venue": "RepL4NLP@ACL",
            "authors": [
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
              },
              {
                "authorId": "144365875",
                "name": "Noah A. Smith"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202539496,
          "isinfluential": false,
          "contexts": [
            "Wadden et al. [11] extracted entities, relations and events using contextualized span representations.",
            "Wadden et al. [11] utilized a pre-trained model to extract contextual features."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
            "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "30051202",
                "name": "David Wadden"
              },
              {
                "authorId": "1387977694",
                "name": "Ulme Wennberg"
              },
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209319719,
          "isinfluential": false,
          "contexts": [
            "In early event extraction tasks, researchers extracted event types and argumentative roles by using template matching and building an additional corpus [5]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Event Extraction From Text",
            "abstract": "Numerous important events happen everyday and everywhere but are reported in different media sources with different narrative styles. How to detect whether real-world events have been reported in articles and posts is one of the main tasks of event extraction. Other tasks include extracting event arguments and identifying their roles, as well as clustering and tracking similar events from different texts. As one of the most important research themes in natural language processing and understanding, event extraction has a wide range of applications in diverse domains and has been intensively researched for decades. This article provides a comprehensive yet up-to-date survey for event extraction from text. We not only summarize the task definitions, data sources and performance evaluations for event extraction, but also provide a taxonomy for its solution approaches. In each solution group, we provide detailed analysis for the most representative methods, especially their origins, basics, strengths and weaknesses. Last, we also present our envisions about future research directions.",
            "year": 2019,
            "venue": "IEEE Access",
            "authors": [
              {
                "authorId": "2052727822",
                "name": "Wei Xiang"
              },
              {
                "authorId": "1719003",
                "name": "Bang Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226096901,
          "isinfluential": true,
          "contexts": [
            "Zhang et al. [18] study an end-to-end model based on BERT, introducing the embedding of event types and subsequent layers of event parameters and recognition roles for entity nodes, representing the relationship between event parameters and roles, and improving classification accuracy with multiple event parameters.",
            "As can be seen from the results, the precision of the model is substantially reduced when we remove the contextual features extracted from the pre-trained BERT layer in the embedding layer.",
            "Researchers have used pre-trained bidirectional transformer models, such as BERT [13], to better capture long-range dependencies compared to RNNs.",
            "The computational formula is as follows: For contextualized embedding, we use the average of the contextual features generated by the 12 layers in the pre-trained BERT-base language model [21] and freeze the weights during training."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "136309159",
                "name": "知秀 柴田"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231728756,
          "isinfluential": false,
          "contexts": [
            "Du et al. [19] propose a generative transformer-based encoder-decoder framework (GRIT), which aims to model context at the document-level.",
            "In order to verify the superiority and effectiveness of the proposed model, we compared the proposed model with other representative document-level event extraction methods, including GLACIER [16], Cohesion Extract [17] and GRIT [19]."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
            "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238660430,
          "isinfluential": false,
          "contexts": [
            "Event extraction is an important task in information extraction that extracts information of interest to the user from unstructured text and represents it in a structured form [1]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Temporal Relation Extraction with Attention Mechanism and Graph Neural Network",
            "abstract": "",
            "year": 2022,
            "venue": "",
            "authors": [
              {
                "authorId": "2115191808",
                "name": "Xiaoliang Xu"
              },
              {
                "authorId": "2065878412",
                "name": "G. Tong"
              },
              {
                "authorId": "2115829666",
                "name": "Yuxiang Wang"
              },
              {
                "authorId": "2131998468",
                "name": "Xinle Xuan"
              }
            ]
          }
        }
      ]
    },
    "218630327": {
      "citing_paper_info": {
        "title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding",
        "abstract": "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",
        "year": 2020,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "13728923",
            "name": "Xinya Du"
          },
          {
            "authorId": "1748501",
            "name": "Claire Cardie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 19,
        "unique_cited_count": 19,
        "influential_count": 2,
        "detailed_records_count": 19
      },
      "cited_papers": [
        "201646551",
        "1957433",
        "52967399",
        "2797612",
        "1697424",
        "1320606",
        "14339673",
        "2524712",
        "6300165",
        "5590763",
        "2114517",
        "6042994",
        "10910955",
        "76666127",
        "189928589",
        "11187670",
        "950755",
        "219683473",
        "4760632"
      ],
      "citation_details": [
        {
          "citedcorpusid": 950755,
          "isinfluential": true,
          "contexts": [
            "TIER (Huang and Riloff, 2011) proposes to first determine the document genre with a classifier and then identify event-relevant sentences and role fillers in the",
            "C L\n] 1\n3 M\nay 2\n02 0\nwith separate classifiers for each type of role and for relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011).",
            "Similar to Huang and Riloff (2012), we also incorporate both intra-sentence and cross-sentence features (paragraph-level features), but instead of using manually designed linguistic information,",
            "TIER (Huang and Riloff, 2011) proposes to first determine the document genre with a classifier and then identify event-relevant sentences and role fillers in the document.",
            "The final extraction decisions are based on the product of normalized sentential and phrasal probabilities; TIER (Huang and Riloff, 2011) proposes a multi-stage approach.",
            "8011 relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011).",
            "Cohesion Extract obtains substantially better precision and with similar level of recall as compared to GLACIER and TIER.",
            "Huang and Riloff (2012) propose a bottom-up approach that first aggressively identifies candidate role fillers (with lexico-syntactic pattern features), and then removes the candidates that are in spurious sentences (i.",
            "08 - - TIER (Huang and Riloff, 2011) 50.",
            "decisions are based on the product of normalized sentential and phrasal probabilities; TIER (Huang and Riloff, 2011) proposes a multi-stage approach."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "--",
            "--",
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1320606,
          "isinfluential": false,
          "contexts": [
            "Ji and Grishman (2008) enforce event role consistency across documents."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Refining Event Extraction through Cross-Document Inference",
            "abstract": "",
            "year": 2008,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1697424,
          "isinfluential": false,
          "contexts": [
            "Many variations of models have been proposed to mitigate the effect of long sequence length, such as Long Short Term Memory (LSTM) Networks (Hochre-iter and Schmidhuber, 1997; Gers et al., 1999; Graves, 2013) and Gated Recurrent Unit Networks (Cho et al., 2014)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Generating Sequences With Recurrent Neural Networks",
            "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
            "year": 2013,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1753223",
                "name": "Alex Graves"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1957433,
          "isinfluential": false,
          "contexts": [
            "…Layer In the embedding layer, we represent each token x i in the input sequence as the concatenation of its word embedding and contextual token representation: • Word Embedding : We use the 100-dimensional GloVe pre-trained word embeddings (Pennington et al., 2014) trained from 6B Web crawl data."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GloVe: Global Vectors for Word Representation",
            "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
              },
              {
                "authorId": "2166511",
                "name": "R. Socher"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2013, 2015) explore various hand-designed features; Nguyen and Grishman (2015); Nguyen et al. (2016); Chen et al. (2015); Liu et al. (2017, 2018) employ deep learning based models such as recurrent neural networks (RNNs) and convolutional neural network (CNN).",
            "features; Nguyen and Grishman (2015); Nguyen et al. (2016); Chen et al.",
            "features; Nguyen and Grishman (2015); Nguyen et al."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2524712,
          "isinfluential": true,
          "contexts": [
            "C L\n] 1\n3 M\nay 2\n02 0\nwith separate classifiers for each type of role and for relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011).",
            "GLACIER (Patwardhan and Riloff, 2009) jointly considers crosssentence and noun phrase evidence in a probabilistic framework to extract role fillers.",
            "We compare to the pipeline and manual feature engineering based systems: GLACIER (Patwardhan and Riloff, 2009) consists of a sentential event classifier and a set of plausible role filler recog-",
            "8011 relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011).",
            "Cohesion Extract obtains substantially better precision and with similar level of recall as compared to GLACIER and TIER.",
            "We compare to the pipeline and manual feature engineering based systems: GLACIER (Patwardhan and Riloff, 2009) consists of a sentential event classifier and a set of plausible role filler recognizers for each event role.",
            "GLACIER (Patwardhan and Riloff, 2009) jointly considers cross-"
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
            "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
            "year": 2009,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2797612,
          "isinfluential": false,
          "contexts": [
            "Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs",
            "abstract": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
            "year": 2017,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              },
              {
                "authorId": "2596310",
                "name": "Chris Quirk"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              },
              {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4760632,
          "isinfluential": false,
          "contexts": [
            "First, capturing long-term dependencies in long sequences remains a fundamental challenge for recurrent neural networks (Trinh et al., 2018).",
            "Capturing Long-term Dependencies for Neural Sequence Models For training neural sequence models such as RNNs, capturing long-term dependencies in sequences remains a fundamental challenge (Trinh et al., 2018)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses",
            "abstract": "Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\\,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.",
            "year": 2018,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "40895509",
                "name": "Trieu H. Trinh"
              },
              {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
              },
              {
                "authorId": "1821711",
                "name": "Thang Luong"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5590763,
          "isinfluential": false,
          "contexts": [
            "Many variations of models have been proposed to mitigate the effect of long sequence length, such as Long Short Term Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997; Gers et al., 1999; Graves, 2013) and Gated Recurrent Unit Networks (Cho et al., 2014).",
            ", 1999; Graves, 2013) and Gated Recurrent Unit Networks (Cho et al., 2014)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
            "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
              },
              {
                "authorId": "3158246",
                "name": "B. V. Merrienboer"
              },
              {
                "authorId": "1854385",
                "name": "Çaglar Gülçehre"
              },
              {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
              },
              {
                "authorId": "2076086",
                "name": "Fethi Bougares"
              },
              {
                "authorId": "144518416",
                "name": "Holger Schwenk"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6042994,
          "isinfluential": false,
          "contexts": [
            "CRF Layer Drawing inspirations for sentencelevel sequence tagging models on tasks like NER (Lample et al., 2016).",
            "We use a multi-layer (3 layers) bi-directional LSTM encoder on top of the token representations, which we denote as BiLSTM:\n{p1,p2, ...,pm} = BiLSTM({x1,x2, ...,xm})\nCRF Layer Drawing inspirations for sentencelevel sequence tagging models on tasks like NER (Lample et al., 2016).",
            "Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Neural Architectures for Named Entity Recognition",
            "abstract": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1830914",
                "name": "Guillaume Lample"
              },
              {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
              },
              {
                "authorId": "50324141",
                "name": "Sandeep Subramanian"
              },
              {
                "authorId": "2189948",
                "name": "Kazuya Kawakami"
              },
              {
                "authorId": "1745899",
                "name": "Chris Dyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6300165,
          "isinfluential": false,
          "contexts": [
            "Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Named Entity Recognition with Bidirectional LSTM-CNNs",
            "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",
            "year": 2015,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2052473561",
                "name": "Jason P. C. Chiu"
              },
              {
                "authorId": "143662759",
                "name": "Eric Nichols"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10910955,
          "isinfluential": false,
          "contexts": [
            "Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distant supervision for relation extraction without labeled data",
            "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.",
            "year": 2009,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "36181176",
                "name": "Mike D. Mintz"
              },
              {
                "authorId": "87299088",
                "name": "Steven Bills"
              },
              {
                "authorId": "144621026",
                "name": "R. Snow"
              },
              {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11187670,
          "isinfluential": false,
          "contexts": [
            "Liao and Grishman (2010) explore event type co-occurrence patterns to propagate event clas-siﬁcation decisions."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Using Document Level Cross-Event Inference to Improve Event Extraction",
            "abstract": "",
            "year": 2010,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39524110",
                "name": "Shasha Liao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "Li et al. (2013, 2015) explore various hand-designed features; Nguyen and Grishman (2015); Nguyen et al. (2016); Chen et al. (2015); Liu et al. (2017, 2018) employ deep learning based models such as recurrent neural networks (RNNs) and convolutional neural network (CNN).",
            "Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Second, although pretrained bi-directional transformer models such as BERT (Devlin et al., 2019) better capture long-distance dependencies as compared to an RNN architecture, they still have a constraint on the maximum length of the sequence, which is below the length of many articles about events.",
            "Transformer based models (Vaswani et al., 2017; Devlin et al., 2019) have also shown improvements in modeling long text.",
            "• Pre-trained LM representation: Contextualized embeddings produced by pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proved to be capable of modeling context beyond the sentence boundary and improve performance on a variety of tasks."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 76666127,
          "isinfluential": false,
          "contexts": [
            "Specifically, we use the average of all the 12 layers’ representations and freeze the\nweights (Peters et al., 2019) during training after empirical trials4.",
            "Specifically, we use the average of all the 12 layers’ representations and freeze the weights (Peters et al., 2019) during training"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
            "abstract": "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.",
            "year": 2019,
            "venue": "RepL4NLP@ACL",
            "authors": [
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
              },
              {
                "authorId": "144365875",
                "name": "Noah A. Smith"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189928589,
          "isinfluential": false,
          "contexts": [
            "There has also been work on unsupervised event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013) and open-domain event extraction (Liu et al., 2019) from documents: the main idea is to group entities corresponding to the same role into an event template."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Open Domain Event Extraction Using Neural Latent Variable Models",
            "abstract": "We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              },
              {
                "authorId": "2145912727",
                "name": "Yue Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 201646551,
          "isinfluential": false,
          "contexts": [
            "Similar results regarding the context length have also been found in document-level coreference resolution (Joshi et al., 2019)."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "BERT for Coreference Resolution: Baselines and Analysis",
            "abstract": "We apply BERT to coreference resolution, achieving a new state of the art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO), but that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. We will release all code and trained models upon publication.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "144863691",
                "name": "Mandar Joshi"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1780531",
                "name": "Daniel S. Weld"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            "We model labeling decisions jointly using a conditional random field (Lafferty et al., 2001)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        }
      ]
    },
    "119308902": {
      "citing_paper_info": {
        "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
        "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
        "year": 2019,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "145119697",
            "name": "Shun Zheng"
          },
          {
            "authorId": "2075436482",
            "name": "Wei Cao"
          },
          {
            "authorId": "145738410",
            "name": "W. Xu"
          },
          {
            "authorId": "152441498",
            "name": "Jiang Bian"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 16,
        "influential_count": 1,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "6628106",
        "11751039",
        "51870827",
        "12108307",
        "1724837",
        "15552794",
        "14339673",
        "12740621",
        "1820089",
        "18198203",
        "51605731",
        "182953200",
        "2386383",
        "2114517",
        "8236317",
        "14175558"
      ],
      "citation_details": [
        {
          "citedcorpusid": 1724837,
          "isinfluential": false,
          "contexts": [
            "For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations.",
            "For instance, [Ren et al. , 2017; Zheng et al. , 2017; Zeng et al. , 2018a; Wang et al. , 2018] focused on jointly extracting entities and inter-entity relations."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases",
            "abstract": "Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object \"translation\" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.",
            "year": 2016,
            "venue": "The Web Conference",
            "authors": [
              {
                "authorId": "145201124",
                "name": "Xiang Ren"
              },
              {
                "authorId": "7806955",
                "name": "Zeqiu Wu"
              },
              {
                "authorId": "2153475859",
                "name": "Wenqi He"
              },
              {
                "authorId": "35955224",
                "name": "Meng Qu"
              },
              {
                "authorId": "1817166",
                "name": "Clare R. Voss"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1730531",
                "name": "T. Abdelzaher"
              },
              {
                "authorId": "145325584",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1820089,
          "isinfluential": false,
          "contexts": [
            "To mitigate such problems, we utilize the scheduled sampling (Bengio et al., 2015) to gradually switch the inputs of document-level entity encoding from ground-truth entity mentions to model recognized ones."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
            "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.",
            "year": 2015,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1751569",
                "name": "Samy Bengio"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "3111912",
                "name": "N. Jaitly"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "Although a great number of efforts [Ahn, 2006; Ji and Gr-ishman, 2008; Liao and Grishman, 2010; Hong et al. , 2011; Riedel and McCallum, 2011; Li et al. , 2013; Li et al. , 2014; Chen et al. , 2015; Yang and Mitchell, 2016; Nguyen et al. , 2016; Liu et al. , 2017; Sha et al. , 2018; Zhang and Ji,…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2386383,
          "isinfluential": false,
          "contexts": [
            "Traditionally, when applying DS to relation extraction, researchers put huge efforts into alleviating labeling noises (Riedel et al., 2010; Lin et al., 2016; Feng et al., 2018; Zheng et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Modeling Relations and Their Mentions without Labeled Text",
            "abstract": "",
            "year": 2010,
            "venue": "ECML/PKDD",
            "authors": [
              {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
              },
              {
                "authorId": "1786422",
                "name": "Limin Yao"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": false,
          "contexts": [
            "Besides, we employ the Adam [ Kingma and Ba, 2015 ] optimizer with the learning rate 1 e − 4 , train for at most 100 epochs and pick the best epoch by the validation score on the development set."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8236317,
          "isinfluential": false,
          "contexts": [
            "…the scaled dot-product attention [Vaswani et al. , 2017] operations: to produce a single embedding ˆ x ∈ R d w , where Q ∈ R d w is a trainable parameter, LayerNorm is the layer normalization [Ba et al. , 2016] and Dropout is an effective technique to avoid overﬁtting [Srivastava et al. , 2014]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Layer Normalization",
            "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
            "year": 2016,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              },
              {
                "authorId": "51131802",
                "name": "J. Kiros"
              },
              {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11751039,
          "isinfluential": false,
          "contexts": [
            "For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations.",
            "For instance, [Ren et al. , 2017; Zheng et al. , 2017; Zeng et al. , 2018a; Wang et al. , 2018] focused on jointly extracting entities and inter-entity relations."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
            "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "37423160",
                "name": "Suncong Zheng"
              },
              {
                "authorId": "2145756722",
                "name": "Feng Wang"
              },
              {
                "authorId": "2682574",
                "name": "Hongyun Bao"
              },
              {
                "authorId": "8361912",
                "name": "Yuexing Hao"
              },
              {
                "authorId": "144032121",
                "name": "P. Zhou"
              },
              {
                "authorId": "2109511511",
                "name": "Bo Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12108307,
          "isinfluential": false,
          "contexts": [
            "Therefore, (Chen et al., 2017; Yang et al., 2018) employed either linguistic resources or predefined dictionaries for trigger-words labeling.",
            "Therefore, [Chen et al. , 2017; Yang et al. , 2018] employed either linguistic resources or predeﬁned dictionaries for trigger-words labeling."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Automatically Labeled Data Generation for Large Scale Event Extraction",
            "abstract": "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "50152545",
                "name": "Shulin Liu"
              },
              {
                "authorId": "2108052731",
                "name": "Xiang Zhang"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12740621,
          "isinfluential": false,
          "contexts": [
            "Moreover, for the entity recognition part, we refer readers to [Huang et al. , 2015] for details about stacking the conditional random ﬁeld (CRF) layer over encoded representations.",
            "We conduct this task at the sentence level and follow a classic method, BI-LSTM-CRF (Huang et al., 2015), that first encodes the token sequence and then adds a conditional random field (CRF) layer to facilitate the sequence tagging."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
            "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
            "year": 2015,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3109481",
                "name": "Zhiheng Huang"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "2150332252",
                "name": "Kai Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14175558,
          "isinfluential": false,
          "contexts": [
            "…same to the focus of this paper, a few studies aimed at designing joint models for the entity and event extraction, such as handcrafted-feature-based [Li et al. , 2014; Yang and Mitchell, 2016; Judea and Strube, 2016] and neural-network-based [ Zhang and Ji, 2018; Nguyen and Nguyen, 2019] models."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Incremental Global Event Extraction",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "3088079",
                "name": "Alex Judea"
              },
              {
                "authorId": "31380436",
                "name": "M. Strube"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "…[Ahn, 2006; Ji and Gr-ishman, 2008; Liao and Grishman, 2010; Hong et al. , 2011; Riedel and McCallum, 2011; Li et al. , 2013; Li et al. , 2014; Chen et al. , 2015; Yang and Mitchell, 2016; Nguyen et al. , 2016; Liu et al. , 2017; Sha et al. , 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019…",
            "Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 20052, an expert-annotated benchmark, which only tagged event arguments within the sentence scope."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15552794,
          "isinfluential": false,
          "contexts": [
            "In the meantime, the same to the focus of this paper, a few studies aimed at designing joint models for the entity and event extraction, such as handcrafted-feature-based (Li et al., 2014; Yang and Mitchell, 2016; Judea and Strube, 2016) and neural-network-based (Zhang and Ji, 2018; Nguyen and Nguyen, 2019) models.",
            "…same to the focus of this paper, a few studies aimed at designing joint models for the entity and event extraction, such as handcrafted-feature-based [Li et al. , 2014; Yang and Mitchell, 2016; Judea and Strube, 2016] and neural-network-based [ Zhang and Ji, 2018; Nguyen and Nguyen, 2019] models.",
            "…number of efforts [Ahn, 2006; Ji and Gr-ishman, 2008; Liao and Grishman, 2010; Hong et al. , 2011; Riedel and McCallum, 2011; Li et al. , 2013; Li et al. , 2014; Chen et al. , 2015; Yang and Mitchell, 2016; Nguyen et al. , 2016; Liu et al. , 2017; Sha et al. , 2018; Zhang and Ji, 2018; Nguyen…"
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Constructing Information Networks Using One Single Model",
            "abstract": "In this paper, we propose a new framework that unifies the output of three information extraction (IE) tasks - entity mentions, relations and events as an information network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many relations can now be considered as the resultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and significantly advances state-of-the-art end-toend event argument extraction.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144873792",
                "name": "Yu Hong"
              },
              {
                "authorId": "1695451",
                "name": "Sujian Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18198203,
          "isinfluential": false,
          "contexts": [
            "Although a great number of efforts [Ahn, 2006; Ji and Gr-ishman, 2008; Liao and Grishman, 2010; Hong et al. , 2011; Riedel and McCallum, 2011; Li et al. , 2013; Li et al. , 2014; Chen et al. , 2015; Yang and Mitchell, 2016; Nguyen et al. , 2016; Liu et al. , 2017; Sha et al. , 2018; Zhang and Ji,…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Fast and Robust Joint Models for Biomedical Event Extraction",
            "abstract": "",
            "year": 2011,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51605731,
          "isinfluential": false,
          "contexts": [
            "For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations.",
            "For instance, [Ren et al. , 2017; Zheng et al. , 2017; Zeng et al. , 2018a; Wang et al. , 2018] focused on jointly extracting entities and inter-entity relations."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Entities and Relations Based on a Novel Graph Scheme",
            "abstract": "Both entity and relation extraction can benefit from being performed jointly, allowing each task to correct the errors of the other. Most existing neural joint methods extract entities and relations separately and achieve joint learning  through parameter sharing, leading to a drawback that information between output entities and relations cannot be fully exploited. In this paper, we convert the joint task into a directed graph by designing a novel graph scheme and propose a transition-based approach to generate the directed graph incrementally, which can achieve joint learning through joint decoding. Our method can model underlying dependencies not only between entities and relations, but also between relations. Experiments on NewYork Times (NYT) corpora show that our approach outperforms the state-of-the-art methods. ",
            "year": 2018,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "38258535",
                "name": "Shaolei Wang"
              },
              {
                "authorId": null,
                "name": "Yue Zhang"
              },
              {
                "authorId": "2256319",
                "name": "Wanxiang Che"
              },
              {
                "authorId": "40282288",
                "name": "Ting Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51870827,
          "isinfluential": true,
          "contexts": [
            "For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations.",
            "On the other hand, another recent work [Zeng et al. , 2018b] showed that directly labeling event arguments without trigger words was also feasible.",
            "For instance, [Ren et al. , 2017; Zheng et al. , 2017; Zeng et al. , 2018a; Wang et al. , 2018] focused on jointly extracting entities and inter-entity relations.",
            "Some previous studies [Zeng et al. , 2018b] attempted to use a similar no-trigger-words design, but they only considered the SEE setting and cannot be directly applied to the DEE setting."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism",
            "abstract": "The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2441459",
                "name": "Xiangrong Zeng"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1954845",
                "name": "Shizhu He"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 182953200,
          "isinfluential": false,
          "contexts": [
            "Traditionally, when applying DS to relation extraction, researchers put huge efforts into alleviating labeling noises (Riedel et al., 2010; Lin et al., 2016; Feng et al., 2018; Zheng et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction",
            "abstract": "Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "50598485",
                "name": "Peilin Yu"
              },
              {
                "authorId": "2115386330",
                "name": "Lu Chen"
              },
              {
                "authorId": "50055322",
                "name": "Ling Huang"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1399906015",
                "name": "Wei Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "To address this problem, some researches attempted to adapt distant supervision (DS) to the EE setting, since DS has shown promising results by employing knowledge bases to generate training data for relation extraction [Mintz et al., 2009] automatically.",
            ", 2018], attempted to explore DEE on ChFinAnn, by employing distant supervision (DS) [Mintz et al., 2009] to generate EE data and performing a two-stage extraction: 1) a sequence tagging model for SEE, and 2) a key-event-sentence detection model to detect the key sentence and an arguments-completion strategy that padded missing arguments from surrounding sentences for DEE.",
            "To address this problem, some researches attempted to adapt distant supervision (DS) to the EE setting, since DS has shown promising results by employing knowledge bases to generate training data for relation extraction [Mintz et al. , 2009] automatically."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "253840075": {
      "citing_paper_info": {
        "title": "ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation",
        "abstract": "Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/",
        "year": 2022,
        "venue": "AACL/IJCNLP",
        "authors": [
          {
            "authorId": "29850167",
            "name": "Debanjana Kar"
          },
          {
            "authorId": "40047941",
            "name": "S. Sarkar"
          },
          {
            "authorId": "51130504",
            "name": "Pawan Goyal"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "212657414",
        "204734128",
        "247155039",
        "233219850",
        "235490449",
        "67855846",
        "60827152",
        "225039792"
      ],
      "citation_details": [
        {
          "citedcorpusid": 60827152,
          "isinfluential": false,
          "contexts": [
            "Using fuzzy string match techniques (Levenshtein, 1965), we only mark the longer argument span in case of redundancy."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Binary codes capable of correcting deletions, insertions, and reversals",
            "abstract": "",
            "year": 1965,
            "venue": "",
            "authors": [
              {
                "authorId": "2154179",
                "name": "V. Levenshtein"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67855846,
          "isinfluential": false,
          "contexts": [
            "We curate such a dataset by collating the following datasets: i) DROP Dataset (Dua et al., 2019) which is an abstractive, reasoning QA dataset with a special focus on numerical reasoning; ii) Hindi annotated instances of MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020) datasets and iii) Bengali annotated instances from TyDi QA dataset (Clark et al., 2020).",
            "We curate such a dataset by collating the following datasets: i) DROP Dataset (Dua et al., 2019) which is an abstractive, reasoning QA dataset with a special focus on numerical reasoning; ii) Hindi annotated instances of MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020) datasets and iii)…"
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
            "abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "33546336",
                "name": "Dheeru Dua"
              },
              {
                "authorId": "1705260",
                "name": "Yizhong Wang"
              },
              {
                "authorId": "2697425",
                "name": "Pradeep Dasigi"
              },
              {
                "authorId": "2157025",
                "name": "Gabriel Stanovsky"
              },
              {
                "authorId": "34650964",
                "name": "Sameer Singh"
              },
              {
                "authorId": "40642935",
                "name": "Matt Gardner"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204734128,
          "isinfluential": false,
          "contexts": [
            "We curate such a dataset by collating the following datasets: i) DROP Dataset (Dua et al., 2019) which is an abstractive, reasoning QA dataset with a special focus on numerical reasoning; ii) Hindi annotated instances of MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020) datasets and iii) Bengali annotated instances from TyDi QA dataset (Clark et al., 2020).",
            "…Dataset (Dua et al., 2019) which is an abstractive, reasoning QA dataset with a special focus on numerical reasoning; ii) Hindi annotated instances of MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020) datasets and iii) Bengali annotated instances from TyDi QA dataset (Clark et al., 2020)."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "abstract": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145222654",
                "name": "Patrick Lewis"
              },
              {
                "authorId": "9185192",
                "name": "Barlas Oğuz"
              },
              {
                "authorId": "1905713",
                "name": "Ruty Rinott"
              },
              {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
              },
              {
                "authorId": "144518416",
                "name": "Holger Schwenk"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212657414,
          "isinfluential": false,
          "contexts": [
            "…Dataset (Dua et al., 2019) which is an abstractive, reasoning QA dataset with a special focus on numerical reasoning; ii) Hindi annotated instances of MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020) datasets and iii) Bengali annotated instances from TyDi QA dataset (Clark et al., 2020)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
            "abstract": "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology—the set of linguistic features each language expresses—such that we expect models performing well on this set to generalize across a large number of the world’s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don’t know the answer yet, and the data is collected directly in each language without the use of translation.",
            "year": 2020,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144797264",
                "name": "J. Clark"
              },
              {
                "authorId": "2890423",
                "name": "Eunsol Choi"
              },
              {
                "authorId": "123052390",
                "name": "Michael Collins"
              },
              {
                "authorId": "2758616",
                "name": "Dan Garrette"
              },
              {
                "authorId": "15652489",
                "name": "T. Kwiatkowski"
              },
              {
                "authorId": "48942032",
                "name": "Vitaly Nikolaev"
              },
              {
                "authorId": "52578817",
                "name": "J. Palomaki"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039792,
          "isinfluential": false,
          "contexts": [
            "In our work, we adopt (Du and Cardie, 2020; Feng et al., 2020)’s idea of reducing our related task of Document-Level Event Argument Aggregation to that of Natural Language Question Answering."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction",
            "abstract": "We study the problem of event extraction from text data, which requires both detecting target event types and their arguments. Typically, both the event detection and argument detection subtasks are formulated as supervised sequence labeling problems. We argue that the event extraction models so trained are inherently label-hungry, and can generalize poorly across domains and text genres.We propose a reading comprehension framework for event extraction.Specifically, we formulate event detection as a textual entailment prediction problem, and argument detection as a question answer-ing problem. By constructing proper query templates, our approach can effectively distill rich knowledge about tasks and label semantics from pretrained reading comprehension models. Moreover, our model can be fine-tuned with a small amount of data to boost its performance. Our experiment results show that our method performs strongly for zero-shot and few-shot event extraction, and it achieves state-of-the-art performance on the ACE 2005 benchmark when trained with full supervision.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2054599518",
                "name": "Rui Feng"
              },
              {
                "authorId": "2118573410",
                "name": "Jie Yuan"
              },
              {
                "authorId": "2152737282",
                "name": "Chao Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": false,
          "contexts": [
            "A very recently published work related to the task of Event Argument Generation is that of (Li et al., 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235490449,
          "isinfluential": false,
          "contexts": [
            "While we use the same English documents as those used in the ArgFuse dataset (Kar et al., 2021), we source the Hindi and Bengali documents from reputed news websites."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument Aggregation",
            "abstract": "Most of the existing information extraction frameworks (Wadden et al., 2019; Veysehet al., 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document. In our endeavour to generate precise document-level information frames from lengthy textual records, we introduce the task of Information Aggregation or Argument Aggregation. More specifically, our aim is to filter irrelevant and redundant argument mentions that were extracted at a sentence level and render a document level information frame. Majority of the existing works have been observed to resolve related tasks of document-level event argument extraction (Yang et al., 2018; Zheng et al., 2019) and salient entity identification (Jain et al., 2020) using supervised techniques. To remove dependency from large amounts of labelled data, we explore the task of information aggregation using weakly supervised techniques. In particular, we present an extractive algorithm with multiple sieves which adopts active learning strategies to work efficiently in low-resource settings. For this task, we have annotated our own test dataset comprising of 131 document information frames and have released the code and dataset to further research prospects in this new domain. To the best of our knowledge, we are the first to establish baseline results for this task in English. Our data and code are publicly available at https://github.com/DebanjanaKar/ArgFuse.",
            "year": 2021,
            "venue": "CASE",
            "authors": [
              {
                "authorId": "29850167",
                "name": "Debanjana Kar"
              },
              {
                "authorId": "40047941",
                "name": "S. Sarkar"
              },
              {
                "authorId": "51130504",
                "name": "Pawan Goyal"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247155039,
          "isinfluential": false,
          "contexts": [
            "Prompt-based methods have recently gained popularity in a number of related tasks like entity extraction(Wang et al., 2022), question answering(Liu et al., 2022) and text generation(Li et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks",
            "abstract": "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46395829",
                "name": "Yufei Wang"
              },
              {
                "authorId": "46747953",
                "name": "Can Xu"
              },
              {
                "authorId": "2112549330",
                "name": "Qingfeng Sun"
              },
              {
                "authorId": "2144026961",
                "name": "Huang Hu"
              },
              {
                "authorId": "8801869",
                "name": "Chongyang Tao"
              },
              {
                "authorId": "2442662",
                "name": "Xiubo Geng"
              },
              {
                "authorId": "2086994543",
                "name": "Daxin Jiang"
              }
            ]
          }
        }
      ]
    },
    "259370545": {
      "citing_paper_info": {
        "title": "CHEER: Centrality-aware High-order Event Reasoning Network for Document-level Event Causality Identification",
        "abstract": "Document-level Event Causality Identification (DECI) aims to recognize causal relations between events within a document. Recent studies focus on building a document-level graph for cross-sentence reasoning, but ignore important causal structures — there are one or two “central” events that prevail throughout the document, with most other events serving as either their cause or consequence. In this paper, we manually annotate central events for a systematical investigation and propose a novel DECI model, CHEER, which performs high-order reasoning while considering event centrality. First, we summarize a general GNN-based DECI model and provide a unified view for better understanding. Second, we design an Event Interaction Graph (EIG) involving the interactions among events (e.g., coreference) and event pairs, e.g., causal transitivity, cause(A, B) AND cause(B, C) → cause(A, C). Finally, we incorporate event centrality information into the EIG reasoning network via well-designed features and multi-task learning. We have conducted extensive experiments on two benchmark datasets. The results present great improvements (5.9% F1 gains on average) and demonstrate the effectiveness of each main component.",
        "year": 2023,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2108612706",
            "name": "Meiqi Chen"
          },
          {
            "authorId": "145014675",
            "name": "Yixin Cao"
          },
          {
            "authorId": "36124320",
            "name": "Yan Zhang"
          },
          {
            "authorId": "40448513",
            "name": "Zhiwei Liu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 12,
        "unique_cited_count": 12,
        "influential_count": 3,
        "detailed_records_count": 12
      },
      "cited_papers": [
        "248218685",
        "9130816",
        "6844431",
        "247084444",
        "9137624",
        "233874227",
        "14068874",
        "53592270",
        "15920102",
        "17227879",
        "240288666",
        "235313618"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6844431,
          "isinfluential": false,
          "contexts": [
            "We apply layer nor-10809 malization (Ba et al., 2016) and dropout (Srivastava et al., 2014) between the EIG reasoning network layers."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
            "year": 2014,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2897313",
                "name": "Nitish Srivastava"
              },
              {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
              },
              {
                "authorId": "2064160",
                "name": "A. Krizhevsky"
              },
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9130816,
          "isinfluential": false,
          "contexts": [
            "Early feature-based methods explore different resources for causal expressions, such as lexical and syntactic patterns (Riaz and Girju, 2013, 2014b,a), causality cues or markers (Do et al., 2011; Hidey and McKeown, 2016), temporal patterns (Ning et al., 2018), statistical information (Hashimoto et…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "In-depth Exploitation of Noun and Verb Semantics to Identify Causation in Verb-Noun Pairs",
            "abstract": "Recognition of causality is important to achieve natural language discourse understanding. Previous approaches rely on shallow linguistic features. In this work, we propose to identify causality in verbnoun pairs by exploiting deeper semantics of nouns and verbs. Particularly, we acquire and employ three novel types of knowledge: (1) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies, (2) data-driven semantic classes of verbal events with the least tendency to encode causality, and (3) tendencies of verb frames to encode causality. Using these knowledge sources, we achieve around 15% improvement in Fscore over a supervised classifier trained using linguistic features.",
            "year": 2014,
            "venue": "SIGDIAL Conference",
            "authors": [
              {
                "authorId": "2852149",
                "name": "M. Riaz"
              },
              {
                "authorId": "2469966",
                "name": "Roxana Girju"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9137624,
          "isinfluential": false,
          "contexts": [
            "…for causal expressions, such as lexical and syntactic patterns (Riaz and Girju, 2013, 2014b,a), causality cues or markers (Do et al., 2011; Hidey and McKeown, 2016), temporal patterns (Ning et al., 2018), statistical information (Hashimoto et al., 2014; Hu et al., 2017), and weakly…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Identifying Causal Relations Using Parallel Wikipedia Articles",
            "abstract": "The automatic detection of causal relationships in text is important for natural language understanding. This task has proven to be difﬁcult, however, due to the need for world knowledge and inference. We focus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality. Unlike the explicit markers, a closed class, these markers vary signiﬁ-cantly in their linguistic forms. We leverage parallel Wikipedia corpora to identify new markers that are variations on known causal phrases, creating a training set via distant supervision. We also train a causal classiﬁer using features from the open class markers and semantic features providing contextual information. The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text.",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3443981",
                "name": "Christopher Hidey"
              },
              {
                "authorId": "145590324",
                "name": "K. McKeown"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14068874,
          "isinfluential": false,
          "contexts": [
            "We also use the Stanford CoreNLP toolkit (Manning et al., 2014) for a supplement."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
            "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              },
              {
                "authorId": "1760868",
                "name": "M. Surdeanu"
              },
              {
                "authorId": "144661918",
                "name": "John Bauer"
              },
              {
                "authorId": "2784228",
                "name": "J. Finkel"
              },
              {
                "authorId": "2105138",
                "name": "Steven Bethard"
              },
              {
                "authorId": "2240597",
                "name": "David McClosky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15920102,
          "isinfluential": false,
          "contexts": [
            "Early feature-based methods explore different resources for causal expressions, such as lexical and syntactic patterns (Riaz and Girju, 2013, 2014b,a), causality cues or markers (Do et al., 2011; Hidey and McKeown, 2016), temporal patterns (Ning et al., 2018), statistical information (Hashimoto et…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations",
            "abstract": "",
            "year": 2013,
            "venue": "SIGDIAL Conference",
            "authors": [
              {
                "authorId": "2852149",
                "name": "M. Riaz"
              },
              {
                "authorId": "2469966",
                "name": "Roxana Girju"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17227879,
          "isinfluential": false,
          "contexts": [
            "…and syntactic patterns (Riaz and Girju, 2013, 2014b,a), causality cues or markers (Do et al., 2011; Hidey and McKeown, 2016), temporal patterns (Ning et al., 2018), statistical information (Hashimoto et al., 2014; Hu et al., 2017), and weakly supervised data (Hashimoto, 2019; Zuo et al., 2021b)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features",
            "abstract": "We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture! exacerbate desertification from the web using semantic relation (between nouns), context, and association features. Experiments show that our method outperforms baselines that are based on state-of-the-art methods. We also propose methods of generating future scenarios like conduct slash-and-burn agriculture! exacerbate desertification! increase Asian dust (from China)! asthma gets worse. Experiments show that we can generate 50,000 scenarios with 68% precision. We also generated a scenario deforestation continues! global warming worsens! sea temperatures rise! vibrio parahaemolyticus fouls (water), which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we “predicted” the future event sequence in a sense.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1760686",
                "name": "Chikara Hashimoto"
              },
              {
                "authorId": "1768754",
                "name": "Kentaro Torisawa"
              },
              {
                "authorId": "1922763",
                "name": "Julien Kloetzer"
              },
              {
                "authorId": "3151314",
                "name": "Motoki Sano"
              },
              {
                "authorId": "2060429542",
                "name": "István Varga"
              },
              {
                "authorId": "1694341",
                "name": "Jong-Hoon Oh"
              },
              {
                "authorId": "2101431",
                "name": "Y. Kidawara"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53592270,
          "isinfluential": false,
          "contexts": [
            "We optimize our model with AdamW (Loshchilov and Hutter, 2019) using a learning rate of 2e-5 with a linear warm-up for the first 8% steps."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233874227,
          "isinfluential": true,
          "contexts": [
            "Due to DSGCN (Zhao et al., 2021) does not provide re-sults on benchmark datasets and does not release codes, we do not compare with it here.",
            "By removing: i) event centrality incorporation, ii) event pair nodes and their relevant edges, iii) edge features and modifying g accordingly, CHEER is scalable to DSGCN. (3) ERGO (Chen et al., 2022) has only event-pair nodes and performs self-attention aggregation: (cid:16) (cid:17) .",
            "DSGCN (Zhao et al., 2021) uses a graph inference mechanism to capture interaction among events.",
            "(2) DSGCN (Zhao et al., 2021) has only event nodes and uses a combination of GCNs: g z (cid:16) (cid:17) , where α k denotes a feature filter."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level event causality identification via graph inference mechanism",
            "abstract": "",
            "year": 2021,
            "venue": "Information Sciences",
            "authors": [
              {
                "authorId": "2088681577",
                "name": "Kun Zhao"
              },
              {
                "authorId": "1719916",
                "name": "D. Ji"
              },
              {
                "authorId": "7250549",
                "name": "Fazhi He"
              },
              {
                "authorId": "2108060975",
                "name": "Yijiang Liu"
              },
              {
                "authorId": "3350168",
                "name": "Yafeng Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313618,
          "isinfluential": true,
          "contexts": [
            "…and syntactic patterns (Riaz and Girju, 2013, 2014b,a), causality cues or markers (Do et al., 2011; Hidey and McKeown, 2016), temporal patterns (Ning et al., 2018), statistical information (Hashimoto et al., 2014; Hu et al., 2017), and weakly supervised data (Hashimoto, 2019; Zuo et al., 2021b).",
            "(3) CauSeRL (Zuo et al., 2021a), which learns contextspecific causal patterns from external causal statements.",
            "(3) CauSeRL (Zuo et al., 2021a), which learns context-specific causal patterns from external causal statements.",
            "To deal with implicit causal relations, Cao et al. (2021) incorporate external knowledge from Con-ceptNet (Speer et al., 2017), and Zuo et al. (2021a) learn context-specific causal patterns from external causal statements.",
            "(4) LearnDA (Zuo et al., 2021b), which uses knowledge bases to augment training data."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement",
            "abstract": "Current models for event causality identification (ECI) mainly adopt a supervised framework, which heavily rely on labeled data for training. Unfortunately, the scale of current annotated datasets is relatively limited, which cannot provide sufficient support for models to capture useful indicators from causal statements, especially for handing those new, unseen cases. To alleviate this problem, we propose a novel approach, shortly named CauSeRL, which leverages external causal statements for event causality identification. First of all, we design a self-supervised framework to learn context-specific causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into the target ECI model. Experimental results show that our method significantly outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively).",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 240288666,
          "isinfluential": false,
          "contexts": [
            "It is a fundamental NLP task and beneficial to various applications, such as question answering (Shi et al., 2021; Sui et al., 2022) and future event forecasting (Hashimoto, 2019; Bai et al., 2021).",
            ", 2022) and future event forecasting (Hashimoto, 2019; Bai et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Integrating Deep Event-Level and Script-Level Information for Script Event Prediction",
            "abstract": "Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information and the script-level information. At the event level, existing studies view an event as a verb with its participants, while neglecting other useful properties, such as the state of the participants. At the script level, most existing studies only consider a single event sequence corresponding to one common protagonist. In this paper, we propose a Transformer-based model, called MCPredictor, which integrates deep event-level and script-level information for script event prediction. At the event level, MCPredictor utilizes the rich information in the text to obtain more comprehensive event semantic representations. At the script-level, it considers multiple event sequences corresponding to different participants of the subsequent event. The experimental results on the widely-used New York Times corpus demonstrate the effectiveness and superiority of the proposed model.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2075398318",
                "name": "Long Bai"
              },
              {
                "authorId": "24749412",
                "name": "Saiping Guan"
              },
              {
                "authorId": "70414094",
                "name": "Jiafeng Guo"
              },
              {
                "authorId": "46947005",
                "name": "Zixuan Li"
              },
              {
                "authorId": "2149111400",
                "name": "Xiaolong Jin"
              },
              {
                "authorId": "1717004",
                "name": "Xueqi Cheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247084444,
          "isinfluential": false,
          "contexts": [
            ", 2021), and event argument extraction (Ma et al., 2022).",
            "Following the success of sentence-level natural language understanding, many tasks are extended to the entire document, such as relation extraction (Yao et al., 2019), natural language inference (Yin et al., 2021), and event argument extraction (Ma et al., 2022)."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2143557418",
                "name": "Yubo Ma"
              },
              {
                "authorId": "2118402851",
                "name": "Zehao Wang"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "1990752926",
                "name": "Kunze Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248218685,
          "isinfluential": true,
          "contexts": [
            "Following ERGO (Chen et al., 2022), we adopt the focal loss (Lin et al.",
            "By removing: i) event centrality incorporation, ii) event pair nodes and their relevant edges, iii) edge features and modifying g accordingly, CHEER is scalable to DSGCN. (3) ERGO (Chen et al., 2022) has only event-pair nodes and performs self-attention aggregation: (cid:16) (cid:17) .",
            "To avoid noisy and exhaustive relation extraction, ERGO (Chen et al., 2022) instead takes each event pair as nodes and leverages GNN on the relational graph for high-order causal transitivity, e.",
            "To avoid noisy and exhaustive relation extraction, ERGO (Chen et al., 2022) instead takes each event pair as nodes and leverages GNN on the relational graph for high-order causal transitivity, e.g., cause(A, B) ∧ cause(B, C) ⇒ cause(A, C) .",
            "(5) ERGO (Chen et al., 2022), which builds a relational graph and model interaction between event pairs.",
            "(3) ERGO (Chen et al., 2022) has only eventpair nodes and performs self-attention aggregation:",
            "ERGO (Chen et al., 2022) builds a relational graph and model interaction between event pairs.",
            "Following conventions (Chen et al., 2022), we add special tokens at the start and end of D (i.e., “ [CLS] ” and “ [SEP] ”), and insert additional special tokens “ <t> ” and “ </t> ”’ at the start and end of all the events to mark the event positions.",
            "Following ERGO (Chen et al., 2022), we adopt the focal loss (Lin et al., 2017) to alleviate the false-negative issue (i.e., the number of negative samples during training far exceeds that of positives).",
            "Following conventions (Chen et al., 2022), we add special tokens at the start and end of D (i."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification",
            "abstract": "Document-level Event Causality Identification (DECI) aims to identify event-event causal relations in a document. Existing works usually build an event graph for global reasoning across multiple sentences. However, the edges between events have to be carefully designed through heuristic rules or external tools. In this paper, we propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI, to ease the graph construction and improve it over the noisy edge issue. Different from conventional event graphs, we define a pair of events as a node and build a complete event relational graph without any prior knowledge or tools. This naturally formulates DECI as a node classification problem, and thus we capture the causation transitivity among event pairs via a graph transformer. Furthermore, we design a criss-cross constraint and an adaptive focal loss for the imbalanced classification, to alleviate the issues of false positives and false negatives. Extensive experiments on two benchmark datasets show that ERGO greatly outperforms previous state-of-the-art (SOTA) methods (12.8% F1 gains on average).",
            "year": 2022,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2108612706",
                "name": "Meiqi Chen"
              },
              {
                "authorId": "145014675",
                "name": "Yixin Cao"
              },
              {
                "authorId": "2162737824",
                "name": "Kunquan Deng"
              },
              {
                "authorId": "2027599235",
                "name": "Mukai Li"
              },
              {
                "authorId": "2119044303",
                "name": "Kunpeng Wang"
              },
              {
                "authorId": "2156121678",
                "name": "Jing Shao"
              },
              {
                "authorId": null,
                "name": "Yan Zhang"
              }
            ]
          }
        }
      ]
    },
    "235681526": {
      "citing_paper_info": {
        "title": "A Prior Information Enhanced Extraction Framework for Document-level Financial Event Extraction",
        "abstract": "Document-level financial event extraction (DFEE) is the task of detecting events and extracting the corresponding event arguments in financial documents, which plays an important role in information extraction in the financial domain. This task is challenging as the financial documents are generally long text and event arguments of one event may be scattered in different sentences. To address this issue, we proposed a novel Prior Information Enhanced Extraction framework (PIEE) for DFEE, leveraging prior information from both event types and pre-trained language models. Specifically, PIEE consists of three components: event detection, event argument extraction, and event table filling. In event detection, we identify the event type. Then, the event type is explicitly used for event argument extraction. Meanwhile, the implicit information within language models also provides considerable cues for event arguments localization. Finally, all the event arguments are filled in an event table by a set of predefined heuristic rules. To demonstrate the effectiveness of our proposed framework, we participated in the share task of CCKS2020 Task 4-2: Document-level Event Arguments Extraction. On both Leaderboard A and Leaderboard B, PIEE took the first place and significantly outperformed the other systems.",
        "year": 2021,
        "venue": "Data Intelligence",
        "authors": [
          {
            "authorId": "2109611257",
            "name": "Haitao Wang"
          },
          {
            "authorId": "1914586128",
            "name": "Tong Zhu"
          },
          {
            "authorId": "48957912",
            "name": "Mingtao Wang"
          },
          {
            "authorId": "2116020013",
            "name": "Guoliang Zhang"
          },
          {
            "authorId": "48993675",
            "name": "Wenliang Chen"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 9,
        "unique_cited_count": 9,
        "influential_count": 1,
        "detailed_records_count": 9
      },
      "cited_papers": [
        "6042994",
        "65179127",
        "2778800",
        "51871198",
        "119308902",
        "16389974",
        "14339673",
        "397533",
        "54437926"
      ],
      "citation_details": [
        {
          "citedcorpusid": 397533,
          "isinfluential": false,
          "contexts": [
            "[21], to exploit the information of all available sentences, we can use the attention mechanism to aggregate sentence-level features."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Neural Relation Extraction with Selective Attention over Instances",
            "abstract": "Distant supervised relation extraction has been widely used to ﬁnd novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level attention-based model for relation extraction. In this model, we employ convolutional neural networks to embed the semantics of sentences. Afterwards, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sentences and effectively reduce the inﬂuence of wrong labelled instances. Our model achieves signiﬁcant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/NRE .",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "2589625",
                "name": "Shiqi Shen"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "3371599",
                "name": "Huanbo Luan"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2778800,
          "isinfluential": false,
          "contexts": [
            "[20] selected the most valuable sentence to represent the whole sentence bag d and the highest probability sentence is defined as follows:"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks",
            "abstract": "Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",
            "year": 2015,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6042994,
          "isinfluential": true,
          "contexts": [
            "In the stage of event argument extraction, both of them regard it as a sequence labeling problem similar to NER, where BiLSTM-CRF [9] is a classic model to address this issue.",
            "BiLSTM-CRF is a classic model to address the NER task and has once achieved the state-of-the-art result in accuracy.",
            "In the stage of event argument extraction, both of them regard it as a sequence labeling problem similar to NER, where BiLSTM-CRF [6] is a classic model to address this issue."
          ],
          "intents": [
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Neural Architectures for Named Entity Recognition",
            "abstract": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1830914",
                "name": "Guillaume Lample"
              },
              {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
              },
              {
                "authorId": "50324141",
                "name": "Sandeep Subramanian"
              },
              {
                "authorId": "2189948",
                "name": "Kazuya Kawakami"
              },
              {
                "authorId": "1745899",
                "name": "Chris Dyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "However, most existing studies merely extract arguments within the sentence scope [1, 2, 3], dubbed as sentence-level EE (SEE)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16389974,
          "isinfluential": false,
          "contexts": [
            "[5] claim that critical information can be also inferred implicitly from all sentences, so a max pooling operation is employed to capture the most valuable features in various aspects from all sentences."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Relation Extraction with Multi-instance Multi-label Convolutional Neural Networks",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2486289",
                "name": "Xiaotian Jiang"
              },
              {
                "authorId": "143906199",
                "name": "Quan Wang"
              },
              {
                "authorId": "2197194739",
                "name": "Peng Li"
              },
              {
                "authorId": null,
                "name": "Bin Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": false,
          "contexts": [
            "Yang et al. [7] and Zheng et al. [8] proposed two different frameworks for DEE."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 54437926,
          "isinfluential": false,
          "contexts": [
            "However, most existing studies merely extract arguments within the sentence scope [2, 14, 15], dubbed as sentence-level EE (SEE)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "One for All: Neural Joint Modeling of Entities and Events",
            "abstract": "The previous work for event extraction has mainly focused on the predictions for event triggers and argument roles, treating entity mentions as being provided by human annotators. This is unrealistic as entity mentions are usually predicted by some existing toolkits whose errors might be propagated to the event trigger and argument role recognition. Few of the recent work has addressed this problem by jointly predicting entity mentions, event triggers and arguments. However, such work is limited to using discrete engineering features to represent contextual information for the individual tasks and their interactions. In this work, we propose a novel model to jointly perform predictions for entity mentions, event triggers and arguments based on the shared hidden representations from deep learning. The experiments demonstrate the benefits of the proposed method, leading to the state-of-the-art performance for event extraction.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "49035085",
                "name": "T. Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 65179127,
          "isinfluential": false,
          "contexts": [
            "[10], to exploit the information of all available sentences, we can use attention mechanism to aggregate sentence-level features."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "abstract": "",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1708114",
                "name": "K. Erk"
              },
              {
                "authorId": "144365875",
                "name": "Noah A. Smith"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "[26] propose two different frameworks for DEE."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        }
      ]
    },
    "233613419": {
      "citing_paper_info": {
        "title": "TDJEE: A Document-Level Joint Model for Financial Event Extraction",
        "abstract": "Extracting financial events from numerous financial announcements is very important for investors to make right decisions. However, it is still challenging that event arguments always scatter in multiple sentences in a financial announcement, while most existing event extraction models only work in sentence-level scenarios. To address this problem, this paper proposes a relation-aware Transformer-based Document-level Joint Event Extraction model (TDJEE), which encodes relations between words into the context and leverages modified Transformer to capture document-level information to fill event arguments. Meanwhile, the absence of labeled data in financial domain could lead models be unstable in extraction results, which is known as the cold start problem. Furthermore, a Fonduer-based knowledge base combined with the distant supervision method is proposed to simplify the event labeling and provide high quality labeled training corpus for model training and evaluating. Experimental results on real-world Chinese financial announcement show that, compared with other models, TDJEE achieves competitive results and can effectively extract event arguments across multiple sentences.",
        "year": 2021,
        "venue": "Electronics",
        "authors": [
          {
            "authorId": "2155302012",
            "name": "Peng Wang"
          },
          {
            "authorId": "103283164",
            "name": "Zhen-ke Deng"
          },
          {
            "authorId": "2087801075",
            "name": "Ruilong Cui"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 15,
        "influential_count": 4,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "224868016",
        "12981484",
        "10638526",
        "41089825",
        "51871927",
        "14339673",
        "119308902",
        "950755",
        "219683473",
        "224948084",
        "208004535",
        "1671874",
        "51871198",
        "15865939",
        "6628106"
      ],
      "citation_details": [
        {
          "citedcorpusid": 950755,
          "isinfluential": true,
          "contexts": [
            "In TIER, event extraction is divided into multi-stage tasks, but error propagation between multiple models is ignored.",
            "Baseline\nWe use TIER [8], DCFEE [29], and Doc2EDAG [9] models as baseline models.",
            "We use TIER [8], DCFEE [29], and Doc2EDAG [9] models as baseline models.",
            "[8] point out that the pipeline architecture with three-stage task can extract documentlevel context information, but error information propagating will be an obstacle to generate correct results.",
            "Training Parameters Optimizer Adam Epoch 30 Learning Rate 10−4 Step [2,4,8] Batch [8,16,32]",
            "TIER employs a pipeline architecture with three-stage task to get document-level context information: classifying narrative document, recognizing event sentence and noun phrase analysis."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1671874,
          "isinfluential": false,
          "contexts": [
            "By incorporating the semantic information of WordNet [17], PALKA can achieve results close to human beings in specific domains."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "WordNet: A Lexical Database for English",
            "abstract": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",
            "year": 1995,
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "authors": [
              {
                "authorId": "144096985",
                "name": "G. Miller"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6628106,
          "isinfluential": true,
          "contexts": [
            "Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv 2014, arXiv:1412.6980.",
            "We employ the Adam [33] optimizer with the learning rate 10−4, and train for at most 30 epochs."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Adam: A Method for Stochastic Optimization",
            "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "2503659",
                "name": "Jimmy Ba"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10638526,
          "isinfluential": false,
          "contexts": [
            "[26] propose a joint event embedding model KGEB, which embeds the knowledge graph into the event vector representation."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Knowledge-Driven Event Embedding for Stock Prediction",
            "abstract": "",
            "year": 2016,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "145411800",
                "name": "Xiao Ding"
              },
              {
                "authorId": null,
                "name": "Yue Zhang"
              },
              {
                "authorId": "40282288",
                "name": "Ting Liu"
              },
              {
                "authorId": "3237142",
                "name": "Junwen Duan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12981484,
          "isinfluential": false,
          "contexts": [
            "Training Parameters Optimizer Adam Epoch 30 Learning Rate 10−4 Step [2,4,8] Batch [8,16,32]",
            "PALKA [16] uses semantic frames and phrase patterns to represent the extraction schema of events.",
            "By incorporating the semantic information of WordNet [17], PALKA can achieve results close to human beings in specific domains."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Acquisition of Linguistic Patterns for Knowledge-Based Information Extraction",
            "abstract": "The paper presents an automatic acquisition of linguistic patterns that can be used for knowledge based information extraction from texts. In knowledge based information extraction, linguistic patterns play a central role in the recognition and classification of input texts. Although the knowledge based approach has been proved effective for information extraction on limited domains, there are difficulties in construction of a large number of domain specific linguistic patterns. Manual creation of patterns is time consuming and error prone, even for a small application domain. To solve the scalability and the portability problem, an automatic acquisition of patterns must be provided. We present the PALKA (Parallel Automatic Linguistic Knowledge Acquisition) system that acquires linguistic patterns from a set of domain specific training texts and their desired outputs. A specialized representation of patterns called FP structures has been defined. Patterns are constructed in the form of FP structures from training texts, and the acquired patterns are tuned further through the generalization of semantic constraints. Inductive learning mechanism is applied in the generalization step. The PALKA system has been used to generate patterns for our information extraction system developed for the fourth Message Understanding Conference (MUC-4). >",
            "year": 1995,
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "authors": [
              {
                "authorId": "2109174525",
                "name": "Jun-Tae Kim"
              },
              {
                "authorId": "40497400",
                "name": "D. Moldovan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14339673,
          "isinfluential": false,
          "contexts": [
            "[6] propose a Dynamic Multi-Pooling Convolution Neural Network (DMCNN) to extract events, which regards event extraction as a two-stage multi-classification task: trigger classification and arguments classification."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks",
            "abstract": "Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "8540973",
                "name": "Liheng Xu"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15865939,
          "isinfluential": false,
          "contexts": [
            "[22] first use the self-training [23] and semi-supervised learning method to extend the labeled corpus."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Acquiring Topic Features to improve Event Extraction: in Pre-selected and Balanced Collections",
            "abstract": "",
            "year": 2011,
            "venue": "Recent Advances in Natural Language Processing",
            "authors": [
              {
                "authorId": "39524110",
                "name": "Shasha Liao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 41089825,
          "isinfluential": false,
          "contexts": [
            "Event extraction [1], which aims to identify event arguments which are primary roles composing an event and fill them into corresponding pre-defined event types, is a challenging task in NLP (Natural Language Process)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of event extraction methods from text for decision support systems",
            "abstract": "",
            "year": 2016,
            "venue": "Decision Support Systems",
            "authors": [
              {
                "authorId": "103368343",
                "name": "Frederik Hogenboom"
              },
              {
                "authorId": "1729599",
                "name": "Flavius Frasincar"
              },
              {
                "authorId": "1678244",
                "name": "U. Kaymak"
              },
              {
                "authorId": "144097974",
                "name": "F. D. Jong"
              },
              {
                "authorId": "34682332",
                "name": "E. Caron"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871198,
          "isinfluential": true,
          "contexts": [
            "DCFEE divides the event extraction into two-stage tasks for processing.",
            "Baseline\nWe use TIER [8], DCFEE [29], and Doc2EDAG [9] models as baseline models.",
            "[29] realizes the extraction of equity freezing, pledge, repurchase, and increase or decrease of holdings in financial announcements.",
            "We use TIER [8], DCFEE [29], and Doc2EDAG [9] models as baseline models.",
            "DCFEE proposed by Yang et al. [29] realizes the extraction of equity freezing, pledge, repurchase, and increase or decrease of holdings in financial announcements.",
            "Specifically, compared with DCFEE, TDJEE improves 9.53%, 18.79%, 13.24%, 18.17%, and 22.49% F1 scores on ER, EF, EP, EO, and EU, respectively.",
            "Although DCFEE uses contextual information to predict event triggers, its context-agnostic arguments completion strategy makes it unable to effectively solve the event arguments scattering problem.",
            "Moreover, the direct document-\nlevel supervision are more robust than the extra sentence-level supervision used in DCFEE, which assumes that the sentences having most event arguments would contain the key event.",
            "The DCFEE model expands the training corpus by using distant supervision, and divides the event extraction into two-stage sub-tasks."
          ],
          "intents": [
            "--",
            "--",
            "['background']",
            "['methodology']",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "DCFEE: A Document-level Chinese Financial Event Extraction System based on Automatically Labeled Training Data",
            "abstract": "We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144955350",
                "name": "Hang Yang"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "2116643823",
                "name": "Yang Xiao"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51871927,
          "isinfluential": false,
          "contexts": [
            "[18] makes full use of document information in event extraction, in which text is encoded by RNN-based fusion hierarchy and attention mechanism, then the representation of text is used to judge event trigger words and types in sentences."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention",
            "abstract": "Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": null,
                "name": "Yue Zhao"
              },
              {
                "authorId": "2149111400",
                "name": "Xiaolong Jin"
              },
              {
                "authorId": "2219600",
                "name": "Yuanzhuo Wang"
              },
              {
                "authorId": "1717004",
                "name": "Xueqi Cheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": true,
          "contexts": [
            "Doc2EDAG uses an end-to-end method to identify event arguments at the sentence level first.",
            "Moreover, Doc2EDAG designs a memory mechanism for path expanding to support the EDAG generation efficiently.",
            "Baseline\nWe use TIER [8], DCFEE [29], and Doc2EDAG [9] models as baseline models.",
            "We use TIER [8], DCFEE [29], and Doc2EDAG [9] models as baseline models.",
            "Therefore, Doc2EDAG may fail to extract such event.",
            "Doc2EDAG et al. [9] transform the event into an entity-based directed acyclic graph (EDAG), which can transform the hard slot-filling task into several sequential path-expanding sub-tasks.",
            "Zheng, S.; Cao, W.; Xu, W.; Bian, J. Doc2EDAG: An end-to-end document-level framework for chinese financial event extraction.\narXiv 2019, arXiv:1904.07535.",
            "[9] use an end-to-end model to identify event arguments at the",
            "[9] transform the event into an entity-based directed acyclic graph (EDAG), which can transform the hard slot-filling task into several sequential path-expanding sub-tasks."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "--",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208004535,
          "isinfluential": false,
          "contexts": [
            "[27] focus on digging out company-related events from text, which includes articles of describing companies on Wikipedia."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Financial Event Extraction Using Wikipedia-Based Weak Supervision",
            "abstract": "Extraction of financial and economic events from text has previously been done mostly using rule-based methods, with more recent works employing machine learning techniques. This work is in line with this latter approach, leveraging relevant Wikipedia sections to extract weak labels for sentences describing economic events. Whereas previous weakly supervised approaches required a knowledge-base of such events, or corresponding financial figures, our approach requires no such additional data, and can be employed to extract economic events related to companies which are not even mentioned in the training data.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1402680837",
                "name": "L. Ein-Dor"
              },
              {
                "authorId": "48835746",
                "name": "Ariel Gera"
              },
              {
                "authorId": "3022994",
                "name": "Orith Toledo-Ronen"
              },
              {
                "authorId": "41127252",
                "name": "Alon Halfon"
              },
              {
                "authorId": "2464133",
                "name": "B. Sznajder"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            "Then, the conditional random fields (CRF) method [12] is used to identify the event arguments.",
            "The Bi-LSTM (Bi-direction Long Short-Term Memory) [30] and CRF (Conditional Random Fields) [12] are combined to label the event elements in a sentence, and the event trigger word is detected through the dictionary."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 224868016,
          "isinfluential": false,
          "contexts": [
            "[20] combine CNN with gate linear mechanism to accelerate the encoding of text."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Entity–Relation Extraction—A Novel and Lightweight Method Based on a Gate Linear Mechanism",
            "abstract": "Entity–relation extraction has attracted considerable attention in recent years as a fundamental task in natural language processing. The goal of entity–relation extraction is to discover the relation structures of entities from a natural language sentence. Most existing models approach this task using recurrent neural nets (RNNs); however, given the sequential nature of RNNs, the states cannot be computed in parallel, which slows the machine comprehension. In this paper, we propose a new end-to-end model based on dilated convolutional units and the gate linear mechanism as an alternative to those recurrent models. We find that relation extraction becomes more difficult as the sentence length increases. In this paper, we introduce dynamic convolutions based on lightweight convolutions to process long sequences, which thus reduces the number of parameters to a low level. Another challenge in relation extraction is relation spans potentially overlapping in a sentence, representing a bottleneck for the detection of multiple relational triplets. To alleviate this problem, we design an entirely new prediction scheme to extract relational pairs and additionally boost performance. We conduct experiments on two widely used datasets, and the results show that our model outperforms the baselines by a large margin.",
            "year": 2020,
            "venue": "",
            "authors": [
              {
                "authorId": "2056013553",
                "name": "Guan-ming Peng"
              },
              {
                "authorId": "66273809",
                "name": "Xiongxiong Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 224948084,
          "isinfluential": false,
          "contexts": [
            "Training Parameters Optimizer Adam Epoch 30 Learning Rate 10−4 Step [2,4,8] Batch [8,16,32]",
            "It has a wide range of applications in the fields such as intelligent question answering, information retrieval [2], automatic summarization [3], recommendation [4], etc."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey of Context-Aware Recommendation Schemes in Event-Based Social Networks",
            "abstract": "In recent years, Event-based social network (EBSN) applications, such as Meetup and DoubanEvent, have received popularity and rapid growth. They provide convenient online platforms for users to create, publish, and organize social events, which will be held in physical places. Additionally, they not only support typical online social networking facilities (e.g., sharing comments and photos), but also promote face-to-face offline social interactions. To provide better service for users, Context-Aware Recommender Systems (CARS) in EBSNs have recently been singled out as a fascinating area of research. CARS in EBSNs provide the suitable recommendation to target users by incorporating the contextual factors into the recommendation process. This paper provides an overview on the development of CARS in EBSNs. We begin by illustrating the concept of the term context and the paradigms of conventional context-aware recommendation process. Subsequently, we introduce the formal definition of an EBSN, the characteristics of EBSNs, the challenges that are faced by CARS in EBSNs, and the implementation process of CARS in EBSNs. We also investigate which contextual factors are considered and how they are represented in the recommendation process. Next, we focus on the state-of-the-art computational techniques regarding CARS in EBSNs. We also overview the datasets and evaluation metrics for evaluation in this research area, and discuss the applications of context-aware recommendation in EBSNs. Finally, we point out research opportunities for the research community.",
            "year": 2020,
            "venue": "Electronics",
            "authors": [
              {
                "authorId": "1390671251",
                "name": "Xiaomei Huang"
              },
              {
                "authorId": "2381461",
                "name": "Guoqiong Liao"
              },
              {
                "authorId": "145826495",
                "name": "N. Xiong"
              },
              {
                "authorId": "1747034",
                "name": "A. Vasilakos"
              },
              {
                "authorId": "1753667759",
                "name": "Tianming Lan"
              }
            ]
          }
        }
      ]
    },
    "235235800": {
      "citing_paper_info": {
        "title": "A Framework for Document-level Cybersecurity Event Extraction from Open Source Data",
        "abstract": "With the rapid development of the Internet, the number of cyber threats increases exponentially. More and more cyber threats come from new and unexpected sources, leading organizations and individuals to facing more security risks and vulnerabilities. Automatically obtaining and structuring security information from cybersecurity news can help security analysts to identify useful information more quickly. Most existing studies on extracting security events merely focused on the event detection task, aiming to discover and categorize cybersecurity events from the plain text. However, such event detection methods cannot capture useful information such as who performed the cyberattack, when the data breach event happened, who was the victim, etc. These arguments of a cybersecurity event are needed for analysts to get cybersecurity event details directly. Several studies have tried to extract rich semantic information of cybersecurity events, but they merely focused on extracting event arguments within the sentence scope. These studies still have limitations when the event arguments needed to recognize spread across multiple sentences. In this paper, we proposed a framework that effectively extracts cybersecurity events at the document-level from cybersecurity news, blogs and announcements. We model the document-level event extraction task as a sequence tagging problem. The goal is to identify the related arguments of cybersecurity events from documents. Firstly, we get the characters embedding and incorporate the word information into the character representations. Then we design a sliding window mechanism to get the cross-sentence context information. Finally, we predict the label of each character. We build a Chinese cybersecurity dataset and use three methods to evaluate our method, and the experimental results demonstrate the effectiveness of the proposed model.",
        "year": 2021,
        "venue": "International Conference on Computer Supported Cooperative Work in Design",
        "authors": [
          {
            "authorId": "2106225541",
            "name": "Ning Luo"
          },
          {
            "authorId": "46993629",
            "name": "Xiangyu Du"
          },
          {
            "authorId": "1438013705",
            "name": "Yitong He"
          },
          {
            "authorId": "2107975546",
            "name": "Jun Jiang"
          },
          {
            "authorId": "2875619",
            "name": "Xuren Wang"
          },
          {
            "authorId": "1500398885",
            "name": "Zhengwei Jiang"
          },
          {
            "authorId": "2153279850",
            "name": "Kai Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 8,
        "unique_cited_count": 8,
        "influential_count": 0,
        "detailed_records_count": 8
      },
      "cited_papers": [
        "131774006",
        "12740621",
        "11574815",
        "2867611",
        "2065400",
        "2114517",
        "12179472",
        "119308902"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2065400,
          "isinfluential": false,
          "contexts": [
            "We recruited three undergraduate students who are major in cybersecurity to perform the data annotation for the processed articles, and the Brat Rapid Annotation tool [25] is used to annotate data."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "brat: a Web-based Tool for NLP-Assisted Text Annotation",
            "abstract": "",
            "year": 2012,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1918552",
                "name": "Pontus Stenetorp"
              },
              {
                "authorId": "1708916",
                "name": "S. Pyysalo"
              },
              {
                "authorId": "2938702",
                "name": "Goran Topic"
              },
              {
                "authorId": "2095533089",
                "name": "Tomoko Ohta"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              },
              {
                "authorId": "1737901",
                "name": "Junichi Tsujii"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "After that, the higher-level features have been investigated to improve the performance, such as cross-document information [5], [6], cross-event information [7], cross-entity information [8], etc. Li et al. [9] ﬁrst proposed joint model to extract event triggers and event arguments together."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2867611,
          "isinfluential": false,
          "contexts": [
            "After that, the higher-level features have been investigated to improve the performance, such as cross-document information [5], [6], cross-event information [7], cross-entity information [8], etc."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Using Cross-Entity Inference to Improve Event Extraction",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144873792",
                "name": "Yu Hong"
              },
              {
                "authorId": "2107968177",
                "name": "Jianfeng Zhang"
              },
              {
                "authorId": "2084599989",
                "name": "Bin Ma"
              },
              {
                "authorId": "2973770",
                "name": "Jianmin Yao"
              },
              {
                "authorId": "143740945",
                "name": "Guodong Zhou"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11574815,
          "isinfluential": false,
          "contexts": [
            "After that, the higher-level features have been investigated to improve the performance, such as cross-document information [5], [6], cross-event information [7], cross-entity information [8], etc. Li et al. [9] ﬁrst proposed joint model to extract event triggers and event arguments together."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross-document Event Extraction and Tracking: Task, Evaluation, Techniques and Challenges",
            "abstract": "",
            "year": 2009,
            "venue": "Recent Advances in Natural Language Processing",
            "authors": [
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              },
              {
                "authorId": "2117202515",
                "name": "Zheng Chen"
              },
              {
                "authorId": "2119997825",
                "name": "Prashant Gupta"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12179472,
          "isinfluential": false,
          "contexts": [
            "An attacker threatens to publish the victim’s data or perpetually block access to it unless a ransom is paid [21]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cryptovirology: extortion-based security threats and countermeasures",
            "abstract": "",
            "year": 1996,
            "venue": "Proceedings 1996 IEEE Symposium on Security and Privacy",
            "authors": [
              {
                "authorId": "3010766",
                "name": "Adam L. Young"
              },
              {
                "authorId": "144068857",
                "name": "M. Yung"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12740621,
          "isinfluential": false,
          "contexts": [
            "With such a layer, we can efﬁciently use past and future tags to predict the current tag, and it has been shown that CRFs can produce higher tagging accuracy in general, such as in the NER task [24]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
            "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.",
            "year": 2015,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "3109481",
                "name": "Zhiheng Huang"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "2150332252",
                "name": "Kai Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 119308902,
          "isinfluential": false,
          "contexts": [
            "[17] proposed Doc2EDAG, which formalized a DEE task with the no-trigger-words design to ease documentlevel event labeling.",
            "[17] proposed a kind of DEE formalization that removes the trigger-words labeling and regards the document-level event extraction task as directly filling event tables based on a document.",
            "A few works presented document-level models [16], [17] that could extract document-level events from Chinese financial announcements."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction",
            "abstract": "Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145119697",
                "name": "Shun Zheng"
              },
              {
                "authorId": "2075436482",
                "name": "Wei Cao"
              },
              {
                "authorId": "145738410",
                "name": "W. Xu"
              },
              {
                "authorId": "152441498",
                "name": "Jiang Bian"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131774006,
          "isinfluential": false,
          "contexts": [
            "In recent research, some researchers have explored the possibility of event detection in the text on the topic of cybersecurity [1], [2].",
            "Severalmethods [1], [2] have been proposed on the event detection task in the cybersecurity domain, which pay more attention to identify and classify cybersecurity events."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Detecting Cybersecurity Events from Noisy Short Text",
            "abstract": "It is very critical to analyze messages shared over social networks for cyber threat intelligence and cyber-crime prevention. In this study, we propose a method that leverages both domain-specific word embeddings and task-specific features to detect cyber security events from tweets. Our model employs a convolutional neural network (CNN) and a long short-term memory (LSTM) recurrent neural network which takes word level meta-embeddings as inputs and incorporates contextual embeddings to classify noisy short text. We collected a new dataset of cyber security related tweets from Twitter and manually annotated a subset of 2K of them. We experimented with this dataset and concluded that the proposed model outperforms both traditional and neural baselines. The results suggest that our method works well for detecting cyber security events from noisy short text.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "40387200",
                "name": "Semih Yagcioglu"
              },
              {
                "authorId": "1997591",
                "name": "M. S. Seyfioglu"
              },
              {
                "authorId": "28282293",
                "name": "Begum Citamak"
              },
              {
                "authorId": "2505143",
                "name": "Batuhan Bardak"
              },
              {
                "authorId": "3448597",
                "name": "Seren Guldamlasioglu"
              },
              {
                "authorId": "108403054",
                "name": "Azmi Yuksel"
              },
              {
                "authorId": "3048213",
                "name": "E. I. Tatli"
              }
            ]
          }
        }
      ]
    },
    "221246218": {
      "citing_paper_info": {
        "title": "Document-level Event-based Extraction Using Generative Template-filling Transformers",
        "abstract": "We revisit the classic information extraction problem of document-level template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is \\emph{implicitly} aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions. Our code for the evaluation script and models will be open-sourced at this https URL for reproduction purposes.",
        "year": 2020,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "13728923",
            "name": "Xinya Du"
          },
          {
            "authorId": "2531268",
            "name": "Alexander M. Rush"
          },
          {
            "authorId": "1748501",
            "name": "Claire Cardie"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 10,
        "influential_count": 1,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "189928589",
        "14365335",
        "14117526",
        "147704286",
        "52967399",
        "102353905",
        "9778664",
        "2524712",
        "7961699",
        "6042994"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2524712,
          "isinfluential": false,
          "contexts": [
            ", 2013) are also able to model the coreference relations and entities at document-level, but have been proved to perform substantially worse than supervised models (Patwardhan and Riloff, 2009).",
            "Evaluation Metric The metric for past work on document-level role-filler mentions extraction (Patwardhan and Riloff, 2009; Huang and Riloff, 2011; Du and Cardie, 2020) calculates mention-level precision across all alternative mentions for each rolefiller entity.",
            "Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
            "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
            "year": 2009,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145984521",
                "name": "Siddharth Patwardhan"
              },
              {
                "authorId": "1691993",
                "name": "E. Riloff"
              }
            ]
          }
        },
        {
          "citedcorpusid": 6042994,
          "isinfluential": false,
          "contexts": [
            "As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are illsuited for the template filling task, which calls for models that encode information and track entities across a longer context."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Neural Architectures for Named Entity Recognition",
            "abstract": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016.",
            "year": 2016,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1830914",
                "name": "Guillaume Lample"
              },
              {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
              },
              {
                "authorId": "50324141",
                "name": "Sandeep Subramanian"
              },
              {
                "authorId": "2189948",
                "name": "Kazuya Kawakami"
              },
              {
                "authorId": "1745899",
                "name": "Chris Dyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7961699,
          "isinfluential": false,
          "contexts": [
            "We treat document-level template filling as a sequence-to-sequence task (Sutskever et al., 2014) in order to better model the cross-role dependencies and cross-sentence noun phrase coreference structure.",
            "Instead of using a sequence-to-sequence learning architecture with separate modules (Sutskever et al., 2014; Bahdanau et al., 2015), we use a single pretrained transformer model (Devlin et al.",
            "Instead of using a sequence-to-sequence learning architecture with separate modules (Sutskever et al., 2014; Bahdanau et al., 2015), we use a single pretrained transformer model (Devlin et al., 2019) for both parts, and introduce no additional fine-tuned parameters."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
            "year": 2014,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9778664,
          "isinfluential": false,
          "contexts": [
            "Duan et al. (2017) and Zhao et al.",
            "Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploiting Document Level Information to Improve Event Detection via Recurrent Neural Networks",
            "abstract": "",
            "year": 2017,
            "venue": "International Joint Conference on Natural Language Processing",
            "authors": [
              {
                "authorId": "31588168",
                "name": "Shaoyang Duan"
              },
              {
                "authorId": "1724097",
                "name": "Ruifang He"
              },
              {
                "authorId": "2118225775",
                "name": "Wenli Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14117526,
          "isinfluential": false,
          "contexts": [
            "Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Improving Event Detection with Abstract Meaning Representation",
            "abstract": "Event Detection (ED) aims to identify instances of specified types of events in text, which is a crucial component in the overall task of event extraction. The commonly used features consist of lexical, syntactic, and entity information, but the knowledge encoded in the Abstract Meaning Representation (AMR) has not been utilized in this task. AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. In this paper, we demonstrate the effectiveness of AMR to capture and represent the deeper semantic contexts of the trigger words in this task. Experimental results further show that adding AMR features on top of the traditional features can achieve 67.8% (with 2.1% absolute improvement) F-measure (F1), which is comparable to the state-of-the-art approaches.",
            "year": 2015,
            "venue": "",
            "authors": [
              {
                "authorId": "2144438930",
                "name": "Xiang Li"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "2065281098",
                "name": "Kai Cao"
              },
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14365335,
          "isinfluential": false,
          "contexts": [
            "We base the event-based template-filling task on the original MUC1 formulation (Sundheim, 1991), but simplify it as done in prior research (Huang and Riloff, 2012; Du and Cardie, 2020).",
            "It is of great importance for automating many real-world tasks, such as event extraction from newswire (Sundheim, 1991)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Third Message Understanding Evaluation and Conference",
            "abstract": "The Naval Ocean Systems Center (NOSC) has conducted the third in a series of evaluations of English text analysis systems. These evaluations are intended to advance our understanding of the merits of current text analysis techniques, as applied to the performance of a realistic information extraction task. The latest one is also intended to provide insight into information retrieval technology (document retrieval and categorization) used instead of or in concert with language understanding technology. The inputs to the analysis/extraction process consist of naturally-occurring texts that were obtained in the form of electronic messages. The outputs of the process are a set of templates or semantic frames resembling the contents of a partially formatted database.",
            "year": 1991,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": true,
          "contexts": [
            "Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction.",
            "Both the baselines are end-toend and fine-tuned BERT (Devlin et al., 2019) contextualized representations with task-specific data.",
            ", 2015), we use a single pretrained transformer model (Devlin et al., 2019) for both parts, and introduce no additional fine-tuned parameters.",
            "Instead of using a sequence-to-sequence learning architecture with separate modules (Sutskever et al., 2014; Bahdanau et al., 2015), we use a single pretrained transformer model (Devlin et al., 2019) for both parts, and introduce no additional fine-tuned parameters."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102353905,
          "isinfluential": false,
          "contexts": [
            "In the scientiïňĄc domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
            "abstract": "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system’s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422908",
                "name": "Robin Jia"
              },
              {
                "authorId": "2109566188",
                "name": "Cliff Wong"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 147704286,
          "isinfluential": false,
          "contexts": [
            "Dong et al. (2019) presents a new unified pre-trained language model that can be fine-tuned for both NLU and NLG tasks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
            "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
            "year": 2019,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "145307652",
                "name": "Li Dong"
              },
              {
                "authorId": "144610884",
                "name": "Nan Yang"
              },
              {
                "authorId": "51456429",
                "name": "Wenhui Wang"
              },
              {
                "authorId": "49807919",
                "name": "Furu Wei"
              },
              {
                "authorId": "46522098",
                "name": "Xiaodong Liu"
              },
              {
                "authorId": "72682749",
                "name": "Yu Wang"
              },
              {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
              },
              {
                "authorId": "143849609",
                "name": "M. Zhou"
              },
              {
                "authorId": "145058181",
                "name": "H. Hon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189928589,
          "isinfluential": false,
          "contexts": [
            "Also on a related note, Chambers and Jurafsky (2011); Chambers (2013); Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Open Domain Event Extraction Using Neural Latent Variable Models",
            "abstract": "We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              },
              {
                "authorId": "2145912727",
                "name": "Yue Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "258959328": {
      "citing_paper_info": {
        "title": "Abstractive Summarization as Augmentation for Document-Level Event Detection",
        "abstract": "Transformer-based models have consistently produced substantial performance gains across a variety of NLP tasks, compared to shallow models. However, deep models are orders of magnitude more computationally expensive than shallow models, especially on tasks with large sequence lengths, such as document-level event detection. In this work, we attempt to bridge the performance gap between shallow and deep models on document-level event detection by using abstractive text summarization as an augmentation method. We augment the DocEE dataset by generating abstractive summaries of examples from low-resource classes. For classification, we use linear SVM with TF-IDF representations and RoBERTa-base. We use BART for zero-shot abstractive summarization, making our augmentation setup less resource-intensive compared to supervised fine-tuning. We experiment with four decoding methods for text generation, namely beam search, top-k sampling, top-p sampling, and contrastive search. Furthermore, we investigate the impact of using document titles as additional input for classification. Our results show that using the document title offers 2.04% and 3.19% absolute improvement in macro F1-score for linear SVM and RoBERTa, respectively. Augmentation via summarization further improves the performance of linear SVM by about 0.5%, varying slightly across decoding methods. Overall, our augmentation setup yields insufficient improvements for linear SVM compared to RoBERTa.",
        "year": 2023,
        "venue": "arXiv.org",
        "authors": [
          {
            "authorId": "2219050207",
            "name": "Janko Vidakovi'c"
          },
          {
            "authorId": "2214998955",
            "name": "Filip Karlo Dosilovic"
          },
          {
            "authorId": "2209683910",
            "name": "Domagoj Pluscec"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "127986954",
        "204960716",
        "15600925",
        "44134226"
      ],
      "citation_details": [
        {
          "citedcorpusid": 15600925,
          "isinfluential": false,
          "contexts": [
            "Generative approaches include back-translation (Sennrich et al., 2016) and summarization (Yang et al., 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Improving Neural Machine Translation Models with Monolingual Data",
            "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.",
            "year": 2015,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2082372",
                "name": "Rico Sennrich"
              },
              {
                "authorId": "2259100",
                "name": "B. Haddow"
              },
              {
                "authorId": "2539211",
                "name": "Alexandra Birch"
              }
            ]
          }
        },
        {
          "citedcorpusid": 44134226,
          "isinfluential": false,
          "contexts": [
            "Secondly, we use top-k sampling, where we set k = 640 , in accordance with Fan et al. (2018).",
            "More recently developed decoding methods include top-k sampling (Fan et al., 2018), top-p (nucleus) sampling (Holtzman et al., 2020), and contrastive search (Su et al., 2022)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Hierarchical Neural Story Generation",
            "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "144270981",
                "name": "Angela Fan"
              },
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "2921469",
                "name": "Yann Dauphin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 127986954,
          "isinfluential": false,
          "contexts": [
            ", 2018), top-p (nucleus) sampling (Holtzman et al., 2020), and contrastive search (Su et al.",
            "More recently developed decoding methods include top-k sampling (Fan et al., 2018), top-p (nucleus) sampling (Holtzman et al., 2020), and contrastive search (Su et al., 2022).",
            "95 (Holtzman et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "The Curious Case of Neural Text Degeneration",
            "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",
            "year": 2019,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "14487640",
                "name": "Ari Holtzman"
              },
              {
                "authorId": "144685020",
                "name": "Jan Buys"
              },
              {
                "authorId": "2152141637",
                "name": "Li Du"
              },
              {
                "authorId": "39191185",
                "name": "Maxwell Forbes"
              },
              {
                "authorId": "1699545",
                "name": "Yejin Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "We use BART (Lewis et al., 2020) to generate zero-shot summaries.",
            "We accomplish this feature by using a version of BART which was fine-tuned on CNN/DailyMail summarization dataset (Nallapati et al., 2016).",
            "We generate abstractive summaries using documents from low-resource classes as inputs to BART generative model."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        }
      ]
    },
    "235490449": {
      "citing_paper_info": {
        "title": "ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument Aggregation",
        "abstract": "Most of the existing information extraction frameworks (Wadden et al., 2019; Veysehet al., 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document. In our endeavour to generate precise document-level information frames from lengthy textual records, we introduce the task of Information Aggregation or Argument Aggregation. More specifically, our aim is to filter irrelevant and redundant argument mentions that were extracted at a sentence level and render a document level information frame. Majority of the existing works have been observed to resolve related tasks of document-level event argument extraction (Yang et al., 2018; Zheng et al., 2019) and salient entity identification (Jain et al., 2020) using supervised techniques. To remove dependency from large amounts of labelled data, we explore the task of information aggregation using weakly supervised techniques. In particular, we present an extractive algorithm with multiple sieves which adopts active learning strategies to work efficiently in low-resource settings. For this task, we have annotated our own test dataset comprising of 131 document information frames and have released the code and dataset to further research prospects in this new domain. To the best of our knowledge, we are the first to establish baseline results for this task in English. Our data and code are publicly available at https://github.com/DebanjanaKar/ArgFuse.",
        "year": 2021,
        "venue": "CASE",
        "authors": [
          {
            "authorId": "29850167",
            "name": "Debanjana Kar"
          },
          {
            "authorId": "40047941",
            "name": "S. Sarkar"
          },
          {
            "authorId": "51130504",
            "name": "Pawan Goyal"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 6,
        "influential_count": 1,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "14293159",
        "3137086",
        "577937",
        "225067984",
        "1508503",
        "3534906"
      ],
      "citation_details": [
        {
          "citedcorpusid": 577937,
          "isinfluential": false,
          "contexts": [
            "• TextRank (Mihalcea and Tarau, 2004): We use the graph based ranking algorithm to rank the sentence-level argument phrases for each argument type in a document extracted using (Kar et al., 2021)’s model."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TextRank: Bringing Order into Text",
            "abstract": "",
            "year": 2004,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145557251",
                "name": "Rada Mihalcea"
              },
              {
                "authorId": "2285843009",
                "name": "P. Tarau"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1508503,
          "isinfluential": false,
          "contexts": [
            "Some of the classic works related to the task of ranking text snippets are that of PageRank (Page et al., 1998) and TextRank (Mi-halcea and Tarau, 2004)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "The PageRank Citation Ranking : Bringing Order to the Web",
            "abstract": "The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.",
            "year": 1999,
            "venue": "The Web Conference",
            "authors": [
              {
                "authorId": "48479130",
                "name": "Lawrence Page"
              },
              {
                "authorId": "1786259",
                "name": "Sergey Brin"
              },
              {
                "authorId": "84095744",
                "name": "R. Motwani"
              },
              {
                "authorId": "1699245",
                "name": "T. Winograd"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3137086,
          "isinfluential": false,
          "contexts": [
            "Event-Argument Extraction, the IE task most related to the task of aggregation has a number of well-documented and reliable datasets annotated at the sentence level in different languages like ACE 2005 and TAC KBP (Mitamura et al., 2015) datasets."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Event Nugget Annotation: Processes and Issues",
            "abstract": "This paper describes the processes and issues of annotating event nuggets based on DEFT ERE Annotation Guidelines v1.3 and TAC KBP Event Detection Annotation Guidelines 1.7. Using Brat Rapid Annotation Tool (brat), newswire and discussion forum documents were annotated. One of the challenges arising from human annotation of documents is annotators’ disagreement about the way of tagging events. We propose using Event Nuggets to help meet the definitions of the specific type/subtypes which are part of this project. We present case studies of several examples of event annotation issues, including discontinuous multi-word events representing single events. Annotation statistics and consistency analysis is provided to characterize the interannotator agreement, considering single term events and multi-word events which are both continuous and discontinuous. Consistency analysis is conducted using a scorer to compare first pass annotated files against adjudicated files.",
            "year": 2015,
            "venue": "EVENTS@HLP-NAACL",
            "authors": [
              {
                "authorId": "1706595",
                "name": "T. Mitamura"
              },
              {
                "authorId": "6115355",
                "name": "Yukari Yamakawa"
              },
              {
                "authorId": "30470090",
                "name": "Susan Holm"
              },
              {
                "authorId": "2114790115",
                "name": "Zhiyi Song"
              },
              {
                "authorId": "3212973",
                "name": "Ann Bies"
              },
              {
                "authorId": "2695965",
                "name": "S. Kulick"
              },
              {
                "authorId": "1754963",
                "name": "Stephanie Strassel"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3534906,
          "isinfluential": false,
          "contexts": [
            "Based on the ﬁndings reported in (Hu et al., 2018), to avoid bias from the previous epoch, we ﬁne-tune the pre-trained BERT-based classiﬁer on the entire annotated dataset for every run of active learning."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Active Learning with Partial Feedback",
            "abstract": "While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback). To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available. To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask. At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class. Each answer eliminates some classes, leaving the learner with a partial label. The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled. Active learning with partial labels requires (i) a sampling strategy to choose (example, class) pairs, and (ii) learning from partial labels between rounds. Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset. Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost. Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.",
            "year": 2018,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2894848",
                "name": "Peiyun Hu"
              },
              {
                "authorId": "32219137",
                "name": "Zachary Chase Lipton"
              },
              {
                "authorId": "2047844",
                "name": "Anima Anandkumar"
              },
              {
                "authorId": "1770537",
                "name": "Deva Ramanan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14293159,
          "isinfluential": false,
          "contexts": [
            "After each epoch of active learning, 50 most uncertain samples are identiﬁed using the Monte Carlo estimation of error reduction (Roy and McCallum, 2001)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Toward Optimal Active Learning through Monte Carlo Estimation of Error Reduction",
            "abstract": "This paper presents an active learning method that directly optimizes expected future error. This is in contrast to many other popular techniques that instead aim to reduce version space size. These methods are popular because for many learning models, closed form calculation of the expected future error is intractable. Our approach is made feasible by taking a Monte Carlo approach to estimating the expected reduction in error due to the labeling of a query. In experimental results on three real-world data sets we reach high accuracy with four times fewer labelled examples than competing methods.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "143724999",
                "name": "N. Roy"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225067984,
          "isinfluential": true,
          "contexts": [
            "Event argument extraction is a well researched information extraction task which has seen a lot of work at the sentence-level (Wang et al., 2019; Wadden et al., 2019; Nguyen et al., 2016; Luan et al., 2019b; Veyseh et al., 2020) but a scarce amount of research has been carried out at the document level.",
            "Most of the existing event-argument extraction systems (Nguyen et al., 2016; Luan et al., 2019a; Wadden et al., 2019; Veyseh et al., 2020) pertain to a sentence-level focus, Figure 1: Example document excerpt from our corpus highlighting the different challenges of the aggregation task.",
            "Most of the existing information extraction frameworks (Wadden et al., 2019; Veyseh et al., 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document.",
            "…argument extraction is a well researched information extraction task which has seen a lot of work at the sentence-level (Wang et al., 2019; Wad-den et al., 2019; Nguyen et al., 2016; Luan et al., 2019b; Veyseh et al., 2020) but a scarce amount of research has been carried out at the document level."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction",
            "abstract": "The goal of Event Argument Extraction (EAE) is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks (GTNs) to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "3460489",
                "name": "Amir Pouran Ben Veyseh"
              },
              {
                "authorId": "150322649",
                "name": "Tuan Ngo Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        }
      ]
    },
    "271064857": {
      "citing_paper_info": {
        "title": "Consistent Document-Level Relation Extraction via Counterfactuals",
        "abstract": "Many datasets have been developed to train and evaluate document-level relation extraction (RE) models. Most of these are constructed using real-world data. It has been shown that RE models trained on real-world data suffer from factual biases. To evaluate and address this issue, we present CovEReD, a counterfactual data generation approach for document-level relation extraction datasets using entity replacement. We first demonstrate that models trained on factual data exhibit inconsistent behavior: while they accurately extract triples from factual data, they fail to extract the same triples after counterfactual modification. This inconsistency suggests that models trained on factual data rely on spurious signals such as specific entities and external knowledge $\\unicode{x2013}$ rather than on the input context $\\unicode{x2013}$ to extract triples. We show that by generating document-level counterfactual data with CovEReD and training models on them, consistency is maintained with minimal impact on RE performance. We release our CovEReD pipeline as well as Re-DocRED-CF, a dataset of counterfactual RE documents, to assist in evaluating and addressing inconsistency in document-level RE.",
        "year": 2024,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2054744",
            "name": "Ali Modarressi"
          },
          {
            "authorId": "1999179692",
            "name": "Abdullatif Köksal"
          },
          {
            "authorId": "2130001188",
            "name": "Hinrich Schutze"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 5,
        "unique_cited_count": 4,
        "influential_count": 0,
        "detailed_records_count": 5
      },
      "cited_papers": [
        "238856938",
        "189898081",
        "235254332",
        "3782112"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3782112,
          "isinfluential": false,
          "contexts": [
            "ENTRE (Wang et al., 2023), a counterfactual modification of TACRED (Zhang et al., 2017), replaces entities to develop a robust sentence-level RE benchmark."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Position-aware Attention and Supervised Data Improve Slot Filling",
            "abstract": "Organized relational knowledge in the form of “knowledge graphs” is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2% to 26.7%.",
            "year": 2017,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49889487",
                "name": "Yuhao Zhang"
              },
              {
                "authorId": "3428769",
                "name": "Victor Zhong"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "32301760",
                "name": "Gabor Angeli"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": false,
          "contexts": [
            "RE datasets such as DocRED (Yao et al., 2019) and Re-DocRED (Tan et al., 2022b) consist of a factual corpus (Wikipedia) annotated with triples.",
            "In a triple r ∈ R , we have the indices ( i ) of head and tail entities, the relation r t and – if the triple comes from the original DocRED (Yao et al., 2019) – the IDs of the sentences that are the evidence for r .",
            "In document-level RE, triples can span multiple sentences (Yao et al., 2019; Tan et al., 2022b; Xiaoyan et al., 2023)."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235254332,
          "isinfluential": false,
          "contexts": [
            "A common case is entity bias: the model relies on entities in its parametric knowledge to make a prediction (Long-pre et al., 2021; Qian et al., 2021; Xu et al., 2022; Chen et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Annotation Inconsistency and Entity Bias in MultiWOZ",
            "abstract": "MultiWOZ (Budzianowski et al., 2018) is one of the most popular multi-domain taskoriented dialog datasets, containing 10K+ annotated dialogs covering eight domains. It has been widely accepted as a benchmark for various dialog tasks, e.g., dialog state tracking (DST), natural language generation (NLG) and end-to-end (E2E) dialog modeling. In this work, we identify an overlooked issue with dialog state annotation inconsistencies in the dataset, where a slot type is tagged inconsistently across similar dialogs leading to confusion for DST modeling. We propose an automated correction for this issue, which is present in 70% of the dialogs. Additionally, we notice that there is significant entity bias in the dataset (e.g., “cambridge” appears in 50% of the destination cities in the train domain). The entity bias can potentially lead to named entity memorization in generative models, which may go unnoticed as the test set suffers from a similar entity bias as well. We release a new test set with all entities replaced with unseen entities. Finally, we benchmark joint goal accuracy (JGA) of the state-of-theart DST baselines on these modified versions of the data. Our experiments show that the annotation inconsistency corrections lead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in JGA when models are evaluated on the new test set with unseen entities.",
            "year": 2021,
            "venue": "SIGDIAL Conferences",
            "authors": [
              {
                "authorId": "2053225294",
                "name": "Kun Qian"
              },
              {
                "authorId": "1791052",
                "name": "Ahmad Beirami"
              },
              {
                "authorId": "3146592",
                "name": "Zhouhan Lin"
              },
              {
                "authorId": "1397387596",
                "name": "Ankita De"
              },
              {
                "authorId": "1979505",
                "name": "A. Geramifard"
              },
              {
                "authorId": "1564034697",
                "name": "Zhou Yu"
              },
              {
                "authorId": "21669342",
                "name": "Chinnadhurai Sankar"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238856938,
          "isinfluential": false,
          "contexts": [
            "We address this by generating CF data and training RE models on them. inferring from the input, they may use their parametric knowledge (McCoy et al., 2019; Kaushik et al., 2020; Paranjape et al., 2022).",
            "Following Paranjape et al. (2022), we use pairwise consistency as our measure."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Retrieval-guided Counterfactual Generation for QA",
            "abstract": "Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals — i.e. minimally perturbed inputs — can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements in a model’s robustness to local perturbations.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2132497930",
                "name": "Bhargavi Paranjape"
              },
              {
                "authorId": "48024953",
                "name": "Matthew Lamm"
              },
              {
                "authorId": "6117577",
                "name": "Ian Tenney"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "A common case is entity bias: the model relies on entities in its parametric knowledge to make a prediction (Long-pre et al., 2021; Qian et al., 2021; Xu et al., 2022; Chen et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "268783461": {
      "citing_paper_info": {
        "title": "Self-distillation framework for document-level relation extraction in low-resource environments",
        "abstract": "The objective of document-level relation extraction is to retrieve the relations existing between entities within a document. Currently, deep learning methods have demonstrated superior performance in document-level relation extraction tasks. However, to enhance the model’s performance, various methods directly introduce additional modules into the backbone model, which often increases the number of parameters in the overall model. Consequently, deploying these deep models in resource-limited environments presents a challenge. In this article, we introduce a self-distillation framework for document-level relational extraction. We partition the document-level relation extraction model into two distinct modules, namely, the entity embedding representation module and the entity pair embedding representation module. Subsequently, we apply separate distillation techniques to each module to reduce the model’s size. In order to evaluate the proposed framework’s performance, two benchmark datasets for document-level relation extraction, namely GDA and DocRED are used in this study. The results demonstrate that our model effectively enhances performance and significantly reduces the model’s size.",
        "year": 2024,
        "venue": "PeerJ Computer Science",
        "authors": [
          {
            "authorId": "2269126650",
            "name": "Hao Wu"
          },
          {
            "authorId": "2294164527",
            "name": "Gang Zhou"
          },
          {
            "authorId": "2148293251",
            "name": "Yi Xia"
          },
          {
            "authorId": "2145559964",
            "name": "Hongbo Liu"
          },
          {
            "authorId": "2294168720",
            "name": "Tianzhi Zhang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 15,
        "unique_cited_count": 15,
        "influential_count": 1,
        "detailed_records_count": 15
      },
      "cited_papers": [
        "102353905",
        "219558831",
        "248300064",
        "131773936",
        "102483181",
        "219559263",
        "52967399",
        "7200347",
        "2797612",
        "39871772",
        "2723173",
        "235485451",
        "53213211",
        "3297437",
        "218613850"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2723173,
          "isinfluential": false,
          "contexts": [
            "Typically, teachers extract knowledge at various levels, such as logits and features ( Romero et al., 2014 ), which can be utilized directly or converted into other networks or kernel functions ( Heo et al., 2019a ; Heo et al., 2019b ; Kim, Park & Kwak, 2018 )."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "FitNets: Hints for Thin Deep Nets",
            "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2069136633",
                "name": "Adriana Romero"
              },
              {
                "authorId": "2482072",
                "name": "Nicolas Ballas"
              },
              {
                "authorId": "3127597",
                "name": "Samira Ebrahimi Kahou"
              },
              {
                "authorId": "3186079",
                "name": "Antoine Chassang"
              },
              {
                "authorId": "143706039",
                "name": "C. Gatta"
              },
              {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2797612,
          "isinfluential": false,
          "contexts": [
            "…and sentences to create a graph representation using heuristic rules and dependency structures is a popular method for document-level relation extraction, e.g. , Peng et al. (2017) , Liu & Lapata (2018) , Christopoulou, Miwa & Ananiadou (2019) , Nan et al. (2020) and Guo, Zhang & Lu (2019) ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs",
            "abstract": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
            "year": 2017,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              },
              {
                "authorId": "2596310",
                "name": "Chris Quirk"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              },
              {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3297437,
          "isinfluential": true,
          "contexts": [
            "For training, we utilize a hybrid precision training model from the Apex library ( Micikevicius et al., 2017 ), optimized by AdamW ( Loshchilov & Hutter, 2019 ) as the learner, with a learning rate of 5e − 5 and linear preheating set to 0.06 ( Goyal et al., 2018 )."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Mixed Precision Training",
            "abstract": "Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1802359",
                "name": "P. Micikevicius"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "67038137",
                "name": "Jonah Alben"
              },
              {
                "authorId": "2040049",
                "name": "G. Diamos"
              },
              {
                "authorId": "152585800",
                "name": "Erich Elsen"
              },
              {
                "authorId": "2082313059",
                "name": "David García"
              },
              {
                "authorId": "31963005",
                "name": "Boris Ginsburg"
              },
              {
                "authorId": "122523478",
                "name": "Michael Houston"
              },
              {
                "authorId": "2787022",
                "name": "Oleksii Kuchaiev"
              },
              {
                "authorId": "145595812",
                "name": "Ganesh Venkatesh"
              },
              {
                "authorId": "1491232360",
                "name": "Hao Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7200347,
          "isinfluential": false,
          "contexts": [
            "Knowledge distillation (KD) ( Pham et al., 2022 ; Gou et al., 2021 ; Hinton, Vinyals & Dean, 2015 ) is a technique that involves transferring learning information from a pre-trained large network (teacher) to a smaller one (student)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distilling the Knowledge in a Neural Network",
            "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
            "year": 2015,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "49959210",
                "name": "J. Dean"
              }
            ]
          }
        },
        {
          "citedcorpusid": 39871772,
          "isinfluential": false,
          "contexts": [
            "…and sentences to create a graph representation using heuristic rules and dependency structures is a popular method for document-level relation extraction, e.g. , Peng et al. (2017) , Liu & Lapata (2018) , Christopoulou, Miwa & Ananiadou (2019) , Nan et al. (2020) and Guo, Zhang & Lu (2019) ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Learning Structured Text Representations",
            "abstract": "In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.",
            "year": 2017,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39798499",
                "name": "Yang Liu"
              },
              {
                "authorId": "1747893",
                "name": "Mirella Lapata"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "The backbone model we employ is built upon ATLOP, with BERT-base ( Devlin et al., 2019 ) serving as the encoder for document encoding in the DocRED (https://github.com/thunlp/ DocRED) and GDA (https://bitbucket.org/alexwuhkucs/gda-extraction/src/master/) datasets."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53213211,
          "isinfluential": false,
          "contexts": [
            "Typically, teachers extract knowledge at various levels, such as logits and features ( Romero et al., 2014 ), which can be utilized directly or converted into other networks or kernel functions ( Heo et al., 2019a ; Heo et al., 2019b ; Kim, Park & Kwak, 2018 )."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons",
            "abstract": "An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classificationfriendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "3086596",
                "name": "Byeongho Heo"
              },
              {
                "authorId": "2646766",
                "name": "Minsik Lee"
              },
              {
                "authorId": "2151587",
                "name": "Sangdoo Yun"
              },
              {
                "authorId": "46174575",
                "name": "J. Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102353905,
          "isinfluential": false,
          "contexts": [
            "To obtain the entity’s embedding for all mentions m ij N ei j = 1 corresponding to e i , logsumexp pooling ( Jia, Wong & Poon, 2019 ) is employed as follows: The pooling operation collects all the information referenced in the document, thereby generating an embedded representation of the entity."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
            "abstract": "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system’s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422908",
                "name": "Robin Jia"
              },
              {
                "authorId": "2109566188",
                "name": "Cliff Wong"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102483181,
          "isinfluential": false,
          "contexts": [
            "Typically, teachers extract knowledge at various levels, such as logits and features ( Romero et al., 2014 ), which can be utilized directly or converted into other networks or kernel functions ( Heo et al., 2019a ; Heo et al., 2019b ; Kim, Park & Kwak, 2018 )."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Comprehensive Overhaul of Feature Distillation",
            "abstract": "We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page.",
            "year": 2019,
            "venue": "IEEE International Conference on Computer Vision",
            "authors": [
              {
                "authorId": "3086596",
                "name": "Byeongho Heo"
              },
              {
                "authorId": "47965071",
                "name": "Jeesoo Kim"
              },
              {
                "authorId": "2151587",
                "name": "Sangdoo Yun"
              },
              {
                "authorId": "46904404",
                "name": "Hyojin Park"
              },
              {
                "authorId": "3160425",
                "name": "Nojun Kwak"
              },
              {
                "authorId": "46174575",
                "name": "J. Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131773936,
          "isinfluential": false,
          "contexts": [
            "Considering a given document d = [ x t ] lt = 1 , we locate the entity mentions by placing a special symbol ‘‘ ∗ ’’ at the start and end of each mention ( Shi & Lin, 2019 ; Soares et al., 2019 ; Zhang et al., 2017 )."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "abstract": "We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1884766",
                "name": "Peng Shi"
              },
              {
                "authorId": "145580839",
                "name": "Jimmy J. Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218613850,
          "isinfluential": false,
          "contexts": [
            "…and sentences to create a graph representation using heuristic rules and dependency structures is a popular method for document-level relation extraction, e.g. , Peng et al. (2017) , Liu & Lapata (2018) , Christopoulou, Miwa & Ananiadou (2019) , Nan et al. (2020) and Guo, Zhang & Lu (2019) ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2056582888",
                "name": "Guoshun Nan"
              },
              {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
              },
              {
                "authorId": "3305422",
                "name": "Ivan Sekulic"
              },
              {
                "authorId": "2153424287",
                "name": "Wei Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219558831,
          "isinfluential": false,
          "contexts": [
            "Furthermore, the teacher can consist of a single pre-training network or a group of multiple pre-training networks,or the results of the previous iteration cycle can be used as the teacher model for the next training cycle ( Zhang & Sabuncu, 2020 ; Shen et al., 2022 )."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Self-Distillation as Instance-Specific Label Smoothing",
            "abstract": "It has been recently demonstrated that multi-generational self-distillation can improve generalization. Despite this intriguing observation, reasons for the enhancement remain poorly understood. In this paper, we first demonstrate experimentally that the improved performance of multi-generational self-distillation is in part associated with the increasing diversity in teacher predictions. With this in mind, we offer a new interpretation for teacher-student training as amortized MAP estimation, such that teacher predictions enable instance-specific regularization. Our framework allows us to theoretically relate self-distillation to label smoothing, a commonly used technique that regularizes predictive uncertainty, and suggests the importance of predictive diversity in addition to predictive uncertainty. We present experimental results using multiple datasets and neural network architectures that, overall, demonstrate the utility of predictive diversity. Finally, we propose a novel instance-specific label smoothing technique that promotes predictive diversity without the need for a separately trained teacher model. We provide an empirical evaluation of the proposed method, which, we find, often outperforms classical label smoothing.",
            "year": 2020,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1491240545",
                "name": "Zhilu Zhang"
              },
              {
                "authorId": "2369409",
                "name": "M. Sabuncu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219559263,
          "isinfluential": false,
          "contexts": [
            "Knowledge distillation (KD) ( Pham et al., 2022 ; Gou et al., 2021 ; Hinton, Vinyals & Dean, 2015 ) is a technique that involves transferring learning information from a pre-trained large network (teacher) to a smaller one (student)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Knowledge Distillation: A Survey",
            "abstract": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
            "year": 2020,
            "venue": "International Journal of Computer Vision",
            "authors": [
              {
                "authorId": "38978232",
                "name": "Jianping Gou"
              },
              {
                "authorId": "2425630",
                "name": "B. Yu"
              },
              {
                "authorId": "144555237",
                "name": "S. Maybank"
              },
              {
                "authorId": "143719920",
                "name": "D. Tao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235485451,
          "isinfluential": false,
          "contexts": [
            "…between entities in a single sentence by using sentence-level analysis, e.g. , Zhao, Gao & Guo (2023) , Zhang et al. (2023) , Li et al. (2022) , Zheng et al. (2021) , Wu et al. (2021) , Zhang et al. (2020) , Wang et al. (2020) , Ye et al. (2020) , Yu et al. (2020) , Zhang et al. (2018) ; Zeng…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction",
            "abstract": "Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples. The source code has been submitted as the supplementary material and will be made publicly available after the blind review.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2115584612",
                "name": "Heng Zheng"
              },
              {
                "authorId": "1381230088",
                "name": "Rui Wen"
              },
              {
                "authorId": "2145307827",
                "name": "Xi Chen"
              },
              {
                "authorId": "1845781852",
                "name": "Yifan Yang"
              },
              {
                "authorId": "2145046450",
                "name": "Yunyan Zhang"
              },
              {
                "authorId": "2030976630",
                "name": "Ziheng Zhang"
              },
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "2113838416",
                "name": "Bin Qin"
              },
              {
                "authorId": "2153555985",
                "name": "Ming Xu"
              },
              {
                "authorId": "2145273405",
                "name": "Yefeng Zheng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248300064,
          "isinfluential": false,
          "contexts": [
            "In addition, Zhang & Cheng (2022) modelled inference as a masked network reconstruction problem, where the entity matrix is viewed as an image, which is then stochastically masked and recovered by an inference module in order to capture correlations between relations."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Masked Image Reconstruction Network for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Compared with its sentence-level counterpart, Document-level relation extraction requires inference over multiple sentences to extract complex relational triples. Previous research normally complete reasoning through information propagation on the mention-level or entity-level document-graphs, regardless of the correlations between the relationships. In this paper, we propose a novel Document-level Relation Extraction model based on a Masked Image Reconstruction network (DRE-MIR), which models inference as a masked image reconstruction problem to capture the correlations between relationships. Specifically, we first leverage an encoder module to get the features of entities and construct the entity-pair matrix based on the features. After that, we look on the entity-pair matrix as an image and then randomly mask it and restore it through an inference module to capture the correlations between the relationships. We evaluate our model on three public document-level relation extraction datasets, i.e. DocRED, CDR, and GDA. Experimental results demonstrate that our model achieves state-of-the-art performance on these three datasets and has excellent robustness against the noises during the inference process.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "48570713",
                "name": "L. Zhang"
              },
              {
                "authorId": "2332172",
                "name": "Yidong Cheng"
              }
            ]
          }
        }
      ]
    },
    "266916709": {
      "citing_paper_info": {
        "title": "A Multiagent-Based Document-Level Relation Extraction System With Entity Pair Awareness and Sentence Significance",
        "abstract": "Document-level relation extraction extracts relations between entity pairs from multiple sentences in a document by reasoning about semantic relationships between entities and entity mentions. In this article, we propose a multiagent system that extracts relations between entity pairs by capturing entity semantics, mentions, and pairs. First, we employ a pretrained language model to capture entities, entity pairs, and sentence context. Then, we design a multisemantic relation graph between entities and entity pairs, based on which a graph convolutional network is constructed for information reasoning and relation extraction in a relation extraction agent. Next, we introduce an evidence sentence extraction agent to focus on a relevant subset of inputs. Finally, we implement collaboration between two agents in a multiagent collaborative optimization module. Experimental results on three public datasets show that our method outperforms the existing methods in terms of F1 (harmonic mean of precision and recall) and IgnF1 (F1 on ignored relations). Specifically, our method outperforms the state-of-the-art method by 0.91 F1 and 0.65 IgnF1 on DocRED, 1.2 F1 on CDR, and 1.2 F1 on GDA.",
        "year": 2024,
        "venue": "IEEE Systems Journal",
        "authors": [
          {
            "authorId": "2278873239",
            "name": "Wenlong Hou"
          },
          {
            "authorId": "2244333441",
            "name": "Ning Jia"
          },
          {
            "authorId": "2275258879",
            "name": "Xianhui Liu"
          },
          {
            "authorId": "2150609976",
            "name": "Weidong Zhao"
          },
          {
            "authorId": "2279113710",
            "name": "Zekai Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 16,
        "influential_count": 1,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "67855713",
        "246288538",
        "240490603",
        "225039888",
        "252626402",
        "202542650",
        "260601397",
        "13756489",
        "221340666",
        "202541610",
        "238667590",
        "253664325",
        "214714027",
        "236980247",
        "5458500",
        "102353905"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5458500,
          "isinfluential": false,
          "contexts": [
            "Then, we use relational graph convolutional network (R-GCN) layers [43] to reason representations of entity semantic nodes."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Modeling Relational Data with Graph Convolutional Networks",
            "abstract": "Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
            "year": 2017,
            "venue": "Extended Semantic Web Conference",
            "authors": [
              {
                "authorId": "8804828",
                "name": "M. Schlichtkrull"
              },
              {
                "authorId": "41016725",
                "name": "Thomas Kipf"
              },
              {
                "authorId": "2789097",
                "name": "Peter Bloem"
              },
              {
                "authorId": "9965217",
                "name": "Rianne van den Berg"
              },
              {
                "authorId": "144889265",
                "name": "Ivan Titov"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "The existing document-level relationextractionmethodsaremainlycomposedofgraph-based methods[17],[18]andtransformer-basedones[19]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67855713,
          "isinfluential": false,
          "contexts": [
            "Restrictions apply. entities [30], events [31], and topics [32], [33] from text using data mining techniques [34], [35], [36]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Utility-Driven Data Analytics on Uncertain Data",
            "abstract": "Modern Internet of Things (IoT) applications generate massive amounts of data, much of them in the form of objects/items of readings, events, and log entries. Specifically, most of the objects in these IoT data contain rich embedded information (e.g., frequency and uncertainty) and different levels of importance (e.g., unit risk/utility of items, interestingness, cost, or weight). Many existing approaches in data mining and analytics have limitations, such as only the binary attribute is considered within a transaction, as well as all the objects/items having equal weights or importance. To solve these drawbacks, a novel utility-driven data analytics algorithm named HUPNU is presented in this article. As a general utility-driven uncertain data mining model, HUPNU can extract High-Utility patterns by considering both Positive and Negative unit utilities from Uncertain data. The qualified high-utility patterns can be effectively discovered for intrusion detection, risk prediction, manufacturing management, and decision-making, among others. By using the developed vertical Probability-Utility list with the positive and negative utilities structure, as well as several effective pruning strategies, experiments showed that the developed HUPNU approach with the pruning strategies performed great in mining the qualified patterns efficiently and effectively.",
            "year": 2019,
            "venue": "IEEE Systems Journal",
            "authors": [
              {
                "authorId": "3045042",
                "name": "Wensheng Gan"
              },
              {
                "authorId": "1690787",
                "name": "Chun-Wei Lin"
              },
              {
                "authorId": "144861189",
                "name": "H. Chao"
              },
              {
                "authorId": "1747034",
                "name": "A. Vasilakos"
              },
              {
                "authorId": "144019071",
                "name": "Philip S. Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102353905,
          "isinfluential": false,
          "contexts": [
            "Hence, document-level relation extraction [16], [17] that can extract relationships among entity pairs from multiple sentences in a document has attracted researchers’ interest.",
            "1) Entity Nodes: The representation of entity node e i is where log represents logsumexp function [16], m j is a mention of e i , h m j = [ avg w k ∈ m j ( h w k )] is m j ’s representation that is calculated by taking the average of its constituent word representations, and w k is a word that…"
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
            "abstract": "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system’s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422908",
                "name": "Robin Jia"
              },
              {
                "authorId": "2109566188",
                "name": "Cliff Wong"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202541610,
          "isinfluential": false,
          "contexts": [
            "Hence, document-level relation extraction [16], [17] that can extract relationships among entity pairs from multiple sentences in a document has attracted researchers’ interest.",
            "The existing document-level relationextractionmethodsaremainlycomposedofgraph-based methods[17],[18]andtransformer-basedones[19]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs",
            "abstract": "Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48810605",
                "name": "Fenia Christopoulou"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202542650,
          "isinfluential": false,
          "contexts": [
            "Existing studies on relation extraction mainly focus on sentence levels [12], [13], [14] that can extract relations of entities from a single sentence only."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy",
            "abstract": "Joint extraction of entities and relations aims to detect entity pairs along with their relations using a single model. Prior work typically solves this task in the extract-then-classify or unified labeling manner. However, these methods either suffer from the redundant entity pairs, or ignore the important inner structure in the process of extracting entities and relations. To address these limitations, in this paper, we first decompose the joint extraction task into two interrelated subtasks, namely HE extraction and TER extraction. The former subtask is to distinguish all head-entities that may be involved with target relations, and the latter is to identify corresponding tail-entities and relations for each extracted head-entity. Next, these two subtasks are further deconstructed into several sequence labeling problems based on our proposed span-based tagging scheme, which are conveniently solved by a hierarchical boundary tagger and a multi-span decoding algorithm. Owing to the reasonable decomposition strategy, our model can fully capture the semantic interdependency between different steps, as well as reduce noise from irrelevant entity pairs. Experimental results show that our method outperforms previous work by 5.2%, 5.9% and 21.5% (F1 score), achieving a new state-of-the-art on three public datasets",
            "year": 2019,
            "venue": "European Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "48613402",
                "name": "Yu Bowen"
              },
              {
                "authorId": "47295143",
                "name": "Zhenyu Zhang"
              },
              {
                "authorId": "51111230",
                "name": "Jianlin Su"
              },
              {
                "authorId": "2115721575",
                "name": "Yubin Wang"
              },
              {
                "authorId": "2079682",
                "name": "Tingwen Liu"
              },
              {
                "authorId": "1864982637",
                "name": "Bin Wang"
              },
              {
                "authorId": "48831399",
                "name": "Sujian Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214714027,
          "isinfluential": false,
          "contexts": [
            "The transformer-based methods mainly use PTMs to capture long-distance dependence information in a document, e.g., Tang et al. [40] apply PTMs but fail to distinguish between intrasentence and intersentence relations.",
            "2) Transformer-Based Methods: They use the PTM to extract information and directly apply relation extraction based on it, including HIN [40], ATLOP [24], SIRE [25], DocUnet [27], RDDCP [26], and SETE [47]."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "HIN: Hierarchical Inference Network for Document-Level Relation Extraction",
            "abstract": "Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the large-scale DocRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance.",
            "year": 2020,
            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "1598319620",
                "name": "Hengzhu Tang"
              },
              {
                "authorId": "9310727",
                "name": "Yanan Cao"
              },
              {
                "authorId": "122542861",
                "name": "Zhenyu Zhang"
              },
              {
                "authorId": "2115871859",
                "name": "Jiangxia Cao"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2108668664",
                "name": "Shi Wang"
              },
              {
                "authorId": "2055975557",
                "name": "Pengfei Yin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221340666,
          "isinfluential": false,
          "contexts": [
            "[41]adoptamultitaskframeworkcontainingdocument-levelrelationextractionandevidencesentenceprediction."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Entity and Evidence Guided Relation Extraction for DocRED",
            "abstract": "Document-level relation extraction is a challenging task which requires reasoning over multiple sentences in order to predict relations in a document. In this paper, we pro-pose a joint training frameworkE2GRE(Entity and Evidence Guided Relation Extraction)for this task. First, we introduce entity-guided sequences as inputs to a pre-trained language model (e.g. BERT, RoBERTa). These entity-guided sequences help a pre-trained language model (LM) to focus on areas of the document related to the entity. Secondly, we guide the fine-tuning of the pre-trained language model by using its internal attention probabilities as additional features for evidence prediction.Our new approach encourages the pre-trained language model to focus on the entities and supporting/evidence sentences. We evaluate our E2GRE approach on DocRED, a recently released large-scale dataset for relation extraction. Our approach is able to achieve state-of-the-art results on the public leaderboard across all metrics, showing that our E2GRE is both effective and synergistic on relation extraction and evidence prediction.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "152530947",
                "name": "Kevin Huang"
              },
              {
                "authorId": "50531624",
                "name": "Peng Qi"
              },
              {
                "authorId": "2148300",
                "name": "Guangtao Wang"
              },
              {
                "authorId": "1901958",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "30768523",
                "name": "Jing Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039888,
          "isinfluential": true,
          "contexts": [
            "We introduce an adaptive threshold loss function and a threshold class TH [24] into relation extraction and solve the imbalance problems by learning an adaptive threshold that optimizes the relationship between TH and the negative classes, i.e., A loss function of the relation extraction agent is…",
            "…total computational complexity of EPASS is 5) We calculate the computational complexity of graph-based DRN [39], PTM-based DocUnet [27], and AT-LOP [24]. a) DRN is mainly composed of encoding and discriminative reasoning modules, and its computational complexity is where N iters is the number of…",
            "2) Transformer-Based Methods: They use the PTM to extract information and directly apply relation extraction based on it, including HIN [40], ATLOP [24], SIRE [25], DocUnet [27], RDDCP [26], and SETE [47].",
            "Somemethods[24],[25],[26] relysolelyonthePTMtoachievegoodresults.",
            "To handle inputs with a length exceeding 512, we employ a dynamic window approach [24] for sequential encoding of documents.",
            "To solve the problems of relationship distribution imbalance and categories’ multilabeling, Zhou et al. [24] employ adaptive threshold and local pooling techniques.",
            "For a given document D , we mark the position of a mention by inserting character “*” at its beginning and end [24]."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling",
            "abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4, and also significantly outperforms existing models on both CDR and GDA. We have released our code at https://github.com/wzhouad/ATLOP.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2203076",
                "name": "Wenxuan Zhou"
              },
              {
                "authorId": "152530947",
                "name": "Kevin Huang"
              },
              {
                "authorId": "1901958",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "30768523",
                "name": "Jing Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236980247,
          "isinfluential": false,
          "contexts": [
            "It also supports many downstream applications, such as question-answering [7], [8] and recommender systems [9], [10], [11]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems",
            "abstract": "Graph neural networks (GNNs) have recently emerged as state-of-the-art collaborative filtering (CF) solution. A fundamental challenge of CF is to distill negative signals from the implicit feedback, but negative sampling in GNN-based CF has been largely unexplored. In this work, we propose to study negative sampling by leveraging both the user-item graph structure and GNNs' aggregation process. We present the MixGCF method---a general negative sampling plugin that can be directly used to train GNN-based recommender systems. In MixGCF, rather than sampling raw negatives from data, we design the hop mixing technique to synthesize hard negatives. Specifically, the idea of hop mixing is to generate the synthetic negative by aggregating embeddings from different layers of raw negatives' neighborhoods. The layer and neighborhood selection process are optimized by a theoretically-backed hard selection strategy. Extensive experiments demonstrate that by using MixGCF, state-of-the-art GNN-based recommendation models can be consistently and significantly improved, e.g., 26% for NGCF and 22% for LightGCN in terms of NDCG@20.",
            "year": 2021,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "50592574",
                "name": "Tinglin Huang"
              },
              {
                "authorId": "2047998",
                "name": "Yuxiao Dong"
              },
              {
                "authorId": "145573466",
                "name": "Ming Ding"
              },
              {
                "authorId": "2149234508",
                "name": "Zhen Yang"
              },
              {
                "authorId": "2114325306",
                "name": "Wenzheng Feng"
              },
              {
                "authorId": "2144708873",
                "name": "Xinyu Wang"
              },
              {
                "authorId": "2109541439",
                "name": "Jie Tang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238667590,
          "isinfluential": false,
          "contexts": [
            "The combination of multiagent systems and knowledge graphs enables the possibility of building multiagent systems with learning and reasoning capabilities [4], [5], [6].",
            "Zheng et al. [4] combine multiagent systems with industrial knowledge graphs to construct cognitive networks."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards Self-X cognitive manufacturing network: An industrial knowledge graph-based multi-agent reinforcement learning approach",
            "abstract": "",
            "year": 2021,
            "venue": "",
            "authors": [
              {
                "authorId": "49721278",
                "name": "Pai Zheng"
              },
              {
                "authorId": "2143947980",
                "name": "Liqiao Xia"
              },
              {
                "authorId": "2145582204",
                "name": "Chengxi Li"
              },
              {
                "authorId": "2108483010",
                "name": "Xinyu Li"
              },
              {
                "authorId": "150044654",
                "name": "Bufan Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 240490603,
          "isinfluential": false,
          "contexts": [
            "It also supports many downstream applications, such as question-answering [7], [8] and recommender systems [9], [10], [11]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Intelligent Edge-Based Recommender System for Internet of Energy Applications",
            "abstract": "Preserving energy in households and office buildings is a significant challenge, mainly due to the recent shortage of energy resources, the uprising of the current environmental problems, and the global lack of utilizing energy-saving technologies. Not to mention, within some regions, COVID-19 social distancing measures have led to a temporary transfer of energy demand from commercial and urban centers to residential areas, causing an increased use and higher charges, and in turn, creating economic impacts on customers. Therefore, the marketplace could benefit from developing an Internet of Things ecosystem that monitors energy consumption habits and promptly recommends action to facilitate energy efficiency. This article aims to present the full integration of a proposed energy efficiency framework into the Home-Assistant platform using an edge-based architecture. End users can visualize their consumption patterns as well as ambient environmental data using the Home-Assistant user interface. More notably, explainable energy-saving recommendations are delivered to end users in the form of notifications via the mobile application to facilitate habit change. In this context, to the best of the authors’ knowledge, this is the first attempt to develop and implement an energy-saving recommender system on edge devices. Thus, ensuring better privacy preservation since data are processed locally on the edge, without the need to transmit them to remote servers, as is the case with cloudlet platforms.",
            "year": 2021,
            "venue": "IEEE Systems Journal",
            "authors": [
              {
                "authorId": "2120553429",
                "name": "A. Sayed"
              },
              {
                "authorId": "2130461741",
                "name": "Yassine Himeur"
              },
              {
                "authorId": "30148974",
                "name": "A. Alsalemi"
              },
              {
                "authorId": "3090801",
                "name": "F. Bensaali"
              },
              {
                "authorId": "144730155",
                "name": "A. Amira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246288538,
          "isinfluential": false,
          "contexts": [
            "The combination of multiagent systems and knowledge graphs enables the possibility of building multiagent systems with learning and reasoning capabilities [4], [5], [6].",
            "Li et al. [5] attempt to combine multiagent systems with a knowledge-graph-enhanced recommendation system by decomposing the task into subtasks and assigning them to multiple agents for cooperative training, forming a more intelligent recommendation system."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Mcore: Multi-Agent Collaborative Learning for Knowledge-Graph-Enhanced Recommendation",
            "abstract": "Recently, knowledge-graph-enhanced recommendation systems have attracted much attention, since knowledge graph (KG) can help improving the dataset quality and offering rich semantics for explainable recommendation. However, current KG-enhanced solutions focus on analyzing user behaviors on the product level and lack effective approaches to extract user preference towards product category, which is essential for better recommendation because users shopping online normally have strong preference towards distinctive product categories, not merely on products, according to various user studies. Moreover, the existing pure embedding-based recommendation methods can only utilize KGs with a limited size, which is not adaptable to many real-world applications. In this paper, we generalize the recommendation problem with preference mining as a compound knowledge reasoning task and propose a novel multi-agent system, called Mcore, which can promote model performance by mining users’ high-level interests and is adaptable to large KGs. Specifically, we split the overall problem and allocate sub-task to each agent: Coordinate Agent takes charge of recognizing the product-category preference of current user, while Relation Agent and Entity Agent perform KG reasoning cooperatively from a user node towards the preferred categories and terminate at a product node as recommendation. To train this heterogeneous multi-agent system, where agents own various functionalities, we propose an asynchronous reinforcement training pipeline, called Multi-agent Collaborative Learning. The extensive experiments on real datasets demonstrate the effectiveness and adaptability of Mcore on recommendation tasks.",
            "year": 2021,
            "venue": "Industrial Conference on Data Mining",
            "authors": [
              {
                "authorId": "2154723109",
                "name": "Xujia Li"
              },
              {
                "authorId": "2923152",
                "name": "Yanyan Shen"
              },
              {
                "authorId": "2146071137",
                "name": "Lei Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252626402,
          "isinfluential": false,
          "contexts": [
            "2) Transformer-Based Methods: They use the PTM to extract information and directly apply relation extraction based on it, including HIN [40], ATLOP [24], SIRE [25], DocUnet [27], RDDCP [26], and SETE [47]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Structure Enhanced Transformer Encoder",
            "abstract": "Document-level relation extraction aims at discovering relational facts among entity pairs in a document, which has attracted more and more attention in recent years. Most existing methods are mainly summarized as graph-based and transformer-based methods. However, previous transformer-based methods neglect structural information between entities, while graph-based methods are unable to extract structural information effectively on account that they isolate the en-coding stage and structure reasoning stage. In this paper, we propose an effective structure enhanced transformer encoder model (SETE), integrating entity structural information into the transformer encoder. We first define a mention-level graph based on mention dependencies and convert it to a token-level graph. Then we design a dual self-attention mechanism, which enriches the structural and contextual information between entities to increase the vanilla transformer encoder inferential capability. Experiments on three public datasets show that the proposed SETE outperforms previous state-of-the-art methods and further analyses illustrate the interpretability of our model.",
            "year": 2022,
            "venue": "IEEE International Joint Conference on Neural Network",
            "authors": [
              {
                "authorId": "2143606898",
                "name": "Wanlong Liu"
              },
              {
                "authorId": "2116635928",
                "name": "Li Zhou"
              },
              {
                "authorId": "2054124872",
                "name": "DingYi Zeng"
              },
              {
                "authorId": "144956396",
                "name": "Hong Qu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253664325,
          "isinfluential": false,
          "contexts": [
            "Previous work has demonstrated the effectiveness of utilizing the distant supervision dataset for relation extraction tasks [45], [46]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Distantly supervised relation extraction with KB-enhanced reconstructed latent iterative graph networks",
            "abstract": "",
            "year": 2022,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "92758499",
                "name": "Qiji Zhou"
              },
              {
                "authorId": null,
                "name": "Yue Zhang"
              },
              {
                "authorId": "145628086",
                "name": "Donghong Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 260601397,
          "isinfluential": false,
          "contexts": [
            "Theformer ones[20],[21],[22]designagraphstructuretomodelentities andmentionsinadocument,whichcanhelpcapturetheir complexandnonlinearrelationships.",
            "Wan et al. [22] use a hierarchical dependence tree and bridge path feature to improve the performance of extracting complex intersentence relations.",
            "1) Graph-Based Methods: They construct graph structures of a document and then perform inference operations, including GLRE [37], LSR [38], GAIN [20], DRN [39], TDGAT [21], and HDT [22]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level relation extraction with hierarchical dependency tree and bridge path",
            "abstract": "",
            "year": 2023,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "145705994",
                "name": "Qian Wan"
              },
              {
                "authorId": "2180683671",
                "name": "Shangheng Du"
              },
              {
                "authorId": "2125067092",
                "name": "Yaqi Liu"
              },
              {
                "authorId": "2190030938",
                "name": "Jing Fang"
              },
              {
                "authorId": "1726043672",
                "name": "Luona Wei"
              },
              {
                "authorId": "29392604",
                "name": "Sannyuya Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Luan et al. [14] employ a graph structure to address information extraction problems.",
            "Existing studies on relation extraction mainly focus on sentence levels [12], [13], [14] that can extract relations of entities from a single sentence only."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Restrictions apply. entities [30], events [31], and topics [32], [33] from text using data mining techniques [34], [35], [36]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "270380221": {
      "citing_paper_info": {
        "title": "On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations",
        "abstract": "Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest. Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names. To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work. We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata. By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results show that both three representative DocRE models and two in-context learned large language models consistently lack sufficient robustness to entity name variations, particularly on cross-sentence relation instances and documents with more entities. Finally, we propose an entity variation robust training method which not only improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities. We further verify that the basic idea of this method can be effectively transferred to in-context learning for DocRE as well.",
        "year": 2024,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2190693111",
            "name": "Shiao Meng"
          },
          {
            "authorId": "2109906988",
            "name": "Xuming Hu"
          },
          {
            "authorId": "10017193",
            "name": "Aiwei Liu"
          },
          {
            "authorId": "2216436828",
            "name": "Fukun Ma"
          },
          {
            "authorId": "2109040042",
            "name": "Yawen Yang"
          },
          {
            "authorId": "2133436155",
            "name": "Shuang Li"
          },
          {
            "authorId": "2114092431",
            "name": "Lijie Wen"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 20,
        "unique_cited_count": 18,
        "influential_count": 0,
        "detailed_records_count": 20
      },
      "cited_papers": [
        "237635295",
        "251518246",
        "259949704",
        "264590646",
        "245144787",
        "215745290",
        "259164990",
        "52118895",
        "21698802",
        "255372865",
        "248496086",
        "258762887",
        "247939302",
        "237491060",
        "59599752",
        "254044368",
        "248227853",
        "235358168"
      ],
      "citation_details": [
        {
          "citedcorpusid": 21698802,
          "isinfluential": false,
          "contexts": [
            "Despite achieving great progress with large pre-trained language models in various tasks, modern NLP models are still brittle to out-of-domain data (Hendrycks et al., 2020), adversarial attacks (McCoy et al., 2019) or small perturbation to the input (Ebrahimi et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "HotFlip: White-Box Adversarial Examples for Text Classification",
            "abstract": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39043512",
                "name": "J. Ebrahimi"
              },
              {
                "authorId": "36290866",
                "name": "Anyi Rao"
              },
              {
                "authorId": "3021654",
                "name": "Daniel Lowd"
              },
              {
                "authorId": "1721158",
                "name": "D. Dou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52118895,
          "isinfluential": false,
          "contexts": [
            "…name variation robustness of DocRE models to more domains such as news (Zaporojets et al., 2021), biomedicine (Li et al., 2016), social media (Hu et al., 2023a) and scientific publications (Luan et al., 2018), and more languages such as Chinese (Cheng et al., 2021) and Korean (Yang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
            "abstract": "We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2265599",
                "name": "Luheng He"
              },
              {
                "authorId": "144339506",
                "name": "Mari Ostendorf"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 59599752,
          "isinfluential": false,
          "contexts": [
            "Despite achieving great progress with large pre-trained language models in various tasks, modern NLP models are still brittle to out-of-domain data (Hendrycks et al., 2020), adversarial attacks (McCoy et al., 2019) or small perturbation to the input (Ebrahimi et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
            "abstract": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145534175",
                "name": "R. Thomas McCoy"
              },
              {
                "authorId": "2949185",
                "name": "Ellie Pavlick"
              },
              {
                "authorId": "2467508",
                "name": "Tal Linzen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215745290,
          "isinfluential": false,
          "contexts": [
            "Despite achieving great progress with large pre-trained language models in various tasks, modern NLP models are still brittle to out-of-domain data (Hendrycks et al., 2020), adversarial attacks (McCoy et al., 2019) or small perturbation to the input (Ebrahimi et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
            "abstract": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
              },
              {
                "authorId": "2118894900",
                "name": "Xiaoyuan Liu"
              },
              {
                "authorId": "145217343",
                "name": "Eric Wallace"
              },
              {
                "authorId": "7485473",
                "name": "Adam Dziedzic"
              },
              {
                "authorId": "1630364521",
                "name": "R. Krishnan"
              },
              {
                "authorId": "143711382",
                "name": "D. Song"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235358168,
          "isinfluential": false,
          "contexts": [
            "The former typically abstract the document by graph structures and perform inference with graph neural networks (Zeng et al., 2020; Zhang et al., 2021; Wei and Li, 2022; Lu et al., 2023), while the latter encode the long-distance contextual dependencies with transformer-only architectures (Zhou et…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Relation Extraction as Semantic Segmentation",
            "abstract": "Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.",
            "year": 2021,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "153773882",
                "name": "Xiang Chen"
              },
              {
                "authorId": "2110972563",
                "name": "Xin Xie"
              },
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2111727840",
                "name": "Chuanqi Tan"
              },
              {
                "authorId": "2108266952",
                "name": "Mosha Chen"
              },
              {
                "authorId": "2087380523",
                "name": "Fei Huang"
              },
              {
                "authorId": "2059080424",
                "name": "Luo Si"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237491060,
          "isinfluential": false,
          "contexts": [
            "By introducing various types of perturbation to entity (names), previous works audit or improve model robustness on different tasks like named entity recognition (Lin et al., 2021), machine reading comprehension (Yan et al., 2022) and dialogue state tracking (Cho et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models",
            "abstract": "To audit the robustness of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in Wikidata; at the context level, we use pre-trained language models (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best model has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the robustness of NER models.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "51583409",
                "name": "Bill Yuchen Lin"
              },
              {
                "authorId": "2257122746",
                "name": "Wenyang Gao"
              },
              {
                "authorId": "49781448",
                "name": "Jun Yan"
              },
              {
                "authorId": "38679591",
                "name": "Ryan Rene Moreno"
              },
              {
                "authorId": "1384550891",
                "name": "Xiang Ren"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237635295,
          "isinfluential": false,
          "contexts": [
            "Recently, a series of DocRE studies propose various novel models and methods, continuously improving the performance on several DocRE benchmarks (Tan et al., 2022a; Zhou and Lee, 2022; Xiao et al., 2022; Sun et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction",
            "abstract": "Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources—relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval on DocRED.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "120727449",
                "name": "Yuxin Xiao"
              },
              {
                "authorId": "1637600997",
                "name": "Zecheng Zhang"
              },
              {
                "authorId": "3375249",
                "name": "Yuning Mao"
              },
              {
                "authorId": "1390553618",
                "name": "Carl Yang"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 245144787,
          "isinfluential": false,
          "contexts": [
            "Consequently, there has been a growing effort to explore robustness issues in NLP, such as building robustness evaluation benchmarks and proposing robustness enhancement strategies (Wang et al., 2022; Liu et al., 2024a,b; Hu et al., 2024b)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Measure and Improve Robustness in NLP Models: A Survey",
            "abstract": "As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1524732527",
                "name": "Xuezhi Wang"
              },
              {
                "authorId": "2340799757",
                "name": "Haohan Wang"
              },
              {
                "authorId": "2143919864",
                "name": "Diyi Yang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247939302,
          "isinfluential": false,
          "contexts": [
            "…of DocRE, which requires the model to jointly perform mention detection (and optionally classification), coreference resolution and relation extraction, aligning better with real-world application scenarios (Eberts and Ulges, 2021; Giorgi et al., 2022; Xu and Choi, 2022; Zhang et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A sequence-to-sequence approach for document-level relation extraction",
            "abstract": "Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https://github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.",
            "year": 2022,
            "venue": "Workshop on Biomedical Natural Language Processing",
            "authors": [
              {
                "authorId": "37585306",
                "name": "John Giorgi"
              },
              {
                "authorId": "144937305",
                "name": "Gary D Bader"
              },
              {
                "authorId": "2153212215",
                "name": "Bo Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248227853,
          "isinfluential": false,
          "contexts": [
            "We calculate the popularity of entities (Huang et al., 2022), i.e., how many times the linked item of the entity appears in a relation instance in Wikidata, in each benchmark’s test set to roughly quantify the entity knowledge."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED",
            "abstract": "DocRED is a widely used dataset for document-level relation extraction. In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload. Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations. However, when comparing DocRED with a subset relabeled from scratch, we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations. Furthermore, we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data. Through the analysis of annotators’ behaviors, we figure out the underlying reason for the problems above: the scheme actually discourages annotators from supplementing adequate instances in the revision phase. We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes. The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2007771781",
                "name": "Quzhe Huang"
              },
              {
                "authorId": "2128965713",
                "name": "Shibo Hao"
              },
              {
                "authorId": "2106717300",
                "name": "Yuan Ye"
              },
              {
                "authorId": "2110041566",
                "name": "Shengqi Zhu"
              },
              {
                "authorId": "2115387922",
                "name": "Yansong Feng"
              },
              {
                "authorId": "144060462",
                "name": "Dongyan Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248496086,
          "isinfluential": false,
          "contexts": [
            "Recently, a series of DocRE studies propose various novel models and methods, continuously improving the performance on several DocRE benchmarks (Tan et al., 2022a; Zhou and Lee, 2022; Xiao et al., 2022; Sun et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "None Class Ranking Loss for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction (RE) aims at extracting relations among entities expressed across multiple sentences, which can be viewed as a multi-label classification problem. In a typical document, most entity pairs do not express any pre-defined relation and are labeled as \"none\" or \"no relation\". For good document-level RE performance, it is crucial to distinguish such none class instances (entity pairs) from those of pre-defined classes (relations). However, most existing methods only estimate the probability of pre-defined relations independently without considering the probability of \"no relation\". This ignores the context of entity pairs and the label correlations between the none class and pre-defined classes, leading to sub-optimal predictions. To address this problem, we propose a new multi-label loss that encourages large margins of label confidence scores between each pre-defined class and the none class, which enables captured label correlations and context-dependent thresholding for label prediction. To gain further robustness against positive-negative imbalance and mislabeled data that could appear in real-world RE datasets, we propose a margin regularization and a margin shifting technique. Experimental results demonstrate that our method significantly outperforms existing multi-label losses for document-level RE and works well in other multi-label tasks such as emotion classification when none class instances are available for training.",
            "year": 2022,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2145500089",
                "name": "Yang Zhou"
              },
              {
                "authorId": "46605464",
                "name": "W. Lee"
              }
            ]
          }
        },
        {
          "citedcorpusid": 251518246,
          "isinfluential": false,
          "contexts": [
            "…former typically abstract the document by graph structures and perform inference with graph neural networks (Zeng et al., 2020; Zhang et al., 2021; Wei and Li, 2022; Lu et al., 2023), while the latter encode the long-distance contextual dependencies with transformer-only architectures (Zhou et…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "SagDRE: Sequence-Aware Graph-Based Document-Level Relation Extraction with Adaptive Margin Loss",
            "abstract": "Relation extraction (RE) is an important task for many natural language processing applications. Document-level relation extraction task aims to extract the relations within a document and poses many challenges to the RE tasks as it requires reasoning across sentences and handling multiple relations expressed in the same document. Existing state-of-the-art document-level RE models use the graph structure to better connect long-distance correlations. In this work, we propose SagDRE model, which further considers and captures the original sequential information from the text. The proposed model learns sentence-level directional edges to capture the information flow in the document and uses the token-level sequential information to encode the shortest paths from one entity to the other. In addition, we propose an adaptive margin loss to address the long-tailed multi-label problem of document-level RE tasks, where multiple relations can be expressed in a document for an entity pair and there are a few popular relations. The loss function aims to encourage separations between positive and negative classes. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed methods.",
            "year": 2022,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "153384423",
                "name": "Ying Wei"
              },
              {
                "authorId": "37696683",
                "name": "Qi Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254044368,
          "isinfluential": false,
          "contexts": [
            "…networks (Zeng et al., 2020; Zhang et al., 2021; Wei and Li, 2022; Lu et al., 2023), while the latter encode the long-distance contextual dependencies with transformer-only architectures (Zhou et al., 2021; Xie et al., 2022; Zhang et al., 2022; Meng et al., 2023; Ma et al., 2023; Hu et al., 2024a)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Towards Better Document-level Relation Extraction via Iterative Inference",
            "abstract": "Document-level relation extraction (RE) aims to extract the relations between entities from the input document that usually containing many difficultly-predicted entity pairs whose relations can only be predicted through relational inference. Existing methods usually directly predict the relations of all entity pairs of input document in a one-pass manner, ignoring the fact that predictions of some entity pairs heavily depend on the predicted results of other pairs. To deal with this issue, in this paper, we propose a novel document-level RE model with iterative inference. Our model is mainly composed of two modules: 1) a base module expected to provide preliminary relation predictions on entity pairs; 2) an inference module introduced to refine these preliminary predictions by iteratively dealing with difficultly-predicted entity pairs depending on other pairs in an easy-to-hard manner. Unlike previous methods which only consider feature information of entity pairs, our inference module is equipped with two Extended Cross Attention units, allowing it to exploit both feature information and previous predictions of entity pairs during relational inference. Furthermore, we adopt a two-stage strategy to train our model. At the first stage, we only train our base module. During the second stage, we train the whole model, where contrastive learning is introduced to enhance the training of inference module. Experimental results on three commonly-used datasets show that our model consistently outperforms other competitive baselines.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48570713",
                "name": "L. Zhang"
              },
              {
                "authorId": "34739384",
                "name": "Jinsong Su"
              },
              {
                "authorId": "47558200",
                "name": "Yidong Chen"
              },
              {
                "authorId": "2154046672",
                "name": "Zhongjian Miao"
              },
              {
                "authorId": "2192607321",
                "name": "Zijun Min"
              },
              {
                "authorId": "2192802944",
                "name": "Qingguo Hu"
              },
              {
                "authorId": "1755321",
                "name": "X. Shi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 255372865,
          "isinfluential": false,
          "contexts": [
            "Therefore, we also conduct an experiment to explore how robust of ICL for DocRE under entity name variations.",
            "Recently large language models (LLMs) (Brown et al., 2020) have achieved promising few-shot re-sults on many tasks through in-context learning (ICL) (Dong et al., 2023)."
          ],
          "intents": [
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Survey on In-context Learning",
            "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2047143813",
                "name": "Qingxiu Dong"
              },
              {
                "authorId": "49192881",
                "name": "Lei Li"
              },
              {
                "authorId": "10780897",
                "name": "Damai Dai"
              },
              {
                "authorId": "2113919886",
                "name": "Ce Zheng"
              },
              {
                "authorId": "150358371",
                "name": "Zhiyong Wu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "2116530295",
                "name": "Xu Sun"
              },
              {
                "authorId": "47883405",
                "name": "Jingjing Xu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258762887,
          "isinfluential": false,
          "contexts": [
            "Recently, a series of DocRE studies propose various novel models and methods, continuously improving the performance on several DocRE benchmarks (Tan et al., 2022a; Zhou and Lee, 2022; Xiao et al., 2022; Sun et al., 2023)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction",
            "abstract": "Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further considering the long-tail problem, we design dynamic uncertainty thresholds for different types of relations to filter high-uncertainty pseudo labels. We conduct experiments on two public datasets. Our framework outperforms strong baselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109170872",
                "name": "Qi Sun"
              },
              {
                "authorId": "2112706130",
                "name": "Kun Huang"
              },
              {
                "authorId": "2135971356",
                "name": "Xiaocui Yang"
              },
              {
                "authorId": "144656297",
                "name": "Pengfei Hong"
              },
              {
                "authorId": "2217679552",
                "name": "Kun Zhang"
              },
              {
                "authorId": "1746416",
                "name": "Soujanya Poria"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259164990,
          "isinfluential": false,
          "contexts": [
            "(Li et al., 2023; Chen et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Rethinking Document-Level Relation Extraction: A Reality Check",
            "abstract": "Recently, numerous efforts have continued to push up performance boundaries of document-level relation extraction (DocRE) and have claimed significant progress in DocRE. In this paper, we do not aim at proposing a novel model for DocRE. Instead, we take a closer look at the field to see if these performance gains are actually true. By taking a comprehensive literature review and a thorough examination of popular DocRE datasets, we find that these performance gains are achieved upon a strong or even untenable assumption in common: all named entities are perfectly localized, normalized, and typed in advance. Next, we construct four types of entity mention attacks to examine the robustness of typical DocRE models by behavioral probing. We also have a close check on model usability in a more realistic setting. Our findings reveal that most of current DocRE models are vulnerable to entity mention attacks and difficult to be deployed in real-world end-user NLP applications. Our study calls more attentions for future research to stop simplifying problem setups, and to model DocRE in the wild rather than in an unrealistic Utopian world.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "92861683",
                "name": "Jing Li"
              },
              {
                "authorId": "2119256294",
                "name": "Yequan Wang"
              },
              {
                "authorId": "1583098764",
                "name": "Shuai Zhang"
              },
              {
                "authorId": "1700777",
                "name": "Min Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259949704,
          "isinfluential": false,
          "contexts": [
            "While covering more realistic scenarios than its sentence-level counterpart (Hu et al., 2023b), DocRE also brings new challenges, requiring a comprehensive modeling of interactions among different mentions of an entity, different entities and different entity pairs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "SelfLRE: Self-refining Representation Learning for Low-resource Relation Extraction",
            "abstract": "Low-resource relation extraction (LRE) aims to extract potential relations from limited labeled corpus to handle the problem of scarcity of human annotations. Previous works mainly consist of two categories of methods: (1) Self-training methods, which improve themselves through the models' predictions, thus suffering from confirmation bias when the predictions are wrong. (2) Self-ensembling methods, which learn task-agnostic representations, therefore, generally do not work well for specific tasks. In our work, we propose a novel LRE architecture named SelfLRE, which leverages two complementary modules, one module uses self-training to obtain pseudo-labels for unlabeled data, and the other module uses self-ensembling learning to obtain the task-agnostic representations, and leverages the existing pseudo-labels to refine the better task-specific representations on unlabeled data. The two models are jointly trained through multi-task learning to iteratively improve the effect of LRE task. Experiments on three public datasets show that SelfLRE achieves 1.81% performance gain over the SOTA baseline. Source code is available at: https://github.com/THU-BPM/SelfLRE.",
            "year": 2023,
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "authors": [
              {
                "authorId": "2109906988",
                "name": "Xuming Hu"
              },
              {
                "authorId": "2223828460",
                "name": "Junzhe Chen"
              },
              {
                "authorId": "2190693111",
                "name": "Shiao Meng"
              },
              {
                "authorId": "2114092431",
                "name": "Lijie Wen"
              },
              {
                "authorId": "2191036692",
                "name": "Philip S. Yu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 264590646,
          "isinfluential": false,
          "contexts": [
            "…abstract the document by graph structures and perform inference with graph neural networks (Zeng et al., 2020; Zhang et al., 2021; Wei and Li, 2022; Lu et al., 2023), while the latter encode the long-distance contextual dependencies with transformer-only architectures (Zhou et al., 2021; Xie et…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Anaphor Assisted Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction (DocRE) involves identifying relations between entities distributed in multiple sentences within a document. Existing methods focus on building a heterogeneous document graph to model the internal structure of an entity and the external interaction between entities. However, there are two drawbacks in existing methods. On one hand, anaphor plays an important role in reasoning to identify relations between entities but is ignored by these methods. On the other hand, these methods achieve cross-sentence entity interactions implicitly by utilizing a document or sentences as intermediate nodes. Such an approach has difficulties in learning fine-grained interactions between entities across different sentences, resulting in sub-optimal performance. To address these issues, we propose an Anaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the widely-used datasets demonstrate that our model achieves a new state-of-the-art performance.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2262515194",
                "name": "Chonggang Lu"
              },
              {
                "authorId": "2109975984",
                "name": "Richong Zhang"
              },
              {
                "authorId": "2262787372",
                "name": "Kai Sun"
              },
              {
                "authorId": "2109177441",
                "name": "Jaein Kim"
              },
              {
                "authorId": "2262935567",
                "name": "Cunwang Zhang"
              },
              {
                "authorId": "2264766340",
                "name": "Yongyi Mao"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Consequently, there has been a growing effort to explore robustness issues in NLP, such as building robustness evaluation benchmarks and proposing robustness enhancement strategies (Wang et al., 2022; Liu et al., 2024a,b; Hu et al., 2024b)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We further apply the proposed pipeline to Do-cRED (Yao et al., 2019) and Re-DocRED (Tan et al., 2022b), due to both being the largest and most widely used DocRE datasets, to create two novel benchmarks, named Env-DocRED and Env-Re-DocRED, for evaluating the robustness of DocRE models to entity name…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "273240753": {
      "citing_paper_info": {
        "title": "A Personalized Federated Framework for Document-Level Biomedical Relation Extraction",
        "abstract": "Document-Ievel relation extraction (RE), a critical task in natural language processing, involves classifying relationships between entities mentioned in documents. Traditional document-level RE methods often rely on centralized datasets, which necessitates the collection of documents from various sources and their storage on a single machine. However, this brings up issues regarding privacy protection, and also limit the diversity of data. In this paper, we solve the document-level RE problem in federated learning scenario and propose a personalized federated learning framework named PF-BRE, specifically designed for biomedical RE. This framework enables multiple clients to collaboratively train a robust model without sharing the raw data. Additionally, to simulate the varying data characteristics of different clients in real-world application scenarios, we process the existing biomedical RE datasets to create distinct clients for personalized federated training. Exper-iments show that, compared to centralized training, the PF - BRE framework not only achieves impressive performance but also protects the data privacy of each client.",
        "year": 2024,
        "venue": "2024 6th International Conference on Data-driven Optimization of Complex Systems (DOCS)",
        "authors": [
          {
            "authorId": "2324964065",
            "name": "Yan Xiao"
          },
          {
            "authorId": "2284823417",
            "name": "Yaochu Jin"
          },
          {
            "authorId": "2280129502",
            "name": "Haoyu Zhang"
          },
          {
            "authorId": "2325282986",
            "name": "Xu Huo"
          },
          {
            "authorId": "151505706",
            "name": "Qiqi Liu"
          },
          {
            "authorId": "2264209226",
            "name": "Zeqi Zheng"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 5,
        "influential_count": 0,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "189898081",
        "3782112",
        "269804131",
        "88817",
        "214641183"
      ],
      "citation_details": [
        {
          "citedcorpusid": 88817,
          "isinfluential": false,
          "contexts": [
            "Specifically, the CDR (Chemical-Disease Reactions) dataset [19] consists of documents that identify relationships between chemicals and diseases, focusing on how certain chemicals impact various diseases."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction",
            "abstract": "Community-run, formal evaluations and manually annotated text corpora are critically important for advancing biomedical text-mining research. Recently in BioCreative V, a new challenge was organized for the tasks of disease named entity recognition (DNER) and chemical-induced disease (CID) relation extraction. Given the nature of both tasks, a test collection is required to contain both disease/chemical annotations and relation annotations in the same set of articles. Despite previous efforts in biomedical corpus construction, none was found to be sufficient for the task. Thus, we developed our own corpus called BC5CDR during the challenge by inviting a team of Medical Subject Headings (MeSH) indexers for disease/chemical entity annotation and Comparative Toxicogenomics Database (CTD) curators for CID relation annotation. To ensure high annotation quality and productivity, detailed annotation guidelines and automatic annotation tools were provided. The resulting BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH as the controlled vocabulary. To ensure accuracy, the entities were first captured independently by two annotators followed by a consensus annotation: The average inter-annotator agreement (IAA) scores were 87.49% and 96.05% for the disease and chemicals, respectively, in the test set according to the Jaccard similarity coefficient. Our corpus was successfully used for the BioCreative V challenge tasks and should serve as a valuable resource for the text-mining research community. Database URL: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr/",
            "year": 2016,
            "venue": "Database J. Biol. Databases Curation",
            "authors": [
              {
                "authorId": "2117970351",
                "name": "Jiao Li"
              },
              {
                "authorId": "2116969756",
                "name": "Yueping Sun"
              },
              {
                "authorId": "2000280",
                "name": "Robin J. Johnson"
              },
              {
                "authorId": "2160494",
                "name": "D. Sciaky"
              },
              {
                "authorId": "3252035",
                "name": "Chih-Hsuan Wei"
              },
              {
                "authorId": "2277706",
                "name": "Robert Leaman"
              },
              {
                "authorId": "108109364",
                "name": "A. P. Davis"
              },
              {
                "authorId": "3283642",
                "name": "C. Mattingly"
              },
              {
                "authorId": "1922749",
                "name": "Thomas C. Wiegers"
              },
              {
                "authorId": "144202084",
                "name": "Zhiyong Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3782112,
          "isinfluential": false,
          "contexts": [
            "The methods mainly validate on well-established and widely used datasets, including SemEval-2010 Task 8 [5], ACE [6], and TACRED [7], and so on."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Position-aware Attention and Supervised Data Improve Slot Filling",
            "abstract": "Organized relational knowledge in the form of “knowledge graphs” is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2% to 26.7%.",
            "year": 2017,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49889487",
                "name": "Yuhao Zhang"
              },
              {
                "authorId": "3428769",
                "name": "Victor Zhong"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              },
              {
                "authorId": "32301760",
                "name": "Gabor Angeli"
              },
              {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": false,
          "contexts": [
            "Previous studies [1], [2] mainly focused on sentence-level RE tasks like the one above, it is noteworthy that document-level RE tasks [3] are commonly in reality, which involves the extraction of relationships between entities mentioned across sentences within a document.",
            "This necessitates advanced techniques to manage these complex linguistic structures, namely, document-level RE [3] technology."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214641183,
          "isinfluential": false,
          "contexts": [
            "Previous studies [1], [2] mainly focused on sentence-level RE tasks like the one above, it is noteworthy that document-level RE tasks [3] are commonly in reality, which involves the extraction of relationships between entities mentioned across sentences within a document."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Hybrid Attention-Based Transformer Block Model for Distant Supervision Relation Extraction",
            "abstract": "",
            "year": 2020,
            "venue": "Neurocomputing",
            "authors": [
              {
                "authorId": "2116643144",
                "name": "Yan Xiao"
              },
              {
                "authorId": "2157835870",
                "name": "Yaochu Jin"
              },
              {
                "authorId": "39336044",
                "name": "Ran Cheng"
              },
              {
                "authorId": "39414900",
                "name": "K. Hao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 269804131,
          "isinfluential": false,
          "contexts": [
            "In the proposed PF-BRE problem, to simulate the heterogeneous data characteristics of different clients in real-world scenarios, we have curated three distinct biomedical document-level RE datasets from [18], each representing a separate client, forming the PF-BRE dataset."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Federated Document-Level Biomedical Relation Extraction with Localized Context Contrast",
            "abstract": "",
            "year": 2024,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2116643144",
                "name": "Yan Xiao"
              },
              {
                "authorId": "2284823417",
                "name": "Yaochu Jin"
              },
              {
                "authorId": "2248434410",
                "name": "Kuangrong Hao"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "In our experiments, we chose Huggingface’s transformers [21], initialized with pretrained cased SciBERT [22] as our base model."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "278206890": {
      "citing_paper_info": {
        "title": "Axial Attention-Infused U-Net for Document-level Relation Extraction",
        "abstract": "Relation extraction (RE) is vital in natural language processing (NLP) for Analyzing the relationships among entities within unstructured text, supporting applications like knowledge graphs and question-answering systems. Existing methods often utilize graph neural networks (GNNs) and pretrained language models such as BERT, but they struggle with long-range dependencies and class imbalance in sparse relationships. In this paper, we introduce AxU-Doc, an innovative model that leverages axial attention within a U-shaped architecture to effectively acquire comprehensive information and promote logical inference between entities. Additionally, we adopt the HingeABLoss function, replacing the conventional cross-entropy loss, focusing on tackling class imbalance and enhance model efficiency. Our experiments on two publicly available datasets shows that AxU-Doc achieves competitive results, confirming its effectiveness in document-level RE tasks.",
        "year": 2024,
        "venue": "2024 3rd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)",
        "authors": [
          {
            "authorId": "2298785730",
            "name": "Hua Yang"
          },
          {
            "authorId": "2298572912",
            "name": "Jie Xiao"
          },
          {
            "authorId": "2358989063",
            "name": "Hao Shen"
          },
          {
            "authorId": "2298736498",
            "name": "Qi Wang"
          },
          {
            "authorId": "2224957053",
            "name": "Shenyang Sheng"
          },
          {
            "authorId": "2298576203",
            "name": "Chengwu Peng"
          },
          {
            "authorId": "2358292146",
            "name": "Sha Li"
          },
          {
            "authorId": "2358402448",
            "name": "Lihao Yu"
          },
          {
            "authorId": "2335838720",
            "name": "Zhaoqi Meng"
          },
          {
            "authorId": "2358412207",
            "name": "Yue Huang"
          },
          {
            "authorId": "2298053198",
            "name": "Rou Fu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 14,
        "unique_cited_count": 13,
        "influential_count": 1,
        "detailed_records_count": 14
      },
      "cited_papers": [
        "202558505",
        "253107167",
        "264412875",
        "3719281",
        "227230416",
        "266176277",
        "218613850",
        "225039888",
        "209323787",
        "88817",
        "261792194",
        "221996144",
        "254877738"
      ],
      "citation_details": [
        {
          "citedcorpusid": 88817,
          "isinfluential": false,
          "contexts": [
            "The performance of our approach was evaluated using experiments conducted on two publicly accessible datasets: ReDocRED[18] and CDR[19]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction",
            "abstract": "Community-run, formal evaluations and manually annotated text corpora are critically important for advancing biomedical text-mining research. Recently in BioCreative V, a new challenge was organized for the tasks of disease named entity recognition (DNER) and chemical-induced disease (CID) relation extraction. Given the nature of both tasks, a test collection is required to contain both disease/chemical annotations and relation annotations in the same set of articles. Despite previous efforts in biomedical corpus construction, none was found to be sufficient for the task. Thus, we developed our own corpus called BC5CDR during the challenge by inviting a team of Medical Subject Headings (MeSH) indexers for disease/chemical entity annotation and Comparative Toxicogenomics Database (CTD) curators for CID relation annotation. To ensure high annotation quality and productivity, detailed annotation guidelines and automatic annotation tools were provided. The resulting BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH as the controlled vocabulary. To ensure accuracy, the entities were first captured independently by two annotators followed by a consensus annotation: The average inter-annotator agreement (IAA) scores were 87.49% and 96.05% for the disease and chemicals, respectively, in the test set according to the Jaccard similarity coefficient. Our corpus was successfully used for the BioCreative V challenge tasks and should serve as a valuable resource for the text-mining research community. Database URL: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr/",
            "year": 2016,
            "venue": "Database J. Biol. Databases Curation",
            "authors": [
              {
                "authorId": "2117970351",
                "name": "Jiao Li"
              },
              {
                "authorId": "2116969756",
                "name": "Yueping Sun"
              },
              {
                "authorId": "2000280",
                "name": "Robin J. Johnson"
              },
              {
                "authorId": "2160494",
                "name": "D. Sciaky"
              },
              {
                "authorId": "3252035",
                "name": "Chih-Hsuan Wei"
              },
              {
                "authorId": "2277706",
                "name": "Robert Leaman"
              },
              {
                "authorId": "108109364",
                "name": "A. P. Davis"
              },
              {
                "authorId": "3283642",
                "name": "C. Mattingly"
              },
              {
                "authorId": "1922749",
                "name": "Thomas C. Wiegers"
              },
              {
                "authorId": "144202084",
                "name": "Zhiyong Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 3719281,
          "isinfluential": false,
          "contexts": [
            "By combining the strengths of the U-Net architecture [15] with axial attention mechanisms [16], the module effectively captures dependencies at multiple scales within the relation matrix."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
            "year": 2015,
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "authors": [
              {
                "authorId": "1737326",
                "name": "O. Ronneberger"
              },
              {
                "authorId": "152702479",
                "name": "P. Fischer"
              },
              {
                "authorId": "1710872",
                "name": "T. Brox"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202558505,
          "isinfluential": false,
          "contexts": [
            "We use BERT base and RoBERTa large from Hugging Face's Transformers library as the encoder for ReDocRED dataset, and SciBERT base [20] for CDR dataset."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              },
              {
                "authorId": "46258841",
                "name": "Kyle Lo"
              },
              {
                "authorId": "2527954",
                "name": "Arman Cohan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209323787,
          "isinfluential": false,
          "contexts": [
            "By combining the strengths of the U-Net architecture [15] with axial attention mechanisms [16], the module effectively captures dependencies at multiple scales within the relation matrix."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Axial Attention in Multidimensional Transformers",
            "abstract": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2126278",
                "name": "Jonathan Ho"
              },
              {
                "authorId": "2583391",
                "name": "Nal Kalchbrenner"
              },
              {
                "authorId": "3319373",
                "name": "Dirk Weissenborn"
              },
              {
                "authorId": "2887364",
                "name": "Tim Salimans"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218613850,
          "isinfluential": false,
          "contexts": [
            "Our comparison models include BRAN [23], DHG [10], LSR [21], SciBERTbase, SSAN-SciBERTbase, ATLOP-SciBERTbase, and DocuNet-SciBERTbase."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2056582888",
                "name": "Guoshun Nan"
              },
              {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
              },
              {
                "authorId": "3305422",
                "name": "Ivan Sekulic"
              },
              {
                "authorId": "2153424287",
                "name": "Wei Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221996144,
          "isinfluential": false,
          "contexts": [
            "[12] introduced the GAIN model, which integrates entity-level and mention-level graphs, leveraging attention mechanisms and reasoning path information to predict relationships between entities, further enhancing performance.",
            "We carried out a case study in accordance with GAIN[12], using the same examples to confirm our model's effectiveness.",
            "The graph-based models include GAIN[12] and SSAN[22], while the transformer-based models include DocuNet[14], KD-DocRE[6], JEREX[24], ATLOP[5] and FM-RKD[25]."
          ],
          "intents": [
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entity-level graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at this https URL .",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039888,
          "isinfluential": true,
          "contexts": [
            "[5] presented the model, which employs adaptive thresholding technique and local context pooling to effectively handle multi-entity classification issues.",
            "The graph-based models include GAIN[12] and SSAN[22], while the transformer-based models include DocuNet[14], KD-DocRE[6], JEREX[24], ATLOP[5] and FM-RKD[25].",
            "We constructed an entity-level relationship matrix to represent the correlations between entities inspired by the context-based strategy[5,14,9].",
            "For a selected document , we make use of the entity marker technique adapted from [5] to tag the positions before and after entity mentions in D with \"*\" and \"/*\" to identify entity locations."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling",
            "abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4, and also significantly outperforms existing models on both CDR and GDA. We have released our code at https://github.com/wzhouad/ATLOP.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2203076",
                "name": "Wenxuan Zhou"
              },
              {
                "authorId": "152530947",
                "name": "Kevin Huang"
              },
              {
                "authorId": "1901958",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "30768523",
                "name": "Jing Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 227230416,
          "isinfluential": false,
          "contexts": [
            "In graph-based research, [10] employed a two-layer heterogeneous graph based on GNNs for document modeling and multi-hop reasoning.",
            "Our comparison models include BRAN [23], DHG [10], LSR [21], SciBERTbase, SSAN-SciBERTbase, ATLOP-SciBERTbase, and DocuNet-SciBERTbase."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Relation Extraction with Dual-tier Heterogeneous Graph",
            "abstract": "Document-level relation extraction (RE) poses new challenges over its sentence-level counterpart since it requires an adequate comprehension of the whole document and the multi-hop reasoning ability across multiple sentences to reach the final result. In this paper, we propose a novel graph-based model with Dual-tier Heterogeneous Graph (DHG) for document-level RE. In particular, DHG is composed of a structure modeling layer followed by a relation reasoning layer. The major advantage is that it is capable of not only capturing both the sequential and structural information of documents but also mixing them together to benefit for multi-hop reasoning and final decision-making. Furthermore, we employ Graph Neural Networks (GNNs) based message propagation strategy to accumulate information on DHG. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on two widely used datasets, and further analyses suggest that all the modules in our model are indispensable for document-level RE.",
            "year": 2020,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "122542861",
                "name": "Zhenyu Zhang"
              },
              {
                "authorId": "48613402",
                "name": "Yu Bowen"
              },
              {
                "authorId": "2269366",
                "name": "Xiaobo Shu"
              },
              {
                "authorId": "2079682",
                "name": "Tingwen Liu"
              },
              {
                "authorId": "1598319620",
                "name": "Hengzhu Tang"
              },
              {
                "authorId": "9309853",
                "name": "Wang Yubin"
              },
              {
                "authorId": "48358041",
                "name": "Li Guo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253107167,
          "isinfluential": false,
          "contexts": [
            "The performance of our approach was evaluated using experiments conducted on two publicly accessible datasets: ReDocRED[18] and CDR[19]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Revisiting DocRED - Addressing the False Negative Problem in Relation Extraction",
            "abstract": "The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "118358816",
                "name": "Qingyu Tan"
              },
              {
                "authorId": "153152460",
                "name": "Lu Xu"
              },
              {
                "authorId": "1996394",
                "name": "Lidong Bing"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              },
              {
                "authorId": "30537701",
                "name": "Sharifah Mahani Aljunied"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254877738,
          "isinfluential": false,
          "contexts": [
            "Additionally, [7] analyzed co-occurrence relationships to tackle long-tail and multi-label problems in document-level RE."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Relation Extraction with Relation Correlations",
            "abstract": "Document-level relation extraction faces two overlooked challenges: long-tail problem and multi-label problem. Previous work focuses mainly on obtaining better contextual representations for entity pairs, hardly address the above challenges. In this paper, we analyze the co-occurrence correlation of relations, and introduce it into DocRE task for the first time. We argue that the correlations can not only transfer knowledge between data-rich relations and data-scarce ones to assist in the training of tailed relations, but also reflect semantic distance guiding the classifier to identify semantically close relations for multi-label entity pairs. Specifically, we use relation embedding as a medium, and propose two co-occurrence prediction sub-tasks from both coarse- and fine-grained perspectives to capture relation correlations. Finally, the learned correlation-aware embeddings are used to guide the extraction of relational facts. Substantial experiments on two popular DocRE datasets are conducted, and our method achieves superior results compared to baselines. Insightful analysis also demonstrates the potential of relation correlations to address the above challenges.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2093920100",
                "name": "Ridong Han"
              },
              {
                "authorId": "2068928731",
                "name": "T. Peng"
              },
              {
                "authorId": "2894465",
                "name": "Benyou Wang"
              },
              {
                "authorId": "2118467569",
                "name": "Lu Liu"
              },
              {
                "authorId": "2101317304",
                "name": "Xiang Wan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261792194,
          "isinfluential": false,
          "contexts": [
            "This matrix construction is analogous to pixel-level feature matrices in images[13]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Research on Detection of Rice Pests and Diseases Based on Improved yolov5 Algorithm",
            "abstract": "Rice pests and diseases have a significant impact on the quality and yield of rice, and even have a certain impact on and cause a loss in the national agricultural industry and economy. The timely and accurate detection of pests and diseases is the basic premise of formulating effective rice pest control and prevention programs. However, the complexity and diversity of pests and diseases and the high similarity between some pests and diseases make the detection and classification task of pests and diseases extremely difficult without detection tools. The existing target detection algorithms can barely complete the task of detecting pests and diseases, but the detection effect is not ideal. In the actual situation of rice disease and insect pest detection, the detection algorithm is required to have fast speed, high accuracy, and good performance for small target detection, and so this paper improved the popular yolov5 algorithm to achieve an ideal detection performance suitable for rice disease and insect pest detection. This paper briefly introduces the current status and influence of rice pests and diseases and several target detection algorithms based on deep learning. Based on the yolov5 algorithm, the RepVGG network structure is introduced, 3*3 convolution is combined with ReLU, a training time model with multi-branch topology is adopted, and the inference time is reduced through layer merging. To improve algorithm detection speed, the SK attention mechanism is introduced to improve the receptive field of the convolution kernel to obtain more information and improve accuracy. In addition, Adaptive NMS is replaced by Adaptive NMS, the dynamic suppression strategy is adopted, and scores for learning density are set, which greatly improves the problems of missing detection and the false detection of small targets. Finally, the improved algorithm model is combined with membrane calculation to further improve the accuracy and speed of the algorithm. According to the experimental results, the accuracy of the improved algorithm is increased by about 2.7 percentage points, and the mAP is increased by 4.3 percentage points, up to 94.4%. The speed is improved by about 2.8 percentage points, and indicators such as recall rate and AP are improved.",
            "year": 2023,
            "venue": "Applied Sciences",
            "authors": [
              {
                "authorId": "2209972973",
                "name": "Hua Yang"
              },
              {
                "authorId": "2240102519",
                "name": "Dang Lin"
              },
              {
                "authorId": "1664724271",
                "name": "Gexiang Zhang"
              },
              {
                "authorId": "2225206448",
                "name": "Haifeng Zhang"
              },
              {
                "authorId": "2225077982",
                "name": "Junxiong Wang"
              },
              {
                "authorId": "2222604017",
                "name": "Shuxiang Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 264412875,
          "isinfluential": false,
          "contexts": [
            "The graph-based models include GAIN[12] and SSAN[22], while the transformer-based models include DocuNet[14], KD-DocRE[6], JEREX[24], ATLOP[5] and FM-RKD[25]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level denoising relation extraction with false-negative mining and reinforced positive-class knowledge distillation",
            "abstract": "",
            "year": 2024,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "2257013376",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2257345620",
                "name": "Jianling Zhu"
              },
              {
                "authorId": "2261249422",
                "name": "Hongting Chen"
              },
              {
                "authorId": "2118794406",
                "name": "Jianhua Dai"
              },
              {
                "authorId": "2228707882",
                "name": "Lincheng Jiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266176277,
          "isinfluential": false,
          "contexts": [
            "We adopted the Adaptive Hinge Balance Loss (HingeAB Loss) [17] as our primary loss function to handle the class imbalance between positive and negative samples."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Adaptive Hinge Balance Loss for Document-Level Relation Extraction",
            "abstract": ",",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2220025503",
                "name": "Jize Wang"
              },
              {
                "authorId": "2273671981",
                "name": "Xinyi Le"
              },
              {
                "authorId": "2274126482",
                "name": "Xiaodi Peng"
              },
              {
                "authorId": "2243318570",
                "name": "Cailian Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "We constructed an entity-level relationship matrix to represent the correlations between entities inspired by the context-based strategy[5,14,9]."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "274270987": {
      "citing_paper_info": {
        "title": "CDM-IE: comprehensive dependence mining for document-level information extraction",
        "abstract": "Information extraction is a fundamental aspect of natural language understanding tasks. Previous works generally utilized a transformer-based architecture as a text encoder. However, gradient vanishing and attention dispersion tend to be inevitable when tacking document-level paragraph texts, which have negative effects on capturing global token relationships. To address these limitations, we propose CDM-IE, an information extraction approach specifically designed for lengthy text input. The CDM-IE is a hybrid CNN-Transformer architecture with dual-branch topology, respectively named a paragraph encoding branch and a dependence mining branch, which excels at learning comprehensive text representation by integrating both global context and local dependence. The two pathways converge at a dependence-guided attention module, which acts as a fusion bridge to feature alignment and synergy. Ablative experiment results on the Medical Entity Extraction (CCKS 2019) and Chinese Machine Reading Comprehension (CMRC 2018) datasets indicate that the proposed CDM-IE showcases improved performance and robustness on information extraction tasks, which provide a valuable solution for text modeling on long sequences.",
        "year": 2024,
        "venue": "Other Conferences",
        "authors": [
          {
            "authorId": "2332345621",
            "name": "Chao Wang"
          },
          {
            "authorId": "2275639810",
            "name": "Qian Chang"
          },
          {
            "authorId": "2275770362",
            "name": "Longgang Zhao"
          },
          {
            "authorId": "2332322618",
            "name": "Qianlan Zhou"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 3,
        "unique_cited_count": 3,
        "influential_count": 0,
        "detailed_records_count": 3
      },
      "cited_papers": [
        "52967399",
        "247619149",
        "3879949"
      ],
      "citation_details": [
        {
          "citedcorpusid": 3879949,
          "isinfluential": false,
          "contexts": [
            "A natural thought is to leverage convolutional neural networks (CNNs) [6] to enhance salient feature extraction in local areas."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Recent advances in convolutional neural networks",
            "abstract": "",
            "year": 2015,
            "venue": "Pattern Recognition",
            "authors": [
              {
                "authorId": "2174964",
                "name": "Jiuxiang Gu"
              },
              {
                "authorId": "2108329894",
                "name": "Zhenhua Wang"
              },
              {
                "authorId": "1859486",
                "name": "Jason Kuen"
              },
              {
                "authorId": "2139431",
                "name": "Lianyang Ma"
              },
              {
                "authorId": "3000984",
                "name": "Amir Shahroudy"
              },
              {
                "authorId": "2521776",
                "name": "Bing Shuai"
              },
              {
                "authorId": "2115433449",
                "name": "Ting Liu"
              },
              {
                "authorId": "2144802333",
                "name": "Xingxing Wang"
              },
              {
                "authorId": "2117909575",
                "name": "Gang Wang"
              },
              {
                "authorId": "1688642",
                "name": "Jianfei Cai"
              },
              {
                "authorId": "40894914",
                "name": "Tsuhan Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Current approaches [1] typically fine-tune Transformer-based [2-3] text embeddings and apply them to downstream tasks, which have demonstrated remarkable language modeling capabilities."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247619149,
          "isinfluential": false,
          "contexts": [
            "Current approaches [1] typically fine-tune Transformer-based [2-3] text embeddings and apply them to downstream tasks, which have demonstrated remarkable language modeling capabilities."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Unified Structure Generation for Universal Information Extraction",
            "abstract": "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1831434",
                "name": "Yaojie Lu"
              },
              {
                "authorId": "2154976399",
                "name": "Qing Liu"
              },
              {
                "authorId": "40495683",
                "name": "Dai Dai"
              },
              {
                "authorId": "2107521158",
                "name": "Xinyan Xiao"
              },
              {
                "authorId": "2116455765",
                "name": "Hongyu Lin"
              },
              {
                "authorId": "2118233348",
                "name": "Xianpei Han"
              },
              {
                "authorId": "2110832778",
                "name": "Le Sun"
              },
              {
                "authorId": "2149181702",
                "name": "Hua Wu"
              }
            ]
          }
        }
      ]
    },
    "273901511": {
      "citing_paper_info": {
        "title": "LogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations",
        "abstract": "Document-level relation extraction (DocRE) aims to identify relationships between entities within a document. Due to the vast number of entity pairs, fully annotating all fact triplets is challenging, resulting in datasets with numerous false negative samples. Recently, self-training-based methods have been introduced to address this issue. However, these methods are purely black-box and sub-symbolic, making them difficult to interpret and prone to overlooking symbolic interdependencies between relations.To remedy this deficiency, our insight is that symbolic knowledge, such as logical rules, can be used as diagnostic tools to identify conflicts between pseudo-labels. By resolving these conflicts through logical diagnoses, we can correct erroneous pseudo-labels, thus enhancing the training of neural models.To achieve this, we propose **LogicST**, a neural-logic self-training framework that iteratively resolves conflicts and constructs the minimal diagnostic set for updating models. Extensive experiments demonstrate that LogicST significantly improves performance and outperforms previous state-of-the-art methods. For instance, LogicST achieves an increase of **7.94%** in F1 score compared to CAST (Tan et al., 2023a) on the DocRED benchmark (Yao et al., 2019). Additionally, LogicST is more time-efficient than its self-training counterparts, requiring only **10%** of the training time of CAST.",
        "year": 2024,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2187456277",
            "name": "Shengda Fan"
          },
          {
            "authorId": "2329871943",
            "name": "Yanting Wang"
          },
          {
            "authorId": "3177863",
            "name": "Shasha Mo"
          },
          {
            "authorId": "2293626051",
            "name": "Jianwei Niu"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 18,
        "unique_cited_count": 18,
        "influential_count": 3,
        "detailed_records_count": 18
      },
      "cited_papers": [
        "259187621",
        "233296843",
        "2381120",
        "271923564",
        "174800839",
        "255372865",
        "20744",
        "263830659",
        "247291740",
        "189898081",
        "263861232",
        "225039888",
        "252918868",
        "259261857",
        "220525799",
        "5509259",
        "10910955",
        "52967399"
      ],
      "citation_details": [
        {
          "citedcorpusid": 20744,
          "isinfluential": false,
          "contexts": [
            "Thirdly, LogicST assumes that entities and their mentions are identified beforehand (Li and Ji, 2014), which falls short of real-world applications."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Incremental Joint Extraction of Entity Mentions and Relations",
            "abstract": "We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search. A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations. Experiments on Automatic Content Extraction (ACE) 1 corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2381120,
          "isinfluential": false,
          "contexts": [
            "Based on Occam’s razor (Domingos, 1999), we only consider the minimal diagnosis , where no subset of the minimal diagnostic set can resolve the conflict."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Role of Occam's Razor in Knowledge Discovery",
            "abstract": "",
            "year": 1999,
            "venue": "Data mining and knowledge discovery",
            "authors": [
              {
                "authorId": "1740213",
                "name": "Pedro M. Domingos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5509259,
          "isinfluential": false,
          "contexts": [
            "However, DocRE tasks often suffer from severe class imbalance (Tan et al., 2022a), causing logits to be biased towards popular classes (Menon et al., 2021) and introducing confirmation bias (Cho and Roy, 2004)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Impact of search engines on page popularity",
            "abstract": "",
            "year": 2004,
            "venue": "The Web Conference",
            "authors": [
              {
                "authorId": "4658767",
                "name": "Junghoo Cho"
              },
              {
                "authorId": "1697207",
                "name": "Sourashis Roy"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10910955,
          "isinfluential": false,
          "contexts": [
            "First, the scoring function of LogicST is designed for settings with incomplete annotations and is not applicable to distant supervision settings (Mintz et al., 2009; Liu et al., 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distant supervision for relation extraction without labeled data",
            "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.",
            "year": 2009,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "36181176",
                "name": "Mike D. Mintz"
              },
              {
                "authorId": "87299088",
                "name": "Steven Bills"
              },
              {
                "authorId": "144621026",
                "name": "R. Snow"
              },
              {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": true,
          "contexts": [
            "Since the advent of pre-trained language models (Devlin et al., 2019; Liu et al., 2019), research in DocRE has experienced significant growth.",
            "They benefit from the powerful representations provided by language models (Devlin et al., 2019), but struggle with symbolic reasoning among entity pairs.",
            "We compare LogicST with LogiRE (Ru et al., 2021), MILR (Fan et al., 2022), and JMRL (Qi et al., 2024) using BERT-base as the encoder.",
            "We use BERT-base (Devlin et al., 2019) and RoBERTa-large (Liu et al., 2019) as the text encoders."
          ],
          "intents": [
            "['background']",
            "['background']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174800839,
          "isinfluential": false,
          "contexts": [
            "We compare LogicST to the following six types of baselines: 1) vanilla baselines, including various top-performing models under fully supervised settings, such as GAIN (Zeng et al., 2020), AT-LOP (Zhou et al., 2021), and KD-DocRE (Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021), and CAST (Tan et al., 2023b); 5) methods based on large language models (LLMs), including LLaMA2-7B (Touvron et al., 2023), GPT-3.",
            "…PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021), and CAST (Tan et al., 2023b); 5) methods based on large language models (LLMs), including LLaMA2-7B…"
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Better Modeling of Incomplete Annotations for Named Entity Recognition",
            "abstract": "Supervised approaches to named entity recognition (NER) are largely developed based on the assumption that the training data is fully annotated with named entity information. However, in practice, annotated data can often be imperfect with one typical issue being the training data may contain incomplete annotations. We highlight several pitfalls associated with learning under such a setup in the context of NER and identify limitations associated with existing approaches, proposing a novel yet easy-to-implement approach for recognizing named entities with incomplete data annotations. We demonstrate the effectiveness of our approach through extensive experiments.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2845078",
                "name": "Zhanming Jie"
              },
              {
                "authorId": "35930962",
                "name": "Pengjun Xie"
              },
              {
                "authorId": "143844110",
                "name": "Wei Lu"
              },
              {
                "authorId": "2058085406",
                "name": "Ruixue Ding"
              },
              {
                "authorId": "2111818678",
                "name": "Linlin Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": false,
          "contexts": [
            "Although semi-automatic strategies, such as the recommend-revise annotation method (Yao et al., 2019), can alleviate annotators’ workload, they still fail to provide gold-quality datasets."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220525799,
          "isinfluential": false,
          "contexts": [
            "However, DocRE tasks often suffer from severe class imbalance (Tan et al., 2022a), causing logits to be biased towards popular classes (Menon et al., 2021) and introducing confirmation bias (Cho and Roy, 2004)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Long-tail learning via logit adjustment",
            "abstract": "Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes naive learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance.",
            "year": 2020,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2844480",
                "name": "A. Menon"
              },
              {
                "authorId": "3078751",
                "name": "Sadeep Jayasumana"
              },
              {
                "authorId": "2241094",
                "name": "A. Rawat"
              },
              {
                "authorId": "2059143344",
                "name": "Himanshu Jain"
              },
              {
                "authorId": "2799898",
                "name": "Andreas Veit"
              },
              {
                "authorId": "152663162",
                "name": "Sanjiv Kumar"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039888,
          "isinfluential": true,
          "contexts": [
            "We compare LogicST to the following six types of baselines: 1) vanilla baselines, including various top-performing models under fully supervised settings, such as GAIN (Zeng et al., 2020), AT-LOP (Zhou et al., 2021), and KD-DocRE (Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021), and CAST (Tan et al., 2023b); 5) methods based on large language models (LLMs), including LLaMA2-7B (Touvron et al., 2023), GPT-3.",
            "Consistent with prior work (Tan et al., 2023a; Wang et al., 2024), we adopt the ATLOP model (Zhou et al., 2021) as our backbone.",
            "Substantial progress has been made through the development of complex neural networks (Zhou et al., 2021; Jiang et al., 2022; Tan et al., 2022a), the integration of evidence sentences (Huang et al., 2021; Xie et al., 2022), and the exploration of loss functions (Zhou and Lee, 2022).",
            "…baselines: 1) vanilla baselines, including various top-performing models under fully supervised settings, such as GAIN (Zeng et al., 2020), AT-LOP (Zhou et al., 2021), and KD-DocRE (Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU…",
            "Following prior work (Tan et al., 2023a), we adopt ATLOP (Zhou et al., 2021) as the backbone and NCRL (Zhou and Lee, 2022) as the loss function."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling",
            "abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4, and also significantly outperforms existing models on both CDR and GDA. We have released our code at https://github.com/wzhouad/ATLOP.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2203076",
                "name": "Wenxuan Zhou"
              },
              {
                "authorId": "152530947",
                "name": "Kevin Huang"
              },
              {
                "authorId": "1901958",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "30768523",
                "name": "Jing Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233296843,
          "isinfluential": false,
          "contexts": [
            "Unlike sentence-level relation extraction, which focuses on individual entity pairs (Stoica et al., 2021), DocRE is challenged by the vast number of potential entity pairs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Re-TACRED: Addressing Shortcomings of the TACRED Dataset",
            "abstract": "TACRED is one of the largest and most widely used sentence-level relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-the-art performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its findings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After verification, we observed that 23.9% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3% and helps uncover significant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.",
            "year": 2021,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2173016",
                "name": "George Stoica"
              },
              {
                "authorId": "144888672",
                "name": "Emmanouil Antonios Platanios"
              },
              {
                "authorId": "2064568169",
                "name": "Barnab'as P'oczos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247291740,
          "isinfluential": false,
          "contexts": [
            "Substantial progress has been made through the development of complex neural networks (Zhou et al., 2021; Jiang et al., 2022; Tan et al., 2022a), the integration of evidence sentences (Huang et al., 2021; Xie et al., 2022), and the exploration of loss functions (Zhou and Lee, 2022)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion",
            "abstract": "Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "1892794261",
                "name": "Yiqing Xie"
              },
              {
                "authorId": "3363642",
                "name": "Jiaming Shen"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "3375249",
                "name": "Yuning Mao"
              },
              {
                "authorId": "2111759643",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252918868,
          "isinfluential": false,
          "contexts": [
            "We compare LogicST to the following six types of baselines: 1) vanilla baselines, including various top-performing models under fully supervised settings, such as GAIN (Zeng et al., 2020), AT-LOP (Zhou et al., 2021), and KD-DocRE (Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021), and CAST (Tan et al., 2023b); 5) methods based on large language models (LLMs), including LLaMA2-7B (Touvron et al., 2023), GPT-3.",
            "There has been extensive research aimed at alleviating the impact of false negative samples (Li et al., 2021; Wang et al., 2022a, 2024).",
            "…et al., 2021), and KD-DocRE (Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021),…"
          ],
          "intents": [
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling",
            "abstract": "Document-level relation extraction (RE) aims to identify relations between entities across multiple sentences. Most previous methods focused on document-level RE under full supervision. However, in real-world scenario, it is expensive and difficult to completely label all relations in a document because the number of entity pairs in document-level RE grows quadratically with the number of entities. To solve the common incomplete labeling problem, we propose a unified positive-unlabeled learning framework - shift and squared ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled (PU) learning on document-level RE for the first time. Considering that labeled data of a dataset may lead to prior shift of unlabeled data, we introduce a PU learning under prior shift of training data. Also, using none-class score as an adaptive threshold, we propose squared ranking loss and prove its Bayesian consistency with multi-label ranking metrics. Extensive experiments demonstrate that our method achieves an improvement of about 14 F1 points relative to the previous baseline with incomplete labeling. In addition, it outperforms previous state-of-the-art results under both fully supervised and extremely unlabeled settings as well.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2185022832",
                "name": "Ye Wang"
              },
              {
                "authorId": "2108998419",
                "name": "Xin-Xin Liu"
              },
              {
                "authorId": "2110232234",
                "name": "Wen-zhong Hu"
              },
              {
                "authorId": "103245682",
                "name": "T. Zhang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 255372865,
          "isinfluential": false,
          "contexts": [
            "5 (Ope-nAI, 2022), and GPT-4o (OpenAI, 2024), as well as techniques utilizing in-context learning (ICL) for task-specific adaptation (Dong et al., 2022), natural language inference (NLI) models for fuzzy matching (Li et al., 2023a), and data programming for label denoising (Gao et al., 2023); and…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Survey on In-context Learning",
            "abstract": "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2047143813",
                "name": "Qingxiu Dong"
              },
              {
                "authorId": "49192881",
                "name": "Lei Li"
              },
              {
                "authorId": "10780897",
                "name": "Damai Dai"
              },
              {
                "authorId": "2113919886",
                "name": "Ce Zheng"
              },
              {
                "authorId": "150358371",
                "name": "Zhiyong Wu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "2116530295",
                "name": "Xu Sun"
              },
              {
                "authorId": "47883405",
                "name": "Jingjing Xu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "3335836",
                "name": "Zhifang Sui"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259187621,
          "isinfluential": true,
          "contexts": [
            "We compare LogicST to the following six types of baselines: 1) vanilla baselines, including various top-performing models under fully supervised settings, such as GAIN (Zeng et al., 2020), AT-LOP (Zhou et al., 2021), and KD-DocRE (Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021), and CAST (Tan et al., 2023b); 5) methods based on large language models (LLMs), including LLaMA2-7B (Touvron et al., 2023), GPT-3.",
            "Moreover, the performance gains from CAST to LogicST increase as the distance grows.",
            "Consistent with prior work (Tan et al., 2023a; Wang et al., 2024), we adopt the ATLOP model (Zhou et al., 2021) as our backbone.",
            "Additionally, LogicST requires only 10 % of the training time of CAST (Tan et al., 2023a).",
            "Previous works have attempted to mitigate this issue by sampling the pseudo-labels based on class frequencies (Wei et al., 2021) or scores calculated on the development set (Tan et al., 2023a)."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data",
            "abstract": "Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which valid relations are falsely annotated as 'no_relation'. Models trained with such data inevitably make similar mistakes during the inference stage. Self-training has been proven effective in alleviating the false negative problem. However, traditional self-training is vulnerable to confirmation bias and exhibits poor performance in minority classes. To overcome this limitation, we proposed a novel class-adaptive re-sampling self-training framework. Specifically, we re-sampled the pseudo-labels for each class by precision and recall scores. Our re-sampling strategy favored the pseudo-labels of classes with high precision and low recall, which improved the overall recall without significantly compromising precision. We conducted experiments on document-level and biomedical relation extraction datasets, and the results showed that our proposed self-training framework consistently outperforms existing competitive methods on the Re-DocRED and ChemDisgene datasets when the training data are incompletely annotated. Our code is released at https://github.com/DAMO-NLP-SG/CAST.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "118358816",
                "name": "Qingyu Tan"
              },
              {
                "authorId": "153152460",
                "name": "Lu Xu"
              },
              {
                "authorId": "1996394",
                "name": "Lidong Bing"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259261857,
          "isinfluential": false,
          "contexts": [
            "Consistent with prior work (Tan et al., 2023a; Wang et al., 2024), we adopt the ATLOP model (Zhou et al., 2021) as our backbone.",
            "…(Tan et al., 2022a); 2) negative sampling methods (Li et al., 2021); 3) PU learning-based methods, including SSR-PU (Wang et al., 2022a) and P 3 M (Wang et al., 2024); 4) sub-symbolic self-training methods, such as VST (Jie et al., 2019), CREST (Wei et al., 2021), and CAST (Tan et al., 2023b); 5)…"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling",
            "abstract": "The goal of document-level relation extraction (RE) is to identify relations between entities that span multiple sentences. Recently, incomplete labeling in document-level RE has received increasing attention, and some studies have used methods such as positive-unlabeled learning to tackle this issue, but there is still a lot of room for improvement. Motivated by this, we propose a positive-augmentation and positive-mixup positive-unlabeled metric learning framework (P3M). Specifically, we formulate document-level RE as a metric learning problem. We aim to pull the distance closer between entity pair embedding and their corresponding relation embedding, while pushing it farther away from the none-class relation embedding. Additionally, we adapt the positive-unlabeled learning to this loss objective. In order to improve the generalizability of the model, we use dropout to augment positive samples and propose a positive-none-class mixup method. Extensive experiments show that P3M improves the F1 score by approximately 4-10 points in document-level RE with incomplete labeling, and achieves state-of-the-art results in fully labeled scenarios. Furthermore, P3M has also demonstrated robustness to prior estimation bias in incomplete labeled scenarios.",
            "year": 2023,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2220239793",
                "name": "Ye Wang"
              },
              {
                "authorId": "2220703235",
                "name": "Huazheng Pan"
              },
              {
                "authorId": "103245682",
                "name": "T. Zhang"
              },
              {
                "authorId": "116354414",
                "name": "Wen Wu"
              },
              {
                "authorId": "2110232234",
                "name": "Wen-zhong Hu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 263830659,
          "isinfluential": false,
          "contexts": [
            "This may be due to LLMs’ difficulty in handling complex reasoning, and domain-specific nuances (Pang et al., 2023 Results on DWIE."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Guideline Learning for In-context Information Extraction",
            "abstract": "Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2257003180",
                "name": "Chaoxu Pang"
              },
              {
                "authorId": "10034341",
                "name": "Yixuan Cao"
              },
              {
                "authorId": "2257004196",
                "name": "Qiang Ding"
              },
              {
                "authorId": "2256989703",
                "name": "Ping Luo"
              }
            ]
          }
        },
        {
          "citedcorpusid": 263861232,
          "isinfluential": false,
          "contexts": [
            "LogicST is implemented within a teacher-student framework (Tarvainen and Valpola, 2017), where the teacher model is first pre-trained to establish a robust initial state."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "abstract": "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "9942966",
                "name": "Antti Tarvainen"
              },
              {
                "authorId": "2132516",
                "name": "Harri Valpola"
              }
            ]
          }
        },
        {
          "citedcorpusid": 271923564,
          "isinfluential": false,
          "contexts": [
            "5 (Ope-nAI, 2022), and GPT-4o (OpenAI, 2024), as well as techniques utilizing in-context learning (ICL) for task-specific adaptation (Dong et al., 2022), natural language inference (NLI) models for fuzzy matching (Li et al., 2023a), and data programming for label denoising (Gao et al., 2023); and 6) logical frameworks designed for supervised DocRE, including LogiRE (Ru et al., 2021), MILR (Fan et al., 2022), and JMRL (Qi et al., 2024).",
            "…al., 2022), natural language inference (NLI) models for fuzzy matching (Li et al., 2023a), and data programming for label denoising (Gao et al., 2023); and 6) logical frameworks designed for supervised DocRE, including LogiRE (Ru et al., 2021), MILR (Fan et al., 2022), and JMRL (Qi et al., 2024).",
            "We compare LogicST with LogiRE (Ru et al., 2021), MILR (Fan et al., 2022), and JMRL (Qi et al., 2024) using BERT-base as the encoder."
          ],
          "intents": [
            "--",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction",
            "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have shown that logical rules can explicitly help capture such interdependencies. These methods either learn logical rules to refine the output of a trained DocRE model, or first learn logical rules from annotated data and then inject the learnt rules into a DocRE model using an auxiliary training objective. However, these learning pipelines may suffer from the issue of error propagation. To mitigate this issue, we propose Joint Modeling Relation extraction and Logical rules or JMRL for short, a novel rule-based framework that jointly learns both a DocRE model and logical rules in an end-to-end fashion. Specifically, we parameterize a rule reasoning module in JMRL to simulate the inference of logical rules, thereby explicitly modeling the reasoning process. We also introduce an auxiliary loss and a residual connection mechanism in JMRL to better reconcile the DocRE model and the rule reasoning module. Experimental results on four benchmark datasets demonstrate that our proposed JMRL framework is consistently superior to existing rule-based frameworks, improving five baseline models for DocRE by a significant margin.",
            "year": 2024,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3119800",
                "name": "Kunxun Qi"
              },
              {
                "authorId": "2266025295",
                "name": "Jianfeng Du"
              },
              {
                "authorId": "2285269859",
                "name": "Hai Wan"
              }
            ]
          }
        }
      ]
    },
    "270703045": {
      "citing_paper_info": {
        "title": "Harvesting Events from Multiple Sources: Towards a Cross-Document Event Extraction Paradigm",
        "abstract": "Document-level event extraction aims to extract structured event information from unstructured text. However, a single document often contains limited event information and the roles of different event arguments may be biased due to the influence of the information source. This paper addresses the limitations of traditional document-level event extraction by proposing the task of cross-document event extraction (CDEE) to integrate event information from multiple documents and provide a comprehensive perspective on events. We construct a novel cross-document event extraction dataset, namely CLES, which contains 20,059 documents and 37,688 mention-level events, where over 70% of them are cross-document. To build a benchmark, we propose a CDEE pipeline that includes 5 steps, namely event extraction, coreference resolution, entity normalization, role normalization and entity-role resolution. Our CDEE pipeline achieves about 72% F1 in end-to-end cross-document event extraction, suggesting the challenge of this task. Our work builds a new line of information extraction research and will attract new research attention.",
        "year": 2024,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2300810930",
            "name": "Qiang Gao"
          },
          {
            "authorId": "2300530472",
            "name": "Zixiang Meng"
          },
          {
            "authorId": "2132446579",
            "name": "Bobo Li"
          },
          {
            "authorId": "2230224962",
            "name": "Jun Zhou"
          },
          {
            "authorId": "2266836320",
            "name": "Fei Li"
          },
          {
            "authorId": "1679617",
            "name": "Chong Teng"
          },
          {
            "authorId": "2237222852",
            "name": "Donghong Ji"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 13,
        "unique_cited_count": 11,
        "influential_count": 0,
        "detailed_records_count": 13
      },
      "cited_papers": [
        "243865600",
        "235253912",
        "258967833",
        "220045465",
        "60709701",
        "216562330",
        "236460259",
        "52816033",
        "233210350",
        "202537206",
        "266163830"
      ],
      "citation_details": [
        {
          "citedcorpusid": 52816033,
          "isinfluential": false,
          "contexts": [
            "Sentence-level event extraction has been extensively researched (Liu et al., 2018; Wadden et al., 2019; Hamborg et al., 2019; Wang et al., 2023b; Xu et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
            "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "49544272",
                "name": "Xiao Liu"
              },
              {
                "authorId": "2730687",
                "name": "Zhunchen Luo"
              },
              {
                "authorId": "4590286",
                "name": "Heyan Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 60709701,
          "isinfluential": false,
          "contexts": [
            "(2) Document-level Encoding: Transformer encoder and RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) tree are used to make entities aware of document-level context."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "RHETORICAL STRUCTURE THEORY: A THEORY OF TEXT ORGANIZATION",
            "abstract": "Abstract : Rhetorical Structure Theory is a descriptive theory of a major aspect of the organization of natural text. It is a linguistically useful method for describing natural texts, characterizing their structure primarily in terms of relations that hold between parts of the text. This paper establishes a new definitional foundation for RST. Definitions are made more systematic and explicit, they introduce a new functional element, and incidentally reflect more experience in text analysis. Along with the definitions, the paper examines three claims and findings of RST: the predominance of nucleus/satellite structural patterns, the functional basis of hierarchy, and the communicative role of text structure. (Author) Keywords: Artificial intelligence; Coherence; Computational linguistics; Discourse; Grammar; Knowledge delivery; Natural language processing; Pragmatics.",
            "year": 1987,
            "venue": "",
            "authors": [
              {
                "authorId": "144847871",
                "name": "W. Mann"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202537206,
          "isinfluential": false,
          "contexts": [
            "Sentence-level event extraction has been extensively researched (Liu et al., 2018; Wadden et al., 2019; Hamborg et al., 2019; Wang et al., 2023b; Xu et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Giveme5W1H: A Universal System for Extracting Main Events from News Articles",
            "abstract": "Event extraction from news articles is a commonly required prerequisite for various tasks, such as article summarization, article clustering, and news aggregation. Due to the lack of universally applicable and publicly available methods tailored to news datasets, many researchers redundantly implement event extraction methods for their own projects. The journalistic 5W1H questions are capable of describing the main event of an article, i.e., by answering who did what, when, where, why, and how. We provide an in-depth description of an improved version of Giveme5W1H, a system that uses syntactic and domain-specific rules to automatically extract the relevant phrases from English news articles to provide answers to these 5W1H questions. Given the answers to these questions, the system determines an article's main event. In an expert evaluation with three assessors and 120 articles, we determined an overall precision of p=0.73, and p=0.82 for answering the first four W questions, which alone can sufficiently summarize the main event reported on in a news article. We recently made our system publicly available, and it remains the only universal open-source 5W1H extractor capable of being applied to a wide range of use cases in news analysis.",
            "year": 2019,
            "venue": "INRA@RecSys",
            "authors": [
              {
                "authorId": "3461253",
                "name": "Felix Hamborg"
              },
              {
                "authorId": "2587724",
                "name": "Corinna Breitinger"
              },
              {
                "authorId": "145151838",
                "name": "Bela Gipp"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216562330,
          "isinfluential": false,
          "contexts": [
            "Du and Cardie (2020) proposed a QA approach for event extraction to avoid the dependency of event extraction results on the previous entity recognition step."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Event Extraction by Answering (Almost) Natural Questions",
            "abstract": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting).",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "1748501",
                "name": "Claire Cardie"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220045465,
          "isinfluential": false,
          "contexts": [
            "Although not many, there have been some studies on cross-document information extraction, such as event coreference resolution (Wu et al., 2020; Held et al., 2021; Eirew et al., 2022) and relation extraction (Yao et al., 2021; Lu et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CorefQA: Coreference Resolution as Query-based Span Prediction",
            "abstract": "In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "145717888",
                "name": "Wei Wu"
              },
              {
                "authorId": "47939052",
                "name": "Fei Wang"
              },
              {
                "authorId": "2061016896",
                "name": "Arianna Yuan"
              },
              {
                "authorId": "144894849",
                "name": "Fei Wu"
              },
              {
                "authorId": "49298465",
                "name": "Jiwei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233210350,
          "isinfluential": false,
          "contexts": [
            "We borrowed the approach from Eirew et al. (2021) and optimized its data collection system to gather data from Wikipedia dump files."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia",
            "abstract": "Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of state-of-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "51150806",
                "name": "Alon Eirew"
              },
              {
                "authorId": "1962331387",
                "name": "Arie Cattan"
              },
              {
                "authorId": "7465342",
                "name": "Ido Dagan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235253912,
          "isinfluential": false,
          "contexts": [
            "Although significant advancements have been made (Xu et al., 2021; Yang et al., 2021a Figure 1: An example of cross-document event extraction, where a comprehensive event is obtained from three event mentions in three documents. et al., 2023a), DEE often encounters limitations in terms of the scope…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
            "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "1500520681",
                "name": "Tianyu Liu"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460259,
          "isinfluential": false,
          "contexts": [
            "Although significant advancements have been made (Xu et al., 2021; Yang et al., 2021a Figure 1: An example of cross-document event extraction, where a comprehensive event is obtained from three event mentions in three documents. et al., 2023a), DEE often encounters limitations in terms of the scope…"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-level Event Extraction via Parallel Prediction Networks",
            "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1845787839",
                "name": "Hang Yang"
              },
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "1799672",
                "name": "Taifeng Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865600,
          "isinfluential": false,
          "contexts": [
            "Although not many, there have been some studies on cross-document information extraction, such as event coreference resolution (Wu et al., 2020; Held et al., 2021; Eirew et al., 2022) and relation extraction (Yao et al., 2021; Lu et al., 2023)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild",
            "abstract": "Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in knowledge bases can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges: Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1390925224",
                "name": "Yuan Yao"
              },
              {
                "authorId": "153140528",
                "name": "Jiaju Du"
              },
              {
                "authorId": "2149202150",
                "name": "Yankai Lin"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "2141313179",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2108485135",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258967833,
          "isinfluential": false,
          "contexts": [
            "…(Xu et al., 2021; Yang et al., 2021a Figure 1: An example of cross-document event extraction, where a comprehensive event is obtained from three event mentions in three documents. et al., 2023a), DEE often encounters limitations in terms of the scope and depth of information that it can provide."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization",
            "abstract": "Document-level multi-event extraction aims to extract the structural information from a given document automatically. Most recent approaches usually involve two steps: (1) modeling entity interactions; (2) decoding entity interactions into events. However, such approaches ignore a global view of inter-dependency of multiple events. Moreover, an event is decoded by iteratively merging its related entities as arguments, which might suffer from error propagation and is computationally inefficient. In this paper, we propose an alternative approach for document-level multi-event extraction with event proxy nodes and Hausdorff distance minimization. The event proxy nodes, representing pseudo-events, are able to build connections with other event proxy nodes, essentially capturing global information. The Hausdorff distance makes it possible to compare the similarity between the set of predicted events and the set of ground-truth events. By directly minimizing Hausdorff distance, the model is trained towards the global optimum directly, which improves performance and reduces training time. Experimental results show that our model outperforms previous state-of-the-art method in F1-score on two datasets with only a fraction of training time.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2144706657",
                "name": "Xinyu Wang"
              },
              {
                "authorId": "145096580",
                "name": "Lin Gui"
              },
              {
                "authorId": "1390509967",
                "name": "Yulan He"
              }
            ]
          }
        },
        {
          "citedcorpusid": 266163830,
          "isinfluential": false,
          "contexts": [
            "Mi-culicich and Henderson (2022) addressed coreference resolution using a graph-based approach, while Chen et al. (2023) introduced discourse information to model documents, resulting in a significant performance improvement."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Cross-Document Event Coreference Resolution on Discourse Structure",
            "abstract": "Cross-document event coreference resolution (CD-ECR) is a task of clustering event mentions across multiple documents that refer to the same real-world events. Previous studies usually model the CD-ECR task as a pairwise similarity comparison problem by using different event mention features, and consider the highly similar event mention pairs in the same cluster as coreferent. In general, most of them only consider the local context of event mentions and ignore their implicit global information, thus failing to capture the interactions of long-distance event mentions. To address the above issue, we regard discourse structure as global information to further improve CD-ECR. First, we use a discourse rhetorical structure constructor to construct tree structures to represent documents. Then, we obtain shortest dependency paths from the tree structures to represent interactions between event mention pairs. Finally, we feed the above information to a multi-layer perceptron to capture the similarities of event mention pairs for resolving coreferent events. Experimental results on the ECB+ dataset show that our proposed model outperforms several baselines and achieves the competitive performance with the start-of-the-art baselines.",
            "year": 2023,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2273481716",
                "name": "Xinyu Chen"
              },
              {
                "authorId": "145846496",
                "name": "Sheng Xu"
              },
              {
                "authorId": "47470867",
                "name": "Peifeng Li"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "Baseline We chose Yu et al. (2022) as the base-line, which determines coreference by enhancing event mention representations with event argument information.",
            "In terms of coreference resolution, Yu et al. (2022) proposed a cross-document coreference resolution model that enhances event mention representation by extracting event arguments."
          ],
          "intents": [
            "--",
            "--"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "With respect to other directions, Caciularu et al. (2021) proposed a novel cross-document pre-training language model to learn rich contextual information across documents."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "257378350": {
      "citing_paper_info": {
        "title": "Document-level Relation Extraction with Cross-sentence Reasoning Graph",
        "abstract": "Relation extraction (RE) has recently moved from the sentence-level to document-level, which requires aggregating document information and using entities and mentions for reasoning. Existing works put entity nodes and mention nodes with similar representations in a document-level graph, whose complex edges may incur redundant information. Furthermore, existing studies only focus on entity-level reasoning paths without considering global interactions among entities cross-sentence. To these ends, we propose a novel document-level RE model with a GRaph information Aggregation and Cross-sentence Reasoning network (GRACR). Specifically, a simplified document-level graph is constructed to model the semantic information of all mentions and sentences in a document, and an entity-level graph is designed to explore relations of long-distance cross-sentence entity pairs. Experimental results show that GRACR achieves excellent performance on two public datasets of document-level RE. It is especially effective in extracting potential relations of cross-sentence entity pairs. Our code is available at https://github.com/UESTC-LHF/GRACR.",
        "year": 2023,
        "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
        "authors": [
          {
            "authorId": "2116272404",
            "name": "Hongfei Liu"
          },
          {
            "authorId": "152200830",
            "name": "Zhao Kang"
          },
          {
            "authorId": "50081620",
            "name": "Lizong Zhang"
          },
          {
            "authorId": "2106775523",
            "name": "Ling Tian"
          },
          {
            "authorId": "2210859111",
            "name": "Fujun Hua"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 10,
        "unique_cited_count": 10,
        "influential_count": 2,
        "detailed_records_count": 10
      },
      "cited_papers": [
        "227231216",
        "236477583",
        "221996144",
        "202541610",
        "214714027",
        "244001886",
        "215768766",
        "6263378",
        "218613850",
        "225039888"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6263378,
          "isinfluential": false,
          "contexts": [
            "Finally, in classiﬁcation module, we merge the context information of relation representations obtained by self-attention [15] to make ﬁnal relation prediction.",
            "Furthermore, following [17], we employ self-attention [15] to capture context relation representations, which can help us exploit the topic information of the document: where W ∈ R d r × d r is a trainable parameter matrix, d r is the dimension of target relation representations. o i is the…"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Context-Aware Representations for Knowledge Base Relation Extraction",
            "abstract": "We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the context representations with an attention mechanism to make the final prediction. We use the Wikidata knowledge base to construct a dataset of multiple relations per sentence and to evaluate our approach. Compared to a baseline system, our method results in an average error reduction of 24 on a held-out set of relations. The code and the dataset to replicate the experiments are made available at https://github.com/ukplab/.",
            "year": 2017,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "24694433",
                "name": "Daniil Sorokin"
              },
              {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202541610,
          "isinfluential": false,
          "contexts": [
            "[1] proposed an edge-oriented model that constructs a document-level graph with different types of nodes and edges to obtain a global representation for relation classiﬁcation.",
            "Following this, [1] constructed a document-level graph with heterogeneous nodes and proposed an edge-oriented model to obtain a global representation."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs",
            "abstract": "Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48810605",
                "name": "Fenia Christopoulou"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 214714027,
          "isinfluential": false,
          "contexts": [
            "[16] applied a hierarchical inference method to aggregate the inference information of different granularity."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "HIN: Hierarchical Inference Network for Document-Level Relation Extraction",
            "abstract": "Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the large-scale DocRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance.",
            "year": 2020,
            "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "1598319620",
                "name": "Hengzhu Tang"
              },
              {
                "authorId": "9310727",
                "name": "Yanan Cao"
              },
              {
                "authorId": "122542861",
                "name": "Zhenyu Zhang"
              },
              {
                "authorId": "2115871859",
                "name": "Jiangxia Cao"
              },
              {
                "authorId": "36595248",
                "name": "Fang Fang"
              },
              {
                "authorId": "2108668664",
                "name": "Shi Wang"
              },
              {
                "authorId": "2055975557",
                "name": "Pengfei Yin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215768766,
          "isinfluential": false,
          "contexts": [
            "[22] captured the coreferential relations in context by a pre-training task."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Coreferential Reasoning Learning for Language Representation",
            "abstract": "Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, existing language representation models seldom consider coreference explicitly, the relationship between noun phrases referring to the same entity, which is essential to a coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model designed to capture the relations between noun phrases that co-refer to each other. According to the experimental results, compared with existing baseline models, the CorefBERT model has made significant progress on several downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "153140528",
                "name": "Jiaju Du"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218613850,
          "isinfluential": true,
          "contexts": [
            "Model F1 intra-F1 inter-F1 LSR [12] 64.",
            "[12] defined the document-level graph as a latent variable to improve the performance of RE models by optimizing the structure of the document-level graph.",
            "Model F 1 intra-F 1 inter-F 1 LSR [12] 64.8 68.9 53.1 DHG [24] 65.9 70.1 54.6 HGNN [14] 64.4 69.2 51.2 MRN [9] 65.9 70.4 54.2 GRACR 68.8 73.9 55.8 Model intra-F 1 inter-F 1 CNN [21] 51.87 37.58 LSTM [21] 56.57 41.47 BiLSTM [21] 57.05 43.39 Context-aware [21] 56.74 42.26 GEDA [7] 61.85 49.46 LSR [12] 65.26 52.05 GRACR 65.88 52.49",
            "[12] defined the document-level graph as a latent variable and induced it based on structured attention to improve the performance of document-level RE models by optimizing the structure of document-level graph."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2056582888",
                "name": "Guoshun Nan"
              },
              {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
              },
              {
                "authorId": "3305422",
                "name": "Ivan Sekulic"
              },
              {
                "authorId": "2153424287",
                "name": "Wei Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221996144,
          "isinfluential": false,
          "contexts": [
            "[23] proposed a double graph-based graph aggregation and inference network (GAIN)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entity-level graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at this https URL .",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039888,
          "isinfluential": false,
          "contexts": [
            "[26] proposed adaptive thresholding and localized context pooling to solve the multi-label and multi-entity problems."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling",
            "abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4, and also significantly outperforms existing models on both CDR and GDA. We have released our code at https://github.com/wzhouad/ATLOP.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2203076",
                "name": "Wenxuan Zhou"
              },
              {
                "authorId": "152530947",
                "name": "Kevin Huang"
              },
              {
                "authorId": "1901958",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "30768523",
                "name": "Jing Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 227231216,
          "isinfluential": false,
          "contexts": [
            "[25] introduced context of entity pairs as edges between entity nodes to model semantic interactions among multiple entities."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Global Context-enhanced Graph Convolutional Networks for Document-level Relation Extraction",
            "abstract": "Document-level Relation Extraction (RE) is particularly challenging due to complex semantic interactions among multiple entities in a document. Among exiting approaches, Graph Convolutional Networks (GCN) is one of the most effective approaches for document-level RE. However, traditional GCN simply takes word nodes and adjacency matrix to represent graphs, which is difficult to establish direct connections between distant entity pairs. In this paper, we propose Global Context-enhanced Graph Convolutional Networks (GCGCN), a novel model which is composed of entities as nodes and context of entity pairs as edges between nodes to capture rich global context information of entities in a document. Two hierarchical blocks, Context-aware Attention Guided Graph Convolution (CAGGC) for partially connected graphs and Multi-head Attention Guided Graph Convolution (MAGGC) for fully connected graphs, could take progressively more global context into account. Meantime, we leverage a large-scale distantly supervised dataset to pre-train a GCGCN model with curriculum learning, which is then fine-tuned on the human-annotated dataset for further improving document-level RE performance. The experimental results on DocRED show that our model could effectively capture rich global context information in the document, leading to a state-of-the-art result. Our code is available at https://github.com/Huiweizhou/GCGCN.",
            "year": 2020,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "2347900",
                "name": "Huiwei Zhou"
              },
              {
                "authorId": "2110166657",
                "name": "Yibin Xu"
              },
              {
                "authorId": "24368213",
                "name": "Weihong Yao"
              },
              {
                "authorId": "47781621",
                "name": "Zhe Liu"
              },
              {
                "authorId": "51066941",
                "name": "Chengkun Lang"
              },
              {
                "authorId": "2152633026",
                "name": "Haibin Jiang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236477583,
          "isinfluential": true,
          "contexts": [
            "Table 3 depicts the comparisons with state-of-the-art models on CDR. Compared to MRN [9], the performance of our model approximately improves about 2.9% for F 1 , and 3.9% for intra-F 1 and 1.6% for inter-F 1 .",
            "[9] proposed a mention-based reasoning network to capture local and global contextual information.",
            "Model F 1 intra-F 1 inter-F 1 LSR [12] 64.8 68.9 53.1 DHG [24] 65.9 70.1 54.6 HGNN [14] 64.4 69.2 51.2 MRN [9] 65.9 70.4 54.2 GRACR 68.8 73.9 55.8 Model intra-F 1 inter-F 1 CNN [21] 51.87 37.58 LSTM [21] 56.57 41.47 BiLSTM [21] 57.05 43.39 Context-aware [21] 56.74 42.26 GEDA [7] 61.85 49.46 LSR…"
          ],
          "intents": [
            "['background']",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction aims to detect the relations within one document, which is challenging since it requires complex reasoning using mentions, entities, local and global contexts. Few previous studies have distinguished local and global reasoning explicitly, which may be problematic because they play different roles in intra-and inter-sentence relations. Moreover, the interactions between local and global contexts should be considered since they could help relation reasoning based on our observation. In this paper, we pro-pose a novel mention-based reasoning (MRN) module based on explicitly and collaboratively local and global reasoning. Based on MRN, we design a co-predictor module to predict entity relations based on local and global entity and relation representations jointly. We evaluate our MRN model on three widely-used benchmark datasets, namely DocRED, CDR, and GDA. Experimental results show that our model outperforms previous state-of-the-art models by a large margin.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2000217741",
                "name": "Jingye Li"
              },
              {
                "authorId": "2113474255",
                "name": "Kang Xu"
              },
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              },
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "3350168",
                "name": "Yafeng Ren"
              },
              {
                "authorId": "1719916",
                "name": "D. Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244001886,
          "isinfluential": false,
          "contexts": [
            "For example, [2] utilized GNN to aggregate the neighborhood information of text graph nodes for text classification."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Graph Fusion Network for Text Classification",
            "abstract": "",
            "year": 2021,
            "venue": "Knowledge-Based Systems",
            "authors": [
              {
                "authorId": "2116918591",
                "name": "Yong Dai"
              },
              {
                "authorId": "24962156",
                "name": "Linjun Shou"
              },
              {
                "authorId": "50175330",
                "name": "Ming Gong"
              },
              {
                "authorId": "2077455484",
                "name": "Xiaoling Xia"
              },
              {
                "authorId": "152200830",
                "name": "Zhao Kang"
              },
              {
                "authorId": "1683510",
                "name": "Zenglin Xu"
              },
              {
                "authorId": "71790825",
                "name": "Daxin Jiang"
              }
            ]
          }
        }
      ]
    },
    "265150105": {
      "citing_paper_info": {
        "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
        "abstract": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for document-level relation extraction due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating a large language model (LLM) and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension.",
        "year": 2023,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2266419394",
            "name": "Junpeng Li"
          },
          {
            "authorId": "1453587987",
            "name": "Zixia Jia"
          },
          {
            "authorId": "2266032392",
            "name": "Zilong Zheng"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 10,
        "influential_count": 0,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "247694170",
        "257766307",
        "248227853",
        "52967399",
        "235358168",
        "247961196",
        "257019958",
        "254564105",
        "60246043",
        "228954221"
      ],
      "citation_details": [
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "† means BERT-base (Devlin et al., 2018) and ‡ means RoBERTa-large (Liu et al., 2019). at once by one prompt rather than enumerate all entity pairs to ask for relations one by one (which is too costly and time-consuming to execute for plenty of entities on document-level RE)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 60246043,
          "isinfluential": false,
          "contexts": [
            "To align GPT-generated relations and predefined relation types, we first combine Natural Language Inference (NLI) models (MacCartney, 2009) with GPT to solve zero-shot DocRE."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Natural language inference.",
            "abstract": "The paper describes the way in which a Preference Semantics system for natural language analysis and generation tackles a difficult class of anaphoric inference problems (finding the correct referent for an English pronoun in context): those requiring either analysis (conceptual) knowledge of a complex sort, or requiring weak inductive knowledge of the course of events in the real world. The method employed converts all available knowledge to a canonical template form and endeavors to create chains of non-deductive inferences from the unknowns to the possible referents. Its method of selecting among possible chains of inferences is consistent with the overall principle of \"semantic preference\" used to set up the original meaning representation, of which these anaphoric inference procedures are a manipulation.",
            "year": 1973,
            "venue": "",
            "authors": [
              {
                "authorId": "2971978",
                "name": "Y. Wilks"
              }
            ]
          }
        },
        {
          "citedcorpusid": 228954221,
          "isinfluential": false,
          "contexts": [
            "Recent studies have utilized GPT (Floridi and Chiriatti, 2020; Chan, 2023) for various structural prediction tasks, such as named entity prediction and relation extraction (Dunn et al., 2022; Gutiér-rez et al., 2022; Wang et al., 2023; Liu et al., 2023; Xu et al., 2023), as well as text…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GPT-3: Its Nature, Scope, Limits, and Consequences",
            "abstract": "In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.",
            "year": 2020,
            "venue": "Minds and Machines",
            "authors": [
              {
                "authorId": "1982425",
                "name": "L. Floridi"
              },
              {
                "authorId": "2037605322",
                "name": "Massimo Chiriatti"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235358168,
          "isinfluential": false,
          "contexts": [
            "Document-level Relation Extraction (DocRE) is a task that focuses on extracting fine-grained relations between entity pairs within a lengthy context (Yao et al., 2019; Nan et al., 2020; Wang et al., 2020; Zhou et al., 2021; Zhang et al., 2021; Ma et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-level Relation Extraction as Semantic Segmentation",
            "abstract": "Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.",
            "year": 2021,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "153773882",
                "name": "Xiang Chen"
              },
              {
                "authorId": "2110972563",
                "name": "Xin Xie"
              },
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2111727840",
                "name": "Chuanqi Tan"
              },
              {
                "authorId": "2108266952",
                "name": "Mosha Chen"
              },
              {
                "authorId": "2087380523",
                "name": "Fei Huang"
              },
              {
                "authorId": "2059080424",
                "name": "Luo Si"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247694170,
          "isinfluential": false,
          "contexts": [
            "Therefore, we employ a NLI model, which has demonstrated effectiveness in assessing factual consistency (Honovich et al., 2022)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TRUE: Re-evaluating Factual Consistency Evaluation",
            "abstract": "",
            "year": 2022,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1754700648",
                "name": "Or Honovich"
              },
              {
                "authorId": "2335771",
                "name": "Roee Aharoni"
              },
              {
                "authorId": "47426264",
                "name": "Jonathan Herzig"
              },
              {
                "authorId": "51258885",
                "name": "Hagai Taitelbaum"
              },
              {
                "authorId": "2162044901",
                "name": "Doron Kukliansy"
              },
              {
                "authorId": "120550181",
                "name": "Vered Cohen"
              },
              {
                "authorId": "2073456043",
                "name": "Thomas Scialom"
              },
              {
                "authorId": "1711977",
                "name": "Idan Szpektor"
              },
              {
                "authorId": "1809983",
                "name": "A. Hassidim"
              },
              {
                "authorId": "1745572",
                "name": "Yossi Matias"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247961196,
          "isinfluential": false,
          "contexts": [
            "Recent studies have utilized GPT (Floridi and Chiriatti, 2020; Chan, 2023) for various structural prediction tasks, such as named entity prediction and relation extraction (Dunn et al., 2022; Gutiér-rez et al., 2022; Wang et al., 2023; Liu et al., 2023; Xu et al., 2023), as well as text…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry",
            "abstract": "This paper examines the ethical solutions raised in response to OpenAI’s language model Generative Pre-trained Transformer-3 (GPT-3) a year and a half from its release. I argue that hype and fear about GPT-3, even within the Natural Language Processing (NLP) industry and AI ethics, have often been underpinned by technologically deterministic perspectives. These perspectives emphasise the autonomy of the language model rather than the autonomy of human actors in AI systems. I highlight the existence of deterministic perspectives in the current AI discourse (which range from technological utopianism to dystopianism), with a specific focus on the two issues of: (1) GPT-3’s potential intentional misuse for manipulation and (2) unintentional harm caused by bias. In response, I find that a contextual approach to GPT-3, which is centred upon wider ecologies of societal harm and benefit, human autonomy, and human values, illuminates practical solutions to concerns about manipulation and bias. Additionally, although OpenAI’s newest 2022 language model InstructGPT represents a small step in reducing toxic language and aligning GPT-3 with user intent, it does not provide any compelling solutions to manipulation or bias. Therefore, I argue that solutions to address these issues must focus on organisational settings as a precondition for ethical decision-making in AI, and high-quality curated datasets as a precondition for less harmful language model outputs.",
            "year": 2022,
            "venue": "AI and Ethics",
            "authors": [
              {
                "authorId": "2189758920",
                "name": "Anastasia Chan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248227853,
          "isinfluential": false,
          "contexts": [
            "Huang et al. (2022) undertook manual annotation from scratch, employing two expert an-notators to annotate 96 documents.",
            "Notably, two representative works, Huang et al. (2022) and Tan et al. (2022b), have contributed to this re-annotation process."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED",
            "abstract": "DocRED is a widely used dataset for document-level relation extraction. In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload. Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations. However, when comparing DocRED with a subset relabeled from scratch, we find that this scheme results in a considerable amount of false negative samples and an obvious bias towards popular entities and relations. Furthermore, we observe that the models trained on DocRED have low recall on our relabeled dataset and inherit the same bias in the training data. Through the analysis of annotators’ behaviors, we figure out the underlying reason for the problems above: the scheme actually discourages annotators from supplementing adequate instances in the revision phase. We appeal to future research to take into consideration the issues with the recommend-revise scheme when designing new models and annotation schemes. The relabeled dataset is released at https://github.com/AndrewZhe/Revisit-DocRED, to serve as a more reliable test set of document RE models.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2007771781",
                "name": "Quzhe Huang"
              },
              {
                "authorId": "2128965713",
                "name": "Shibo Hao"
              },
              {
                "authorId": "2106717300",
                "name": "Yuan Ye"
              },
              {
                "authorId": "2110041566",
                "name": "Shengqi Zhu"
              },
              {
                "authorId": "2115387922",
                "name": "Yansong Feng"
              },
              {
                "authorId": "144060462",
                "name": "Dongyan Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 254564105,
          "isinfluential": false,
          "contexts": [
            "…GPT (Floridi and Chiriatti, 2020; Chan, 2023) for various structural prediction tasks, such as named entity prediction and relation extraction (Dunn et al., 2022; Gutiér-rez et al., 2022; Wang et al., 2023; Liu et al., 2023; Xu et al., 2023), as well as text classification labeling (Gilardi…"
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "abstract": "Intelligently extracting and linking complex scientific information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientific text. The approach leverages a pre-trained large language model (LLM), GPT-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of JSON objects. We demonstrate that LLMs trained in this way are capable of accurately extracting useful records of complex scientific knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-flexible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.",
            "year": 2022,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "144979831",
                "name": "Alex Dunn"
              },
              {
                "authorId": "23138704",
                "name": "John Dagdelen"
              },
              {
                "authorId": "2190503824",
                "name": "N. Walker"
              },
              {
                "authorId": "2144572916",
                "name": "Sanghoon Lee"
              },
              {
                "authorId": "80083779",
                "name": "Andrew S. Rosen"
              },
              {
                "authorId": "2807088",
                "name": "G. Ceder"
              },
              {
                "authorId": "2152444800",
                "name": "Kristin A. Persson"
              },
              {
                "authorId": "2541031",
                "name": "Anubhav Jain"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257019958,
          "isinfluential": false,
          "contexts": [
            "We test the SOTA document-level RE model (Ma et al., 2023) In addition, we conducted experiments using two other DocRE models, ATLOP (Zhou et al., 2021) and KD-DocRE (Tan et al., 2022a), by leveraging their officially provided code.",
            "Document-level Relation Extraction (DocRE) is a task that focuses on extracting fine-grained relations between entity pairs within a lengthy context (Yao et al., 2019; Nan et al., 2020; Wang et al., 2020; Zhou et al., 2021; Zhang et al., 2021; Ma et al., 2023)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction (DocRE) is the task of identifying all relations between each entity pair in a document. Evidence, defined as sentences containing clues for the relationship between an entity pair, has been shown to help DocRE systems focus on relevant texts, thus improving relation extraction. However, evidence retrieval (ER) in DocRE faces two major issues: high memory consumption and limited availability of annotations. This work aims at addressing these issues to improve the usage of ER in DocRE. First, we propose DREEAM, a memory-efficient approach that adopts evidence information as the supervisory signal, thereby guiding the attention modules of the DocRE system to assign high weights to evidence. Second, we propose a self-training strategy for DREEAM to learn ER from automatically-generated evidence on massive data without evidence annotations. Experimental results reveal that our approach exhibits state-of-the-art performance on the DocRED benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the first approach to employ ER self-training.",
            "year": 2023,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1453595477",
                "name": "Youmi Ma"
              },
              {
                "authorId": "2116561203",
                "name": "An Wang"
              },
              {
                "authorId": "1764004",
                "name": "Naoaki Okazaki"
              }
            ]
          }
        },
        {
          "citedcorpusid": 257766307,
          "isinfluential": false,
          "contexts": [
            "…2020; Chan, 2023) for various structural prediction tasks, such as named entity prediction and relation extraction (Dunn et al., 2022; Gutiér-rez et al., 2022; Wang et al., 2023; Liu et al., 2023; Xu et al., 2023), as well as text classification labeling (Gilardi et al., 2023; Törnberg, 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ChatGPT outperforms crowd workers for text-annotation tasks",
            "abstract": "Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.",
            "year": 2023,
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "authors": [
              {
                "authorId": "46210729",
                "name": "F. Gilardi"
              },
              {
                "authorId": "2253532",
                "name": "Meysam Alizadeh"
              },
              {
                "authorId": "69039257",
                "name": "M. Kubli"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "…2020; Chan, 2023) for various structural prediction tasks, such as named entity prediction and relation extraction (Dunn et al., 2022; Gutiér-rez et al., 2022; Wang et al., 2023; Liu et al., 2023; Xu et al., 2023), as well as text classification labeling (Gilardi et al., 2023; Törnberg, 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "243865596": {
      "citing_paper_info": {
        "title": "Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification",
        "abstract": "Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level, which easily leads to conflicts between different mentions of the same event. To this end, we study the problem of document-level event factuality identification, which determines the event factuality from the view of a document. For this task, we need to consider two important characteristics: Local Uncertainty and Global Structure, which can be utilized to improve performance. In this paper, we propose an Uncertain Local-to-Global Network (ULGN) to make use of these two characteristics. Specifically, we devise a Local Uncertainty Estimation module to model the uncertainty of local information. Moreover, we propose an Uncertain Information Aggregation module to leverage the global structure for integrating the local information. Experimental results demonstrate the effectiveness of our proposed method, outperforming the previous state-of-the-art model by 8.4% and 11.45% of F1 score on two widely used datasets.",
        "year": 2021,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2142758019",
            "name": "Pengfei Cao"
          },
          {
            "authorId": "152829071",
            "name": "Yubo Chen"
          },
          {
            "authorId": "2145435513",
            "name": "Yuqing Yang"
          },
          {
            "authorId": "77397868",
            "name": "Kang Liu"
          },
          {
            "authorId": "11447228",
            "name": "Jun Zhao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 21,
        "unique_cited_count": 21,
        "influential_count": 3,
        "detailed_records_count": 21
      },
      "cited_papers": [
        "196215305",
        "13756489",
        "215737171",
        "51605619",
        "160705",
        "195833740",
        "1915014",
        "2239324",
        "9885298",
        "236460024",
        "216078090",
        "2100831",
        "14124213",
        "804903",
        "174800759",
        "203620105",
        "8992998",
        "5730838",
        "12873739",
        "207228784",
        "13468104"
      ],
      "citation_details": [
        {
          "citedcorpusid": 160705,
          "isinfluential": false,
          "contexts": [
            "However, such a point estimate is not sufﬁcient to express uncertainty, as point-based methods assume that learned features are always correct (Gal and Ghahramani, 2016; Kendall and Gal, 2017)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
            "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",
            "year": 2015,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "2681954",
                "name": "Y. Gal"
              },
              {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
              }
            ]
          }
        },
        {
          "citedcorpusid": 804903,
          "isinfluential": false,
          "contexts": [
            "The early work on this problem has mainly employed rule-based methods (Nairn et al., 2006; Saurı, 2008; Lotan et al., 2013) or machine learning methods (with manually designed features) (Diab et al.",
            "Generally, event factuality can be classified into five categories (Saurı, 2008): Certain Positive (certainly happening, denoted as CT+), Certain Negative (certainly not happening, CT-), Possible Positive (possibly happening, PS+), Possible Negative (possibly not happening, PS-) and Underspecified (events’ factuality cannot be identified, Uu)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A factuality profiler for eventualities in text",
            "abstract": "Event factuality is the level of information expressing the factual status of eventualities mentioned in text. That is, it conveys whether eventualities are characterized as corresponding to facts, to possibilities, or to situations that do not hold in the world. As such, it touches on two categories more standardly assumed in the literature: modality and evidentiality. They both have been widely discussed in linguistics and philosophy, but it is not until recently that have started to receive some attention within the area of NLP. \nFactuality is a necessary component for reasoning about eventualities in discourse. Inferences derived from events that have not happened, or that are possible, are different from those derived from events judged as factual. It is also essential for any task involving temporal ordering. The creation of event timelines needs to be aware of the different status of eventualities presented as uncertain or counterfactual. \nMy dissertation aims at designing and developing a factuality profiler, namely a tool devoted to the identification of the factuality degree associated to eventualities mentioned in discourse. \nEvent factuality cannot be conceived independently from language users, who are understood here as the sources of factuality information. Their inclusion in the model is fundamental. Two sources can assign different factuality values to the same event. Because of that, the factuality profiler must be capable of representing different and possibly contradictory information about the factuality nature of any event. \nDe Facto, the tool I am presenting here, is grounded on the linguistic strategies we speakers employ to signal degrees of factuality in discourse. These involve information at different levels: lexical, syntactic, and rhetoric. De Facto implements an algorithm based on the grammatical structuring of factuality in languages like English, and is informed with a set of linguistic resources compiled from a data-driven approach. \nFor evaluating De Facto, I created FactBank, a corpus annotated with factuality information. The interannotation agreement score for the task of assigning factuality values to events is kcohen = 0.81. Running De Facto against the gold standard results in F1=0.74 (macro-averaging), F1=0.85 (micro-averaging) and, in terms of interannotation agreement, kcohen =0.72.",
            "year": 2008,
            "venue": "",
            "authors": [
              {
                "authorId": "1707726",
                "name": "J. Pustejovsky"
              },
              {
                "authorId": "144327698",
                "name": "R. Saurí"
              }
            ]
          }
        },
        {
          "citedcorpusid": 1915014,
          "isinfluential": false,
          "contexts": [
            ", 2014) and long short-term memory networks (Hochreiter and Schmidhuber, 1997) can also be employed as encoders."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Long Short-Term Memory",
            "abstract": "",
            "year": 1997,
            "venue": "Neural Computation",
            "authors": [
              {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
              },
              {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2100831,
          "isinfluential": false,
          "contexts": [
            "…event is PS+. EFI is an important task in natural language processing (NLP) area, which is beneﬁcial for a wide range of NLP applications, such as rumor detection (Qazvinian et al., 2011), sentiment analysis (Klenner and Clematide, 2016) and machine reading comprehension (Richardson et al., 2013)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
            "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone’s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today’s computers and algorithms.",
            "year": 2013,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "144422314",
                "name": "Matthew Richardson"
              },
              {
                "authorId": "2676309",
                "name": "C. Burges"
              },
              {
                "authorId": "1859813",
                "name": "Erin Renshaw"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2239324,
          "isinfluential": false,
          "contexts": [
            "In recent years, neural networks have been introduced into the EFI task, and achieved state-of-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Sheng et al., 2019; Huang et al., 2019; Veyseh et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Are You Sure That This Happened? Assessing the Factuality Degree of Events in Text",
            "abstract": "",
            "year": 2012,
            "venue": "International Conference on Computational Logic",
            "authors": [
              {
                "authorId": "144327698",
                "name": "R. Saurí"
              },
              {
                "authorId": "1707726",
                "name": "J. Pustejovsky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5730838,
          "isinfluential": false,
          "contexts": [
            "…has mainly employed rule-based methods (Nairn et al., 2006; Saurı, 2008; Lotan et al., 2013) or machine learning methods (with manually designed features) (Diab et al., 2009; Prabhakaran et al., 2010; De Marneffe et al., 2012; Saurí and Pustejovsky, 2012; Lee et al., 2015; Qian et al., 2015)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Automatic Committed Belief Tagging",
            "abstract": "",
            "year": 2010,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "3331141",
                "name": "Vinodkumar Prabhakaran"
              },
              {
                "authorId": "1702447",
                "name": "Owen Rambow"
              },
              {
                "authorId": "1700007",
                "name": "Mona T. Diab"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8992998,
          "isinfluential": false,
          "contexts": [
            ", 2013) or machine learning methods (with manually designed features) (Diab et al., 2009; Prabhakaran et al., 2010; De Marneffe et al., 2012; Saurí and Pustejovsky, 2012; Lee et al., 2015; Qian et al., 2015).",
            "…has mainly employed rule-based methods (Nairn et al., 2006; Saurı, 2008; Lotan et al., 2013) or machine learning methods (with manually designed features) (Diab et al., 2009; Prabhakaran et al., 2010; De Marneffe et al., 2012; Saurí and Pustejovsky, 2012; Lee et al., 2015; Qian et al., 2015)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A two-step approach for event factuality identification",
            "abstract": "",
            "year": 2015,
            "venue": "International Conference on Asian Language Processing",
            "authors": [
              {
                "authorId": "46314205",
                "name": "Zhong Qian"
              },
              {
                "authorId": "47470867",
                "name": "Peifeng Li"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9885298,
          "isinfluential": false,
          "contexts": [
            "The early work on this problem has mainly employed rule-based methods (Nairn et al., 2006; Saurı, 2008; Lotan et al., 2013) or machine learning methods (with manually designed features) (Diab et al., 2009; Prabhakaran et al., 2010; De Marneffe et al., 2012; Saurí and Pustejovsky, 2012; Lee et al.,…"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "TruthTeller: Annotating Predicate Truth",
            "abstract": "",
            "year": 2013,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "50703118",
                "name": "A. Lotan"
              },
              {
                "authorId": "32590708",
                "name": "Asher Stern"
              },
              {
                "authorId": "7465342",
                "name": "Ido Dagan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12873739,
          "isinfluential": false,
          "contexts": [
            "In fact, other models like convolutional neural networks (Zeng et al., 2014) and long short-term memory networks (Hochreiter and Schmidhuber, 1997) can also be employed as encoders."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Relation Classification via Convolutional Deep Neural Network",
            "abstract": "",
            "year": 2014,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "38431523",
                "name": "Siwei Lai"
              },
              {
                "authorId": "143652253",
                "name": "Guangyou Zhou"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13468104,
          "isinfluential": false,
          "contexts": [
            "For example, Vilnis and McCallum (2015) utilize Gaussian embeddings to represent words, where the covariance naturally measures the ambiguity of the words."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Word Representations via Gaussian Embedding",
            "abstract": "Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.",
            "year": 2014,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "2546951",
                "name": "L. Vilnis"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "Our local context encoder is based on the Transformer architecture (Vaswani et al., 2017)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14124213,
          "isinfluential": false,
          "contexts": [
            "…event is PS+. EFI is an important task in natural language processing (NLP) area, which is beneﬁcial for a wide range of NLP applications, such as rumor detection (Qazvinian et al., 2011), sentiment analysis (Klenner and Clematide, 2016) and machine reading comprehension (Richardson et al., 2013).",
            "…the size of randomly initialized parameters may not be beneﬁcial for BERT ﬁne-tuning. can beneﬁt many NLP applications, including rumor detection (Qazvinian et al., 2011), sentiment analysis (Klenner and Clematide, 2016), event causality identiﬁcation (Cao et al., 2021; Tran Phu and Nguyen,…"
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Rumor has it: Identifying Misinformation in Microblogs",
            "abstract": "",
            "year": 2011,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1705983",
                "name": "Vahed Qazvinian"
              },
              {
                "authorId": "3237213",
                "name": "Emily Rosengren"
              },
              {
                "authorId": "9215251",
                "name": "Dragomir R. Radev"
              },
              {
                "authorId": "1743469",
                "name": "Qiaozhu Mei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51605619,
          "isinfluential": true,
          "contexts": [
            "In recent years, various neural models have been proposed for sentence-level EFI, and achieve state-of-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Veyseh et al., 2019).",
            "In recent years, neural networks have been introduced into the EFI task, and achieved state-of-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Sheng et al., 2019; Huang et al., 2019; Veyseh et al., 2019).",
            "In recent years, neural networks have been introduced into the EFI task, and achieved state-ofthe-art performance (Rudinger et al., 2018; Qian et al., 2018; Sheng et al., 2019; Huang et al., 2019; Veyseh et al., 2019).",
            "In recent years, various neural models have been proposed for sentence-level EFI, and achieve stateof-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Veyseh et al., 2019)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Event Factuality Identification via Generative Adversarial Networks with Auxiliary Classification",
            "abstract": "Event factuality identification is an important semantic task in NLP. Traditional research heavily relies on annotated texts. This paper proposes a two-step framework, first extracting essential factors related with event factuality from raw texts as the input, and then identifying the factuality of events via a Generative Adversarial Network with Auxiliary Classification (AC-GAN). The use of AC-GAN allows the model to learn more syntactic information and address the imbalance among factuality values. Experimental results on FactBank show that our method significantly outperforms several state-of-the-art baselines, particularly on events with embedded sources, speculative and negative factuality values.",
            "year": 2018,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "46314205",
                "name": "Zhong Qian"
              },
              {
                "authorId": "47470867",
                "name": "Peifeng Li"
              },
              {
                "authorId": null,
                "name": "Yue Zhang"
              },
              {
                "authorId": "143740945",
                "name": "Guodong Zhou"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 174800759,
          "isinfluential": true,
          "contexts": [
            "For a fair comparison with previous work (Qian et al., 2019), we both perform 10-fold cross-validation on English and Chinese corpora.",
            "Therefore, following previous work (Qian et al., 2019), we mainly focus on the performance of CT+, CT-and PS+.",
            "To further investigate the effectiveness of our method for document-level EFI, we compare our method with Att-Adv on the (1) Compared with improvements over the Att-Adv (Qian et al., 2019) when n = 1, our method achieves more improvements when n> 1.",
            "We compare the proposed approach ULGN with the following methods: (1) MaxEntVote (Qian et al., 2019), which first uses maximum entropy model to identify sentencelevel event factuality, and then votes, i.",
            "From the table, we have two important observations: (1) Compared with improvements over the AttAdv (Qian et al., 2019) when n=1, our method achieves more improvements when n>1.",
            "To this end, Qian et al. (2019) propose the document-level EFI task.",
            "(3) The BERT Model achieves comparable performance with complex state-of-the-art methods such as Att-Adv (Qian et al., 2019) on these two datasets, which indicates that the BERT is able to extract useful text features for the task.",
            "Therefore, following previous work (Qian et al., 2019), we mainly focus on the performance of CT+, CT- and PS+.",
            "We compare the proposed approach ULGN with the following methods: (1) MaxEntVote (Qian et al., 2019), which ﬁrst uses maximum entropy model to identify sentence-level event factuality, and then votes, i.e., choosing the value committed by the most sentences as the document-level factuality value.",
            "(3) Att-Adv (Qian et al., 2019), which leverages the intra-sentence and inter-sentence attention to learn the document representation, and utilizes adversarial training to improve the robustness.",
            "For example, compared with the previous state-of-the-art model Att-Adv (Qian et al., 2019), our method achieves 11.45% improvements of macro-F1 score on the Chinese event factuality dataset.",
            "(2) BiLSTM-Att (Qian et al., 2019), which employs the bidirectional long short-term memory network (BiLSTM) to extract features, and uses the intra-sentence attention to capture the most important information in the sentence.",
            "For example, compared with the previous state-of-the-art model Att-Adv (Qian et al., 2019), our method achieves 11.",
            "According to our statistics on the English and Chinese event factuality datasets (Qian et al., 2019), 25.",
            "According to our statistics on the English and Chinese event factuality datasets (Qian et al., 2019), 25.7% (English) and 37.8% (Chinese) of instances have the problem of event factuality conﬂict at sentence level for the same event, which is not negligible.",
            "We evaluate our proposed method on two widely used datasets, English and Chinese event factuality datasets (Qian et al., 2019)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Factuality Identification via Adversarial Neural Network",
            "abstract": "Document-level event factuality identification is an important subtask in event factuality and is crucial for discourse understanding in Natural Language Processing (NLP). Previous studies mainly suffer from the scarcity of suitable corpus and effective methods. To solve these two issues, we first construct a corpus annotated with both document- and sentence-level event factuality information on both English and Chinese texts. Then we present an LSTM neural network based on adversarial training with both intra- and inter-sequence attentions to identify document-level event factuality. Experimental results show that our neural network model can outperform various baselines on the constructed corpus.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46314205",
                "name": "Zhong Qian"
              },
              {
                "authorId": "47470867",
                "name": "Peifeng Li"
              },
              {
                "authorId": "7703092",
                "name": "Qiaoming Zhu"
              },
              {
                "authorId": "143740945",
                "name": "Guodong Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 195833740,
          "isinfluential": false,
          "contexts": [
            "In recent years, various neural models have been proposed for sentence-level EFI, and achieve state-of-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Veyseh et al., 2019).",
            "Most existing EFI studies are limited to the sentence-level task (Saurí and Pustejovsky, 2012; De Marneffe et al., 2012; Rudinger et al., 2018; Veyseh et al., 2019).",
            "In recent years, neural networks have been introduced into the EFI task, and achieved state-of-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Sheng et al., 2019; Huang et al., 2019; Veyseh et al., 2019)."
          ],
          "intents": [
            "['background']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures",
            "abstract": "Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important context words. The previous work for EFP has only combined these information in a simple way that cannot fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed model for EFP.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3460489",
                "name": "Amir Pouran Ben Veyseh"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              },
              {
                "authorId": "1721158",
                "name": "D. Dou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 196215305,
          "isinfluential": false,
          "contexts": [
            "Inspired by Zhu et al. (2019), we formally utilize an uncertain graph convolution layer (UGCL) to perform convolution operations between Gaussian distributions."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Robust Graph Convolutional Networks Against Adversarial Attacks",
            "abstract": "Graph Convolutional Networks (GCNs) are an emerging type of neural network model on graphs which have achieved state-of-the-art performance in the task of node classification. However, recent studies show that GCNs are vulnerable to adversarial attacks, i.e. small deliberate perturbations in graph structures and node attributes, which poses great challenges for applying GCNs to real world applications. How to enhance the robustness of GCNs remains a critical open problem. To address this problem, we propose Robust GCN (RGCN), a novel model that \"fortifies'' GCNs against adversarial attacks. Specifically, instead of representing nodes as vectors, our method adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer. In this way, when the graph is attacked, our model can automatically absorb the effects of adversarial changes in the variances of the Gaussian distributions. Moreover, to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances when performing convolutions. Extensive experimental results demonstrate that our proposed method can effectively improve the robustness of GCNs. On three benchmark graphs, our RGCN consistently shows a substantial gain in node classification accuracy compared with state-of-the-art GCNs against various adversarial attack strategies.",
            "year": 2019,
            "venue": "Knowledge Discovery and Data Mining",
            "authors": [
              {
                "authorId": "51139354",
                "name": "Dingyuan Zhu"
              },
              {
                "authorId": "48806258",
                "name": "Ziwei Zhang"
              },
              {
                "authorId": "143738684",
                "name": "Peng Cui"
              },
              {
                "authorId": "145583986",
                "name": "Wenwu Zhu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203620105,
          "isinfluential": false,
          "contexts": [
            "In recent years, neural networks have been introduced into the EFI task, and achieved state-of-the-art performance (Rudinger et al., 2018; Qian et al., 2018; Sheng et al., 2019; Huang et al., 2019; Veyseh et al., 2019).",
            "In recent years, neural networks have been introduced into the EFI task, and achieved state-ofthe-art performance (Rudinger et al., 2018; Qian et al., 2018; Sheng et al., 2019; Huang et al., 2019; Veyseh et al., 2019)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Chinese Event Factuality Detection",
            "abstract": "",
            "year": 2019,
            "venue": "Natural Language Processing and Chinese Computing",
            "authors": [
              {
                "authorId": "1381733985",
                "name": "Jiaxuan Sheng"
              },
              {
                "authorId": "3078054",
                "name": "Bowei Zou"
              },
              {
                "authorId": "3210894",
                "name": "Zhengxian Gong"
              },
              {
                "authorId": "2009642867",
                "name": "Yu Hong"
              },
              {
                "authorId": "143740945",
                "name": "Guodong Zhou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207228784,
          "isinfluential": false,
          "contexts": [
            "He et al. (2015) attempt to leverage the Gaussian distribution to represent the entity and relation, which aims to model the uncertainty of entities and relations in knowledge graphs."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Learning to Represent Knowledge Graphs with Gaussian Embedding",
            "abstract": "",
            "year": 2015,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "1954845",
                "name": "Shizhu He"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "2055422009",
                "name": "Guoliang Ji"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 215737171,
          "isinfluential": false,
          "contexts": [
            "Longformer 4 (Beltagy et al., 2020) to extract the global feature for prediction; 2) BERT-GCN and BERT-GAT, which ﬁrst uses the BERT to the lo-cal information, and then employs GCN and GAT (Veliˇckovi´c et al., 2017) for integrating the local information, respectively."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Longformer: The Long-Document Transformer",
            "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
            "year": 2020,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "46181066",
                "name": "Iz Beltagy"
              },
              {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
              },
              {
                "authorId": "2527954",
                "name": "Arman Cohan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 216078090,
          "isinfluential": true,
          "contexts": [
            "Thus, we use the reparameterization trick (Kingma and Welling, 2013) to bypass the problem.",
            "…(2) Uncertain Information Aggregation ( § 2.2), which leverages the global structure to integrate the local information; (3) Reparameterization for Prediction ( § 2.3), which utilizes the reparameterization trick (Kingma and Welling, 2013) to obtain the global representation for ﬁnal prediction."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Auto-Encoding Variational Bayes",
            "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
            "year": 2013,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236460024,
          "isinfluential": false,
          "contexts": [
            "…of randomly initialized parameters may not be beneﬁcial for BERT ﬁne-tuning. can beneﬁt many NLP applications, including rumor detection (Qazvinian et al., 2011), sentiment analysis (Klenner and Clematide, 2016), event causality identiﬁcation (Cao et al., 2021; Tran Phu and Nguyen, 2021) and so on.",
            ", 2011), sentiment analysis (Klenner and Clematide, 2016), event causality identification (Cao et al., 2021; Tran Phu and Nguyen, 2021) and so on."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks",
            "abstract": "Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49776272",
                "name": "Pengfei Cao"
              },
              {
                "authorId": "87696608",
                "name": "Xinyu Zuo"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "11447228",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2145264600",
                "name": "Yuguang Chen"
              },
              {
                "authorId": "49161576",
                "name": "Weihua Peng"
              }
            ]
          }
        }
      ]
    },
    "259949858": {
      "citing_paper_info": {
        "title": "Linked-DocRED - Enhancing DocRED with Entity-Linking to Evaluate End-To-End Document-Level Information Extraction Pipelines",
        "abstract": "Information Extraction (IE) pipelines aim to extract meaningful entities and relations from documents and structure them into a knowledge graph that can then be used in downstream applications. Training and evaluating such pipelines requires a dataset annotated with entities, coreferences, relations, and entity-linking. However, existing datasets either lack entity-linking labels, are too small, not diverse enough, or automatically annotated (that is, without a strong guarantee of the correction of annotations). Therefore, we propose Linked-DocRED, to the best of our knowledge, the first manually-annotated, large-scale, document-level IE dataset. We enhance the existing and widely-used DocRED dataset with entity-linking labels that are generated thanks to a semi-automatic process that guarantees high-quality annotations. In particular, we use hyperlinks in Wikipedia articles to provide disambiguation candidates. We also propose a complete framework of metrics to benchmark end-to-end IE pipelines, and we define an entity-centric metric to evaluate entity-linking. The evaluation of a baseline shows promising results while highlighting the challenges of an end-to-end IE pipeline. Linked-DocRED, the source code for the entity-linking, the baseline, and the metrics are distributed under an open-source license and can be downloaded from a public repository.",
        "year": 2023,
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "authors": [
          {
            "authorId": "2187857024",
            "name": "Pierre-Yves Genest"
          },
          {
            "authorId": "2769723",
            "name": "P. Portier"
          },
          {
            "authorId": "1403092231",
            "name": "Elöd Egyed-Zsigmond"
          },
          {
            "authorId": "90093415",
            "name": "M. Lovisetto"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 17,
        "unique_cited_count": 15,
        "influential_count": 6,
        "detailed_records_count": 17
      },
      "cited_papers": [
        "15926286",
        "88817",
        "4612975",
        "59528287",
        "53080736",
        "221970808",
        "17606580",
        "18930466",
        "252904978",
        "52967399",
        "189898081",
        "207169186",
        "261317734",
        "67856607",
        "243865630"
      ],
      "citation_details": [
        {
          "citedcorpusid": 88817,
          "isinfluential": true,
          "contexts": [
            "To train and evaluate IE pipelines, numerous datasets have been proposed, covering a large spectrum of settings and applications:\n• Some focus on general domain information (e.g., TREx [10], DocRED [36], or HacRED [5]), other on very specific domains (scientific literature for SciERC [20], biomedicine for FewRel 2.0 [12], or BC5CDR [19]).",
            "DWIE [37] Similar to KnowledgeNet and BC5CDR, DWIE contains documents labeled for the four tasks.",
            "Indeed, FewRel [15, 12] lacks documents and novel entities; T-REx [10] lacks manual annotations and novel entities; KnowledgeNet [23] and BC5CDR [19] are too small and not diverse enough; DWIE has automatic entity-linking annotations [37]; and HacRED [5], and DocRED [36] lack annotation for entitylinking.",
            "However, BC5CDR and KnowledgeNet are too small (see Table 1) and not diverse enough (with only 15 relations types for KnowledgeNet and 1 for BC5CDR), which raises questions regarding their representativeness for realistic IE scenarios.",
            "KnowledgeNet [23] and BC5CDR [19] They contain documents and are annotated for the four tasks.",
            "Similarly to FewRel, BC5CDR has no new knowledge (all entities are already present in the knowledge base)."
          ],
          "intents": [
            "--",
            "--",
            "['background']",
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction",
            "abstract": "Community-run, formal evaluations and manually annotated text corpora are critically important for advancing biomedical text-mining research. Recently in BioCreative V, a new challenge was organized for the tasks of disease named entity recognition (DNER) and chemical-induced disease (CID) relation extraction. Given the nature of both tasks, a test collection is required to contain both disease/chemical annotations and relation annotations in the same set of articles. Despite previous efforts in biomedical corpus construction, none was found to be sufficient for the task. Thus, we developed our own corpus called BC5CDR during the challenge by inviting a team of Medical Subject Headings (MeSH) indexers for disease/chemical entity annotation and Comparative Toxicogenomics Database (CTD) curators for CID relation annotation. To ensure high annotation quality and productivity, detailed annotation guidelines and automatic annotation tools were provided. The resulting BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH as the controlled vocabulary. To ensure accuracy, the entities were first captured independently by two annotators followed by a consensus annotation: The average inter-annotator agreement (IAA) scores were 87.49% and 96.05% for the disease and chemicals, respectively, in the test set according to the Jaccard similarity coefficient. Our corpus was successfully used for the BioCreative V challenge tasks and should serve as a valuable resource for the text-mining research community. Database URL: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr/",
            "year": 2016,
            "venue": "Database J. Biol. Databases Curation",
            "authors": [
              {
                "authorId": "2117970351",
                "name": "Jiao Li"
              },
              {
                "authorId": "2116969756",
                "name": "Yueping Sun"
              },
              {
                "authorId": "2000280",
                "name": "Robin J. Johnson"
              },
              {
                "authorId": "2160494",
                "name": "D. Sciaky"
              },
              {
                "authorId": "3252035",
                "name": "Chih-Hsuan Wei"
              },
              {
                "authorId": "2277706",
                "name": "Robert Leaman"
              },
              {
                "authorId": "108109364",
                "name": "A. P. Davis"
              },
              {
                "authorId": "3283642",
                "name": "C. Mattingly"
              },
              {
                "authorId": "1922749",
                "name": "Thomas C. Wiegers"
              },
              {
                "authorId": "144202084",
                "name": "Zhiyong Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 4612975,
          "isinfluential": true,
          "contexts": [
            "To train and evaluate IE pipelines, numerous datasets have been proposed, covering a large spectrum of settings and applications:\n• Some focus on general domain information (e.g., TREx [10], DocRED [36], or HacRED [5]), other on very specific domains (scientific literature for SciERC [20], biomedicine for FewRel 2.0 [12], or BC5CDR [19]).",
            "Indeed, FewRel [15, 12] lacks documents and novel entities; T-REx [10] lacks manual annotations and novel entities; KnowledgeNet [23] and BC5CDR [19] are too small and not diverse enough; DWIE has automatic entity-linking annotations [37]; and HacRED [5], and DocRED [36] lack annotation for entitylinking.",
            "T-REx [10] Of the seven datasets, it is by far the largest, with around 4.6 million documents.",
            "Information extraction can be seen as a supervised task [37, 28, 32], a weakly-supervised task [10], or an unsupervised task [13, 2], the most common setting being supervised information extraction.",
            "• Some are manually annotated (e.g., DocRED [36], FewRel [15] or HacRED [5]), others automatically generated such as T-REx [10] or NYT-10 [29].",
            "T-REx [10] Of the seven datasets, it is by far the largest, with around 4.",
            ", using sentences and not documents), or automatically annotated [23, 37, 10, 12].",
            ", DocRED [36], FewRel [15] or HacRED [5]), others automatically generated such as T-REx [10] or NYT-10 [29].",
            "FewRel [15, 12] - 1 397k 112k 2k 112k 0 56k 80 T-REx [10] 4 650.",
            ", TREx [10], DocRED [36], or HacRED [5]), other on very specific domains (scientific literature for SciERC [20], biomedicine for FewRel 2."
          ],
          "intents": [
            "--",
            "['background']",
            "--",
            "['background']",
            "--",
            "['background']",
            "['methodology']",
            "['methodology']",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples",
            "abstract": "",
            "year": 2018,
            "venue": "International Conference on Language Resources and Evaluation",
            "authors": [
              {
                "authorId": "2218938",
                "name": "Hady ElSahar"
              },
              {
                "authorId": "7631872",
                "name": "P. Vougiouklis"
              },
              {
                "authorId": "41127671",
                "name": "Arslen Remaci"
              },
              {
                "authorId": "1701478",
                "name": "C. Gravier"
              },
              {
                "authorId": "3724810",
                "name": "Jonathon S. Hare"
              },
              {
                "authorId": "2141475",
                "name": "F. Laforest"
              },
              {
                "authorId": "2927032",
                "name": "E. Simperl"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15926286,
          "isinfluential": false,
          "contexts": [
            "On this sample, we compute the Cohen’s kappa coefficient [7], and obtain"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Coefficient of Agreement for Nominal Scales",
            "abstract": "",
            "year": 1960,
            "venue": "",
            "authors": [
              {
                "authorId": "145670758",
                "name": "Jacob Cohen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 17606580,
          "isinfluential": false,
          "contexts": [
            "This metric, among others, is recommended to evaluate coreference resolution models [27, 24, 37]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric",
            "abstract": "Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identiﬁcation effect in the B 3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this ﬂaw is MUC , which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the ofﬁcial CoNLL scorer.",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2182290",
                "name": "N. Moosavi"
              },
              {
                "authorId": "31380436",
                "name": "M. Strube"
              }
            ]
          }
        },
        {
          "citedcorpusid": 18930466,
          "isinfluential": false,
          "contexts": [
            "This metric is recommended to evaluate coreference resolution models [24, 27, 37]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation",
            "abstract": "The definitions of two coreference scoring metrics- B3 and CEAF-are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms.",
            "year": 2014,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1735131",
                "name": "Sameer Pradhan"
              },
              {
                "authorId": "34501578",
                "name": "Xiaoqiang Luo"
              },
              {
                "authorId": "144409897",
                "name": "Marta Recasens"
              },
              {
                "authorId": "144547315",
                "name": "E. Hovy"
              },
              {
                "authorId": "145106110",
                "name": "Vincent Ng"
              },
              {
                "authorId": "31380436",
                "name": "M. Strube"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "This model relies on BERT [8], which can only handle documents with at most 512 tokens.",
            "As we have documents with more than 512 tokens, we propose to replace BERT with Longformer [3], which can encode documents up to 4 096 tokens, with only a marginal decrease in performance compared to BERT.",
            "[38, 39, 37, 40, 6]), who often represent the knowledge explicitly as a graph, which can be processed with Graph Neural Networks (GNN) for inference; Zhou et al. [42] propose to use implicit knowledge representations produced with BERT, which results in a simple, efficient and effective model."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53080736,
          "isinfluential": true,
          "contexts": [
            "FewRel [15, 12] It is large-scale, diverse (it contains many different relation types), and annotated for the four tasks, but it does not contain documents.",
            "Indeed, FewRel [15, 12] lacks documents and novel entities; T-REx [10] lacks manual annotations and novel entities; KnowledgeNet [23] and BC5CDR [19] are too small and not diverse enough; DWIE has automatic entity-linking annotations [37]; and HacRED [5], and DocRED [36] lack annotation for entitylinking.",
            ", FewRel [15, 12], or NYT10 [29]) others on documents (DocRED [36], KnowledgeNet [23], or DWIE [37]).",
            ", DocRED [36], FewRel [15] or HacRED [5]), others automatically generated such as T-REx [10] or NYT-10 [29].",
            "FewRel [15, 12] - 1 397k 112k 2k 112k 0 56k 80 T-REx [10] 4 650."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation",
            "abstract": "We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "144459859",
                "name": "Hao Zhu"
              },
              {
                "authorId": "92720938",
                "name": "Pengfei Yu"
              },
              {
                "authorId": "2124823889",
                "name": "Ziyun Wang"
              },
              {
                "authorId": "144779803",
                "name": "Y. Yao"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 59528287,
          "isinfluential": false,
          "contexts": [
            "The resulting knowledge graph can then be used for multiple downstream tasks such as recommender systems [14], logical reasoning [4], or question answering [17]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Knowledge Graph Embedding Based Question Answering",
            "abstract": "Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.",
            "year": 2019,
            "venue": "Web Search and Data Mining",
            "authors": [
              {
                "authorId": "97620379",
                "name": "Xiao Huang"
              },
              {
                "authorId": "49050393",
                "name": "Jingyuan Zhang"
              },
              {
                "authorId": "34377382",
                "name": "Dingcheng Li"
              },
              {
                "authorId": "1739405774",
                "name": "Ping Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 67856607,
          "isinfluential": false,
          "contexts": [
            "[38, 39, 37, 40, 6]), who often represent the knowledge explicitly as a graph, which can be processed with Graph Neural Networks (GNN) for inference; Zhou et al."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks",
            "abstract": "We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate “few-shot” models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other baselines, especially for long-tail relations.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "13223878",
                "name": "Zhanlin Sun"
              },
              {
                "authorId": "2119999",
                "name": "Guanying Wang"
              },
              {
                "authorId": "48283576",
                "name": "Xi Chen"
              },
              {
                "authorId": "2155468731",
                "name": "Wei Zhang"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": true,
          "contexts": [
            "DocRED [36] and HacRED [5] They contain around two to five times more documents and annotations than the other manually annotated datasets, which makes them more suitable to train and evaluate IE pipelines.",
            "At the same time, we notice that one existing dataset, DocRED [36], is almost adequate to train and evaluate an IE pipeline, except for the lack of entity-linking annotations.",
            "On the one hand, most datasets focus on NER, Coref, and RE, ignoring the last entity-linking step [36, 5].",
            "Indeed, FewRel [15, 12] lacks documents and novel entities; T-REx [10] lacks manual annotations and novel entities; KnowledgeNet [23] and BC5CDR [19] are too small and not diverse enough; DWIE has automatic entity-linking annotations [37]; and HacRED [5], and DocRED [36] lack annotation for entitylinking.",
            "The most recent ones [36, 37, 23] focus on document-level information extraction, a more realistic, albeit more challenging scenario than sentencelevel IE.",
            "Instead of creating a dataset from scratch, we enhance the widely-used DocRED dataset [36] (already labeled with entities, coreferences, and relations) by annotating each entity with entity-linking.",
            ", FewRel [15, 12], or NYT10 [29]) others on documents (DocRED [36], KnowledgeNet [23], or DWIE [37]).",
            "2k 1 141k 99k 9 19k 68k 26 DocRED [36] 5.",
            ", DocRED [36], FewRel [15] or HacRED [5]), others automatically generated such as T-REx [10] or NYT-10 [29].",
            ", TREx [10], DocRED [36], or HacRED [5]), other on very specific domains (scientific literature for SciERC [20], biomedicine for FewRel 2."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207169186,
          "isinfluential": false,
          "contexts": [
            "Information extraction can be seen as a supervised task [28, 32, 37], a weakly-supervised task [10], or an unsupervised task [2, 13], the most common setting being supervised information extraction."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Open Information Extraction from the Web",
            "abstract": "Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER’s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000more abstract assertions.",
            "year": 2007,
            "venue": "CACM",
            "authors": [
              {
                "authorId": "2339397",
                "name": "Michele Banko"
              },
              {
                "authorId": "1725561",
                "name": "Michael J. Cafarella"
              },
              {
                "authorId": "144295318",
                "name": "S. Soderland"
              },
              {
                "authorId": "50452701",
                "name": "M. Broadhead"
              },
              {
                "authorId": "1741101",
                "name": "Oren Etzioni"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221970808,
          "isinfluential": true,
          "contexts": [
            "LinkedDocRED contains four times more entities and two times more relations than its closest competitor DWIE [37].",
            "• Some are manually annotated (e.g., DocRED [36], FewRel [15] or HacRED [5]), others automatically generated such as T-REx [10] or • Some focus on sentences (e.g., FewRel [15, 12], or NYT-10 [29]) others on documents (DocRED [36], Knowled-geNet [23], or DWIE [37]).",
            "DWIE [37] Similar to KnowledgeNet and BC5CDR, DWIE contains documents labeled for the four tasks.",
            "The most recent ones [36, 37, 23] focus on document-level information extraction, a more realistic, albeit more challenging scenario than sentencelevel IE.",
            "A similar observation was made by Prieur et al. [28] on the DWIE dataset.",
            ", FewRel [15, 12], or NYT10 [29]) others on documents (DocRED [36], KnowledgeNet [23], or DWIE [37]).",
            "[37] by establishing a clear and coherent set of entity-centric metrics to evaluate the performance of an IE pipeline.",
            "As we have said in the introduction, we define information extraction as a four-step process with [28, 37]:",
            "Information extraction can be seen as a supervised task [37, 28, 32], a weakly-supervised task [10], or an unsupervised task [13, 2], the most common setting being supervised information extraction.",
            "Indeed, FewRel [15, 12] lacks documents and novel entities; T-REx [10] lacks manual annotations and novel entities; KnowledgeNet [23] and BC5CDR [19] are too small and not diverse enough; DWIE has automatic entity-linking annotations [37]; and HacRED [5], and DocRED [36] lack annotation for entity-linking.",
            "5k 343k 10k 2 19k 10k 0 48k 1 DWIE [37] 0.",
            ", using sentences and not documents), or automatically annotated [23, 37, 10, 12].",
            "Similarly to [37, 28], we define IE as a four-step process with:",
            "Linked-DocRED contains four times more entities and two times more relations than its closest competitor DWIE [37]."
          ],
          "intents": [
            "['background']",
            "--",
            "['methodology']",
            "['background']",
            "--",
            "['background']",
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "--",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "DWIE: an entity-centric dataset for multi-task document-level information extraction",
            "abstract": "",
            "year": 2020,
            "venue": "Information Processing & Management",
            "authors": [
              {
                "authorId": "46214177",
                "name": "Klim Zaporojets"
              },
              {
                "authorId": "2630759",
                "name": "Johannes Deleu"
              },
              {
                "authorId": "2489892",
                "name": "Chris Develder"
              },
              {
                "authorId": "1388296896",
                "name": "Thomas Demeester"
              }
            ]
          }
        },
        {
          "citedcorpusid": 243865630,
          "isinfluential": false,
          "contexts": [
            "Recent papers often consider the ﬁrst three tasks [40, 38, 16, 20, 33, 31], setting entity-linking aside."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Incorporating medical knowledge in BERT for clinical relation extraction",
            "abstract": "In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia versus clinic notes), these models may not be ideal for domain-specific tasks (e.g., extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": null,
                "name": "Arpita Roy"
              },
              {
                "authorId": "2239443126",
                "name": "Shimei Pan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 252904978,
          "isinfluential": false,
          "contexts": [
            "Information extraction can be seen as a supervised task [37, 28, 32], a weakly-supervised task [10], or an unsupervised task [13, 2], the most common setting being supervised information extraction."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "PromptORE - A Novel Approach Towards Fully Unsupervised Relation Extraction",
            "abstract": "Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a \"Prompt-based Open Relation Extraction\" model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Results on three general and specific domain datasets show that PromptORE consistently outperforms state-of-the-art models with a relative gain of more than 40% in B3, V-measure and ARI. Qualitative analysis also indicates PromptORE's ability to identify semantically coherent clusters that are very close to true relations.",
            "year": 2022,
            "venue": "International Conference on Information and Knowledge Management",
            "authors": [
              {
                "authorId": "2187857024",
                "name": "Pierre-Yves Genest"
              },
              {
                "authorId": "2769723",
                "name": "P. Portier"
              },
              {
                "authorId": "1403092231",
                "name": "Elöd Egyed-Zsigmond"
              },
              {
                "authorId": "2749242",
                "name": "L. Goix"
              }
            ]
          }
        },
        {
          "citedcorpusid": 261317734,
          "isinfluential": true,
          "contexts": [
            "This pipeline is trained using the hyperparameter values proposed by the authors of PURE [41], NeuralCoref, and ATLOP [42].",
            "We propose to use the simple yet eﬀective span-based NER proposed by Zhong and Chen [41, 33, 21] (PURE).",
            "In the rest of the paper, we call this NER-Coref-RE ensemble PNA (for PURE [41], NeuralCoref, and ATLOP [42]).",
            "Similarly to previous works (e.g., Zhong and Chen [41]), we consider a predicted mention to be correct if its boundaries and type are the same as the ones of a ground truth mention."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "A Frustratingly Easy Approach for Entity and Relation Extraction",
            "abstract": "End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "49164966",
                "name": "Zexuan Zhong"
              },
              {
                "authorId": "50536468",
                "name": "Danqi Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "As we have documents with more than 512 tokens, we pro-pose to replace BERT with Longformer [3], which can encode documents up to 4 096 tokens, with only a marginal decrease in performance compared to"
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "To evaluate coreferences, we use the B3 metric [1], which is used to evaluate clustering."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {}
        }
      ]
    },
    "263334323": {
      "citing_paper_info": {
        "title": "Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?",
        "abstract": "Two distinct approaches have been proposed for relational triple extraction - pipeline and joint. Joint models, which capture interactions across triples, are the more recent development, and have been shown to outperform pipeline models for sentence-level extraction tasks. Document-level extraction is a more challenging setting where interactions across triples can be long-range, and individual triples can also span across sentences. Joint models have not been applied for document-level tasks so far. In this paper, we benchmark state-of-the-art pipeline and joint extraction models on sentence-level as well as document-level datasets. Our experiments show that while joint models outperform pipeline models significantly for sentence-level extraction, their performance drops sharply below that of pipeline models for the document-level dataset.",
        "year": 2023,
        "venue": "International Joint Conference on Natural Language Processing",
        "authors": [
          {
            "authorId": "40813358",
            "name": "Pratik Saini"
          },
          {
            "authorId": "144329677",
            "name": "Tapas Nayak"
          },
          {
            "authorId": "2249761906",
            "name": "Indrajit Bhattacharya"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 16,
        "unique_cited_count": 16,
        "influential_count": 3,
        "detailed_records_count": 16
      },
      "cited_papers": [
        "235313883",
        "235790751",
        "247594480",
        "53250562",
        "2778800",
        "16483125",
        "52967399",
        "2386383",
        "247291740",
        "221996144",
        "2476229",
        "237507023",
        "208248243",
        "52115592",
        "237346893",
        "202889074"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2386383,
          "isinfluential": false,
          "contexts": [
            "Traditional pipeline approaches (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2014, 2015; Nayak and Ng, 2019; Jat et al., 2017) first identify entities followed by relation identification one entity pair at a time.",
            "Joint models for relation extraction outperform traditional pipeline models for sentence-level datasets such as NYT (Riedel et al., 2010).",
            "NYT (Riedel et al., 2010) is a large-scale and popular benchmark for sentence-level RE, and we use this dataset as it is for our sentence-level experiments."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Modeling Relations and Their Mentions without Labeled Text",
            "abstract": "",
            "year": 2010,
            "venue": "ECML/PKDD",
            "authors": [
              {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
              },
              {
                "authorId": "1786422",
                "name": "Limin Yao"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2476229,
          "isinfluential": false,
          "contexts": [
            "Miwa and Bansal (2016) proposed a model that trained the NER and RC module in a single model."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
            "abstract": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "143977268",
                "name": "Mohit Bansal"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2778800,
          "isinfluential": false,
          "contexts": [
            "Traditional pipeline approaches (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2014, 2015; Nayak and Ng, 2019; Jat et al., 2017) first identify entities followed by relation identification one entity pair at a time.",
            "The second step, Relation Classification (RC), identifies pairwise relations between the extracted entities (Zeng et al., 2014, 2015; Jat et al., 2017; Nayak and Ng, 2019)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks",
            "abstract": "Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",
            "year": 2015,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1763402",
                "name": "Yubo Chen"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 16483125,
          "isinfluential": false,
          "contexts": [
            "Traditional pipeline approaches (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2014, 2015; Nayak and Ng, 2019; Jat et al., 2017) first identify entities followed by relation identification one entity pair at a time."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations",
            "abstract": "",
            "year": 2011,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2566295",
                "name": "Raphael Hoffmann"
              },
              {
                "authorId": "1799338",
                "name": "Congle Zhang"
              },
              {
                "authorId": "145787377",
                "name": "Xiao Ling"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              },
              {
                "authorId": "1780531",
                "name": "Daniel S. Weld"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52115592,
          "isinfluential": false,
          "contexts": [
            "Earlier works (Peng et al., 2021; Quirk and Poon, 2017; Song et al., 2018; Jia et al., 2019) used dependency graph be-tween the two entities to find the relations."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "N-ary Relation Extraction using Graph-State LSTM",
            "abstract": "Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1748796",
                "name": "Linfeng Song"
              },
              {
                "authorId": null,
                "name": "Yue Zhang"
              },
              {
                "authorId": "40296541",
                "name": "Zhiguo Wang"
              },
              {
                "authorId": "1793218",
                "name": "D. Gildea"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52967399,
          "isinfluential": false,
          "contexts": [
            "Parameter Settings: We use BERT BASE (cased) (Devlin et al., 2019) for document encoding for all the models except REBEL for which we have used BART BASE (Lewis et al., 2019).",
            "Parameter Settings: We use BERTBASE (cased) (Devlin et al., 2019) for document encoding for all the models except REBEL for which we"
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "39172707",
                "name": "Jacob Devlin"
              },
              {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
              },
              {
                "authorId": "2544107",
                "name": "Kenton Lee"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53250562,
          "isinfluential": false,
          "contexts": [
            "In contrast, more recent joint approaches (Zeng et al., 2018; Takanobu et al., 2019; Nayak and Ng, 2020; Wei et al., 2020; Wang et al., 2020b; Zhong and Chen, 2021; Zheng et al., 2021; Li et al., 2021; Wei et al., 2020; Yan et al., 2021; Shang et al., 2022) not only identify entities and relations…"
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "A Hierarchical Framework for Relation Extraction with Reinforcement Learning",
            "abstract": "Most existing methods determine relation types only after all the entities have been recognized, thus the interaction between relation types and entity mentions is not fully modeled. This paper presents a novel paradigm to deal with relation extraction by regarding the related entities as the arguments of a relation. We apply a hierarchical reinforcement learning (HRL) framework in this paradigm to enhance the interaction between entity mentions and relation types. The whole extraction process is decomposed into a hierarchy of two-level RL policies for relation detection and entity extraction respectively, so that it is more feasible and natural to deal with overlapping relations. Our model was evaluated on public datasets collected via distant supervision, and results show that it gains better performance than existing methods and is more powerful for extracting overlapping relations1.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "51055574",
                "name": "Ryuichi Takanobu"
              },
              {
                "authorId": "50615630",
                "name": "Tianyang Zhang"
              },
              {
                "authorId": "3486119",
                "name": "Jiexi Liu"
              },
              {
                "authorId": "1730108",
                "name": "Minlie Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 202889074,
          "isinfluential": false,
          "contexts": [
            "More recent Transformer-based approaches (Wang et al., 2019; Tang et al., 2020; Huang et al., 2021; Xu et al., 2021a; Zhou et al., 2021; Xie et al., 2022)"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Fine-tune Bert for DocRED with Two-step Process",
            "abstract": "Modelling relations between multiple entities has attracted increasing attention recently, and a new dataset called DocRED has been collected in order to accelerate the research on the document-level relation extraction. Current baselines for this task uses BiLSTM to encode the whole document and are trained from scratch. We argue that such simple baselines are not strong enough to model to complex interaction between entities. In this paper, we further apply a pre-trained language model (BERT) to provide a stronger baseline for this task. We also find that solving this task in phases can further improve the performance. The first step is to predict whether or not two entities have a relation, the second step is to predict the specific relation.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "46507182",
                "name": "Hong Wang"
              },
              {
                "authorId": "31474976",
                "name": "C. Focke"
              },
              {
                "authorId": "1381213572",
                "name": "Rob Sylvester"
              },
              {
                "authorId": "2061040882",
                "name": "Nilesh Mishra"
              },
              {
                "authorId": "152876475",
                "name": "W. Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208248243,
          "isinfluential": true,
          "contexts": [
            "PtrNet (Nayak and Ng, 2020) and REBEL (Huguet Cabot and Navigli, 2021) use the Seq2Seq approach.",
            "…by presenting triples as a sequence of text and uses special separator tokens, as markers, to achieve the linearization.. WordDecoder model of (Nayak and Ng, 2020) uses a similar approach using LSTMs whereas REBEL is a BART-based Seq2Seq model that utilizes the advantages of transformer model…",
            "Out of the 5 joint models, we see significantly higher drop in F1 score for OneRel and PtrNet than REBEL, GRTE, and BiRTE models.",
            "In contrast, more recent joint approaches (Zeng et al., 2018; Takanobu et al., 2019; Nayak and Ng, 2020; Wei et al., 2020; Wang et al., 2020b; Zhong and Chen, 2021; Zheng et al., 2021; Li et al., 2021; Wei et al., 2020; Yan et al., 2021; Shang et al., 2022) not only identify entities and relations…",
            "We chose PtrNet (Nayak and Ng, 2020) and REBEL (Huguet Cabot and Navigli, 2021) as they used Seq2Seq model for this task.",
            "A.1 Details of Joint Models A.1.1 PtrNet (Nayak and Ng, 2020) PtrNet utilizes a seq2seq approach along with the pointer network-based decoding for jointly extracting entities and relations.",
            "Additionally, PtrNet extracts index positions for the entities.",
            "PtrNet and REBEL are Seq2Seq models which use a decoder to extract the triples, so they possibly need more training data to learn from longer document contexts.",
            "Nayak and Ng (2020); Cabot and Navigli (2021) propose seq2seq models for extracting the triples in a sequence."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction",
            "abstract": "A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.",
            "year": 2019,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "144329677",
                "name": "Tapas Nayak"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221996144,
          "isinfluential": false,
          "contexts": [
            "Recent works (Guo et al., 2019; Nan et al., 2020; Wang et al., 2020a; Zeng et al., 2020, 2021; Xu et al., 2021c,b) proposed graph-based approaches that use advanced neural techniques to do multi-hop reasoning."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entity-level graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at this https URL .",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313883,
          "isinfluential": false,
          "contexts": [
            "Recent works (Guo et al., 2019; Nan et al., 2020; Wang et al., 2020a; Zeng et al., 2020, 2021; Xu et al., 2021c,b) proposed graph-based approaches that use advanced neural techniques to do multi-hop reasoning."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction has attracted much attention in recent years. It is usually formulated as a classification problem that predicts relations for all entity pairs in the document. However, previous works indiscriminately represent intra- and inter-sentential relations in the same way, confounding the different patterns for predicting them. Besides, they create a document graph and use paths between entities on the graph as clues for logical reasoning. However, not all entity pairs can be connected with a path and have the correct logical reasoning paths in their graph. Thus many cases of logical reasoning cannot be covered. This paper proposes an effective architecture, SIRE, to represent intra- and inter-sentential relations in different ways. We design a new and straightforward form of logical reasoning module that can cover more logical reasoning chains. Experiments on the public datasets show SIRE outperforms the previous state-of-the-art methods. Further analysis shows that our predictions are reliable and explainable. Our code is available at https://github.com/DreamInvoker/SIRE.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "3028245",
                "name": "Yuting Wu"
              },
              {
                "authorId": "39488576",
                "name": "Baobao Chang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235790751,
          "isinfluential": false,
          "contexts": [
            "Zhang et al. (2017); Wang et al. (2020b, 2021); Shang et al. (2022) formulate the NER and RC tasks as table filling problem where each cell of the table represents the interaction between two tokens."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "UniRE: A Unified Label Space for Entity Relation Extraction",
            "abstract": "Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks’ label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell’s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46395583",
                "name": "Yijun Wang"
              },
              {
                "authorId": "2118133838",
                "name": "Changzhi Sun"
              },
              {
                "authorId": "3174675",
                "name": "Yuanbin Wu"
              },
              {
                "authorId": "2111824520",
                "name": "Hao Zhou"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              },
              {
                "authorId": "3063894",
                "name": "Junchi Yan"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237346893,
          "isinfluential": false,
          "contexts": [
            "Ren et al. (2022); Zheng et al. (2021); Li et al. (2021); Yan et al. (2021); Wei et al. (2020) have separate NER and RC modules in the same model trained in an end-to-end fashion.",
            "In contrast, more recent joint approaches (Zeng et al., 2018; Takanobu et al., 2019; Nayak and Ng, 2020; Wei et al., 2020; Wang et al., 2020b; Zhong and Chen, 2021; Zheng et al., 2021; Li et al., 2021; Wei et al., 2020; Yan et al., 2021; Shang et al., 2022) not only identify entities and relations for the same triple together but also extract all relational triples together.",
            "…et al., 2019; Nayak and Ng, 2020; Wei et al., 2020; Wang et al., 2020b; Zhong and Chen, 2021; Zheng et al., 2021; Li et al., 2021; Wei et al., 2020; Yan et al., 2021; Shang et al., 2022) not only identify entities and relations for the same triple together but also extract all relational triples…"
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A Partition Filter Network for Joint Entity and Relation Extraction",
            "abstract": "In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where feature encoding is decomposed into two steps: partition and filter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task-specific features is dependent upon each other. Experiment results on six public datasets show that our model performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "113087283",
                "name": "Zhiheng Yan"
              },
              {
                "authorId": "2111387504",
                "name": "Chong Zhang"
              },
              {
                "authorId": "41037252",
                "name": "Jinlan Fu"
              },
              {
                "authorId": "1409702669",
                "name": "Qi Zhang"
              },
              {
                "authorId": "2118602528",
                "name": "Zhongyu Wei"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237507023,
          "isinfluential": true,
          "contexts": [
            "OneRel (Shang et al., 2022) used table-filling method whereas BiRTE (Ren et al., 2022) and GRTE (Ren et al., 2021) used sequentially extracting entities and relations in their end-to-end model. with adaptive thresholding and localized context pooling.",
            "The tagging approaches of BiRTE (Ren et al., 2022) and GRTE (Ren et al., 2021) have a separate entity extraction process in their end-to-end modeling.",
            "A.1.3 GRTE (Ren et al., 2021) GRTE utilizes individual tables for each relation."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling",
            "abstract": "Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated table features. Next, the mined global associations are integrated into the table feature of each relation. This “generate-mine-integrate” process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation’s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed model on three benchmark datasets. Experimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: https://github.com/neukg/GRTE.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "34387678",
                "name": "Feiliang Ren"
              },
              {
                "authorId": "2108282817",
                "name": "Longhui Zhang"
              },
              {
                "authorId": "37792146",
                "name": "Shujuan Yin"
              },
              {
                "authorId": "2124906187",
                "name": "Xiaofeng Zhao"
              },
              {
                "authorId": "2593449",
                "name": "Shilei Liu"
              },
              {
                "authorId": "2132473866",
                "name": "Bochao Li"
              },
              {
                "authorId": "2583064",
                "name": "Yaduo Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247291740,
          "isinfluential": false,
          "contexts": [
            "More recent Transformer-based approaches (Wang et al., 2019; Tang et al., 2020; Huang et al., 2021; Xu et al., 2021a; Zhou et al., 2021; Xie et al., 2022)"
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion",
            "abstract": "Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "1892794261",
                "name": "Yiqing Xie"
              },
              {
                "authorId": "3363642",
                "name": "Jiaming Shen"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "3375249",
                "name": "Yuning Mao"
              },
              {
                "authorId": "2111759643",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247594480,
          "isinfluential": true,
          "contexts": [
            "We use PL-Marker (Ye et al., 2022) as the NER module and KD-DocRE (Tan et al., 2022), SSAN (Xu et al., 2021a), and experiment with SAIS (Xiao et al., 2022) as relation classification models for our experiments, and train these for specific datasets.",
            "A.2.2 KD-DocRE (Tan et al., 2022) This paper suggests a semi-supervised framework for extracting document-level relations.",
            ", 2022) as the NER module and KD-DocRE (Tan et al., 2022), SSAN (Xu et al."
          ],
          "intents": [
            "['methodology']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation",
            "abstract": "Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.",
            "year": 2022,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "118358816",
                "name": "Qingyu Tan"
              },
              {
                "authorId": "22272507",
                "name": "Ruidan He"
              },
              {
                "authorId": "1996394",
                "name": "Lidong Bing"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              }
            ]
          }
        }
      ]
    },
    "267044015": {
      "citing_paper_info": {
        "title": "Document-Level Mathematical Relation Extraction Using Pre-Training Models",
        "abstract": "Mathematical relation extraction is pivotal for identifying relationships between entities in texts, a crucial step in aiding automated systems to comprehend the semantics of mathematical content. While prior studies have concentrated on sentence-level relation extraction (RE), their applicability in real-world scenarios remains limited. Document-level RE (DocRE), in contrast, poses a more complex challenge, necessitating multi-sentence reasoning and the prediction of relationships across entire documents. The scarcity of high- quality datasets, attributed to the constraints of manual data annotation, further complicates this task. In our study, we focus on extracting relationships from mathematical documents through document-level RE techniques. To effectively tackle the complexities of multi-label and multi-entity scenarios in document-level RE, we introduce two innovative methods: adaptive thresholding and evidence context pooling. These approaches allow document-level RE systems to focus on relevant text, significantly enhancing relation extraction capabilities. Furthermore, we implement a self-training strategy to learn evidence retrieval knowledge from a large amount of distantly-supervised data that lacks specific evidence annotations. Our experimental findings affirm the success of these methods, showcasing an impressive accuracy rate reaching up to 90.14% in the domain of mathematical relation extraction.",
        "year": 2023,
        "venue": "2023 20th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)",
        "authors": [
          {
            "authorId": "2279911891",
            "name": "Zhengyong Gong"
          },
          {
            "authorId": "2279933503",
            "name": "Xiuqin Zhong"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 7,
        "unique_cited_count": 7,
        "influential_count": 0,
        "detailed_records_count": 7
      },
      "cited_papers": [
        "237635295",
        "2797612",
        "190001673",
        "12873739",
        "15359942",
        "13756489",
        "102353905"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2797612,
          "isinfluential": false,
          "contexts": [
            "One approach constructs a global information graph using structured attention, dependency structures, or heuristics [4][7-9], followed by the application of graph neural models for reasoning [10-11].",
            "However, a significant portion of relations in mathematical problems span across multiple sentences [4-6]."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs",
            "abstract": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
            "year": 2017,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              },
              {
                "authorId": "2596310",
                "name": "Chris Quirk"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              },
              {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
              }
            ]
          }
        },
        {
          "citedcorpusid": 12873739,
          "isinfluential": false,
          "contexts": [
            "Past research primarily focused on sentence-level relation extraction [1-3]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Relation Classification via Convolutional Deep Neural Network",
            "abstract": "",
            "year": 2014,
            "venue": "International Conference on Computational Linguistics",
            "authors": [
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "38431523",
                "name": "Siwei Lai"
              },
              {
                "authorId": "143652253",
                "name": "Guangyou Zhou"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13756489,
          "isinfluential": false,
          "contexts": [
            "We then employ a Transformer-based PLM [21] to encode the document D and generate contextual embeddings."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Attention is All you Need",
            "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
            "year": 2017,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "3877127",
                "name": "Niki Parmar"
              },
              {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
              },
              {
                "authorId": "145024664",
                "name": "Llion Jones"
              },
              {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
              },
              {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
              },
              {
                "authorId": "3443442",
                "name": "I. Polosukhin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15359942,
          "isinfluential": false,
          "contexts": [
            "To address the challenges of manual data annotation in relation extraction tasks, distant supervision has become a widely used technique [14][19-20]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Distant Supervision for Relation Extraction beyond the Sentence Boundary",
            "abstract": "The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross-sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.",
            "year": 2016,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2596310",
                "name": "Chris Quirk"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 102353905,
          "isinfluential": false,
          "contexts": [
            "The logsumexp pooling technique is applied to obtain ℎ 𝑒 𝑖 , a method that has demonstrated effectiveness in prior research [22]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level N-ary Relation Extraction with Multiscale Representation Learning",
            "abstract": "Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system’s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods.",
            "year": 2019,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422908",
                "name": "Robin Jia"
              },
              {
                "authorId": "2109566188",
                "name": "Cliff Wong"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 190001673,
          "isinfluential": false,
          "contexts": [
            "One approach constructs a global information graph using structured attention, dependency structures, or heuristics [4][7-9], followed by the application of graph neural models for reasoning [10-11]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Attention Guided Graph Convolutional Networks for Relation Extraction",
            "abstract": "Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
              },
              {
                "authorId": "39831806",
                "name": "Yan Zhang"
              },
              {
                "authorId": "143844110",
                "name": "Wei Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 237635295,
          "isinfluential": false,
          "contexts": [
            "Furthermore, Xiao [17] developed a neural classifier for automatic evidence retrieval in conjunction with RE."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction",
            "abstract": "Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources—relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval on DocRED.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "120727449",
                "name": "Yuxin Xiao"
              },
              {
                "authorId": "1637600997",
                "name": "Zecheng Zhang"
              },
              {
                "authorId": "3375249",
                "name": "Yuning Mao"
              },
              {
                "authorId": "1390553618",
                "name": "Carl Yang"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        }
      ]
    },
    "264452034": {
      "citing_paper_info": {
        "title": "From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction",
        "abstract": "Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of\"memory\", where the results of already predicted events are cached and can be retrieved to help the prediction of upcoming events. These methods extract events according to their appearance order in the document, however, the event that appears in the first sentence does not mean that it is the easiest to extract. Existing methods might introduce noise to the extraction of upcoming events if they rely on an incorrect prediction of previous events. In order to provide more reliable memory, we propose a simple-to-complex progressive framework for document-level EAE. Specifically, we first calculate the difficulty of each event and then, we conduct the extraction following a simple-to-complex order. In this way, the memory will store the most certain results, and the model could use these reliable sources to help the prediction of more difficult events. Experiments on WikiEvents show that our model outperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex framework is useful in the EAE task.",
        "year": 2023,
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "authors": [
          {
            "authorId": "2007771781",
            "name": "Quzhe Huang"
          },
          {
            "authorId": "2261678163",
            "name": "Yanxi Zhang"
          },
          {
            "authorId": "2261698348",
            "name": "Dongyan Zhao"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 13,
        "unique_cited_count": 13,
        "influential_count": 4,
        "detailed_records_count": 13
      },
      "cited_papers": [
        "9946972",
        "253510351",
        "28671436",
        "258967387",
        "131773936",
        "201646309",
        "212747810",
        "247450599",
        "248780117",
        "15552794",
        "235694418",
        "6292807",
        "233219850"
      ],
      "citation_details": [
        {
          "citedcorpusid": 6292807,
          "isinfluential": false,
          "contexts": [
            "In this work, the temperature T is selected by minimizing the Expected Calibration Error (ECE) (Pakdaman Naeini et al., 2015) on the validation set, and we denote the temperature with the lowest ECE as T ′ ."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Obtaining Well Calibrated Probabilities Using Bayesian Binning",
            "abstract": "Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets.",
            "year": 2015,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "1739626",
                "name": "Mahdi Pakdaman Naeini"
              },
              {
                "authorId": "1726406",
                "name": "G. Cooper"
              },
              {
                "authorId": "1731761",
                "name": "M. Hauskrecht"
              }
            ]
          }
        },
        {
          "citedcorpusid": 9946972,
          "isinfluential": false,
          "contexts": [
            "We evaluate our framework on W IKI E VENTS (Li et al., 2021) as it annotates all the events in a document (averagely 16 events per document), while existing document-level datasets such as DocEE (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) only annotate at most 3 events per document.",
            "We focus on document-level IAE (Li et al., 2021) (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) that only annotate at most 3 events per document, W IKI E VENTS annotates all the events in a document, with an average of 16 events per document.",
            "…framework on W IKI E VENTS (Li et al., 2021) as it annotates all the events in a document (averagely 16 events per document), while existing document-level datasets such as DocEE (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) only annotate at most 3 events per document."
          ],
          "intents": [
            "--",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Overview of the Fourth Message Understanding Evaluation and Conference",
            "abstract": "The Fourth Message Understanding Conference (MUC-4) is the latest in a series of conferences that concern the evaluation of natural language processing (NLP) systems. These conferences have reported on progress being made both in the development of systems capable of analyzing relatively short English texts and in the definition of a rigorous performance evaluation methodology. MUC-4 was preceded by a period of intensive system development by each of the participating organizations and blind testing using materials prepared by NRaD and SAIC that are described in this paper, other papers in this volume, and the MUC-3 proceedings [1].",
            "year": 1992,
            "venue": "Message Understanding Conference",
            "authors": [
              {
                "authorId": "2620384",
                "name": "B. Sundheim"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15552794,
          "isinfluential": true,
          "contexts": [
            "1 Introduction Document-level Event Argument Extraction (EAE) aims at identifying the participants of multiple events from a document and classifying them into proper roles (Li et al., 2021; Du et al., 2022; Xu et al., 2022; Huang et al., 2022; Yang et al., 2023).",
            "Also, it provides complete coreference annotation for evaluating document-level IAE. Recently, generation-based methods have been proposed for document-level EAE.",
            "Generation-based document-level EAE methods are widely used in recent works (Li et al., 2021; Du et al., 2022; Du and Ji, 2022; Huang et al., 2022).",
            "Unlike sentence-level EAE (Li et al., 2014; Du and Cardie, 2020; Xiangyu et al., 2021 and their participants usually spread across the document in document-level EAE.",
            "Further, Du et al. (2022); Du and Ji (2022) introduced the idea of “memory” to document-level EAE, where predictions of already predicted events were utilized as additional input."
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Constructing Information Networks Using One Single Model",
            "abstract": "In this paper, we propose a new framework that unifies the output of three information extraction (IE) tasks - entity mentions, relations and events as an information network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many relations can now be considered as the resultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and significantly advances state-of-the-art end-toend event argument extraction.",
            "year": 2014,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144873792",
                "name": "Yu Hong"
              },
              {
                "authorId": "1695451",
                "name": "Sujian Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 28671436,
          "isinfluential": true,
          "contexts": [
            "Due to its low time overhead and low ECE property, we adopt it in our work.",
            "Specifically, we adopt temperature scaling (Guo et al., 2017; Desai and Durrett, 2020), a simple and effective method for calibration.",
            "Modern DNNs are prone to over-confidence, which implies that the model’s confidence is not reliable (Guo et al., 2017).",
            "Among modern calibration approaches, temperature scaling is a simple and effective method (Desai and Durrett, 2020) which can produce low ECE (Guo et al., 2017; Chen et al., 2023).",
            "However, some studies reveal that current Deep Neural Networks (DNNs) are prone to over-confidence , which implies that the model’s confidence is not reliable (Guo et al., 2017).",
            "In this work, the temperature T is selected by minimizing the Expected Calibration Error (ECE) (Pakdaman Naeini et al., 2015) on the validation set, and we denote the temperature with the lowest ECE as T ′ .",
            "Other works focus on methods such as label smoothing (Pereyra et al., 2017) and data augmentation (Hendrycks* et al., 2020), but these methods cannot produce as low ECE as temperature scaling (Chen et al., 2023)."
          ],
          "intents": [
            "--",
            "['methodology']",
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "On Calibration of Modern Neural Networks",
            "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.",
            "year": 2017,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "144993411",
                "name": "Chuan Guo"
              },
              {
                "authorId": "10804137",
                "name": "Geoff Pleiss"
              },
              {
                "authorId": "2117103358",
                "name": "Yu Sun"
              },
              {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131773936,
          "isinfluential": false,
          "contexts": [
            "We compare our framework with a series of competitive baselines: (1) BERT-CRF (Shi and Lin, 2019), a simple BERT-based model without incorporating lexical or syntactic features for argument identification and classification.",
            "• All models augmented with retrieval (i.e., w/ M) perform better compared with BERT-CRF and raw BART-Gen, showing the importance of modeling inter-event dependencies."
          ],
          "intents": [
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "abstract": "We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1884766",
                "name": "Peng Shi"
              },
              {
                "authorId": "145580839",
                "name": "Jimmy J. Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 201646309,
          "isinfluential": false,
          "contexts": [
            "Here, we plot the process of predicting the arguments of E 2 . similarity between E i ’s context c i and each prediction in m using S-BERT (Reimers and Gurevych, 2019) embeddings, and select the prediction with the highest score as additional input to help the prediction of E i : where SBERT() denotes S-BERT encoding, m Ri denotes the retrieved prediction that E i relies on.",
            "Here, we plot the process of predicting the arguments of E 2 . similarity between E i ’s context c i and each prediction in m using S-BERT (Reimers and Gurevych, 2019) embeddings, and select the prediction with the highest score as additional input to help the prediction of E i : where SBERT()…"
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
            "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2959414",
                "name": "Nils Reimers"
              },
              {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
              }
            ]
          }
        },
        {
          "citedcorpusid": 212747810,
          "isinfluential": false,
          "contexts": [
            "Studies on the calibration of natural language models have been drawing attention recently (Desai and Durrett, 2020; Park and Caragea, 2022; Kim et al., 2023).",
            "Among modern calibration approaches, temperature scaling is a simple and effective method (Desai and Durrett, 2020) which can produce low ECE (Guo et al., 2017; Chen et al., 2023).",
            "Specifically, we adopt temperature scaling (Guo et al., 2017; Desai and Durrett, 2020), a simple and effective method for calibration."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Calibration of Pre-trained Transformers",
            "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "120777041",
                "name": "Shrey Desai"
              },
              {
                "authorId": "1814094",
                "name": "Greg Durrett"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233219850,
          "isinfluential": true,
          "contexts": [
            "1 Introduction Document-level Event Argument Extraction (EAE) aims at identifying the participants of multiple events from a document and classifying them into proper roles (Li et al., 2021; Du et al., 2022; Xu et al., 2022; Huang et al., 2022; Yang et al., 2023).",
            "We formulate document-level IAE as a generative template-filling task following Li et al. (2021) and Du et al. (2022).",
            "(2) BART-Gen (Li et al., 2021), a conditional neural text generation model that generates a filled template for each event given the event template and context words.",
            "From the results, we can conclude that: • Our S2C-CD model outperforms all previous methods on W IKI E VENTS as to document-level IAE, with an average gain of 1.4% in F1 on all four settings.",
            "Following previous studies on document-level IAE (Li et al., 2021; Du et al., 2022), we adopt Head Word Match (Head F1) (Huang and Riloff, 2021) and Coreferential Match (Coref F1) (Ji and Grish-man, 2008) to judge whether the predicted argu-ment span matches the gold argument span.",
            "First, we introduce our memory-enhanced IAE model (Section 3.1).",
            "The main results for document-level IAE are presented in Table 2.",
            "We evaluate our framework on W IKI E VENTS (Li et al., 2021) as it annotates all the events in a document (averagely 16 events per document), while existing document-level datasets such as DocEE (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) only annotate at most 3 events…",
            "Also, it provides complete coreference annotation for evaluating document-level IAE. Recently, generation-based methods have been proposed for document-level EAE.",
            "We conduct experiments on a widely used benchmark W IKI E VENTS (Li et al., 2021), and our proposed simple-to-complex framework outperforms the previous SOTA by 1.4% in F1, illustrating the effectiveness of our method.",
            "Generation-based document-level EAE methods are widely used in recent works (Li et al., 2021; Du et al., 2022; Du and Ji, 2022; Huang et al., 2022).",
            "Our memory-enhanced IAE model is based on a generative model.",
            "We focus on document-level IAE (Li et al., 2021) (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) that only annotate at most 3 events per document, W IKI E VENTS annotates all the events in a document, with an average of 16 events per document.",
            "Also, it provides complete coreference annotation for document-level IAE.",
            "Among them, one line of studies (Li et al., 2021; Huang et al., 2022) treats each event independently and ignores the underlying correlations between events in real-world documents.",
            "We calculate the cosine Figure 1: Our simple-to-complex progressive framework for document-level IAE.",
            "It is the SOTA model on document-level IAE, but still extracts events according to their appearance order in the document.",
            "Retrieval-Augmented Generation In the input stage (both for training and testing), we augment our model with similarity-based retrieval following Du et al. (2022) to make it capable of finding argument mentions beyond the context of an event, especially informative ones (Li et al., 2021).",
            "In this work, we focus on document-level Informa-tive Argument Extraction 1 (IAE) (Li et al., 2021), where informative arguments are far more distant than local/uninformative ones and provide more useful information about an event."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']",
            "--",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "--",
            "['methodology']",
            "['methodology']",
            "--",
            "--",
            "--",
            "['background']",
            "--",
            "--",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "153034701",
                "name": "Jiawei Han"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235694418,
          "isinfluential": false,
          "contexts": [
            "Unlike sentence-level EAE (Li et al., 2014; Du and Cardie, 2020; Xiangyu et al., 2021 and their participants usually spread across the document in document-level EAE."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder",
            "abstract": "Capturing interactions among event arguments is an essential step towards robust event argument extraction (EAE). However, existing efforts in this direction suffer from two limitations: 1) The argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) The argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an event mention, is not well characterized. To tackle the above two bottlenecks, we formalize EAE as a Seq2Seq-like learning problem for the first time, where a sentence with a specific event trigger is mapped to a sequence of event argument roles. A neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) is proposed to generate argument roles by incorporating contextual entities’ argument role predictions, like a word-by-word text generation process, thereby distinguishing implicit argument distribution patterns within an event more accurately.",
            "year": 2021,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2268306",
                "name": "Xiangyu Xi"
              },
              {
                "authorId": "145235143",
                "name": "Wei Ye"
              },
              {
                "authorId": "1705434",
                "name": "Shikun Zhang"
              },
              {
                "authorId": "12696819",
                "name": "Quanxiu Wang"
              },
              {
                "authorId": "2309680",
                "name": "Huixing Jiang"
              },
              {
                "authorId": "2118974140",
                "name": "Wei Wu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247450599,
          "isinfluential": false,
          "contexts": [
            "Studies on the calibration of natural language models have been drawing attention recently (Desai and Durrett, 2020; Park and Caragea, 2022; Kim et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118885320",
                "name": "Seohong Park"
              },
              {
                "authorId": "2140493460",
                "name": "Cornelia Caragea"
              }
            ]
          }
        },
        {
          "citedcorpusid": 248780117,
          "isinfluential": true,
          "contexts": [
            "1 Introduction Document-level Event Argument Extraction (EAE) aims at identifying the participants of multiple events from a document and classifying them into proper roles (Li et al., 2021; Du et al., 2022; Xu et al., 2022; Huang et al., 2022; Yang et al., 2023).",
            "Constrained Decoding In the output stage, we introduce argument pair constraints following Du et al. (2022) to constrain the decoding of arguments with conflicting roles.",
            "We formulate document-level IAE as a generative template-filling task following Li et al. (2021) and Du et al. (2022).",
            "Among them, template generation-based approaches (Li et al., 2021; Huang et al., 2022; Du et al., 2022) are widely utilized.",
            "Following previous studies on document-level IAE (Li et al., 2021; Du et al., 2022), we adopt Head Word Match (Head F1) (Huang and Riloff, 2021) and Coreferential Match (Coref F1) (Ji and Grish-man, 2008) to judge whether the predicted argu-ment span matches the gold argument span.",
            "Generation-based document-level EAE methods are widely used in recent works (Li et al., 2021; Du et al., 2022; Du and Ji, 2022; Huang et al., 2022).",
            "In this section, we first compare our bounded constraints with those presented in Du et al. (2022), then analyze the impact of the lower and upper bounds individually.",
            "Other works (Du et al., 2022; Du and Ji, 2022) start to consider inter-event dependencies and model them by introducing the idea of “memory”, where event predictions (e.g., arguments, roles) are cached and can be retrieved to help the prediction of the upcoming events in a document.",
            "However, once an incorrect prediction is used to constrain another, it may cause more errors (Du et al., 2022).",
            "In Table 5, we observe that when applying the original constraints (Du et al., 2022 performs only comparably with our S2C model."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']",
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Dynamic Global Memory for Document-level Argument Extraction",
            "abstract": "Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.",
            "year": 2022,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2109154767",
                "name": "Sha Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 253510351,
          "isinfluential": false,
          "contexts": [
            "Further, Du et al. (2022); Du and Ji (2022) introduced the idea of “memory” to document-level EAE, where predictions of already predicted events were utilized as additional input.",
            "Other works (Du et al., 2022; Du and Ji, 2022) start to consider inter-event dependencies and model them by introducing the idea of “memory”, where event predictions (e.g., arguments, roles) are cached and can be retrieved to help the prediction of the upcoming events in a document.",
            "Generation-based document-level EAE methods are widely used in recent works (Li et al., 2021; Du et al., 2022; Du and Ji, 2022; Huang et al., 2022)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Retrieval-Augmented Generative Question Answering for Event Argument Extraction",
            "abstract": "Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models’ capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example’s context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performances.",
            "year": 2022,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "13728923",
                "name": "Xinya Du"
              },
              {
                "authorId": "2072975661",
                "name": "Heng Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 258967387,
          "isinfluential": false,
          "contexts": [
            "1 Introduction Document-level Event Argument Extraction (EAE) aims at identifying the participants of multiple events from a document and classifying them into proper roles (Li et al., 2021; Du et al., 2022; Xu et al., 2022; Huang et al., 2022; Yang et al., 2023)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "An AMR-based Link Prediction Approach for Document-level Event Argument Extraction",
            "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2145435513",
                "name": "Yuqing Yang"
              },
              {
                "authorId": "3187768",
                "name": "Qipeng Guo"
              },
              {
                "authorId": "12040998",
                "name": "Xiangkun Hu"
              },
              {
                "authorId": "39939186",
                "name": "Yue Zhang"
              },
              {
                "authorId": "1767521",
                "name": "Xipeng Qiu"
              },
              {
                "authorId": "1852415",
                "name": "Zheng Zhang"
              }
            ]
          }
        }
      ]
    },
    "267200174": {
      "citing_paper_info": {
        "title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction",
        "abstract": "Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which Generates labeled data by Retrieval and Denoising Knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.",
        "year": 2024,
        "venue": "The Web Conference",
        "authors": [
          {
            "authorId": "2258678494",
            "name": "Qi Sun"
          },
          {
            "authorId": "2112706130",
            "name": "Kun Huang"
          },
          {
            "authorId": "2135971356",
            "name": "Xiaocui Yang"
          },
          {
            "authorId": "2280906554",
            "name": "Rong Tong"
          },
          {
            "authorId": "2217679552",
            "name": "Kun Zhang"
          },
          {
            "authorId": "1746416",
            "name": "Soujanya Poria"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 11,
        "unique_cited_count": 11,
        "influential_count": 1,
        "detailed_records_count": 11
      },
      "cited_papers": [
        "235313469",
        "233210556",
        "238583580",
        "225039888",
        "11751039",
        "246897443",
        "793385",
        "53250562",
        "259108325",
        "221996144",
        "53607073"
      ],
      "citation_details": [
        {
          "citedcorpusid": 793385,
          "isinfluential": false,
          "contexts": [
            "In sentence-level synthetic data, there exists merely one relation triplet within a sentence.",
            "Next, we construct cross-document knowledge graphs according to the pseudo labels and original labels of synthetic data."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Zero-Shot Relation Extraction via Reading Comprehension",
            "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.",
            "year": 2017,
            "venue": "Conference on Computational Natural Language Learning",
            "authors": [
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "4418074",
                "name": "Minjoon Seo"
              },
              {
                "authorId": "2890423",
                "name": "Eunsol Choi"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 11751039,
          "isinfluential": true,
          "contexts": [
            ": [{\" head entity \": \"The Godfather\", \" tail entity \": \"Mario Puzo\", \" relation type \": \"screenwriter\", \" reasoning explanation \": \"Mario Puzo was another co-writer of the screenplay for …\", \" index of supporting sentence \": 2 }, … ]} Figure 4: A sample of the proposed chain-of-retrieval.",
            "The Godfather 7, we can observe that the performance of different unseen relation types significantly improves with the denoised synthetic data on both Re-DocRED and DocRED datasets.",
            "It can be observed that our GenRDK is able to reduce label noises in synthetic data by 1) Adding correct relational facts by the cross-document knowledge graph, such as the triplets (The Godfather, Francis Ford Coppola, screenwriter) and (Michael Jordan, Chicago Bulls, member of sports team) ; 2) Reducing the false relational facts by the consistency of knowledge, such as the triplet (Michael Jordan, National Basketball Association, member of sports team) .",
            "Although previous approaches (b) Document-level Generation [1]The Godfatherisa1972AmericanfilmdirectedbyFrancis Ford Coppola.",
            "As shown in Table 4, it can be observed that all backbone models [1] The Godfather is a 1972 American film directed by Francis Ford Coppola.",
            "One is reducing the incorrect triplet as shown in the red dotted line (The Godfather, Vito Corleone, cast member) , and another is adding the missing triplet as shown in the green solid line (The Godfather, Francis Ford Coppola, screenwriter) .",
            ":[{“ entity ”: “The Godfather” “ entity type ”: “Miscellaneous”}, {“ entity ”: “Francis Ford Coppola” “ entity type ”: “Person”}, {“ entity ”: “Mario Puzo” “ entity type ”: “Person”}, {“ entity ”: “Marlon Brando” “ entity type ”: “Person”}, {“ entity ”: “Al Pacino” “ entity type ”: “Person”}, {“ entity ”: “James Caan” “ entity type ”: “Person”},"
          ],
          "intents": [
            "--",
            "--",
            "--",
            "['result']",
            "--",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
            "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
            "year": 2017,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "37423160",
                "name": "Suncong Zheng"
              },
              {
                "authorId": "2145756722",
                "name": "Feng Wang"
              },
              {
                "authorId": "2682574",
                "name": "Hongyun Bao"
              },
              {
                "authorId": "8361912",
                "name": "Yuexing Hao"
              },
              {
                "authorId": "144032121",
                "name": "P. Zhou"
              },
              {
                "authorId": "2109511511",
                "name": "Bo Xu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53250562,
          "isinfluential": false,
          "contexts": [
            "Although previous approaches (b) Document-level Generation [1]The Godfatherisa1972AmericanfilmdirectedbyFrancis Ford Coppola."
          ],
          "intents": [
            "['result']"
          ],
          "cited_paper_info": {
            "title": "A Hierarchical Framework for Relation Extraction with Reinforcement Learning",
            "abstract": "Most existing methods determine relation types only after all the entities have been recognized, thus the interaction between relation types and entity mentions is not fully modeled. This paper presents a novel paradigm to deal with relation extraction by regarding the related entities as the arguments of a relation. We apply a hierarchical reinforcement learning (HRL) framework in this paradigm to enhance the interaction between entity mentions and relation types. The whole extraction process is decomposed into a hierarchy of two-level RL policies for relation detection and entity extraction respectively, so that it is more feasible and natural to deal with overlapping relations. Our model was evaluated on public datasets collected via distant supervision, and results show that it gains better performance than existing methods and is more powerful for extracting overlapping relations1.",
            "year": 2018,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "51055574",
                "name": "Ryuichi Takanobu"
              },
              {
                "authorId": "50615630",
                "name": "Tianyang Zhang"
              },
              {
                "authorId": "3486119",
                "name": "Jiexi Liu"
              },
              {
                "authorId": "1730108",
                "name": "Minlie Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53607073,
          "isinfluential": false,
          "contexts": [
            "In the case of document-level synthetic data, there are more than 22 relation triplets distributed across different sentences.",
            "Next, we construct cross-document knowledge graphs according to the pseudo labels and original labels of synthetic data."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Zero-shot Relation Classification as Textual Entailment",
            "abstract": "We consider the task of relation classification, and pose this task as one of textual entailment. We show that this formulation leads to several advantages, including the ability to (i) perform zero-shot relation classification by exploiting relation descriptions, (ii) utilize existing textual entailment models, and (iii) leverage readily available textual entailment datasets, to enhance the performance of relation classification systems. Our experiments show that the proposed approach achieves 20.16% and 61.32% in F1 zero-shot classification performance on two datasets, which further improved to 22.80% and 64.78% respectively with the use of conditional encoding.",
            "year": 2018,
            "venue": "FEVER@EMNLP",
            "authors": [
              {
                "authorId": "22313325",
                "name": "A. Obamuyide"
              },
              {
                "authorId": "2064056928",
                "name": "Andreas Vlachos"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221996144,
          "isinfluential": false,
          "contexts": [
            "The main contributions of our work are summarized as follows: • We explore [12, 14, 31, 38] and the graph-based [3, 7, 18, 20, 24, 27, 28, 32, 33] models to extract contextual and non-local structural information for aggregating entity representations [12, 14, 31, 38]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entity-level graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at this https URL .",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "48486877",
                "name": "Shuang Zeng"
              },
              {
                "authorId": "1748844142",
                "name": "Runxin Xu"
              },
              {
                "authorId": "7267809",
                "name": "Baobao Chang"
              },
              {
                "authorId": "143900005",
                "name": "Lei Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 225039888,
          "isinfluential": false,
          "contexts": [
            "The main contributions of our work are summarized as follows: • We explore [12, 14, 31, 38] and the graph-based [3, 7, 18, 20, 24, 27, 28, 32, 33] models to extract contextual and non-local structural information for aggregating entity representations [12, 14, 31, 38].",
            "We present extensive experimental results of different popular DocRE backbone models [30, 38] trained on original and denoised synthetic data."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling",
            "abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4, and also significantly outperforms existing models on both CDR and GDA. We have released our code at https://github.com/wzhouad/ATLOP.",
            "year": 2020,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2203076",
                "name": "Wenxuan Zhou"
              },
              {
                "authorId": "152530947",
                "name": "Kevin Huang"
              },
              {
                "authorId": "1901958",
                "name": "Tengyu Ma"
              },
              {
                "authorId": "30768523",
                "name": "Jing Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 233210556,
          "isinfluential": false,
          "contexts": [
            "[2]ThescreenplaywaswrittenbyCoppolaandMario Puzo,basedonPuzo‘sbest-sell… [3]ThefilmstarsMarlon BrandoasthepatriarchoftheCorleonefamily,withAl Pacino,James Caan,Richard S .",
            "Next, we construct cross-document knowledge graphs according to the pseudo labels and original labels of synthetic data."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning",
            "abstract": "While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, Zero-Shot BERT (ZS-BERT), to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their seen relations, ZS-BERT learns two functions that project sentences and relations into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54% improvement on F1 score.",
            "year": 2021,
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2109675427",
                "name": "Chih-Yao Chen"
              },
              {
                "authorId": "2169355",
                "name": "Cheng-te Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235313469,
          "isinfluential": false,
          "contexts": [
            "The main contributions of our work are summarized as follows: • We explore [12, 14, 31, 38] and the graph-based [3, 7, 18, 20, 24, 27, 28, 32, 33] models to extract contextual and non-local structural information for aggregating entity representations [12, 14, 31, 38]."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Discriminative Reasoning for Document-level Relation Extraction",
            "abstract": "Document-level relation extraction (DocRE) models generally use graph networks to implicitly model the reasoning skill (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the relation between one entity pair in a document. In this paper, we propose a novel discriminative reasoning framework to explicitly model the paths of these reasoning skills between each entity pair in this document. Thus, a discriminative reasoning network is designed to estimate the relation probability distribution of different reasoning paths based on the constructed graph and vectorized document contexts for each entity pair, thereby recognizing their relation. Experimental results show that our method outperforms the previous state-of-the-art performance on the large-scale DocRE dataset. The code is publicly available at https://github.com/xwjim/DRN.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2110725401",
                "name": "Wang Xu"
              },
              {
                "authorId": "2152954660",
                "name": "Kehai Chen"
              },
              {
                "authorId": "145382463",
                "name": "T. Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 238583580,
          "isinfluential": false,
          "contexts": [
            "However, synthetic data for document-level relation triplet extraction usually contain complex semantic structures and various relation triplets."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
            "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.",
            "year": 2021,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "6215698",
                "name": "Junxian He"
              },
              {
                "authorId": "2384711",
                "name": "Chunting Zhou"
              },
              {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
              },
              {
                "authorId": "1400419309",
                "name": "Taylor Berg-Kirkpatrick"
              },
              {
                "authorId": "1700325",
                "name": "Graham Neubig"
              }
            ]
          }
        },
        {
          "citedcorpusid": 246897443,
          "isinfluential": false,
          "contexts": [
            "Relation Triplet Extraction (RTE) aims to extract the entity pair and the semantic relation type from the unstructured text, which plays a vital role in various downstream Natural Language Processing (NLP) applications, including knowledge graph construction and information retrieval [15, 22, 29]."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "TransO: a knowledge-driven representation learning method with ontology information constraints",
            "abstract": "",
            "year": 2022,
            "venue": "World wide web (Bussum)",
            "authors": [
              {
                "authorId": "48459088",
                "name": "Zhao Li"
              },
              {
                "authorId": "2146074361",
                "name": "Xin Liu"
              },
              {
                "authorId": "2153691544",
                "name": "Xin Wang"
              },
              {
                "authorId": "152814510",
                "name": "Pengkai Liu"
              },
              {
                "authorId": "2152571428",
                "name": "Yuxin Shen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 259108325,
          "isinfluential": false,
          "contexts": [
            "[2]ThescreenplaywaswrittenbyCoppolaandMario Puzo,basedonPuzo‘sbest-sell… [3]ThefilmstarsMarlon BrandoasthepatriarchoftheCorleonefamily,withAl Pacino,James Caan,Richard S .",
            "Next, we construct cross-document knowledge graphs according to the pseudo labels and original labels of synthetic data."
          ],
          "intents": [
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction",
            "abstract": "Semantic matching is a mainstream paradigm of zero-shot relation extraction, which matches a given input with a corresponding label description. The entities in the input should exactly match their hypernyms in the description, while the irrelevant contexts should be ignored when matching.However, general matching methods lack explicit modeling of the above matching pattern. In this work, we propose a fine-grained semantic matching method tailored for zero-shot relation extraction. Guided by the above matching pattern, we decompose the sentence-level similarity score into the entity matching score and context matching score. Considering that not all contextual words contribute equally to the relation semantics, we design a context distillation module to reduce the negative impact of irrelevant components on context matching. Experimental results show that our method achieves higher matching accuracy and more than 10 times faster inference speed, compared with the state-of-the-art methods.",
            "year": 2023,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2145804572",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2187461272",
                "name": "Wenyu Zhan"
              },
              {
                "authorId": "2183015489",
                "name": "Xin Zhao"
              },
              {
                "authorId": "47835189",
                "name": "Qi Zhang"
              },
              {
                "authorId": "2067331064",
                "name": "Tao Gui"
              },
              {
                "authorId": "2118602528",
                "name": "Zhongyu Wei"
              },
              {
                "authorId": null,
                "name": "Junzhe Wang"
              },
              {
                "authorId": "24859244",
                "name": "Minlong Peng"
              },
              {
                "authorId": "2219726324",
                "name": "Mingming Sun"
              }
            ]
          }
        }
      ]
    },
    "12451195": {
      "citing_paper_info": {
        "title": "Document-level adverse drug reaction event extraction on electronic health records in Spanish",
        "abstract": "We outline an Adverse Drug Reaction (ADRs) extraction system for Electronic Health Records (EHRs) written in Spanish. The goal of the system is to assist experts on pharmacy in making the decision of whether a patient suffers from one or more ADRs. The core of the system is a predictive model inferred from a manually tagged corpus that counts on both semantic and syntactically features. This model is able to extract ADRs from disease-drug pairs in a given EHR. Finally, the ADRs automatically extracted are post-processed using a heuristic to present the information in a compact way. This stage reports the drugs and diseases of the document together with their frequency, and it also links the pairs related as ADRs. In brief, the system not only presents the ADRs in the text but also provides concise information on request by experts in pharmacy (the potential users of the system). Keywords: Event Extraction; Adverse Drug Reactions; Text Mining.",
        "year": 2016,
        "venue": "Proces. del Leng. Natural",
        "authors": [
          {
            "authorId": "3375366",
            "name": "Sara Santiso"
          },
          {
            "authorId": "145483491",
            "name": "Arantza Casillas"
          },
          {
            "authorId": "2110387414",
            "name": "Alicia Pérez"
          },
          {
            "authorId": "1718560",
            "name": "M. Oronoz"
          },
          {
            "authorId": "1748238",
            "name": "Koldo Gojenola"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 4,
        "unique_cited_count": 4,
        "influential_count": 1,
        "detailed_records_count": 4
      },
      "cited_papers": [
        "2626916",
        "10557799",
        "13861754",
        "14886018"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2626916,
          "isinfluential": false,
          "contexts": [
            "As a general-purpose analyser would be of little use, due to the use of medical language, we resorted to FreeLing-Med, an analyser adapted to the clinical domain, operating both in Spanish and English (Oronoz et al., 2013; Gojenola et al., 2014).",
            "Most of the analysed studies had English as their target language, and fewer works have been carried out for other languages.",
            "As a second challenge, we should mention the fact that the EHRs are written in Spanish and so far the clinical literature has focused primarily in English while there are some preliminary works, as well, in Spanish social media (de la Peña et al., 2014; Segura-Bedmar et al., 2015)."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "IxaMed: Applying Freeling and a Perceptron Sequential Tagger at the Shared Task on Analyzing Clinical Texts",
            "abstract": "This paper presents the results of the IxaMed team at the SemEval-2014 Shared Task 7 on Analyzing Clinical Texts. We have developed three different systems based on: a) exact match, b) a general-purpose morphosyntactic analyzer enriched with the SNOMED CT terminology content, and c) a perceptron sequential tagger based on a Global Linear Model. The three individual systems result in similar f-score while they vary in their precision and recall. We have also tried direct combinations of the individual systems, obtaining considerable improvements in performance.",
            "year": 2014,
            "venue": "International Workshop on Semantic Evaluation",
            "authors": [
              {
                "authorId": "1748238",
                "name": "Koldo Gojenola"
              },
              {
                "authorId": "1718560",
                "name": "M. Oronoz"
              },
              {
                "authorId": "2110387414",
                "name": "Alicia Pérez"
              },
              {
                "authorId": "145483491",
                "name": "Arantza Casillas"
              }
            ]
          }
        },
        {
          "citedcorpusid": 10557799,
          "isinfluential": false,
          "contexts": [
            "…al. (2009) presented the steps taken towards an automated processing of clinical Finnish, focusing on daily nursing notes in a Finnish Intensive Care Unit (ICU). de la Peña et al. (2014) and Segura-Bedmar et al. (2015) wrote some works that we are aware of that tackle adverse effects for Spanish.",
            "As a second challenge, we should mention the fact that the EHRs are written in Spanish and so far the clinical literature has focused primarily in English while there are some preliminary works, as well, in Spanish social media (de la Peña et al., 2014; Segura-Bedmar et al., 2015)."
          ],
          "intents": [
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "ADRSpanishTool: a tool for extracting adverse drug reactions and indications",
            "abstract": "on de Informaci on, Medios Sociales Abstract: We present a tool based on co-occurrences of drug-eect pairs to detect adverse drug reactions and drug indications from user messages that were collected from an online Spanish health forum. In addition, we also describe the automatic construction of the",
            "year": 2014,
            "venue": "Proces. del Leng. Natural",
            "authors": [
              {
                "authorId": "12580482",
                "name": "S. Peña"
              },
              {
                "authorId": "1401373532",
                "name": "Isabel Segura-Bedmar"
              },
              {
                "authorId": "144646131",
                "name": "Paloma Martínez"
              },
              {
                "authorId": "1401521973",
                "name": "J. Martínez-Fernández"
              }
            ]
          }
        },
        {
          "citedcorpusid": 13861754,
          "isinfluential": true,
          "contexts": [
            "6.9 libraries (Hall et al., 2009)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "The WEKA data mining software: an update",
            "abstract": "",
            "year": 2009,
            "venue": "SKDD",
            "authors": [
              {
                "authorId": "118860642",
                "name": "M. Hall"
              },
              {
                "authorId": "1767318",
                "name": "E. Frank"
              },
              {
                "authorId": "144282963",
                "name": "G. Holmes"
              },
              {
                "authorId": "1737420",
                "name": "Bernhard Pfahringer"
              },
              {
                "authorId": "2840271",
                "name": "P. Reutemann"
              },
              {
                "authorId": "9419406",
                "name": "I. Witten"
              }
            ]
          }
        },
        {
          "citedcorpusid": 14886018,
          "isinfluential": false,
          "contexts": [
            "This proposal was put in practice by Wang et al. (2009) for narrative reports."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Active computerized pharmacovigilance using natural language processing, statistics, and electronic health records: a feasibility study.",
            "abstract": "OBJECTIVE It is vital to detect the full safety profile of a drug throughout its market life. Current pharmacovigilance systems still have substantial limitations, however. The objective of our work is to demonstrate the feasibility of using natural language processing (NLP), the comprehensive Electronic Health Record (EHR), and association statistics for pharmacovigilance purposes. DESIGN Narrative discharge summaries were collected from the Clinical Information System at New York Presbyterian Hospital (NYPH). MedLEE, an NLP system, was applied to the collection to identify medication events and entities which could be potential adverse drug events (ADEs). Co-occurrence statistics with adjusted volume tests were used to detect associations between the two types of entities, to calculate the strengths of the associations, and to determine their cutoff thresholds. Seven drugs/drug classes (ibuprofen, morphine, warfarin, bupropion, paroxetine, rosiglitazone, ACE inhibitors) with known ADEs were selected to evaluate the system. RESULTS One hundred thirty-two potential ADEs were found to be associated with the 7 drugs. Overall recall and precision were 0.75 and 0.31 for known ADEs respectively. Importantly, qualitative evaluation using historic roll back design suggested that novel ADEs could be detected using our system. CONCLUSIONS This study provides a framework for the development of active, high-throughput and prospective systems which could potentially unveil drug safety profiles throughout their entire market life. Our results demonstrate that the framework is feasible although there are some challenging issues. To the best of our knowledge, this is the first study using comprehensive unstructured data from the EHR for pharmacovigilance.",
            "year": 2009,
            "venue": "JAMIA Journal of the American Medical Informatics Association",
            "authors": [
              {
                "authorId": "2118775737",
                "name": "Xiaoyan Wang"
              },
              {
                "authorId": "1686114",
                "name": "G. Hripcsak"
              },
              {
                "authorId": "1806164",
                "name": "M. Markatou"
              },
              {
                "authorId": "145133587",
                "name": "C. Friedman"
              }
            ]
          }
        }
      ]
    },
    "247939302": {
      "citing_paper_info": {
        "title": "A sequence-to-sequence approach for document-level relation extraction",
        "abstract": "Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https://github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.",
        "year": 2022,
        "venue": "Workshop on Biomedical Natural Language Processing",
        "authors": [
          {
            "authorId": "37585306",
            "name": "John Giorgi"
          },
          {
            "authorId": "144937305",
            "name": "Gary D Bader"
          },
          {
            "authorId": "2153212215",
            "name": "Bo Wang"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 22,
        "unique_cited_count": 22,
        "influential_count": 3,
        "detailed_records_count": 22
      },
      "cited_papers": [
        "244119359",
        "221738957",
        "218613850",
        "51870827",
        "208248243",
        "231879840",
        "260429228",
        "52115592",
        "235358786",
        "231750020",
        "209515730",
        "226283579",
        "226237654",
        "204960716",
        "8174613",
        "7961699",
        "204838007",
        "2797612",
        "5112317",
        "220919723",
        "2003493",
        "52118895"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2003493,
          "isinfluential": true,
          "contexts": [
            "…entity annotations may be overly optimistic and corroborates previous work demonstrating the beneﬁts of jointly learning entity and relation extraction (Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016; Li et al., 2016a, 2017; Nguyen and Verspoor, 2019a; Yu et al., 2020).",
            "…we show in §5.2, jointly learning to extract entities and relations can improve performance (Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016; Li et al., 2016a, 2017; Nguyen and Verspoor, 2019a; Yu et al., 2020) and may be more efﬁcient due to shared parameters and training steps."
          ],
          "intents": [
            "['result']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "A neural joint model for entity and relation extraction from biomedical text",
            "abstract": "Extracting biomedical entities and their relations from text has important applications on biomedical research. Previous work primarily utilized feature-based pipeline models to process this task. Many efforts need to be made on feature engineering when feature-based models are employed. Moreover, pipeline models may suffer error propagation and are not able to utilize the interactions between subtasks. Therefore, we propose a neural joint model to extract biomedical entities as well as their relations simultaneously, and it can alleviate the problems above. Our model was evaluated on two tasks, i.e., the task of extracting adverse drug events between drug and disease entities, and the task of extracting resident relations between bacteria and location entities. Compared with the state-of-the-art systems in these tasks, our model improved the F1 scores of the first task by 5.1% in entity recognition and 8.0% in relation extraction, and that of the second task by 9.2% in relation extraction. The proposed model achieves competitive performances with less work on feature engineering. We demonstrate that the model based on neural networks is effective for biomedical entity and relation extraction. In addition, parameter sharing is an alternative method for neural models to jointly process this task. Our work can facilitate the research on biomedical text mining.",
            "year": 2017,
            "venue": "BMC Bioinformatics",
            "authors": [
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              },
              {
                "authorId": "2678094",
                "name": "Meishan Zhang"
              },
              {
                "authorId": "2059275",
                "name": "G. Fu"
              },
              {
                "authorId": "1719916",
                "name": "D. Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 2797612,
          "isinfluential": false,
          "contexts": [
            "A popular approach involves graph-based methods, which have the advantage of naturally modelling inter-sentence relations (Peng et al., 2017; Song et al., 2018; Christopoulou et al., 2019; Nan et al., 2020; Minh Tran et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs",
            "abstract": "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
            "year": 2017,
            "venue": "Transactions of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3157053",
                "name": "Nanyun Peng"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              },
              {
                "authorId": "2596310",
                "name": "Chris Quirk"
              },
              {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
              },
              {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
              }
            ]
          }
        },
        {
          "citedcorpusid": 5112317,
          "isinfluential": false,
          "contexts": [
            "Most approaches are restricted to intra-sentence RE (Bekoulis et al., 2018; Luan et al., 2018; Nguyen and Verspoor, 2019b; Wadden et al., 2019; Giorgi et al., 2019) and have only recently been extended to DocRE (Eberts and Ulges, 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint entity recognition and relation extraction as a multi-head selection problem",
            "abstract": "",
            "year": 2018,
            "venue": "Expert systems with applications",
            "authors": [
              {
                "authorId": "8859751",
                "name": "Giannis Bekoulis"
              },
              {
                "authorId": "2630759",
                "name": "Johannes Deleu"
              },
              {
                "authorId": "1388296896",
                "name": "Thomas Demeester"
              },
              {
                "authorId": "2489892",
                "name": "Chris Develder"
              }
            ]
          }
        },
        {
          "citedcorpusid": 7961699,
          "isinfluential": false,
          "contexts": [
            "A less popular end-to-end approach is to frame RE as a generative task with sequence-to-sequence (seq2seq) learning (Sutskever et al., 2014)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Sequence to Sequence Learning with Neural Networks",
            "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
            "year": 2014,
            "venue": "Neural Information Processing Systems",
            "authors": [
              {
                "authorId": "1701686",
                "name": "I. Sutskever"
              },
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "2827616",
                "name": "Quoc V. Le"
              }
            ]
          }
        },
        {
          "citedcorpusid": 8174613,
          "isinfluential": false,
          "contexts": [
            "Therefore, we follow previous work (Gu et al., 2016b, 2017; Verga et al., 2018; Christopoulou et al., 2019; Zhou et al., 2021) by ﬁltering negative relations like these, with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract, according to…",
            "To enable copying of input tokens during decoding, we use a copying mechanism (Gu et al., 2016a)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
            "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.",
            "year": 2016,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3016273",
                "name": "Jiatao Gu"
              },
              {
                "authorId": "11955007",
                "name": "Zhengdong Lu"
              },
              {
                "authorId": "49404233",
                "name": "Hang Li"
              },
              {
                "authorId": "2052674293",
                "name": "V. Li"
              }
            ]
          }
        },
        {
          "citedcorpusid": 51870827,
          "isinfluential": true,
          "contexts": [
            "Our use of the copy mechanism is similar to previous seq2seq-based approaches for RE (Zeng et al., 2018, 2020).",
            "CopyRE (Zeng et al., 2018) uses an encoder-decoder architecture with a copy mechanism, similar to our approach, but is restricted to intra-sentence relations.",
            "CopyRE (Zeng et al., 2018) uses an encoder-decoder architecture with a copy mech-",
            "However, existing work stops short, focusing on intra-sentence binary relations (Zeng et al., 2018; Zhang et al., 2020; Nayak and Ng, 2020; Zeng et al., 2020)."
          ],
          "intents": [
            "['methodology']",
            "['methodology']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism",
            "abstract": "The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2441459",
                "name": "Xiangrong Zeng"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "1954845",
                "name": "Shizhu He"
              },
              {
                "authorId": "2200096",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52115592,
          "isinfluential": false,
          "contexts": [
            "A popular approach involves graph-based methods, which have the advantage of naturally modelling inter-sentence relations (Peng et al., 2017; Song et al., 2018; Christopoulou et al., 2019; Nan et al., 2020; Minh Tran et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "N-ary Relation Extraction using Graph-State LSTM",
            "abstract": "Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "1748796",
                "name": "Linfeng Song"
              },
              {
                "authorId": null,
                "name": "Yue Zhang"
              },
              {
                "authorId": "40296541",
                "name": "Zhiguo Wang"
              },
              {
                "authorId": "1793218",
                "name": "D. Gildea"
              }
            ]
          }
        },
        {
          "citedcorpusid": 52118895,
          "isinfluential": false,
          "contexts": [
            "Most approaches are restricted to intra-sentence RE (Bekoulis et al., 2018; Luan et al., 2018; Nguyen and Verspoor, 2019b; Wadden et al., 2019; Giorgi et al., 2019) and have only recently been extended to DocRE (Eberts and Ulges, 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
            "abstract": "We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",
            "year": 2018,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "145081697",
                "name": "Yi Luan"
              },
              {
                "authorId": "2265599",
                "name": "Luheng He"
              },
              {
                "authorId": "144339506",
                "name": "Mari Ostendorf"
              },
              {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204838007,
          "isinfluential": false,
          "contexts": [
            "Several recent works, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), have proposed pretraining strategies for entire encoder-decoder architectures, which can be ﬁne-tuned on downstream tasks.",
            "Several recent works, such as T5 (Raffel et al., 2020) and BART (Lewis et al.",
            "9 More generally, our paper is related to a recently proposed “text-to-text” framework (Raffel et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "year": 2019,
            "venue": "Journal of machine learning research",
            "authors": [
              {
                "authorId": "2402716",
                "name": "Colin Raffel"
              },
              {
                "authorId": "1846258",
                "name": "Noam M. Shazeer"
              },
              {
                "authorId": "145625142",
                "name": "Adam Roberts"
              },
              {
                "authorId": "3844009",
                "name": "Katherine Lee"
              },
              {
                "authorId": "46617804",
                "name": "Sharan Narang"
              },
              {
                "authorId": "1380243217",
                "name": "Michael Matena"
              },
              {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
              },
              {
                "authorId": "2157338362",
                "name": "Wei Li"
              },
              {
                "authorId": "35025299",
                "name": "Peter J. Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": false,
          "contexts": [
            "Several recent works, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), have proposed pretraining strategies for entire encoder-decoder architectures, which can be ﬁne-tuned on downstream tasks."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 208248243,
          "isinfluential": false,
          "contexts": [
            "A similar approach was published concurrently but was again limited to intra-sentence binary relations (Nayak and Ng, 2020).",
            "However, existing work stops short, focusing on intra-sentence binary relations (Zeng et al., 2018; Zhang et al., 2020; Nayak and Ng, 2020; Zeng et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction",
            "abstract": "A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.",
            "year": 2019,
            "venue": "AAAI Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "144329677",
                "name": "Tapas Nayak"
              },
              {
                "authorId": "34789794",
                "name": "H. Ng"
              }
            ]
          }
        },
        {
          "citedcorpusid": 209515730,
          "isinfluential": false,
          "contexts": [
            "Most approaches are restricted to intra-sentence RE (Bekoulis et al., 2018; Luan et al., 2018; Nguyen and Verspoor, 2019b; Wadden et al., 2019; Giorgi et al., 2019) and have only recently been extended to DocRE (Eberts and Ulges, 2021)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "End-to-end Named Entity Recognition and Relation Extraction using Pre-trained Language Models",
            "abstract": "Named entity recognition (NER) and relation extraction (RE) are two important tasks in information extraction and retrieval (IE \\& IR). Recent work has demonstrated that it is beneficial to learn these tasks jointly, which avoids the propagation of error inherent in pipeline-based systems and improves performance. However, state-of-the-art joint models typically rely on external natural language processing (NLP) tools, such as dependency parsers, limiting their usefulness to domains (e.g. news) where those tools perform well. The few neural, end-to-end models that have been proposed are trained almost completely from scratch. In this paper, we propose a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. Because the bulk of our model's parameters are pre-trained and we eschew recurrence for self-attention, our model is fast to train. On 5 datasets across 3 domains, our model matches or exceeds state-of-the-art performance, sometimes by a large margin.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "37585306",
                "name": "John Giorgi"
              },
              {
                "authorId": "2108048327",
                "name": "Xindi Wang"
              },
              {
                "authorId": "2067001450",
                "name": "Nicola Sahar"
              },
              {
                "authorId": "1471879558",
                "name": "W. Shin"
              },
              {
                "authorId": "144937305",
                "name": "Gary D Bader"
              },
              {
                "authorId": "2153212215",
                "name": "Bo Wang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 218613850,
          "isinfluential": false,
          "contexts": [
            "A popular approach involves graph-based methods, which have the advantage of naturally modelling inter-sentence relations (Peng et al., 2017; Song et al., 2018; Christopoulou et al., 2019; Nan et al., 2020; Minh Tran et al., 2020).",
            "• Nan et al. (2020) propose LSR (Latent Structure Reﬁnement)."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
            "year": 2020,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2056582888",
                "name": "Guoshun Nan"
              },
              {
                "authorId": "2681038",
                "name": "Zhijiang Guo"
              },
              {
                "authorId": "3305422",
                "name": "Ivan Sekulic"
              },
              {
                "authorId": "2153424287",
                "name": "Wei Lu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 220919723,
          "isinfluential": false,
          "contexts": [
            "When training and evaluating on biomedical corpora, we use PubMedBERT (Gu et al., 2020), and BERT BASE (Devlin et al., 2019) otherwise.",
            "Where Jia et al. (2019) use a BiLSTM that is trained from scratch, we use PubMedBERT, a much larger model that has been pretrained on abstracts and full-text ar-Table",
            "When training and evaluating on biomedical corpora, we use PubMedBERT (Gu et al., 2020), and BERTBASE (Devlin et al."
          ],
          "intents": [
            "['methodology']",
            "--",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB.",
            "year": 2020,
            "venue": "ACM Trans. Comput. Heal.",
            "authors": [
              {
                "authorId": "2112677245",
                "name": "Yu Gu"
              },
              {
                "authorId": "1846722967",
                "name": "Robert Tinn"
              },
              {
                "authorId": "47413820",
                "name": "Hao Cheng"
              },
              {
                "authorId": "152277155",
                "name": "Michael R. Lucas"
              },
              {
                "authorId": "2637252",
                "name": "N. Usuyama"
              },
              {
                "authorId": "46522098",
                "name": "Xiaodong Liu"
              },
              {
                "authorId": "40466858",
                "name": "Tristan Naumann"
              },
              {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 221738957,
          "isinfluential": false,
          "contexts": [
            "Previous work has addressed this issue with various strategies, including reinforcement learning (Zeng et al., 2019), unordered-multi-tree decoders (Zhang et al., 2020), and non-autoregressive decoders (Sui et al., 2020).",
            ", 2019), unordered-multi-tree decoders (Zhang et al., 2020), and non-autoregressive de-",
            "However, existing work stops short, focusing on intra-sentence binary relations (Zeng et al., 2018; Zhang et al., 2020; Nayak and Ng, 2020; Zeng et al., 2020)."
          ],
          "intents": [
            "--",
            "--",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation Extraction",
            "abstract": "Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unordered triplets and involves a large decoding length associated with error accumulation. These methods introduce exposure bias, which may cause the models overfit to the frequent label combination, thus limiting the generalization ability. We propose a novel Sequence-to-Unordered-Multi-Tree (Seq2UMTree) model to minimize the effects of exposure bias by limiting the decoding length to three within a triplet and removing the order among triplets. We evaluate our model on two datasets, DuIE and NYT, and systematically study how exposure bias alters the performance of Seq2Seq models. Experiments show that the state-of-the-art Seq2Seq model overfits to both datasets while Seq2UMTree shows significantly better generalization. Our code is available at https://github.com/WindChimeRan/OpenJERE.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "46702624",
                "name": "H. Zhang"
              },
              {
                "authorId": "4289746",
                "name": "Qianying Liu"
              },
              {
                "authorId": "1945651182",
                "name": "Aysa Xuemo Fan"
              },
              {
                "authorId": "2113323573",
                "name": "Heng Ji"
              },
              {
                "authorId": "1796706",
                "name": "Daojian Zeng"
              },
              {
                "authorId": "49412583",
                "name": "Fei Cheng"
              },
              {
                "authorId": "2368642",
                "name": "Daisuke Kawahara"
              },
              {
                "authorId": "1795664",
                "name": "S. Kurohashi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226237654,
          "isinfluential": false,
          "contexts": [
            "Previous work has addressed this issue with various strategies, including reinforcement learning (Zeng et al., 2019), unordered-multi-tree decoders (Zhang et al., 2020), and non-autoregressive decoders (Sui et al., 2020)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Joint Entity and Relation Extraction With Set Prediction Networks",
            "abstract": "Joint entity and relation extraction is an important task in natural language processing, which aims to extract all relational triples mentioned in a given sentence. In essence, the relational triples mentioned in a sentence are in the form of a set, which has no intrinsic order between elements and exhibits the permutation invariant feature. However, previous seq2seq-based models require sorting the set of relational triples into a sequence beforehand with some heuristic global rules, which destroys the natural set structure. In order to break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model is not burdened with predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. In contrast to autoregressive approaches that generate triples one by one in a specific order, the proposed networks are able to directly output the final set of relational triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions through bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Various experiments on two benchmark datasets demonstrate that our proposed model significantly outperforms the current state-of-the-art (SoTA) models. Training code and trained models are now publicly available at https://github.com/DianboWork/SPN4RE.",
            "year": 2020,
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "authors": [
              {
                "authorId": "1381062467",
                "name": "Dianbo Sui"
              },
              {
                "authorId": "152829071",
                "name": "Yubo Chen"
              },
              {
                "authorId": "77397868",
                "name": "Kang Liu"
              },
              {
                "authorId": "1390572170",
                "name": "Jun Zhao"
              },
              {
                "authorId": "2441459",
                "name": "Xiangrong Zeng"
              },
              {
                "authorId": "2035396",
                "name": "Shengping Liu"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226283579,
          "isinfluential": false,
          "contexts": [
            "A popular approach involves graph-based methods, which have the advantage of naturally modelling inter-sentence relations (Peng et al., 2017; Song et al., 2018; Christopoulou et al., 2019; Nan et al., 2020; Minh Tran et al., 2020).",
            "• Minh Tran et al. (2020) propose EoGANE (EoG model Augmented with Node Represen-tations), which extends the edge-orientated model proposed by Christopoulou et al. (2019) to include explicit node representations which are used during relation classiﬁcation."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction",
            "abstract": "The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for representation learning. However, this model does not capture the representations for the nodes in the graphs, thus preventing it from effectively encoding the specific and relevant information of the nodes for DRE. To address this issue, we propose to explicitly compute the representations for the nodes in the graph-based edge-oriented model for DRE. These node representations allow us to introduce two novel representation regularization mechanisms to improve the representation vectors for DRE. The experiments show that our model achieves state-of-the-art performance on two benchmark datasets.",
            "year": 2020,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "144356780",
                "name": "H. Tran"
              },
              {
                "authorId": "49035085",
                "name": "T. Nguyen"
              },
              {
                "authorId": "1811211",
                "name": "Thien Huu Nguyen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231750020,
          "isinfluential": false,
          "contexts": [
            "2 We experimented with the common approach of inserting marker tokens before and after each entity mention (Zhou and Chen, 2021) but found this to perform worse."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "An Improved Baseline for Sentence-level Relation Extraction",
            "abstract": "Sentence-level relation extraction (RE) aims at identifying the relationship between two entities in a sentence. Many efforts have been devoted to this problem, while the best performing methods are still far from perfect. In this paper, we revisit two problems that affect the performance of existing RE models, namely entity representation and noisy or ill-defined labels. Our improved RE baseline, incorporated with entity representations with typed markers, achieves an F1 of 74.6% on TACRED, significantly outperforms previous SOTA methods. Furthermore, the presented new baseline achieves an F1 of 91.1% on the refined Re-TACRED dataset, demonstrating that the pretrained language models (PLMs) achieve high performance on this task. We release our code to the community for future research.",
            "year": 2021,
            "venue": "AACL",
            "authors": [
              {
                "authorId": "2203076",
                "name": "Wenxuan Zhou"
              },
              {
                "authorId": "1998918",
                "name": "Muhao Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 231879840,
          "isinfluential": true,
          "contexts": [
            ", 2019) and have only recently been extended to DocRE (Eberts and Ulges, 2021).",
            "In Table 3 we compare to an existing end-to-end approach on DocRED, JEREX (Eberts and Ulges, 2021).",
            "We use the same split as previous end-to-end methods (Eberts and Ulges, 2021), which has 3,008 documents in the training set, 300 in the validation set and 700 in the test set3.",
            "We ﬁnd that although our model is arguably simpler (JEREX contains four task-speciﬁc sub-components, each with its own loss) it only slightly underperforms JEREX, mainly due to recall."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning",
            "abstract": "We present a joint model for entity-level relation extraction from documents. In contrast to other approaches - which focus on local intra-sentence mention pairs and thus require annotations on mention level - our model operates on entity level. To do so, a multi-task approach is followed that builds upon coreference resolution and gathers relevant signals via multi-instance learning with multi-level representations combining global entity and local mention information. We achieve state-of-the-art relation extraction results on the DocRED dataset and report the first entity-level end-to-end relation extraction results for future reference. Finally, our experimental results suggest that a joint approach is on par with task-specific learning, though more efficient due to shared parameters and training steps.",
            "year": 2021,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "21034061",
                "name": "Markus Eberts"
              },
              {
                "authorId": "1782440",
                "name": "A. Ulges"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235358786,
          "isinfluential": false,
          "contexts": [
            "This framework has recently been applied to biomedical literature to perform named entity recognition, relation extraction (binary, intra-sentence), natural language inference, and question answering (Phan et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "SciFive: a text-to-text transformer model for biomedical literature",
            "abstract": "In this report, we introduce SciFive, a domain-specific T5 model that has been pre-trained on large biomedical corpora. Our model outperforms the current SOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation, relation extraction, natural language inference, and question-answering. We show that text-generation methods have significant potential in a broad array of biomedical NLP tasks, particularly those requiring longer, more complex outputs. Our results support the exploration of more difficult text generation tasks and the development of new methods in this area",
            "year": 2021,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2066589762",
                "name": "Long Phan"
              },
              {
                "authorId": "1404222397",
                "name": "J. Anibal"
              },
              {
                "authorId": "2057078797",
                "name": "H. Tran"
              },
              {
                "authorId": "48539437",
                "name": "Shaurya Chanana"
              },
              {
                "authorId": "2107060508",
                "name": "Erol Bahadroglu"
              },
              {
                "authorId": "2093916650",
                "name": "Alec Peltekian"
              },
              {
                "authorId": "1398310427",
                "name": "G. Altan-Bonnet"
              }
            ]
          }
        },
        {
          "citedcorpusid": 244119359,
          "isinfluential": false,
          "contexts": [
            "cently, GenerativeRE (Cao and Ananiadou, 2021) proposed a novel copy mechanism to improve performance on multi-token entities.",
            "Most recently, GenerativeRE (Cao and Ananiadou, 2021) proposed a novel copy mechanism to improve performance on multi-token entities."
          ],
          "intents": [
            "['background']",
            "--"
          ],
          "cited_paper_info": {
            "title": "GenerativeRE: Incorporating a Novel Copy Mechanism and Pretrained Model for Joint Entity and Relation Extraction",
            "abstract": "Previous neural seq2seq models have shown the e ﬀ ectiveness for jointly extracting relation triplets. However, most of these models suf-fer from incompletion and disorder problems when they extract multi-token entities from input sentences. To tackle these problems, we propose a generative, multi-task learning framework, named GenerativeRE. We ﬁrstly propose a special entity labelling method on both input and output sequences. During the training stage, GenerativeRE ﬁne-tunes the pretrained generative model and learns the special entity labels simultaneously. During the inference stage, we propose a novel copy mechanism equipped with three mask strategies, to generate the most probable tokens by diminishing the scope of the model decoder. Experimental results show that our model achieves 4.6% and 0.9% F1 score im-provements over the current state-of-the-art methods in the NYT24 and NYT29 benchmark datasets respectively.",
            "year": 2021,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "84104199",
                "name": "Jiarun Cao"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 260429228,
          "isinfluential": false,
          "contexts": [
            "During training, this enforces an unnecessary decoding order and may make the model prone to overfit frequent token combinations in the training set (Vinyals et al., 2016; Yang et al., 2019).",
            "During training, this enforces an unnecessary decoding order and may make the model prone to overﬁt frequent token combinations in the training set (Vinyals et al., 2016; Yang et al., 2019)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Order Matters: Sequence to sequence for sets",
            "abstract": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.",
            "year": 2015,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1689108",
                "name": "O. Vinyals"
              },
              {
                "authorId": "1751569",
                "name": "Samy Bengio"
              },
              {
                "authorId": "1942300",
                "name": "M. Kudlur"
              }
            ]
          }
        }
      ]
    },
    "249191749": {
      "citing_paper_info": {
        "title": "Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction",
        "abstract": "Compared with traditional sentence-level relation extraction, document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However, most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features, and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result, the distinct semantics between the different mentions of an entity are overlooked. To address this problem, we propose RSMAN in this paper which performs selective attentions over different entity mentions with respect to candidate relations. In this manner, the flexible and relation-specific representations of entities are obtained which indeed benefit relation classification. Our extensive experiments upon two benchmark datasets show that our RSMAN can bring significant improvements for some backbone models to achieve state-of-the-art performance, especially when an entity have multiple mentions in the document.",
        "year": 2022,
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "2116034604",
            "name": "Jiaxin Yu"
          },
          {
            "authorId": "1944126000",
            "name": "Deqing Yang"
          },
          {
            "authorId": "2149512772",
            "name": "Shuyu Tian"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 6,
        "unique_cited_count": 6,
        "influential_count": 0,
        "detailed_records_count": 6
      },
      "cited_papers": [
        "277550631",
        "53592270",
        "5458500",
        "184487889",
        "235358168",
        "236477583"
      ],
      "citation_details": [
        {
          "citedcorpusid": 5458500,
          "isinfluential": false,
          "contexts": [
            "Wang et al. (2020) constructed a global heterogeneous graph and used a stacked R-GCN (Schlichtkrull et al., 2018) to encode the document information.",
            "(2020) constructed a global heterogeneous graph and used a stacked R-GCN (Schlichtkrull et al., 2018) to encode the document information."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Modeling Relational Data with Graph Convolutional Networks",
            "abstract": "Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
            "year": 2017,
            "venue": "Extended Semantic Web Conference",
            "authors": [
              {
                "authorId": "8804828",
                "name": "M. Schlichtkrull"
              },
              {
                "authorId": "41016725",
                "name": "Thomas Kipf"
              },
              {
                "authorId": "2789097",
                "name": "Peter Bloem"
              },
              {
                "authorId": "9965217",
                "name": "Rianne van den Berg"
              },
              {
                "authorId": "144889265",
                "name": "Ivan Titov"
              },
              {
                "authorId": "1678311",
                "name": "M. Welling"
              }
            ]
          }
        },
        {
          "citedcorpusid": 53592270,
          "isinfluential": false,
          "contexts": [
            "In addition, we adopted AdamW (Loshchilov and Hutter, 2018) as our optimizer and used learning rate linear schedule with warming up based on Huggingface’s Transformers (Wolf et al., 2019)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Decoupled Weight Decay Regularization",
            "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL",
            "year": 2017,
            "venue": "International Conference on Learning Representations",
            "authors": [
              {
                "authorId": "1678656",
                "name": "I. Loshchilov"
              },
              {
                "authorId": "144661829",
                "name": "F. Hutter"
              }
            ]
          }
        },
        {
          "citedcorpusid": 184487889,
          "isinfluential": false,
          "contexts": [
            "Recently, many scholars have paid more attention to document-level RE (Sahu et al., 2019; Yao et al., 2019) which aims to identify the relations of all entity pairs in a document, since it is more in demand than sentencelevel RE in various real scenarios."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network",
            "abstract": "Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3422905",
                "name": "Sunil Kumar Sahu"
              },
              {
                "authorId": "48810605",
                "name": "Fenia Christopoulou"
              },
              {
                "authorId": "1731657",
                "name": "Makoto Miwa"
              },
              {
                "authorId": "1881965",
                "name": "S. Ananiadou"
              }
            ]
          }
        },
        {
          "citedcorpusid": 235358168,
          "isinfluential": false,
          "contexts": [
            "To this end, previous RE models simply apply average pooling (Ye et al., 2020; Xu et al., 2021), max pooling (Li et al., 2021), or logsumexp pooling (Zhou et al., 2021; Zhang et al., 2021)."
          ],
          "intents": [
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Document-level Relation Extraction as Semantic Segmentation",
            "abstract": "Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.",
            "year": 2021,
            "venue": "International Joint Conference on Artificial Intelligence",
            "authors": [
              {
                "authorId": "2608639",
                "name": "Ningyu Zhang"
              },
              {
                "authorId": "153773882",
                "name": "Xiang Chen"
              },
              {
                "authorId": "2110972563",
                "name": "Xin Xie"
              },
              {
                "authorId": "152931849",
                "name": "Shumin Deng"
              },
              {
                "authorId": "2111727840",
                "name": "Chuanqi Tan"
              },
              {
                "authorId": "2108266952",
                "name": "Mosha Chen"
              },
              {
                "authorId": "2087380523",
                "name": "Fei Huang"
              },
              {
                "authorId": "2059080424",
                "name": "Luo Si"
              },
              {
                "authorId": "49178307",
                "name": "Huajun Chen"
              }
            ]
          }
        },
        {
          "citedcorpusid": 236477583,
          "isinfluential": false,
          "contexts": [
            ", 2021), max pooling (Li et al., 2021), or logsumexp pooling (Zhou et al.",
            "To this end, previous RE models simply apply average pooling (Ye et al., 2020; Xu et al., 2021), max pooling (Li et al., 2021), or logsumexp pooling (Zhou et al., 2021; Zhang et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction",
            "abstract": "Document-level relation extraction aims to detect the relations within one document, which is challenging since it requires complex reasoning using mentions, entities, local and global contexts. Few previous studies have distinguished local and global reasoning explicitly, which may be problematic because they play different roles in intra-and inter-sentence relations. Moreover, the interactions between local and global contexts should be considered since they could help relation reasoning based on our observation. In this paper, we pro-pose a novel mention-based reasoning (MRN) module based on explicitly and collaboratively local and global reasoning. Based on MRN, we design a co-predictor module to predict entity relations based on local and global entity and relation representations jointly. We evaluate our MRN model on three widely-used benchmark datasets, namely DocRED, CDR, and GDA. Experimental results show that our model outperforms previous state-of-the-art models by a large margin.",
            "year": 2021,
            "venue": "Findings",
            "authors": [
              {
                "authorId": "2000217741",
                "name": "Jingye Li"
              },
              {
                "authorId": "2113474255",
                "name": "Kang Xu"
              },
              {
                "authorId": "2109530930",
                "name": "Fei Li"
              },
              {
                "authorId": "46959445",
                "name": "Hao Fei"
              },
              {
                "authorId": "3350168",
                "name": "Yafeng Ren"
              },
              {
                "authorId": "1719916",
                "name": "D. Ji"
              }
            ]
          }
        },
        {
          "citedcorpusid": 277550631,
          "isinfluential": false,
          "contexts": [
            "In addition, we adopted AdamW (Loshchilov and Hutter, 2018) as our optimizer and used learning rate linear schedule with warming up based on Huggingface’s Transformers (Wolf et al., 2019)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "2257007291",
                "name": "Thomas Wolf"
              },
              {
                "authorId": "1380459402",
                "name": "Lysandre Debut"
              },
              {
                "authorId": "51918868",
                "name": "Victor Sanh"
              },
              {
                "authorId": "40811585",
                "name": "Julien Chaumond"
              },
              {
                "authorId": "40899333",
                "name": "Clement Delangue"
              },
              {
                "authorId": "1382164294",
                "name": "Anthony Moi"
              },
              {
                "authorId": "1382164165",
                "name": "Pierric Cistac"
              },
              {
                "authorId": "1382164170",
                "name": "Tim Rault"
              },
              {
                "authorId": "2185329",
                "name": "Rémi Louf"
              },
              {
                "authorId": "97662964",
                "name": "Morgan Funtowicz"
              },
              {
                "authorId": "48776237",
                "name": "Joe Davison"
              },
              {
                "authorId": "88728159",
                "name": "Sam Shleifer"
              },
              {
                "authorId": "138609838",
                "name": "Patrick von Platen"
              },
              {
                "authorId": "2257128341",
                "name": "Clara Ma"
              },
              {
                "authorId": "2268491803",
                "name": "Yacine Jernite"
              },
              {
                "authorId": "3008389",
                "name": "J. Plu"
              },
              {
                "authorId": "2257127518",
                "name": "Canwen Xu"
              },
              {
                "authorId": "1379806208",
                "name": "Teven Le Scao"
              },
              {
                "authorId": "103682620",
                "name": "Sylvain Gugger"
              },
              {
                "authorId": "2125818054",
                "name": "Mariama Drame"
              },
              {
                "authorId": "2113836945",
                "name": "Quentin Lhoest"
              },
              {
                "authorId": "2260132137",
                "name": "Alexander M. Rush"
              }
            ]
          }
        }
      ]
    },
    "248780117": {
      "citing_paper_info": {
        "title": "Dynamic Global Memory for Document-level Argument Extraction",
        "abstract": "Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.",
        "year": 2022,
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "authors": [
          {
            "authorId": "13728923",
            "name": "Xinya Du"
          },
          {
            "authorId": "2109154767",
            "name": "Sha Li"
          },
          {
            "authorId": "144016781",
            "name": "Heng Ji"
          }
        ]
      },
      "citation_summary": {
        "citation_count": 16,
        "unique_cited_count": 13,
        "influential_count": 1,
        "detailed_records_count": 16
      },
      "cited_papers": [
        "203701085",
        "247797575",
        "15359942",
        "226262410",
        "21700944",
        "189898081",
        "204960716",
        "207853145",
        "201646309",
        "189762527",
        "219683473",
        "131773936",
        "2114517"
      ],
      "citation_details": [
        {
          "citedcorpusid": 2114517,
          "isinfluential": false,
          "contexts": [
            "We consider an argument span to be correctly identiﬁed if its offsets match any of the gold/reference informative arguments of the current event (i.e., argument identiﬁcation); and it is correctly classiﬁed if its semantic role also matches (i.e., argument classiﬁcation) (Li et al., 2013)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Joint Event Extraction via Structured Prediction with Global Features",
            "abstract": "",
            "year": 2013,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2118912383",
                "name": "Qi Li"
              },
              {
                "authorId": "144016781",
                "name": "Heng Ji"
              },
              {
                "authorId": "144768480",
                "name": "Liang Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 15359942,
          "isinfluential": false,
          "contexts": [
            "Apart from event extraction, in the future, it’s worth investigating how to leverage the global memory idea for other document-level IE problems like (N ary) relation extraction (Quirk and Poon, 2017; Yao et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Distant Supervision for Relation Extraction beyond the Sentence Boundary",
            "abstract": "The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross-sentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.",
            "year": 2016,
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2596310",
                "name": "Chris Quirk"
              },
              {
                "authorId": "1759772",
                "name": "Hoifung Poon"
              }
            ]
          }
        },
        {
          "citedcorpusid": 21700944,
          "isinfluential": false,
          "contexts": [
            ", 2021) and ignores the global context partially because of the pretrained models’ length limit and their lack of attention for distant context (Khandelwal et al., 2018)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
            "abstract": "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",
            "year": 2018,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "3030219",
                "name": "Urvashi Khandelwal"
              },
              {
                "authorId": "144533687",
                "name": "He He"
              },
              {
                "authorId": "50531624",
                "name": "Peng Qi"
              },
              {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
              }
            ]
          }
        },
        {
          "citedcorpusid": 131773936,
          "isinfluential": false,
          "contexts": [
            "(Shi and Lin, 2019) is a popular baseline for semantic role labeling (predicate-argument prediction)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "abstract": "We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",
            "year": 2019,
            "venue": "arXiv.org",
            "authors": [
              {
                "authorId": "1884766",
                "name": "Peng Shi"
              },
              {
                "authorId": "145580839",
                "name": "Jimmy J. Lin"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189762527,
          "isinfluential": false,
          "contexts": [
            "Bosselut et al. (2019) use generative language models to generate new event knowledge based on crowdsourced triples."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction",
            "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "2691021",
                "name": "Antoine Bosselut"
              },
              {
                "authorId": "2516777",
                "name": "Hannah Rashkin"
              },
              {
                "authorId": "2729164",
                "name": "Maarten Sap"
              },
              {
                "authorId": "8805254",
                "name": "Chaitanya Malaviya"
              },
              {
                "authorId": "1709797",
                "name": "Asli Celikyilmaz"
              },
              {
                "authorId": "1699545",
                "name": "Yejin Choi"
              }
            ]
          }
        },
        {
          "citedcorpusid": 189898081,
          "isinfluential": false,
          "contexts": [
            "Apart from event extraction, in the future, it’s worth investigating how to leverage the global memory idea for other document-level IE problems like ( N -ary) relation extraction (Quirk and Poon, 2017; Yao et al., 2019)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset",
            "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "46461580",
                "name": "Yuan Yao"
              },
              {
                "authorId": "50816334",
                "name": "Deming Ye"
              },
              {
                "authorId": "144326610",
                "name": "Peng Li"
              },
              {
                "authorId": "48506411",
                "name": "Xu Han"
              },
              {
                "authorId": "2427350",
                "name": "Yankai Lin"
              },
              {
                "authorId": "49047064",
                "name": "Zhenghao Liu"
              },
              {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
              },
              {
                "authorId": "2110799018",
                "name": "Lixin Huang"
              },
              {
                "authorId": "49178343",
                "name": "Jie Zhou"
              },
              {
                "authorId": "1753344",
                "name": "Maosong Sun"
              }
            ]
          }
        },
        {
          "citedcorpusid": 201646309,
          "isinfluential": false,
          "contexts": [
            "To retrieve the most relevant “event” (i.e., a generated sequence) from the memory store m = { m 1 , m 2 , ... } , we use S-BERT (Reimers and Gurevych, 2019) for dense retrieval (i.e., retrieval with dense representations provided by NN).",
            "S-BERT is a modiﬁcation of the BERT model (Devlin et al., 2019) that uses siamese and triplet network structures to obtain semantically meaningful embeddings for text sequences.",
            "Since we ﬁx the parameters from S-BERT, the retrieval module’s parameters are not updated during training."
          ],
          "intents": [
            "['methodology']",
            "--",
            "--"
          ],
          "cited_paper_info": {
            "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
            "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
            "year": 2019,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "2959414",
                "name": "Nils Reimers"
              },
              {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
              }
            ]
          }
        },
        {
          "citedcorpusid": 203701085,
          "isinfluential": false,
          "contexts": [
            "Event Extraction has long been studied as a local sentence-level task (Grishman and Sundheim, 1996; Ji and Grishman, 2008b; Grishman, 2019; Lin et al., 2020)."
          ],
          "intents": [
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Twenty-five years of information extraction",
            "abstract": "Abstract Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.",
            "year": 2019,
            "venue": "Natural Language Engineering",
            "authors": [
              {
                "authorId": "1788050",
                "name": "R. Grishman"
              }
            ]
          }
        },
        {
          "citedcorpusid": 204960716,
          "isinfluential": true,
          "contexts": [
            "• As compared to the raw BART-Gen, with our memory-based training – leveraging previously closest extracted event information substantially helps increase precision (P) and F-1 scores, with smaller but notable improvement in recall especially under Coref Match.",
            "Our best model with argu-ment pair constrained decoding outperforms substantially both BART-Gen and our memory-based training model.",
            "Following Li et al. (2021), the main model of our framework is based on the pretrained encoder-decoder model BART (Lewis et al., 2020).",
            "We observe that: • The neural generation-based models (BART-Gen and our framework) are superior in this document-level informative argument extraction problem, as compared to the sequence labeling-based approaches.",
            "Li et al. (2021) propose to use conditional neural text generation model for the document-level argument extraction problem, it handles each event in isolation ( BART-Gen ).",
            "(2021), the main model of our framework is based on the pretrained encoderdecoder model BART (Lewis et al., 2020).",
            "The intuition behind using BART for the extraction task is that it is pre-trained as a denoising autoencoder – reconstruct the original input sequence."
          ],
          "intents": [
            "--",
            "--",
            "['methodology']",
            "--",
            "--",
            "['methodology']",
            "--"
          ],
          "cited_paper_info": {
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "35084211",
                "name": "M. Lewis"
              },
              {
                "authorId": "11323179",
                "name": "Yinhan Liu"
              },
              {
                "authorId": "39589154",
                "name": "Naman Goyal"
              },
              {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
              },
              {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
              },
              {
                "authorId": "39455775",
                "name": "Omer Levy"
              },
              {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
              },
              {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
              }
            ]
          }
        },
        {
          "citedcorpusid": 207853145,
          "isinfluential": false,
          "contexts": [
            "To support the progress for the problem, Ebner et al. (2020) built RAMS dataset, and it contains annotations for cross-sentence arguments but for each document it contains only one event.",
            "In recent years, there have been efforts focusing on event extraction beyond sentence boundaries with end-to-end learning (Ebner et al., 2020; Du, 2021; Li et al., 2021)."
          ],
          "intents": [
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Multi-Sentence Argument Linking",
            "abstract": "We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",
            "year": 2019,
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "authors": [
              {
                "authorId": "78150202",
                "name": "Seth Ebner"
              },
              {
                "authorId": "2465658",
                "name": "Patrick Xia"
              },
              {
                "authorId": "41017370",
                "name": "Ryan Culkin"
              },
              {
                "authorId": "1740418",
                "name": "Kyle Rawlins"
              },
              {
                "authorId": "7536576",
                "name": "Benjamin Van Durme"
              }
            ]
          }
        },
        {
          "citedcorpusid": 219683473,
          "isinfluential": false,
          "contexts": [
            ", 2019) and uses Conditional Random Fields (Lafferty et al., 2001) for structured prediction (BERT-CRF).",
            "It performs sequence labeling based on automatically extracted features from BERT (Devlin et al., 2019) and uses Conditional Random Fields (Lafferty et al., 2001) for structured prediction ( BERT-CRF )."
          ],
          "intents": [
            "['methodology']",
            "['methodology']"
          ],
          "cited_paper_info": {
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "venue": "International Conference on Machine Learning",
            "authors": [
              {
                "authorId": "1739581",
                "name": "J. Lafferty"
              },
              {
                "authorId": "143753639",
                "name": "A. McCallum"
              },
              {
                "authorId": "113414328",
                "name": "Fernando Pereira"
              }
            ]
          }
        },
        {
          "citedcorpusid": 226262410,
          "isinfluential": false,
          "contexts": [
            "For example, if one person has been identified as a JAILER in an event, it’s unlikely that the same person is an ATTACKER in another event in the document (Figure 1), according to world event knowledge (Sap et al., 2019; Yao et al., 2020).",
            "Yao et al. (2020) propose a weakly-supervised approach to extract sub-event relation tuples from the text.",
            "For example, if one person has been identiﬁed as a JAILER in an event, it’s unlikely that the same person is an ATTACKER in another event in the document (Figure 1), according to world event knowledge (Sap et al., 2019; Yao et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['methodology']",
            "['background']"
          ],
          "cited_paper_info": {
            "title": "Weakly Supervised Subevent Knowledge Acquisition",
            "abstract": "Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.",
            "year": 2020,
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "authors": [
              {
                "authorId": "9185405",
                "name": "Wenlin Yao"
              },
              {
                "authorId": "22194540",
                "name": "Zeyu Dai"
              },
              {
                "authorId": "2073611308",
                "name": "Maitreyi Ramaswamy"
              },
              {
                "authorId": "1875233",
                "name": "Bonan Min"
              },
              {
                "authorId": "40372969",
                "name": "Ruihong Huang"
              }
            ]
          }
        },
        {
          "citedcorpusid": 247797575,
          "isinfluential": false,
          "contexts": [
            "In recent years, there have been efforts focusing on event extraction beyond sentence boundaries with end-to-end learning (Ebner et al., 2020; Du, 2021; Li et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards More Intelligent Extraction of Information from Documents",
            "abstract": "In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.",
            "year": 2021,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": 247797575,
          "isinfluential": false,
          "contexts": [
            "In recent years, there have been efforts focusing on event extraction beyond sentence boundaries with end-to-end learning (Ebner et al., 2020; Du, 2021; Li et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards More Intelligent Extraction of Information from Documents",
            "abstract": "In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.",
            "year": 2021,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": 247797575,
          "isinfluential": false,
          "contexts": [
            "In recent years, there have been efforts focusing on event extraction beyond sentence boundaries with end-to-end learning (Ebner et al., 2020; Du, 2021; Li et al., 2021)."
          ],
          "intents": [
            "--"
          ],
          "cited_paper_info": {
            "title": "Towards More Intelligent Extraction of Information from Documents",
            "abstract": "In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.",
            "year": 2021,
            "venue": "",
            "authors": []
          }
        },
        {
          "citedcorpusid": null,
          "isinfluential": false,
          "contexts": [
            "For example, if one person has been identified as a JAILER in an event, it’s unlikely that the same person is an ATTACKER in another event in the document (Figure 1), according to world event knowledge (Sap et al., 2019; Yao et al., 2020).",
            "Sap et al. (2019) propose to use crowdsourcing for obtaining if-then relations be-tween events.",
            "For example, if one person has been identiﬁed as a JAILER in an event, it’s unlikely that the same person is an ATTACKER in another event in the document (Figure 1), according to world event knowledge (Sap et al., 2019; Yao et al., 2020)."
          ],
          "intents": [
            "['background']",
            "['background']",
            "['background']"
          ],
          "cited_paper_info": {}
        }
      ]
    }
  }
}