Analysis_Source	Name (extracted)	Citing Article	Citied Article	Features	Name_Variants	Homepage_URL
citing_context	2022)	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/2022.acl-long.66 (2021)	The PMR dataset (Dong et al., 2022) is used for visual question answering, specifically focusing on premise-based multimodal reasoning that integrates textual and visual clues. It enables researchers to explore how joint textual and visual information can be effectively utilized to answer complex questions, enhancing the understanding of multimodal data integration and reasoning.		
citing_context	3RScan	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.1109/ICCV.2019.00775 (2019)	The 3RScan dataset is used to source complex real-world scenes, providing a range of 3D indoor scenarios for generating diverse spatial locations and viewpoints. It enables researchers to study and model various indoor environments, enhancing the realism and diversity of spatial data in computer vision and robotics applications. The dataset's rich 3D content supports the development and evaluation of algorithms that require detailed and varied indoor scene representations.		
cited_context	400 million image-text pairs	https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The 400 million image-text pairs dataset is primarily used to pre-train CLIP, a model designed to learn the correspondence between raw text and images. This enhances multi-modal representation learning, enabling the model to effectively match textual descriptions with visual content. The dataset's large scale and diverse content are crucial for training robust and versatile multi-modal models.		
citing_context	5	https://doi.org/10.48550/arXiv.2504.10074 (2025)		The dataset of 5,800 test samples is used to evaluate the performance of models in multi-modal knowledge graph reasoning, specifically focusing on single-hop and two-hop reasoning questions involving Wikipedia entities. This dataset enables researchers to assess how effectively models can reason across different types of data and relationships within a knowledge graph.		
citing_context	800 test samples	https://doi.org/10.48550/arXiv.2504.10074 (2025)		The dataset of 5,800 test samples is used to evaluate the performance of models in multi-modal knowledge graph reasoning, specifically focusing on single-hop and two-hop reasoning questions involving Wikipedia entities. This dataset enables researchers to assess how effectively models can reason across different types of data and relationships within a knowledge graph.		
citing_context	A-OKVQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.18653/v1/2021.acl-long.528 (2021)	The A-OKVQA dataset is primarily used to evaluate open-ended visual question answering systems, focusing on generating accurate and coherent responses. It is employed to assess models' ability to understand complex visual scenes and provide contextually appropriate answers. This dataset emphasizes the importance of coherence and accuracy in responses, making it valuable for improving and benchmarking visual question answering methodologies.		
citing_context	AI2D	https://doi.org/10.18653/v1/P19-1347 (2018)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The AI2D dataset is used for knowledge extraction from diagrams, enhancing multimodal reasoning capabilities. It provides a unique type of data that supports the development and evaluation of models capable of integrating visual and textual information. This dataset enables researchers to address challenges in understanding and interpreting complex diagrams, contributing to advancements in multimodal reasoning systems.		
citing_context	allrecipes.com	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.18653/v1/N19-1423 (2019)	The allrecipes.com dataset is used to study a wide range of recipes, contributing to the development of multi-modal reasoning systems that understand diverse culinary content. It enables researchers to analyze and reason about various cooking techniques, ingredients, and preparation methods, enhancing the system's ability to process and generate culinary-related information.		
cited_context	and Tone	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/3511808.3557233 (2022)	The Global Database of Events, Language, and Tone is used to construct a dense knowledge graph for temporal knowledge graph completion. Researchers focus on analyzing events and tones over time, employing methodologies that enable the derivation of temporal relationships and patterns. This dataset facilitates research into how events and their associated tones evolve, providing insights into dynamic social and political phenomena.		
citing_context	and Tone (GDELT)	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1145/2187980.2188242 (2012)	The Global Database of Events, Language, and Tone (GDELT) is used for dynamic Knowledge Graph Reasoning (KGR) tasks, providing event data and linguistic tone analysis for global events. This dataset enables researchers to analyze and reason over temporal and spatial dynamics of world events, enhancing understanding of global phenomena through structured event data and sentiment analysis.		
citing_context	ARKitScenes	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.1109/ICCV.2019.00775 (2019)	The ARKitScenes dataset is used to generate diverse real-world scenes by leveraging mobile RGB-D data, enabling 3D indoor scene understanding. It provides spatial locations and viewpoints, facilitating research in 3D reconstruction and scene analysis. This dataset supports methodologies that require detailed, realistic indoor environments for testing and validating algorithms.		
cited_context | citing_context	Aser	https://doi.org/10.1145/3573201 (2022), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022), https://doi.org/10.1145/3573201 (2022), https://doi.org/10.1109/TKDE.2024.3352100 (2023)	https://doi.org/10.1145/3366423.3380107 (2019)	The ASER dataset is primarily recognized for being the largest knowledge graph with eventualities as semantic units and relations as edges. However, specific methodologies or research questions directly employing ASER are not detailed in the provided descriptions. Its scale and structure suggest potential utility in large-scale knowledge representation and reasoning tasks, though explicit research applications are not specified.; The ASER dataset is used to construct large-scale, multi-modal knowledge graphs by leveraging defined patterns and an automatic pipeline, focusing on scalable graph construction. It is also utilized to study causal relationships between events, enhancing commonsense reasoning through graph-based methods. ASER is noted for being the largest knowledge graph with eventualities as semantic units and relations as edges, though specific research applications beyond these areas are not detailed.	ASER	
citing_context	aspect catalog	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1145/3340531.3412875 (2020)	The 'aspect catalog' dataset is used to define aspects for entity instances, enhancing the representation of entities in a knowledge graph. It specifically supports entity aspect linking by providing detailed attributes that enrich entity representations, enabling more accurate and nuanced connections within the knowledge graph. This dataset facilitates research in improving the granularity and specificity of entity data, which is crucial for advanced knowledge graph applications.		
cited_context	ATOMIC	https://doi.org/10.1109/TKDE.2024.3352100 (2023)	https://doi.org/10.1609/aaai.v35i7.16792 (2020)	The ATOMIC dataset is used to explore causal effects between events for commonsense reasoning, employing both symbolic and neural representations. It focuses on understanding how these representations can capture and reason about the relationships and effects between different events, enabling research into more nuanced and context-aware AI systems.		
citing_context	AudioSet	https://doi.org/10.1609/aaai.v35i2.16231 (2021)	https://doi.org/10.1109/ICASSP.2017.7952261 (2017)	The AudioSet dataset is used for extracting 96-second Mel Spectrogram patches to classify audio events, focusing on the representation of audio features in multi-modal reasoning. This methodology supports research in audio event classification, leveraging the dataset's extensive audio content to enhance the accuracy and robustness of audio feature representations.		
cited_context | citing_context	Awa2	https://doi.org/10.1109/ICTAI56018.2022.00064 (2022), https://doi.org/10.48550/arXiv.2207.01328 (2022)	https://doi.org/10.1109/CVPR.2019.01175 (2018), https://www.semanticscholar.org/paper/ad7ddcc14984caae308c397f1a589aae75d4ab71 (2020)	The AWA2 dataset is used to evaluate zero-shot learning models, particularly focusing on the performance of implicit and explicit correlation methods in knowledge graph propagation. It enables researchers to assess how well these models can predict unseen classes by leveraging existing knowledge. The dataset's characteristics support the evaluation of model accuracy in propagating knowledge across different categories.; The AWA2 dataset is primarily used to evaluate vision transformer encoders, particularly in zero-shot learning tasks for animal classification. It enables researchers to assess model performance in recognizing unseen animal categories, leveraging the dataset's rich visual and semantic attributes. This focus supports advancements in zero-shot learning methodologies within the domain of computer vision.	AWA2	
cited_context	AWA2-KG	https://doi.org/10.48550/arXiv.2207.01328 (2022)	https://doi.org/10.1145/3442381.3450042 (2021)	The AWA2-KG dataset is primarily used for evaluating and validating models in zero-shot learning tasks, particularly focusing on knowledge graph-based approaches. It provides triples for prompting in ontology-enhanced zero-shot learning, emphasizing relations between classes, entities, and their attributes. This dataset facilitates the assessment of model performance in handling ontology-enhanced attributes and supports the development of flexible systems capable of reasoning with limited data.		
cited_context	Babel-Net	https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	Babel-Net is used to build a multilingual knowledge graph, serving as a large-scale lexicalized semantic network. It provides lexical knowledge and supports the creation of multilingual resources. While listed as an example of a knowledge graph, it is not specifically used for multi-modal Knowledge Graph Reasoning research. Its extensive lexical coverage and multilingual capabilities enable the development of comprehensive semantic networks.		
citing_context	BabelNet	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	BabelNet is primarily listed as an example of a knowledge graph that covers lexical knowledge. It is not used in specific research contexts or methodologies related to multi-modal Knowledge Graph Reasoning. The dataset's comprehensive lexical coverage makes it a reference point in discussions about knowledge graphs, but its direct application in research is not specified in the provided descriptions.		
cited_context	BioRel	https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126 (2023)	https://doi.org/10.1186/s12859-020-03889-5 (2020)	The BioRel dataset is used for empirical evaluation and large-scale biomedical relation extraction, focusing on the performance of models in extracting biomedical relations from text. It leverages distant supervision to compile a comprehensive dataset for training and testing, enabling researchers to assess and improve relation extraction methodologies in the biomedical domain.		
cited_context | citing_context	Bookcorpus	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The BookCorpus dataset is used to enhance event knowledge graphs by providing textual data that offers context for event relations. This textual information is integrated into the knowledge graph to enrich the representation and understanding of events, thereby improving the accuracy and depth of event-related research. The dataset's extensive textual content is crucial for contextualizing and linking events within the graph.; The BookCorpus dataset is used to provide textual data for event classification and relation extraction, which enhances the construction and enrichment of multi-modal event knowledge graphs. This dataset enables researchers to improve the accuracy and comprehensiveness of event-related information in knowledge graphs by leveraging its extensive textual content.	BookCorpus	
cited_context | citing_context	C4(News)	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The C4(news) dataset is used to enrich event knowledge graphs with current textual news data, incorporating new events and relations. This methodology involves updating knowledge graphs dynamically to reflect recent developments, enhancing their relevance and accuracy. The dataset's timely and diverse content supports research in maintaining up-to-date knowledge bases, particularly in areas requiring real-time information.; The C4(news) dataset provides news-related textual data primarily used for event classification and relation extraction. It supports the construction and enhancement of multi-modal event knowledge graphs by offering rich, structured textual content. This dataset enables researchers to accurately classify events and extract relationships, contributing to more robust and detailed knowledge graphs.	C4(news)	
cited_context | citing_context	Cc12M	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The CC12M dataset is used to support event recognition and relation extraction in multi-modal knowledge graphs. It provides visual data that enhances the accuracy and richness of these tasks, enabling researchers to build more comprehensive and contextually aware knowledge graphs. This dataset's visual content is crucial for integrating and reasoning about complex, multi-source information.; The CC12M dataset is used to supply additional visual data for object recognition and event relation extraction, enhancing the construction and expansion of multi-modal event knowledge graphs. This dataset provides rich visual content that complements textual data, enabling more comprehensive and accurate event relation extraction and object recognition in research.	CC12M	
cited_context | citing_context	Cc3M	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The CC3M dataset is used to support event recognition and relation extraction in multi-modal knowledge graphs. It provides visual data that enhances the ability to extract and recognize events and their relationships, contributing to more accurate and contextually rich knowledge graphs. This dataset enables researchers to integrate visual and textual information effectively, improving the overall performance of multi-modal reasoning systems.; The CC3M dataset is used to enhance multi-modal data in research, specifically for object recognition and event relation extraction, contributing to the enrichment of multi-modal event knowledge graphs. It is also utilized to augment training sets, improving the performance of image captioning models by providing additional visual data.	CC3M	
cited_context	CC3M&12M	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://doi.org/10.18653/v1/P18-1238 (2018)	The CC3M&12M dataset is used to train and evaluate multi-modal knowledge graph reasoning systems, specifically focusing on image-caption pairs. It enhances event extraction and representation by leveraging these pairs, enabling researchers to improve the accuracy and depth of event understanding in multi-modal contexts.		
cited_context	Chunyu	https://doi.org/10.1145/3343031.3351033 (2019)	https://doi.org/10.1145/3209978.3210081 (2018)	The Chunyu dataset is used to evaluate the performance of KABLSTM in medical question answering by comparing it with SMatrix and K-NRM. It demonstrates the utility of incorporating knowledge graphs in this context, focusing on enhancing the accuracy and relevance of medical answers. The dataset's multi-modal nature supports the integration of textual and structured data, enabling more effective reasoning and response generation in medical inquiries.		
citing_context	CI-FAR	https://doi.org/10.1145/3627673.3679175 (2024)	https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086 (2009)	The CI-FAR dataset is primarily used for small-scale image recognition and object localization. Researchers extract bounding boxes of objects from the images to construct VisionKG, a knowledge graph focused on visual data. This methodology enables the creation of structured visual information, facilitating research in object detection and image understanding.		
cited_context | citing_context	Clevr	https://doi.org/10.18653/v1/P19-1347 (2018), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The CLEVR dataset is used to address the visual grounding problem by focusing on compositional language and elementary visual reasoning. It employs diagnostic tasks to evaluate models' ability to understand and reason about visual scenes described in natural language. This dataset enables researchers to test and improve algorithms that integrate linguistic and visual information, enhancing the interpretability and accuracy of visual reasoning systems.; The CLEVR dataset is primarily used to evaluate compositional reasoning abilities, focusing on visual and linguistic understanding. It assesses models' capabilities in visual question answering, particularly in understanding complex queries involving multiple objects and attributes. The dataset is designed to test elementary visual reasoning and compositional language skills in a controlled environment, enabling researchers to evaluate and improve knowledge graph reasoning models.	CLEVR	
citing_context	Clotho	https://doi.org/10.1609/aaai.v35i2.16231 (2021)	https://doi.org/10.1109/ICASSP40776.2020.9052990 (2019)	The Clotho dataset is used for generating natural language captions for audio/visual content, integrating audio and visual modalities in multi-modal reasoning. It enables researchers to develop and evaluate models that can effectively combine these modalities to produce descriptive captions, addressing challenges in multi-modal understanding and synthesis.		
cited_context | citing_context	Cm3Kg	https://doi.org/10.18653/v1/2022.emnlp-demos.15 (2022)	https://doi.org/10.18653/v1/2020.acl-main.703 (2019)	The CM3KG dataset is used as an open-sourced multi-modal knowledge graph to support research in multi-modal knowledge graph reasoning. It provides a rich resource for integrating and reasoning across different modalities, enabling researchers to explore complex relationships and interactions within and between various data types. This dataset facilitates the development and evaluation of methods that can effectively combine textual, visual, and other forms of data to enhance reasoning capabilities.	CM3KG	
cited_context | citing_context	Cn-Dbpedia	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	CN-DBpedia is used as a structured data resource derived from Chinese Wikipedia, serving semantic web applications. It provides a Chinese-language knowledge graph, enabling researchers to leverage encyclopedia knowledge for various semantic web tasks. The dataset's structured format facilitates integration and querying, supporting applications that require rich, interconnected data.; The CN-DBpedia dataset is primarily listed as an example of a knowledge graph containing encyclopedia knowledge. It is referenced in research contexts but not utilized for specific methodologies or research questions. Its role is more illustrative, highlighting the availability of structured encyclopedia data, rather than being directly employed in empirical studies or multi-modal Knowledge Graph Reasoning.	CN-DBpedia	
citing_context	CNVD	https://doi.org/10.1109/ISADS56919.2023.10092024 (2023)		The CNVD dataset is used to enrich the ICS-Het-KG by providing additional vulnerability data for industrial control systems. This enhances the graph's information, contributing to more comprehensive knowledge representation. The dataset's specific role is to augment the graph with detailed vulnerability data, which supports research focused on improving the security and resilience of industrial control systems.		
cited_context	COCO Caption	https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The COCO Caption dataset is primarily used to assess image-text retrieval capabilities, focusing on caption generation and the alignment between images and textual descriptions. Researchers employ this dataset to evaluate models' ability to generate accurate captions and align them with corresponding images, enhancing the understanding of visual and textual data integration.		
citing_context	Common Sense Knowledge Graph (CSKG)	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-030-77385-4_41 (2020)	The Common Sense Knowledge Graph (CSKG) is used to integrate and explore common sense knowledge across various dimensions, supporting the development of rich, well-connected, and heterogeneous knowledge graphs. It facilitates the integration of diverse data types and enhances multi-modal reasoning, enabling researchers to explore the breadth of common sense knowledge effectively.		
cited_context	ComplexWebQuestion (CWQ)	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.18653/v1/N18-1059 (2018)	The ComplexWebQuestion (CWQ) dataset is used to derive FB400k, focusing on complex web questions and their answers. It enhances multi-modal knowledge graph reasoning by providing a rich set of complex queries and corresponding answers, which are utilized to improve the reasoning capabilities of knowledge graphs in handling intricate web-based questions.		
cited_context	Compositional ZSL	https://doi.org/10.48550/arXiv.2207.01328 (2022)	https://doi.org/10.1109/CVPR46437.2021.00101 (2021)	The Compositional ZSL dataset is used to explore zero-shot learning scenarios, specifically focusing on the composition of visual primitives to enhance feature-to-sequence transformation techniques. It enables researchers to investigate how class descriptions composed of visual primitives can improve model performance in predicting unseen classes. This dataset facilitates the development and evaluation of methods that leverage compositional structures in visual data.		
cited_context | citing_context	Conceptnet	https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1145/3573201 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022) (+5), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.48550/arXiv.2205.11501 (2022), https://doi.org/10.1145/3573201 (2022) (+7)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	ConceptNet is used to enhance knowledge graphs and multi-modal reasoning by representing commonsense knowledge in a graph format, where nodes are concepts and edges are relations. It is leveraged as an external knowledge corpus to enrich relation learning, object graphs, and scene graphs, particularly in visual question answering and event information granularity. Its multilingual support and general knowledge base enable researchers to integrate external knowledge, improving multi-modal reasoning capabilities and system understanding of everyday concepts and relationships.; ConceptNet is used to gather and enrich knowledge bases with general and commonsense knowledge, particularly through multilingual and open-source data. It contributes to constructing knowledge graphs by providing a large number of facts and commonsense triples, which enhance reasoning capabilities in various applications. Specifically, it is utilized to improve Visual Question Answering (VQA) systems by supplying relevant linguistic subgraphs and general knowledge, thereby enhancing the system's ability to reason about everyday concepts and integrate background knowledge. Additionally, ConceptNet aids in retrieving 1-hop neighbor nodes and pruning irrelevant nodes, improving the relevance of concept nodes in knowledge graph settings.	ConceptNet	
cited_context	ConceptNet KG	https://doi.org/10.48550/arXiv.2205.11501 (2022)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	The ConceptNet KG dataset is used to enhance multi-modal reasoning by retrieving concept-level knowledge from a general-domain knowledge graph. It integrates textual and visual information, enabling researchers to improve the understanding and processing of complex, multi-faceted data in various applications. This dataset facilitates the development of more robust and context-aware reasoning systems.		
citing_context	Conceptual Captions	https://doi.org/10.18653/v1/2023.acl-industry.16 (2023)	https://doi.org/10.1162/tacl_a_00166 (2014)	The Conceptual Captions dataset is used to analyze and compare the unique characteristics of image-text pairs, particularly in the product domain. Researchers employ this dataset to highlight differences in data distribution and content, focusing on how these pairs differ from other domains. This analysis aids in understanding the specific nuances and patterns within product-related image-text data, which can inform the development of more tailored multimodal models.		
citing_context	CORD-19	https://doi.org/10.3390/app13127115 (2023)	https://www.semanticscholar.org/paper/bc411487f305e451d7485e53202ec241fcc97d3b (2020)	The CORD-19 dataset is used to integrate textual and structured data in COVID-19 research, enhancing multi-modal reasoning capabilities. Researchers apply this dataset to study how combining different data types can improve understanding and analysis in the context of the pandemic. This integration supports more comprehensive and nuanced research questions and methodologies, leveraging the dataset's rich textual and structured content.		
cited_context	Countries	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://www.semanticscholar.org/paper/03dfada96b88c741bb26bd4ce7b5ae4232157d37 (2015)	The 'Countries' dataset is used to represent and reason about geographical relations among countries, employing low-rank vector spaces for approximate reasoning. This approach facilitates the analysis of complex geographical relationships, enabling researchers to explore and model interactions between countries in a computationally efficient manner.		
citing_context	CRAG	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2406.04744 (2024)	The CRAG dataset is used to benchmark RAG models, specifically focusing on comprehensive evaluation across various reasoning tasks and modalities. It enables researchers to assess model performance in multi-modal knowledge graph reasoning, ensuring robustness and versatility in handling diverse data types and reasoning challenges.		
citing_context	cross kg datasets	https://doi.org/10.3390/app13116747 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The 'cross kg datasets' are used for evaluating the robustness of multi-modal reasoning systems by varying the percentage of aligned entity pairs (20%, 50%, 80%). This assessment helps researchers understand how these systems perform under different levels of alignment, providing insights into their reliability and effectiveness in multi-modal knowledge graph reasoning tasks.		
citing_context	CSKG	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-030-77385-4_41 (2020)	The CSKG dataset is used to consolidate and integrate heterogeneous common sense knowledge from multiple sources, enhancing multi-modal reasoning capabilities. It is employed to enrich language models and scene graphs with relevant common sense information, improving tasks such as common sense question answering and visual understanding. The dataset's comprehensive and systematically consolidated nature allows researchers to compute node similarity, extract relevant triplets, and refine graph structures, thereby enhancing the reasoning capabilities of multi-modal systems.		
cited_context	CT	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The CT dataset is used to integrate visual information with textual descriptions, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This approach leverages the dataset's ability to combine imaging data with clinical text, improving diagnostic outcomes.		
citing_context	CVE	https://doi.org/10.1109/ISADS56919.2023.10092024 (2023)		The CVE dataset is used to enhance the construction of the ICS-Het-KG, a heterogeneous knowledge graph for industrial control systems. It provides comprehensive and accurate vulnerability data, which enriches the graph's structure and content. This dataset supports research focused on improving the security and resilience of industrial control systems by integrating detailed vulnerability information into the knowledge graph.		
cited_context	CWQ	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.18653/v1/N18-1059 (2018)	The CWQ dataset is used for evaluating complex query answering in knowledge graphs. It provides SPARQL annotations for validation and test questions, enabling researchers to assess the performance of query answering systems. This supports the development and refinement of methods for handling complex queries in knowledge graphs, focusing on accuracy and effectiveness in retrieving information.		
cited_context | citing_context	Cyc	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The Cyc dataset is primarily listed as an example of a knowledge graph that covers common sense knowledge. It is not utilized in specific research contexts or methodologies, nor is it directly involved in addressing particular research questions or applications. Its main relevance lies in its representation of common sense knowledge, which can inform the design and evaluation of other knowledge graphs and reasoning systems.; The Cyc dataset is primarily listed as an example of a knowledge graph that contains common sense knowledge. It is referenced in research contexts but not utilized for specific empirical studies or methodologies. Its role is more illustrative, highlighting the availability of common sense knowledge in knowledge graphs, rather than being directly employed in multi-modal Knowledge Graph Reasoning or other specific research applications.	Cyc	
citing_context	DB-Img-Few	https://doi.org/10.1109/ICDE60146.2024.00061 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The DB-Img-Few dataset is used to provide image embeddings for multi-modal knowledge graph reasoning, specifically in few-shot learning scenarios. This dataset enables researchers to integrate visual information into knowledge graphs, enhancing reasoning capabilities with limited data. The focus on few-shot learning highlights its utility in scenarios where labeled data is scarce.		
cited_context | citing_context	Db100K	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DB100K dataset is primarily used for knowledge graph embedding, where the focus is on learning representations of entities and relations within a large-scale knowledge graph. This methodology involves converting graph data into vector spaces to capture semantic relationships, enabling more effective reasoning and prediction tasks. The dataset's scale and structure support advanced embedding techniques, facilitating research in areas such as entity linking and relation prediction.; The DB100K dataset is used for testing the scalability and robustness of knowledge graph embedding and reasoning algorithms. It provides a larger dataset to evaluate how well these models generalize and perform under increased data volume, focusing on the capabilities of algorithms to handle and reason over extensive knowledge graphs.	DB100K	
cited_context | citing_context	Db15K	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://doi.org/10.1609/aaai.v39i12.33454 (2024), https://doi.org/10.1145/3545573 (2022) (+7), https://doi.org/10.1609/aaai.v39i12.33454 (2024), https://doi.org/10.1145/3545573 (2022), https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The DB15K dataset is primarily used for evaluating and extending multi-modal knowledge graph reasoning, particularly in cross-lingual and cross-modal entity and relation prediction tasks. It enhances the original DBpedia dataset by incorporating images, enabling researchers to study diverse entity categories and improve entity representation and reasoning. The dataset is utilized to train and evaluate models, focusing on metrics like MRR, Hit@1, Hit@3, and Hit@10, and to assess computational efficiency and performance improvements in link prediction and entity alignment.; The DB15K dataset is primarily used in multi-modal knowledge graph reasoning research, specifically for evaluating and enhancing model performance in tasks such as cross-modal entity alignment and relation prediction. It is often used alongside other datasets like FB15K and YAGO15K, with splits of 2:8, 5:5, and 8:2 for training and testing. The dataset facilitates the assessment of state-of-the-art methods, focusing on metrics like MRR, Hit@1, Hit@3, and Hit@10, and is also used to measure computational efficiency on a single GPU. It supports the selection of diverse entity categories (e.g., artist, city, organization, country) for constructing robust multi-modal knowledge graphs.	DB15K	
cited_context | citing_context	Db15K Mmkgs	https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The DB15K MMKGs dataset is used alongside FB15K for entity alignment, specifically focusing on multi-modal aspects. It is split into training and testing sets to evaluate the performance of entity alignment methods. This dataset enables researchers to assess and improve the accuracy of aligning entities across different knowledge graphs by incorporating multi-modal data.	DB15K MMKGs	
citing_context	DB15K-IMG	https://doi.org/10.3390/s24237605 (2024)	https://doi.org/10.1145/3474085.3475470 (2021)	The DB15K-IMG dataset is used to evaluate models in cross-lingual knowledge graph reasoning with visual context, specifically focusing on entity alignment. It enables researchers to test the effectiveness of their models in aligning entities across different languages while incorporating visual information. This dataset is crucial for advancing multi-modal reasoning techniques in knowledge graphs.		
cited_context | citing_context	Dbp15K	https://doi.org/10.48550/arXiv.2307.16210 (2023), https://doi.org/10.3390/app13116747 (2023), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024) (+3), https://doi.org/10.1145/3581783.3611786 (2022), https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.48550/arXiv.2209.00891 (2022)	The DBP15K dataset is primarily used for evaluating cross-lingual entity alignment methods, focusing on attribute-preserving embeddings and the integration of visual features. It is employed in experiments using pre-trained ResNet-152 and VGG vision encoders, assessing performance metrics like Hits@1, Hits@10, and MRR. The dataset supports research on model stability, robustness, and efficiency in multi-modal knowledge graph alignment across various language pairs, including Chinese-English, Japanese-English, and French-English.; The DBP15K dataset is primarily used to evaluate cross-lingual entity alignment methods, particularly those involving attribute-preserving embeddings and multi-modal knowledge graph alignment. It focuses on entities with attached images and is often used to assess the performance and stability of ResNet-152 vision encoders with 2048-dimensional vision features. This dataset enables researchers to test and refine models that integrate visual and textual data for improved entity alignment across multiple language pairs.	DBP15K	
citing_context	DBP15K FR - EN	https://doi.org/10.48550/arXiv.2407.19625 (2024)	https://doi.org/10.1007/978-3-319-68288-4_37 (2017)	The DBP15K FR - EN dataset is used for bilingual entity alignment between French and English DBpedia versions, focusing on cross-lingual knowledge graph reasoning. It enables researchers to develop and evaluate methods for aligning entities across different language versions of DBpedia, enhancing the interoperability and integration of multilingual knowledge graphs.		
citing_context	DBP15K JA - EN	https://doi.org/10.48550/arXiv.2407.19625 (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024)	https://doi.org/10.1609/aaai.v35i5.16550 (2020)	The DBP15K JA - EN dataset is used for bilingual entity alignment between Japanese and English DBpedia versions, focusing on cross-lingual knowledge graph reasoning. It is employed to evaluate methods like EVA, measuring their performance in aligning entities across languages. This dataset enables researchers to assess the effectiveness of entity alignment techniques in a cross-lingual context.		
citing_context	DBP15K ZH - EN	https://doi.org/10.48550/arXiv.2407.19625 (2024)	https://doi.org/10.1007/978-3-319-68288-4_37 (2017)	The DBP15K ZH - EN dataset is used for bilingual entity alignment between Chinese and English DBpedia versions, focusing on cross-lingual knowledge graph reasoning. It enables researchers to develop and evaluate methods for aligning entities across different language versions, enhancing the interoperability and integration of multilingual knowledge graphs. This dataset supports research in improving cross-lingual information retrieval and semantic web technologies.		
cited_context | citing_context	Dbpedia	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1145/3474085.3475470 (2021) (+8), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1145/3474085.3475470 (2021) (+4)	https://doi.org/10.3233/SW-140134 (2015), https://doi.org/10.1145/2556195.2556245 (2014)	DBpedia is used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing structured information for constructing and enhancing knowledge graphs. It is utilized to extract raw triplet facts, including hasPart/isA relationships, and to offer categorical knowledge for Visual Question Answering. The dataset highlights the incompleteness of entity-image associations, focusing on the integration of structured information and the lack of visual data for many entities.; DBpedia is used to extract structured information from Wikipedia, constructing a large-scale, linked data resource that enhances knowledge graphs with relational facts. It provides hasPart and isA triples, contributing over 193,449 facts. The dataset supports knowledge-driven applications like recommendation systems, information retrieval, and machine learning, and is utilized to provide categorical knowledge for Visual Question Answering (VQA), enhancing the system's ability to understand and answer questions about visual content. DBpedia's structured content from Wikimedia projects enriches multi-modal knowledge graph reasoning, offering diverse and interconnected data.	DBpedia	
cited_context	DBpedia15K	https://doi.org/10.1145/3545573 (2022)	https://doi.org/10.18653/v1/N19-1423 (2019)	The DBpedia15K dataset is used to collect textual information for entities, primarily focusing on relationship prediction in knowledge graphs. Researchers employ this dataset to enhance the accuracy of predicting relationships between entities by leveraging the rich textual data it provides. This approach supports the development of more robust and contextually informed knowledge graph models.		
cited_context | citing_context	Dbpedia50	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DBpedia50 dataset is used for knowledge graph completion, specifically focusing on entity linking and relation prediction within a smaller subset of DBpedia. Researchers employ this dataset to enhance the accuracy of these tasks by leveraging its structured data, enabling more precise and contextually relevant predictions in knowledge graphs.; The DBpedia50 dataset is used for knowledge graph completion, specifically focusing on entity linking and relation prediction within a smaller subset of DBpedia. It is employed to evaluate and validate knowledge graph completion methods, enabling efficient testing and validation on smaller-scale entity sets. This dataset facilitates research by providing a manageable yet representative sample for these tasks.	DBpedia50	
cited_context | citing_context	Dbpedia500	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DBpedia500 dataset is used for knowledge graph completion, specifically focusing on entity linking and relation prediction within a larger subset of DBpedia. Researchers employ this dataset to enhance the accuracy and robustness of knowledge graphs by improving the linking of entities and predicting relationships between them. This enables more effective and comprehensive knowledge representation and reasoning tasks.; The DBpedia500 dataset is used for evaluating and assessing the performance and robustness of knowledge graph completion models. It focuses on scalability and model performance on a moderately sized dataset, enabling researchers to test the effectiveness of knowledge graph embedding techniques in a controlled yet challenging environment.	DBpedia500	
citing_context	DBPedia50k	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1162/tacl_a_00360 (2019)	The DBPedia50k dataset is used for inductive knowledge graph reasoning, particularly focusing on embedding and pre-trained language representation. It enables researchers to explore how pre-trained models can be effectively utilized in inductive settings, enhancing the reasoning capabilities over knowledge graphs. This dataset supports the development and evaluation of methods that integrate language understanding with graph embeddings, facilitating more robust and scalable knowledge graph reasoning systems.		
citing_context	DiaKG	https://doi.org/10.3390/app13127115 (2023)	https://www.semanticscholar.org/paper/bc411487f305e451d7485e53202ec241fcc97d3b (2020)	The DiaKG dataset is utilized as a medical knowledge graph to evaluate the performance of reasoning models in healthcare applications. It enables researchers to assess how effectively these models can reason within a medical context, focusing on the accuracy and reliability of their outputs in practical healthcare scenarios.		
cited_context	Dingxiang	https://doi.org/10.1145/3343031.3351033 (2019)	https://doi.org/10.1145/3209978.3210081 (2018)	The Dingxiang dataset is used to evaluate the precision of KABLSTM, a model that integrates knowledge graphs in medical domain applications. It specifically verifies the effectiveness of knowledge graph integration, focusing on high-precision outcomes in medical reasoning tasks. This dataset enables researchers to test and validate models designed for medical knowledge representation and inference.		
cited_context | citing_context	Docred	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021), https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126 (2023)	https://doi.org/10.18653/v1/D17-1004 (2017), https://doi.org/10.1186/s12859-020-03889-5 (2020)	The DocRED dataset is primarily used for training models, such as the ATLOP model, to perform document-level relation extraction. This involves identifying and extracting relationships between entities within complex documents, which contributes to the construction of comprehensive knowledge graphs. The dataset's focus on document-level relations enables researchers to develop more sophisticated and context-aware models for knowledge graph construction.; The DocRED dataset is primarily used for empirical evaluation in relation extraction, specifically to assess model performance in identifying and extracting relationships from textual data. It enables researchers to test and compare various relation extraction models, focusing on their accuracy and effectiveness in handling complex document-level relations.	DocRED	
citing_context	E-VQA	https://doi.org/10.48550/arXiv.2504.10074 (2025)	https://doi.org/10.1109/ICCV51070.2023.00289 (2023)	The E-VQA dataset, comprising 221,000 question-answer pairs linked to 16,700 fine-grained Wikipedia entities, is used to enhance entity understanding and integrate detailed properties of fine-grained categories. It supports both single-hop and two-hop reasoning, pushing research towards comprehensive entity understanding and the integration of fine-grained details from Wikipedia.		
citing_context	EAL dataset	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1145/3340531.3412875 (2020)	The EAL dataset is used in research for entity aspect linking, specifically to calculate similarity between context and aspects and to evaluate the performance of methods in linking entities to their aspects. It provides aspect content essential for these calculations, enabling detailed experimental analysis in this domain.		
citing_context	eal-dataset-2020	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1145/3340531.3412875 (2020)	The eal-dataset-2020 is used to evaluate the AspectMMKG model, specifically for entity aspect linking tasks. This involves linking entities to their relevant aspects in multi-modal knowledge graphs, aiming to achieve state-of-the-art performance. The dataset's focus on entity aspects and its integration with multi-modal data enables researchers to enhance the precision and effectiveness of knowledge graph reasoning models.		
cited_context	electronic medical database	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1038/s41598-017-05778-z (2017)	The electronic medical database is used to construct a health knowledge graph, enabling reasoning tasks by extracting meaningful information from patient records. This involves methodologies focused on data integration and semantic extraction to enhance understanding and utility of medical data in research settings.		
citing_context	electronic medical records	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1016/j.bdr.2020.100174 (2020)	The electronic medical records dataset is used to integrate with a medical knowledge graph for safe medication recommendation, focusing on patient safety and drug interactions. This integration employs a knowledge graph reasoning approach to enhance the accuracy and safety of medication recommendations by leveraging detailed patient data and existing medical knowledge.		
citing_context	ETH	https://doi.org/10.1109/ACCESS.2020.2991435 (2020)	https://doi.org/10.1109/ICCV.2009.5459260 (2009)	The ETH dataset is used to evaluate model performance in multi-modal reasoning, particularly focusing on multi-agent interactions and social behavior. It is applied in multi-target tracking scenarios, enabling researchers to assess how models reason about complex social dynamics and interactions in real-world settings.		
citing_context	EVA-Dataset	https://doi.org/10.1145/3627673.3679126 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The EVA-Dataset is used to evaluate the performance of multi-modal reasoning models, particularly in entity linking and alignment tasks. It equips entities with visual information, though it has low image coverage, which affects the comprehensive representation of entities in multi-modal knowledge graphs. This dataset enables researchers to assess model effectiveness in integrating textual and visual data.		
cited_context	external knowledge base	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2019.00686 (2019)	The 'external knowledge base' dataset is used alongside the Visual Genome scene graph to generate question-answer pairs, enhancing the creation of unbiased Visual Question Answering (VQA) datasets. This methodology leverages the dataset to improve the diversity and fairness of VQA datasets, addressing research questions related to bias reduction in machine learning models.		
citing_context	Fakeddit	https://doi.org/10.48550/arXiv.2505.14714 (2025), https://doi.org/10.1145/3555178 (2022)	https://doi.org/10.1089/big.2020.0062 (2018)	The Fakeddit dataset is used for evaluating fine-grained fake news detection, particularly focusing on multimodal content and social context. It is also utilized to analyze millions of fauxtography posts from Reddit, examining visual and textual elements to understand the spread of misleading images. This dataset enables researchers to develop and test methods for detecting and analyzing misinformation by integrating both visual and textual data.		
citing_context	Fashion-MMKG	https://doi.org/10.18653/v1/2023.acl-industry.16 (2023)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The Fashion-MMKG dataset is used to enhance e-commerce image-text retrieval by incorporating cross-modal fashion knowledge into the training of a CLIP-style model. This approach improves the model's understanding of fashion-related multimodal data, enabling more accurate and contextually relevant search results. The dataset's focus on fashion-specific multimodal information is crucial for this application.		
cited_context	FB-IMG	https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb (2022)	https://doi.org/10.24963/ijcai.2017/438 (2016)	The FB-IMG dataset is used to evaluate multi-modal knowledge graph representation learning, specifically focusing on image-embodied knowledge representation and translation-based approaches. It enables researchers to assess how effectively models can integrate visual and textual information within knowledge graphs, addressing research questions related to the accuracy and robustness of these representations.		
citing_context	FB-Img-Few	https://doi.org/10.1109/ICDE60146.2024.00061 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The FB-Img-Few dataset is used to provide image embeddings for multi-modal knowledge graph reasoning, particularly in few-shot learning scenarios. It evaluates the performance of methods like MULTIFORM, focusing on enhancing entity representations through multi-modal data. The dataset is utilized to compare these methods against baselines, enabling researchers to assess improvements in entity representation and reasoning capabilities.		
cited_context | citing_context	Fb-Img-Txt	https://doi.org/10.1109/ICDE55515.2023.00015 (2022), https://doi.org/10.1109/TKDE.2025.3546686 (2023), https://doi.org/10.1109/ICDE55515.2023.00015 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/S18-2027 (2018)	The FB-IMG-TXT dataset is used to enhance the data diversity of multi-modal knowledge graphs by integrating textual descriptions and images into entities, improving the richness of auxiliary data. It supports multi-modal knowledge graph reasoning, particularly in a transductive setting, by verifying reasoning performance and setting embedding dimensions for image features. The dataset includes 100 images per entity, aiding in representation learning and addressing challenges related to sparsity and complexity in multi-modal data.; The FB-IMG-TXT dataset is used for multimodal knowledge graph representation learning, combining textual descriptions and images. It supports tasks such as multimodal translation-based representation learning and enhancing data diversity by integrating visual and textual information. The dataset features 100 images per entity and sets the embedding dimension for image features to 128, aiding in the comparison of sparsity and complexity in multimodal knowledge graphs.	FB-IMG-TXT	
cited_context | citing_context	Fb122	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB122 dataset is used to evaluate the performance of knowledge graph completion models. It serves as a moderate-sized dataset with a diverse set of relations, enabling researchers to assess model accuracy and effectiveness in completing knowledge graphs. This dataset facilitates the comparison of different models and approaches in knowledge graph reasoning tasks.; The FB122 dataset is used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase entities and relations. It is also employed for evaluating knowledge graph reasoning methods, particularly in entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting missing links and relationships within complex knowledge graphs.	FB122	
cited_context | citing_context	Fb13	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB13 dataset is used to evaluate knowledge graph completion methods, particularly focusing on smaller-scale entity sets and relation types. It enables researchers to assess the performance of these methods in predicting missing links within the graph, facilitating advancements in knowledge graph reasoning techniques.; The FB13 dataset is used to train and evaluate knowledge graph completion models, focusing on a smaller subset of Freebase entities and relations. It is employed in evaluating knowledge graph reasoning methods, particularly for entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in these specific areas, providing a benchmark for comparing different approaches.	FB13	
cited_context	FB15-237	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.1609/aaai.v32i1.11573 (2017)	The FB15-237 dataset is used to train and evaluate multi-modal knowledge graph embedding models, specifically for relation prediction tasks. It employs a filtered set of relations to enhance model performance and accuracy. This dataset enables researchers to focus on improving the predictive capabilities of models in complex knowledge graphs.		
cited_context | citing_context	Fb15K	https://doi.org/10.48550/arXiv.2408.11526 (2024), https://doi.org/10.1145/3637528.3671769 (2024), https://doi.org/10.1145/3656579 (2024) (+7), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1145/3308558.3313612 (2019), https://doi.org/10.1109/TPAMI.2024.3417451 (2022) (+4)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.1609/aaai.v32i1.11535 (2017)	FB15K is primarily used for evaluating and benchmarking knowledge graph embedding models, focusing on link prediction and relational reasoning tasks. It is often extended to include images and additional relation types, enhancing its utility for multi-modal knowledge graph reasoning. The dataset supports entity and relation prediction, entity alignment, and cross-KG reasoning experiments, typically split into training and testing sets to assess model performance.; The FB15K dataset is primarily used for evaluating and training knowledge graph embedding models, focusing on relation prediction and link prediction tasks. It is employed to assess models' ability to predict missing links in multi-relational data, often using a medium-sized subset of Freebase entities and relations. The dataset supports research in knowledge graph completion, entity linking, and entity alignment, with some studies incorporating multimodal reasoning by adding images to entities. It is frequently split into training and testing sets to evaluate model performance, particularly in tasks such as TransE modeling.	FB15K	
citing_context	FB15k-(237)	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The FB15k-(237) dataset is used for evaluating multi-modal knowledge graph reasoning, specifically focusing on more challenging and less noisy relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting relationships within knowledge graphs, enhancing the accuracy and robustness of these predictions.		
cited_context | citing_context	Fb15K-237	https://doi.org/10.1145/3545573 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1145/3394171.3413736 (2020) (+3), https://doi.org/10.1145/3545573 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022) (+1)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.1609/aaai.v32i1.11535 (2017)	FB15k-237 is primarily used to evaluate and enhance knowledge graph completion models, focusing on link prediction and reducing redundancy. It supports multi-modal knowledge graph reasoning by integrating visual and textual data with relational triples. The dataset is utilized to train and evaluate models like Query2GMM, CMGNN, and HRGAT, addressing complex relations, entity linking, and bias reduction. It also facilitates comparisons of convergence speeds and performance improvements in knowledge base completion tasks using methods such as relational graph convolutional networks.; FB15k-237 is primarily used to train and evaluate knowledge graph completion models, focusing on entity linking, relation prediction, and entity prediction tasks. It is employed to reduce redundancy and improve evaluation, often serving as a baseline for comparing different methods, including graph convolutional networks and multimodal approaches that integrate visual and textual data. The dataset enables researchers to assess model performance and convergence speeds in various knowledge graph reasoning tasks.	FB15k-237	
cited_context | citing_context	Fb15K-237-Img	https://doi.org/10.48550/arXiv.2307.03591 (2023), https://doi.org/10.3390/s24237605 (2024), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/1376616.1376746 (2008), https://doi.org/10.1145/3477495.3531992 (2022)	The FB15k-237-IMG dataset is used to enhance multi-modal knowledge graph representation learning by extending entities in FB15k-237 with images. This extension supports visual context integration and entity linking, enabling researchers to evaluate models' performance on multi-modal knowledge graph reasoning tasks. The dataset's inclusion of images for each entity improves the visual aspect of entity relationships, facilitating more comprehensive and contextually rich reasoning.; The FB15K-237-IMG dataset is used to extend the scope of triplets in multimodal knowledge graph completion by integrating image data with textual information. This integration enhances the representation and reasoning capabilities of knowledge graphs, enabling more comprehensive and contextually rich completions. The dataset's unique combination of images and textual data supports advanced multimodal reasoning tasks, facilitating research in areas such as enhanced entity linking and semantic understanding.	FB15K-237-IMG, FB15k-237-IMG	
cited_context	FB15k-237-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The FB15k-237-sparse dataset is used to evaluate and train multi-modal knowledge graph reasoning methods, particularly focusing on the sparse subset of the filtered Freebase knowledge graph. It is utilized to assess the performance of models like IterE+axioms, emphasizing the impact of deductive capabilities. The dataset supports hyperparameter tuning, including embedding dimensions and learning rate, and is used for training and validation with a maximum of 10 iterations.		
cited_context | citing_context	Fb15K-Db15K	https://doi.org/10.3390/app13116747 (2023), https://doi.org/10.48550/arXiv.2407.19625 (2024), https://doi.org/10.1145/3637528.3671769 (2024) (+3), https://doi.org/10.1145/3534678.3539244 (2022), https://doi.org/10.1145/3581783.3611786 (2022), https://doi.org/10.1609/aaai.v38i8.28762 (2024) (+1)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-DB15K dataset is primarily used for cross-KG entity alignment and reasoning, focusing on the alignment between Freebase and DBpedia subsets. It is employed in evaluating the performance of pre-trained models like VGG-16 for visual pivoting and assessing multi-modal reasoning in cross-KG settings. The dataset supports ablation experiments to verify correction schemes and cross-lingual alignment, and it is split into training and testing sets in various proportions (2:8, 5:5, 8:2) to evaluate model performance and computational challenges.; The FB15K-DB15K dataset is primarily used for evaluating and training entity alignment methods in multi-modal knowledge graphs. It focuses on aligning entities across different modalities and datasets, such as FB15K and DB15K. Research employs this dataset to compare various alignment techniques (e.g., MSNEA, PoE, MMEA, EVA) using metrics like Hits@1, Hits@10, and MRR. Studies often use different training-testing splits (2:8, 5:5, 8:2) and explore the impact of multi-modal knowledge on alignment performance, particularly in monolingual settings.	FB15K-DB15K	
cited_context	FB15k-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The FB15k-sparse dataset is used to evaluate and train multi-modal knowledge graph reasoning methods, specifically focusing on the sparse subset of the Freebase knowledge graph. It is utilized for hyperparameter tuning, such as adjusting embedding dimensions and learning rates, with a maximum of 50 training iterations. This dataset enables researchers to assess the performance of their models in handling sparse data, enhancing the robustness of knowledge graph reasoning techniques.		
cited_context | citing_context	Fb15K-Yago15K	https://doi.org/10.1145/3637528.3671769 (2024), https://doi.org/10.48550/arXiv.2407.19625 (2024), https://doi.org/10.1609/aaai.v38i8.28762 (2024), https://doi.org/10.1145/3581783.3611786 (2022), https://doi.org/10.1609/aaai.v38i8.28762 (2024)	https://doi.org/10.1609/aaai.v35i5.16550 (2020), https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-YAGO15K dataset is primarily used for evaluating entity alignment in multi-modal knowledge graphs, particularly focusing on cross-KG entity alignment tasks. Researchers use it to assess the performance of methods like pre-trained VGG-16 for visual pivoting and the impact of seed entities or pre-aligned entities on alignment accuracy. This dataset facilitates the evaluation of cross-KG reasoning and alignment between subsets of Freebase and YAGO.; The FB15K-YAGO15K dataset is used for evaluating and training entity alignment in multi-modal knowledge graphs, particularly focusing on cross-KG entity matching and alignment accuracy. It supports monolingual experiments within the MMKG framework, enabling researchers to assess the impact of varying proportions of reference entity alignment pairs on model performance. This dataset facilitates the development and testing of methods for improving the accuracy and robustness of entity alignment in multi-modal settings.	FB15K-YAGO15K	
cited_context | citing_context	Fb15K-Yg15K	https://doi.org/10.1145/3534678.3539244 (2022)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-YG15K dataset is used to evaluate entity alignment methods in multi-modal knowledge graph reasoning. It specifically focuses on aligning entities with 20% alignment seeds, enabling researchers to assess the effectiveness of these methods in integrating and reasoning across different knowledge graphs. This dataset facilitates the development and testing of algorithms designed to improve entity alignment accuracy.; The FB15K-YG15K dataset is used for evaluating entity alignment methods in multi-modal knowledge graph reasoning. It focuses on aligning entities across different modalities, using 20% alignment seeds to assess the effectiveness of these methods. This dataset enables researchers to test and improve algorithms designed for cross-modal entity alignment in knowledge graphs.	FB15K-YG15K	
citing_context	FB15M	https://doi.org/10.1145/3534678.3539244 (2022)	https://doi.org/10.1007/978-3-030-75762-5_40 (2021)	The dataset 'FB15M' is mentioned in the citation context but lacks detailed descriptions of its usage in research. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling features in any particular research area.		
cited_context | citing_context	Fb20K	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB20k dataset is used to evaluate the scalability and performance of knowledge graph completion algorithms. Researchers employ this dataset to test how well these algorithms handle larger-scale data, focusing on their efficiency and effectiveness in completing knowledge graphs. This enables the assessment of algorithmic capabilities in managing extensive and complex graph structures.; The FB20k dataset is used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase entities and relations. It is also employed for evaluating knowledge graph reasoning methods, particularly in entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting missing links and relationships within complex knowledge graphs.	FB20k	
cited_context | citing_context	Fb24K	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB24k dataset is used to evaluate the effectiveness of knowledge graph completion methods, particularly on a dataset with a slightly larger entity set. It enables researchers to assess the performance and scalability of these methods, focusing on the accuracy and efficiency of completing knowledge graphs.; The FB24k dataset is used to train and evaluate knowledge graph completion models, specifically focusing on entity linking and relation prediction tasks within a subset of the Freebase dataset. It enables researchers to assess the performance of knowledge graph reasoning methods by providing a structured set of entities and relations, facilitating the development and testing of algorithms designed to predict missing links and relationships.	FB24k	
cited_context	FB400k	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.18653/v1/N18-1059 (2018)	The FB400k dataset is used for validating and testing SPARQL annotations and evaluating knowledge graph reasoning, particularly focusing on complex questions and their answers within the knowledge graph. It serves as a subset of Freebase, enabling researchers to assess the performance of reasoning systems on intricate web queries.		
cited_context | citing_context	Fb5M	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB5M dataset is used to evaluate the performance of knowledge graph completion models, particularly focusing on their scalability and efficiency when handling very large datasets. This dataset enables researchers to test and compare different methodologies in knowledge graph completion, ensuring they can effectively manage extensive data volumes.; The FB5M dataset is used to train and evaluate knowledge graph completion models, focusing on a large subset of Freebase entities and relations. It is employed in evaluating knowledge graph reasoning methods, particularly for entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting missing links and relationships within extensive knowledge graphs.	FB5M	
cited_context | citing_context	Fb60K-Nyt10	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB60k-NYT10 dataset is used to test the integration of knowledge graphs with textual data, specifically focusing on linking entities from New York Times articles. This involves methodologies that combine structured knowledge graph data with unstructured textual information to enhance entity recognition and linking. The dataset enables researchers to evaluate and improve the accuracy of entity linking in multi-source information systems.; The FB60k-NYT10 dataset is used to train and evaluate knowledge graph completion models, specifically focusing on entity linking and relation prediction tasks. It combines Freebase entities with New York Times articles to create a multimodal dataset, enabling researchers to assess the performance of knowledge graph reasoning methods in a rich, real-world context.	FB60k-NYT10	
cited_context	FBDB15K	https://doi.org/10.1145/3581783.3611786 (2022)	https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108 (2014)	The FBDB15K dataset is used for multi-modal knowledge graph reasoning, particularly focusing on entity alignment with vision features. It employs the VGG-16 vision encoder and 4096-dimensional vision features to evaluate and demonstrate model stability in integrating visual and textual data within knowledge graphs. This dataset enables researchers to assess the effectiveness of vision encoders in enhancing multi-modal reasoning tasks.		
cited_context	FBYG15K	https://doi.org/10.1145/3581783.3611786 (2022)	https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108 (2014)	The FBYG15K dataset is used for multi-modal knowledge graph reasoning, particularly focusing on entity alignment with vision features. It employs the VGG-16 vision encoder and 4096-dimensional vision features to evaluate and demonstrate model stability in integrating visual and textual data. This dataset enables researchers to assess the effectiveness of vision encoders in enhancing multi-modal knowledge graph reasoning tasks.		
citing_context	FER2013	https://doi.org/10.1145/3583690 (2023)	https://doi.org/10.1016/j.neunet.2014.09.005 (2013)	The FER2013 dataset is used to train a FaceNet model for extracting facial emotion features from detected faces in video frames. This involves employing deep learning techniques to recognize and classify emotions, enabling research focused on facial emotion recognition in various applications such as human-computer interaction and affective computing. The dataset's large-scale and diverse set of labeled facial images facilitates robust model training and validation.		
cited_context | citing_context	Flick30K Entities	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Flick30k Entities dataset is used to enhance entity recognition in images and their associated text, facilitating fine-grained multimedia knowledge extraction. Researchers apply this dataset to improve the accuracy of entity detection and linking in visual and textual data, supporting more robust and detailed multimedia analysis. This dataset enables the development and evaluation of models that integrate visual and textual information for enhanced knowledge extraction.; The Flick30k Entities dataset is used to train detectors with supervised data, focusing on image-text pairs annotated with entities. This supports multi-modal reasoning tasks by leveraging the aligned visual and textual information. The dataset's entity annotations enable researchers to develop models that can reason across both modalities, enhancing the understanding and integration of visual and textual data.	Flick30k Entities	
citing_context	Flickr-0	https://doi.org/10.1609/aaai.v36i2.20123 (2022)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The Flickr-0 dataset, a sub-sampled split of Flickr30K Entities, is used for training and evaluating models that focus on region-phrase correspondences. This involves aligning textual phrases with specific regions in images, enabling research in visual grounding and multimodal understanding. The dataset's region-phrase annotations facilitate the development and assessment of algorithms that can accurately map textual descriptions to corresponding image segments.		
citing_context	Flickr-1	https://doi.org/10.1609/aaai.v36i2.20123 (2022)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The Flickr-1 dataset, a sub-sampled split of Flickr30K Entities, is used for training and evaluating models that focus on region-phrase correspondences. This involves aligning textual phrases with specific regions in images, enabling research in visual grounding and multimodal understanding. The dataset's region-phrase annotations facilitate the development and assessment of algorithms that can accurately map textual descriptions to corresponding image segments.		
cited_context | citing_context	Flickr30K	https://doi.org/10.1609/aaai.v38i12.29280 (2024), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2501.04173 (2025) (+2), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014), https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Flickr30k dataset is primarily used for evaluating and training models in multi-modal tasks, such as image captioning, visual grounding, and image-text matching. It provides a diverse set of images with multiple captions, enabling researchers to fine-tune and evaluate models on tasks like generating textual descriptions from images, aligning visual and textual modalities, and cross-modal hashing retrieval. The dataset supports methodologies including contrastive knowledge distillation, weakly supervised learning, and feature extraction with mechanisms like Faster RCNN and BUTD. It is also used to set hyperparameters for momentum encoders and to improve detectors on image-caption pairs, enhancing the alignment and reasoning capabilities between visual and textual data.; The Flickr30k dataset is primarily used for evaluating and training multi-modal systems, specifically in visual grounding and image-text matching. It supports fine-grained multimedia knowledge extraction and aligns textual descriptions with corresponding images. The dataset's image-text pairs enable researchers to assess the accuracy of these alignments and train detectors for multi-modal reasoning tasks.	Flickr30k	
citing_context	Flickr30K Entities	https://doi.org/10.1609/aaai.v36i2.20123 (2022), https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The Flickr30K Entities dataset is used for pre-training models like the Oscar-based chunk-aware semantic interactor for phrase-level semantic alignment between text and images, particularly in multi-modal knowledge graph reasoning. It is also utilized for zero-shot phrase grounding experiments, providing region-to-phrase correspondences to enhance image-to-sentence models. Researchers often use sub-sampled splits to focus on specific research needs, enriching the models with detailed region-phrase mappings.		
citing_context	food.com	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.18653/v1/N19-1423 (2019)	The food.com dataset is used to analyze recipes from both home cooks and celebrity chefs, with a focus on integrating food news and pop culture into multi-modal reasoning in culinary contexts. This involves enhancing the understanding of culinary practices and trends through the analysis of textual and visual data, enabling more nuanced reasoning about food and cooking.		
cited_context	FrameNet	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	FrameNet is integrated into ontologies to provide frame-based semantic structures, which support the reasoning process in multi-modal knowledge graphs. This integration enhances the representation and processing of complex, multi-modal data by leveraging FrameNet's detailed semantic frames, enabling more sophisticated and context-aware reasoning capabilities.		
cited_context | citing_context	Freebase	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.1109/ICDE60146.2024.00061 (2024) (+7), https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.1145/3581783.3612151 (2023) (+7)	https://doi.org/10.1145/1376616.1376746 (2008)	Freebase is used as a collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia. It supports various research tasks, including entity matching, recommendation systems, and multimedia reasoning. Researchers use it to evaluate entity matching approaches, provide pre-trained entity embeddings for multi-relational data modeling, and represent entities and relations in a graph structure. Its large, structured format enables the evaluation of multi-modal reasoning and the integration of diverse information sources.; Freebase is used as a collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia and NNDB. It supports knowledge-driven applications such as recommendation systems, information retrieval, and machine learning. Freebase is also utilized for tasks like question answering and evaluating entity matching approaches, focusing on linking entities across different knowledge bases. Its extensive structured data enables researchers to enhance and evaluate various knowledge-driven systems.	Freebase	
cited_context	Freebase KG	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.1145/3397271.3401172 (2020)	The Freebase KG dataset is used to conduct multi-hop reasoning experiments and train knowledge graph embeddings, focusing on the scalability and efficiency of these processes in large-scale graphs with 86M nodes and 338M edges. This enables researchers to evaluate and improve embedding algorithms for handling extensive and complex knowledge graphs.		
citing_context	Freebase-15k (237) Multi-Modal (FB15k-(237))	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The Freebase-15k (237) Multi-Modal (FB15k-(237)) dataset is used to enhance multi-modal knowledge graphs by integrating visual and textual data. It focuses on 237 relations to improve entity linking and reasoning, enabling more accurate and contextually rich knowledge graph constructions. This dataset supports research in multi-modal knowledge graph enhancement, specifically addressing the integration of diverse data types to refine entity relationships and reasoning capabilities.		
citing_context	Freebase-15k Multi-Modal (FB15k)	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The Freebase-15k Multi-Modal (FB15k) dataset is used to enhance knowledge graph reasoning by integrating visual and textual information, extending the original FB15k dataset. This integration supports research in multi-modal data fusion, enabling more comprehensive and contextually rich reasoning tasks. The dataset's multi-modal features facilitate the development and evaluation of models that can leverage both visual and textual inputs for improved reasoning capabilities.		
cited_context | citing_context	Fvqa	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1609/aaai.v33i01.33018876 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TNNLS.2020.3045034 (2020) (+1)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The FVQA dataset is used for fact-based visual question answering, integrating factual knowledge with visual information. It captures relationships among mentions and entities, providing knowledge about named entities and their relations in images. This integration allows researchers to enhance the accuracy and depth of answers by leveraging supporting fact subgraphs, making it suitable for tasks that require both visual and textual understanding.; The FVQA dataset is used to evaluate methods that integrate knowledge graphs with image representations for factual visual question answering. It focuses on the accuracy of answers generated from combined visual and textual inputs, emphasizing detailed visual reasoning, commonsense reasoning, and the integration of external knowledge sources. This dataset captures relationships among mentions and entities, providing knowledge about named entities and their relations in images, and is used to obtain questions directly from existing knowledge bases. It enables research by facilitating the evaluation of models that combine textual and visual information to answer questions accurately.	FVQA	
citing_context	Geometry3K	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.18653/v1/2021.acl-long.528 (2021)	The Geometry3K dataset is used to evaluate geometry problem-solving capabilities, particularly focusing on formal language and symbolic reasoning in multimodal contexts. It enables researchers to assess and improve multimodal reasoning methods, specifically in handling geometry problems that require both visual and textual understanding.		
cited_context | citing_context	Global Database Of Events	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/2187980.2188242 (2012), https://doi.org/10.1145/3511808.3557233 (2022)	The Global Database of Events, Language, and Tone (GDELT) is used for dynamic Knowledge Graph Reasoning (KGR) tasks, providing event data and linguistic tone analysis for global events. This dataset enables researchers to analyze and reason over temporal and spatial dynamics of world events, enhancing understanding of global phenomena through structured event data and sentiment analysis.; The Global Database of Events, Language, and Tone is used to construct a dense knowledge graph for temporal knowledge graph completion. Researchers focus on analyzing events and tones over time, employing methodologies that enable the derivation of temporal relationships and patterns. This dataset facilitates research into how events and their associated tones evolve, providing insights into dynamic social and political phenomena.	Global Database of Events	
cited_context | citing_context	Goodnews	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TMM.2023.3301279 (2021), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1646396.1646452 (2009)	The GoodNews dataset is primarily used for image captioning in news contexts, leveraging multi-modal data and graph-structured information to enhance context understanding. It supports entity-aware captioning, ensuring descriptions are context-driven and entity-relevant. The dataset includes images, captions, and news articles from the New York Times, annotated with ground-truth captions, enabling researchers to generate more accurate and contextually rich image descriptions.; The GoodNews dataset is used for enhancing image captioning by integrating textual descriptions with visual content through graph-structured information. This approach improves reasoning capabilities, allowing for more accurate and contextually rich captions. The dataset's multi-modal nature, combining text and images, is crucial for training models that can effectively relate and generate descriptions based on visual inputs.	GoodNews	
citing_context	GossipCop	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/N16-1174 (2016)	The GossipCop dataset is used to evaluate the performance of KGF and KGT models in detecting fake news, specifically focusing on the impact of different embedding dimensions. This dataset enables researchers to assess how varying embedding sizes affect the accuracy and efficiency of these models in distinguishing between real and fake news.		
cited_context | citing_context	Gqa	https://doi.org/10.1145/3579051.3579073 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1007/s11263-016-0981-7 (2016), https://doi.org/10.1109/CVPR.2019.00331 (2019)	The GQA dataset is utilized for grounded question answering and visual reasoning, providing rich annotations and scene graphs that integrate visual and textual information. It supports the evaluation of various visual question answering methods, focusing on relational reasoning and complex question answering tasks that require multi-step reasoning over images and knowledge graphs. The dataset captures relationships among entities, enabling research that combines external knowledge with visual data.; The GQA dataset is used to test and enhance models' abilities in multi-modal reasoning, specifically focusing on visual scene understanding and compositional question answering. It evaluates how well models can integrate visual and textual information to reason about complex queries and ground answers in context. The dataset captures relationships among entities in images, providing rich knowledge about named entities and their interactions, which is crucial for grounded question answering and visual reasoning tasks.	GQA	
cited_context	HacRED	https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126 (2023)	https://doi.org/10.1186/s12859-020-03889-5 (2020)	The HacRED dataset is used for empirical evaluation in relation extraction, specifically to assess model performance in extracting relations from text. It enables researchers to test and compare various relation extraction models, focusing on their accuracy and effectiveness in identifying relationships within textual data.		
cited_context | citing_context	Haspart Kb	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The 'hasPart KB' dataset is used to provide part-whole relationships, specifically through has-Part triples, which contribute to the construction of knowledge graphs. It aids in the comprehension of complex objects and their components, particularly in the context of Visual Question Answering. This dataset enables researchers to enhance the reasoning capabilities of systems by integrating detailed part-whole relationships into knowledge graphs.; The 'hasPart KB' dataset is used to provide hasPart triples, which contribute to the construction of knowledge graphs for open-domain Visual Question Answering (VQA). It offers part-whole relationships, aiding in the understanding of complex objects and their components. This dataset enhances VQA systems by integrating explicit knowledge, improving the accuracy and depth of answers.	hasPart KB	
citing_context	HICO-DET	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.1109/WACV.2018.00048 (2017)	The HICO-DET dataset is primarily used to detect human-object interactions in visual scenes. Researchers employ manually labeled object bounding boxes to enhance multi-modal reasoning, focusing on the accurate identification and interaction analysis between humans and objects. This dataset enables detailed studies on visual scene understanding and interaction detection, leveraging its rich annotations to improve the performance of multi-modal reasoning models.		
citing_context	HotPotQA	https://doi.org/10.48550/arXiv.2501.04173 (2025)	https://doi.org/10.1109/CVPR.2019.01094 (2018)	The HotPotQA dataset is used to develop and evaluate text-based question answering (QA) systems that require reasoning over multiple documents. It enhances multi-modal knowledge graph reasoning capabilities, focusing on complex queries that necessitate information integration from various sources. This dataset supports research in improving the accuracy and reasoning abilities of QA systems, particularly in handling multi-document contexts.		
citing_context	Houston dataset	https://doi.org/10.1080/01431161.2023.2240032 (2023)	https://doi.org/10.1016/J.ISPRSJPRS.2021.05.011 (2021)	The Houston dataset is used for multimodal remote sensing, focusing on land cover classification by integrating Hyperspectral Imaging (HSI) and Multispectral Imaging (MSI) data. Researchers employ shared and specific feature learning models to enhance classification accuracy, leveraging the dataset's detailed spectral and spatial information. This approach addresses the challenge of accurately classifying diverse land cover types in urban environments.		
citing_context	ICD/ATC	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1109/TCYB.2021.3109881 (2021)	The ICD/ATC dataset is used as a hierarchical dataset for self-supervised graph learning in healthcare, specifically to explore the relationships between International Classification of Diseases (ICD) and Anatomical Therapeutic Chemical (ATC) codes. This approach leverages the structured nature of the dataset to enhance understanding and prediction in medical coding and classification systems.		
citing_context	ICD/ATC  CO	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1109/TCYB.2021.3109881 (2021)	The ICD/ATC  CO dataset is used to enhance the representation of medical events in graph learning models by incorporating co-occurrence information. This extended version of ICD/ATC is specifically employed to improve the accuracy and richness of medical event representations in knowledge graphs, enabling more effective graph learning and reasoning tasks.		
citing_context	ICEWS	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	The ICEWS dataset is used to provide event data with daily temporal granularity, focusing on military and political events. It is employed in multi-modal knowledge graph reasoning research, enabling the analysis and reasoning over complex, time-sensitive event data. This dataset facilitates the exploration of relationships and patterns in military and political activities, enhancing the understanding of dynamic socio-political phenomena.		
cited_context	ICEWS-WIKI	https://doi.org/10.18653/v1/2024.findings-emnlp.148 (2024)	https://doi.org/10.48550/arXiv.2304.03468 (2023)	The ICEWS-WIKI dataset is extended to a multi-modal version to enhance multi-modal Knowledge Graph Reasoning. It integrates textual and visual information, enabling researchers to explore the integration of these modalities in knowledge graphs. This approach focuses on improving reasoning capabilities by leveraging both text and images, addressing research questions related to multi-modal data fusion and representation.		
cited_context	ICEWS-YAGO	https://doi.org/10.18653/v1/2024.findings-emnlp.148 (2024)	https://doi.org/10.48550/arXiv.2304.03468 (2023)	The ICEWS-YAGO dataset is extended to a multi-modal version to enhance multi-modal Knowledge Graph Reasoning, specifically by integrating textual and visual information. This extension supports research focused on improving the reasoning capabilities of knowledge graphs through the incorporation of diverse data types, enabling more comprehensive and contextually rich analyses.		
citing_context	ICS Vulnerability Database	https://doi.org/10.1109/ISADS56919.2023.10092024 (2023)		The ICS Vulnerability Database is used to construct the ICS-Het-KG, a specialized knowledge graph for industrial control systems. It provides detailed vulnerability data, enhancing the graph's relevance and specificity. This dataset improves the accuracy and detail of the knowledge graph, supporting research focused on enhancing security in industrial control systems.		
citing_context	ImageGraph	https://doi.org/10.1145/3656579 (2024)	https://doi.org/10.24432/C56P45 (2017)	ImageGraph is used to support visual-relational queries in web-extracted knowledge graphs by extending FB15K with 829,931 images and 1,330 relation types. Researchers employ this dataset to enhance multi-modal reasoning capabilities, enabling more complex and contextually rich queries that integrate both textual and visual data. This extension facilitates the exploration of how visual information can complement and refine relational knowledge in large-scale graphs.		
cited_context | citing_context	Imagenet	https://doi.org/10.1609/aaai.v36i2.20123 (2022), https://doi.org/10.1145/3637528.3671769 (2024), https://doi.org/10.1145/3394171.3413711 (2020) (+2), https://doi.org/10.1109/IJCNN55064.2022.9892382 (2022)	https://doi.org/10.1109/cvpr.2016.90 (2015), https://doi.org/10.1109/CVPR.2009.5206848 (2009)	ImageNet is primarily used to pre-train deep learning models such as ResNet-50, ResNet-152, and VGG-16, which extract high-dimensional feature vectors from images. These pre-trained features are then utilized to initialize the backbone image encoders in models like SSD and RetinaNet, enhancing their performance in multi-modal reasoning tasks. The dataset's large-scale hierarchical structure and rich semantic information enable effective transfer learning, improving the models' ability to learn and generalize from visual data.; The ImageNet dataset is primarily used to train deep learning models like ResNet50, focusing on hierarchical image classification. It enables researchers to extract robust visual features from video frames, enhancing the accuracy of image and video analysis tasks. The large-scale, hierarchically organized nature of ImageNet supports the development and evaluation of advanced visual recognition systems.	ImageNet	
cited_context | citing_context	Imdb	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	https://doi.org/10.1007/978-3-642-13489-0_23 (2010), https://doi.org/10.1093/nar/gkv1075 (2015)	The IMDB dataset is used to evaluate entity matching approaches, particularly in the context of the OAEI, by focusing on linking entities across different knowledge bases. This involves comparing and aligning entities from various sources to enhance data integration and interoperability. The dataset's rich entity relationships and attributes facilitate robust testing and validation of entity matching algorithms.; The IMDB dataset is used as a domain-specific knowledge graph to provide structured information about movies, actors, and directors. It is employed to evaluate entity matching approaches, focusing on linking entities across different knowledge bases. This enables researchers to enhance the accuracy and efficiency of data integration and knowledge base alignment methods.	IMDB	
cited_context | citing_context	Imgpedia	https://doi.org/10.1109/TKDE.2025.3546686 (2023), https://doi.org/10.1609/aaai.v38i17.29828 (2024), https://doi.org/10.1145/3627673.3679175 (2024), https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1007/978-3-319-68204-4_8 (2017)	The IMGpedia dataset is used to enhance multi-modal knowledge graph reasoning by providing content-based analysis of Wikimedia images. It links image metadata and annotations to structured data in DBpedia, grounding visual content and enriching linked data. This enables researchers to explore and reason about the relationships between visual and textual information in a more integrated manner.; The IMGpedia dataset is used to explore multi-modal relationships within a large knowledge graph, specifically focusing on image-to-image and image-to-text connections. This enhances the understanding of multimedia content by integrating visual and textual data, enabling researchers to analyze and reason about complex multi-modal interactions.	IMGpedia	
cited_context	imSitu	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://www.semanticscholar.org/paper/d53bcbac7ea19173e95d3bd855b998fab765737d (1998)	The imSitu dataset is used to enhance multi-modal knowledge graphs by incorporating situation recognition and visual semantic role labeling. It enriches the visual ontology through the alignment of WordNet synsets to annotated frames, thereby improving the semantic representation of visual scenes. This methodology supports research in extending and refining visual ontologies, enabling more nuanced understanding and interpretation of visual data.		
citing_context	InfoSeek	https://doi.org/10.48550/arXiv.2504.10074 (2025)	https://doi.org/10.1109/ICCV51070.2023.00289 (2023)	The InfoSeek dataset is used to evaluate multi-modal knowledge retrieval systems, integrating Wikipedia-scale knowledge with 1.3 million image-question pairs linked to 11,000 Wikipedia pages. It assesses models' abilities to connect visual and textual information, emphasizing comprehensive entity understanding and fine-grained details. Research focuses on validating these capabilities using 73,000 validation samples, highlighting the dataset's role in advancing multi-modal reasoning.		
citing_context	Integrated Crisis Early Warning System (ICEWS)	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1145/2187980.2188242 (2012)	The Integrated Crisis Early Warning System (ICEWS) dataset is used for dynamic Knowledge Graph Reasoning (KGR) tasks, providing structured data on political events and crisis situations. It enables researchers to analyze temporal patterns and relationships within these events, facilitating the development of predictive models for crisis early warning systems. The dataset's structured nature supports the integration of event data into knowledge graphs, enhancing the accuracy and robustness of KGR methodologies.		
citing_context	Karpathy	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://doi.org/10.1109/CVPR.2015.7298932 (2014)	The Karpathy dataset is primarily used for training and evaluating image captioning and vision-language models, focusing on the alignment of visual and semantic information. It supports research in generating accurate image descriptions and aligning visual and textual data, enabling the development and assessment of deep visual-semantic alignment models.		
cited_context | citing_context	Kb-Vqa	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/aaai.v33i01.33018876 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.18653/v1/2020.findings-emnlp.44 (2020)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016), https://doi.org/10.24963/IJCAI.2017/179 (2015)	The KB-VQA dataset is used to enhance visual question answering by integrating external knowledge from knowledge bases. It captures relationships among mentions and entities, providing detailed knowledge about named entities and their relations in images. This integration allows researchers to improve the accuracy and context-awareness of visual question answering systems, leveraging structured external information to better understand and answer complex queries.; The KB-VQA dataset is used for knowledge-based visual question answering, focusing on integrating external knowledge bases with image understanding. It is employed to compare against datasets with limited images and external knowledge, emphasizing the need for comprehensive data in multi-modal knowledge graph reasoning. The dataset supports explicit knowledge-based reasoning, generating template-based questions and capturing relationships among mentions and entities in images. This enables researchers to enhance visual question answering by leveraging structured knowledge graphs.	KB-VQA	
cited_context | citing_context	Kgbench	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1007/978-3-030-77385-4_37 (2021)	The kgbench dataset is used to evaluate relational and multimodal machine learning models by providing a collection of knowledge graph datasets for benchmarking. It enables researchers to assess model performance across various knowledge graphs, focusing on tasks such as entity prediction and relation classification. The dataset's diverse and multimodal nature supports the development and comparison of advanced machine learning algorithms in knowledge graph reasoning.; The kgbench dataset is used to evaluate and benchmark relational and multimodal machine learning methods across a collection of knowledge graph datasets. It focuses on assessing performance across different modalities, enabling researchers to compare and analyze the effectiveness of various models in handling complex, multimodal data.	kgbench	
citing_context	KGF	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.1007/978-3-030-00671-6_39 (2018)	The KGF dataset is used to construct a knowledge graph focused on fake news, specifically analyzing the structure and content of misleading news articles. This dataset enables researchers to develop and test methods for detecting fake news by leveraging the detailed structural and textual features of the articles.		
citing_context	KGT	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.1007/978-3-030-00671-6_39 (2018)	The KGT dataset is used to construct a knowledge graph of real news, focusing on the structure and content of authentic news articles. This dataset enables researchers to analyze and detect patterns in news articles by leveraging the structured representation of news data, enhancing the accuracy of news detection methodologies.		
citing_context	KI-VQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.18653/v1/2021.acl-long.528 (2021)	The KI-VQA dataset is used to test knowledge-intensive visual question answering, specifically focusing on integrating external knowledge with visual data. This dataset enables researchers to evaluate models that combine textual and visual information, addressing research questions related to the effective fusion of these modalities for more accurate and contextually rich answers.		
citing_context	KiloGram	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2211.16492 (2022)	The KiloGram dataset is used for data augmentation in multi-modal reasoning tasks, particularly in ScienceQA. It addresses sparse data issues and facilitates the construction of multi-step, multi-modal samples, enhancing the robustness and complexity of reasoning tasks. This dataset enables researchers to improve model performance by providing diverse and rich data samples.		
citing_context	Kinetics-TPS	https://doi.org/10.1145/3503161.3548257 (2022)	https://doi.org/10.1609/AAAI.V34I07.6836 (2019)	The Kinetics-TPS dataset is used to enhance multi-modal reasoning in video recognition tasks by providing over 15 million part-level annotations for detailed human action understanding. It is employed to construct and enrich visual knowledge graphs, focusing on action parsing and temporal segment annotation, which improves the accuracy and detail of human action recognition in videos.		
cited_context	Knowledge Vault	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The Knowledge Vault dataset is used as a large-scale knowledge graph to integrate diverse sources of information, supporting comprehensive knowledge representation. It enables researchers to aggregate and unify data from various sources, facilitating the creation of a more holistic and interconnected knowledge base. This integration supports research requiring extensive and multifaceted information, enhancing the depth and breadth of knowledge available for analysis.		
citing_context	KVC16K	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://doi.org/10.1145/3626772.3657800 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The KVC16K dataset is used in multi-modal Knowledge Graph Reasoning research to evaluate and validate trends, particularly in knowledge graph completion tasks. It focuses on predicting missing links using both textual and visual data, assessing models like M O M O K through performance metrics such as MRR and Hit@1. This dataset enables researchers to test the consistency and effectiveness of multi-modal reasoning approaches across different datasets.		
cited_context | citing_context	Kvqa	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1109/CVPR.2019.00331 (2019), https://doi.org/10.1109/CVPR.2017.215 (2016)	The KVQA dataset is used to test and evaluate models' ability to reason about visual content using external knowledge, incorporating knowledge graphs to answer questions that require structured data reasoning. It captures relationships among mentions and entities, providing knowledge about named entities and their relations in images, specifically designed to challenge models with complex reasoning tasks.; The KVQA dataset is primarily used for visual question answering (VQA) research, focusing on integrating visual data with structured knowledge graphs. It evaluates and trains models on their ability to reason about complex visual content using external knowledge, specifically designed to challenge state-of-the-art methods in multi-modal reasoning. The dataset captures relationships among mentions and entities, enabling researchers to study VQA over knowledge graphs and assess the integration of world knowledge with visual data.	KVQA	
citing_context	LAION-5B	https://doi.org/10.1145/3592573.3593102 (2023)	https://doi.org/10.48550/arXiv.2210.08402 (2022)	The LAION-5B dataset is primarily used to train the OpenCLIP model, focusing on capturing semantic relationships between images and text. This supports the development of advanced multi-modal reasoning models, enabling more sophisticated image-text understanding. The dataset's large scale and diverse content are crucial for training robust models that can handle complex reasoning tasks.		
cited_context | citing_context	Language	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/2187980.2188242 (2012), https://doi.org/10.1145/3511808.3557233 (2022)	The Global Database of Events, Language, and Tone (GDELT) is used for dynamic Knowledge Graph Reasoning (KGR) tasks, providing event data and linguistic tone analysis for global events. This dataset enables researchers to analyze and reason over temporal and spatial dynamics of world events, enhancing understanding of global phenomena through structured event data and sentiment analysis.; The Global Database of Events, Language, and Tone is used to construct a dense knowledge graph for temporal knowledge graph completion. Researchers focus on analyzing events and tones over time, employing methodologies that enable the derivation of temporal relationships and patterns. This dataset facilitates research into how events and their associated tones evolve, providing insights into dynamic social and political phenomena.	Language	
citing_context	large-scale noisy image-text pairs	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99 (2022)	The 'large-scale noisy image-text pairs' dataset is used to pre-train a multi-modal mixture of the encoder-decoder model. Researchers focus on enhancing model robustness by injecting diverse synthetic captions and removing noisy captions. This approach aims to improve the quality and reliability of the model's outputs in multi-modal tasks.		
citing_context	LVIS	https://doi.org/10.1145/3583690 (2023)	https://doi.org/10.1109/CVPR.2019.00550 (2019)	The LVIS dataset is used to pretrain the ResNet152 model for large vocabulary instance segmentation, enhancing multi-modal reasoning capabilities. It is also utilized to detect objects and extract features, which contribute to the object component in multi-modal Knowledge Graph Reasoning. This dataset enables researchers to improve the accuracy and richness of feature extraction and object detection, crucial for advanced multi-modal reasoning tasks.		
citing_context	M2KR	https://doi.org/10.48550/arXiv.2504.10074 (2025)		The M2KR dataset is used to evaluate multi-modal knowledge retrieval systems, specifically focusing on tasks that involve retrieving information from multi-modal knowledge graphs. The dataset enables researchers to assess the performance of these systems in handling and integrating different types of data, though details about its content and structure are not provided. This evaluation helps address research questions related to the effectiveness and efficiency of multi-modal knowledge retrieval methods.		
citing_context	MathVista	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The MathVista dataset is used to test mathematical problem-solving skills, particularly in multimodal settings. It integrates visual and textual data to comprehensively evaluate and assess the capabilities of models in solving complex mathematical problems. The dataset emphasizes the combination of text and images, enabling researchers to focus on multi-modal reasoning and the effective integration of diverse information types.		
citing_context	MEISD	https://doi.org/10.1145/3593583 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.393 (2020)	The MEISD dataset is used for assessing multi-modal emotion recognition and sentiment analysis. It includes multi-label emotion, intensity, and sentiment dialogue data, enabling researchers to evaluate models that can recognize and analyze emotions and sentiments across multiple modalities. This dataset supports the development and testing of algorithms designed to handle complex emotional and sentimental content in dialogues.		
citing_context	MELD	https://doi.org/10.1145/3593583 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.393 (2020)	The MELD dataset is used to evaluate multi-modal emotion recognition and sentiment analysis, specifically focusing on textual and visual utterances annotated with sentiment and emotion labels. Researchers employ this dataset to develop and test models that can accurately recognize emotions and sentiments from both text and visual data, enhancing the understanding of human communication in multi-modal contexts.		
cited_context | citing_context	Microsoft Coco	https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-46448-0_51 (2016), https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The Microsoft COCO dataset is primarily used to evaluate visual context models, focusing on metrics such as recall at K and mean R@K. It is employed to assess the performance of proposed approaches in visual recognition tasks, enabling researchers to benchmark their methods against established standards. The dataset's rich annotations and diverse images facilitate robust evaluation in these areas.; The Microsoft COCO dataset is primarily used to evaluate model performance in identifying a diverse set of common objects within complex scenes. It serves as a benchmark for object detection and image recognition tasks, enabling researchers to assess the accuracy and robustness of their models in real-world scenarios. The dataset's rich annotations and varied imagery facilitate rigorous testing and comparison across different methodologies.	Microsoft COCO	
cited_context	Microsoft Concept Graph	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The Microsoft Concept Graph is used as a general knowledge graph to represent and categorize concepts and their relationships. It aids in text understanding and classification by providing structured information that enhances the interpretation and categorization of textual data. This dataset enables researchers to improve the accuracy and depth of text analysis through its comprehensive representation of concept relationships.		
citing_context	MIMIC-CXR	https://doi.org/10.1109/BIBM58861.2023.10386013 (2023)	https://doi.org/10.1093/jamia/ocv080 (2015)	The MIMIC-CXR dataset is primarily used for generating radiology reports, focusing on chest X-ray images and their associated reports. It is part of the MIMIC series and is widely utilized in medical imaging research. The dataset enables researchers to develop and evaluate algorithms for automated report generation, enhancing the accuracy and efficiency of radiological interpretations.		
cited_context | citing_context	Mirflickr	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1460096.1460104 (2008)	The MIRFlickr dataset is used to enhance multi-modal reasoning in image tagging tasks by disambiguating concepts and improving image-tag relationships. This involves employing methodologies that integrate image and tag data to refine the accuracy and relevance of tags associated with images, thereby supporting more effective and contextually appropriate image annotation and retrieval.; The MIRFlickr dataset is used for multi-label image annotation and retrieval, focusing on disambiguating concepts and improving their relation to images. It enables researchers to enhance the accuracy and relevance of image annotations, which is crucial for effective image retrieval systems. The dataset's large collection of images and associated tags supports these research objectives.	MIRFlickr	
cited_context | citing_context	Mkg-W	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024), https://doi.org/10.1109/KSE63888.2024.11063656 (2024) (+1), https://doi.org/10.1609/aaai.v39i12.33454 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MKG-W dataset is primarily used to evaluate and enhance multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information. It is employed in assessing model performance in tasks such as link prediction, using metrics like MRR and Hit@1. The dataset supports ablation studies to analyze the impact of different components and is used to configure vision encoders like BEiT. It facilitates the enhancement of representation learning with multimodal data, specifically for reasoning tasks.; The MKG-W dataset is used to evaluate the effectiveness of models in multi-modal knowledge graph completion, focusing on the integration of textual and visual information. This dataset enables researchers to assess how well models can complete knowledge graphs by leveraging both text and images, addressing the specific challenge of combining these modalities in reasoning tasks.	MKG-W	
cited_context	MKG-Wikipedia	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.14778/3407790.3407828 (2020)	The MKG-Wikipedia dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. Researchers employ this dataset to assess the performance of algorithms in aligning entities and predicting relations within a multimodal context, leveraging the rich textual and structural information from Wikipedia. This enables the development and refinement of techniques for enhancing knowledge graph reasoning.		
cited_context | citing_context	Mkg-Y	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024), https://doi.org/10.1109/KSE63888.2024.11063656 (2024) (+1), https://doi.org/10.1609/aaai.v39i12.33454 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MKG-Y dataset is used in multi-modal knowledge graph reasoning research, specifically to evaluate and enhance models' capabilities in handling diverse modalities such as textual and visual information. It is employed to test robustness in entity linking, relation extraction, and cross-modal entity alignment. The dataset facilitates the integration of structured and unstructured data, often using vision encoders like BEiT, to improve multimodal representation learning and reasoning.; The MKG-Y dataset is used to evaluate the robustness of models in handling multi-modal data, focusing on scenarios with complex relationships and diverse modalities. It enables researchers to test how well models can integrate and reason across different types of data, ensuring they perform reliably in challenging, real-world conditions.	MKG-Y	
cited_context	MKG-YAGO	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/3503161.3548388 (2022)	The MKG-YAGO dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. Researchers employ this dataset to assess the performance of algorithms in these tasks, leveraging the YAGO data's rich structure and multimodal features. This enables the development and refinement of techniques for enhancing knowledge graph reasoning.		
citing_context	MM-Vet	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The MM-Vet dataset is used for verifying the correctness and consistency of multimodal reasoning outputs in veterinary question answering. It tests models' ability to accurately interpret and respond to complex inputs combining visual and textual information, ensuring that predictions are reliable and consistent across modalities. This dataset enables researchers to evaluate and improve the performance of multimodal reasoning systems in a specialized domain.		
citing_context	MME	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The MME dataset is utilized to benchmark and evaluate multimodal large language models across various reasoning tasks, including image-text alignment and reasoning. It provides a comprehensive evaluation framework, enabling researchers to assess model performance in educational settings, particularly in integrating visual and textual data for problem-solving.		
cited_context | citing_context	Mmea Datasets	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The MMEA datasets are used to evaluate multi-modal entity alignment methods, addressing the challenges of aligning entities across different modalities in knowledge graphs. Research focuses on assessing model performance across bilingual, monolingual, and high-degree categories using pre-trained visual encoders like ResNet-152 and CLIP, under both standard and iterative settings. This enables researchers to compare and refine alignment techniques effectively.; The MMEA datasets are used to evaluate multi-modal entity alignment methods, addressing the challenges of aligning entities across different modalities in knowledge graphs. They are also employed to assess model performance across bilingual, monolingual, and high-degree categories using pre-trained visual encoders such as ResNet-152 and CLIP, in both standard and iterative settings. This enables researchers to compare and refine alignment techniques and model effectiveness in multi-modal contexts.	MMEA datasets	
cited_context	MMEKG	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The MMEKG dataset is used to represent and integrate multi-modal events into a knowledge graph, facilitating universal representation across diverse data types and modalities. It advances the field of event knowledge graphs by providing a large-scale ontology with 990,000 concept events and 644 relation types, enabling comprehensive coverage of real-world happenings. This dataset supports research in creating more robust and versatile knowledge graphs.		
cited_context | citing_context	Mmkg	https://doi.org/10.1145/3627673.3679126 (2024), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1145/3583780.3614782 (2023), https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2024.3352100 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.1145/3340531.3417439 (2020)	The MMKG dataset is used to evaluate and enhance multi-modal knowledge graph reasoning, integrating visual, textual, and numerical information. It facilitates the incorporation of visual data into entities, though with low image coverage, which affects comprehensive entity representation. The dataset supports the integration of symbolic knowledge with pre-trained visual-language models, improving performance in open-domain VQA tasks. Additionally, it aids in retrieving deep learning paper implementations, enhancing access to associated code in academic research.; The MMKG dataset is used to enhance retrieval capabilities for deep learning papers and codes, generate a metallic materials knowledge graph by integrating structured data from DBpedia and Wikipedia, and incorporate both text and image information into knowledge graphs for multi-modal reasoning tasks. This dataset supports research in improving search functionalities and integrating diverse data types for enhanced knowledge representation and reasoning.	MMKG	
cited_context	MMKG-DB15k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MMKG-DB15k dataset is used to integrate specific knowledge graphs with numeric literals, facilitating multi-modal reasoning in research. This integration supports the development and evaluation of methods that can handle both symbolic and numeric data, enhancing the capability to reason across diverse data types. The dataset's unique feature of incorporating numeric literals enables more robust and nuanced reasoning tasks in multi-modal knowledge graph research.		
cited_context	MMKG-FB15k-IMG	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MMKG-FB15k-IMG dataset is used to integrate specific knowledge graphs with images, enhancing multi-modal reasoning capabilities in research. This integration supports the development of models that can effectively combine textual and visual information, improving the accuracy and robustness of multi-modal reasoning tasks. The dataset's unique combination of structured knowledge and image data enables researchers to explore and develop advanced multi-modal reasoning techniques.		
citing_context	MMKG1	https://doi.org/10.1109/ICDE60146.2024.00274 (2024)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The MMKG1 dataset is used to identify and analyze semantic inconsistencies in the Multi-Modal Entity Alignment (MMEA) task. It focuses on aligning entities across different modalities, enabling researchers to evaluate and improve the consistency and accuracy of multi-modal knowledge graph reasoning systems. This dataset highlights specific challenges in entity alignment, facilitating the development of more robust alignment methodologies.		
citing_context	MMKG2	https://doi.org/10.1109/ICDE60146.2024.00274 (2024)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The MMKG2 dataset is used to identify and analyze semantic inconsistencies in the Multi-Modal Entity Alignment (MMEA) task. It focuses on aligning entities across different modalities, enabling researchers to evaluate and improve the consistency and accuracy of entity relationships in multi-modal knowledge graphs. This dataset highlights specific challenges in cross-modal alignment, facilitating the development of more robust alignment methods.		
cited_context	Movie-QA	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1109/CVPR.2016.501 (2015)	The Movie-QA dataset is used to evaluate the ability to reason about visual and textual information in movie clips through question-answering tasks. It focuses on understanding stories in movies by integrating both visual and textual data, enabling researchers to assess systems' capabilities in comprehending complex narratives. This dataset supports research in natural language processing and computer vision, particularly in the context of multi-modal reasoning.		
cited_context	MS-Celebs	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-018-1116-0 (2016)	The MS-Celebs dataset is used to link visual data with structured knowledge, specifically by associating cropped celebrity faces with Freebase IDs to enhance multi-modal reasoning. It is also employed to merge entities from MS-Celebs and KVQA, focusing on entity recognition and integration, thereby facilitating the creation of comprehensive person lists. These applications leverage the dataset's rich visual and structured data to improve entity linking and multi-modal reasoning tasks.		
cited_context | citing_context	Mscoco	https://doi.org/10.1609/aaai.v38i12.29280 (2024), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/aaai.v36i2.20123 (2022) (+1), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/AAAI.V34I07.6731 (2020), https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The MSCOCO dataset is primarily used for training and evaluating models in multi-modal tasks, including object recognition, cross-modal hashing, and image-text matching. It provides bounding boxes and categories for object detection, enabling the construction of multi-modal knowledge graphs and vision-based knowledge graphs. The dataset supports the evaluation of various algorithms and methods, such as cross-modal hashing, image-to-text and text-to-image retrieval, and multi-modal reasoning tasks, often using metrics like Mean Average Precision (MAP). It also facilitates the investigation of learning components and data augmentation techniques in intra-modal contrastive learning.; The MSCOCO dataset is extensively used in research for image captioning, object detection, and visual recognition, often to evaluate and compare the performance of different models. It contains over 100K images with multiple captions each, enabling detailed assessments of image-text alignment and multi-modal reasoning capabilities. Researchers utilize it to enhance the precision of visual object extraction, generate textual descriptions from images, and conduct multi-label classification experiments, leveraging its rich annotations and diverse visual contexts.	MSCOCO	
citing_context	MSCOCO captions	https://doi.org/10.48550/arXiv.2501.04173 (2025)	https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628 (2015)	The MSCOCO captions dataset is primarily used for fine-tuning models on image captioning tasks. It provides a large set of images paired with descriptive captions, enabling researchers to train and evaluate models that generate natural language descriptions of visual content. This dataset supports the development and improvement of image-to-text generation models, enhancing their ability to produce accurate and contextually relevant captions.		
citing_context	MSED	https://doi.org/10.1145/3593583 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.393 (2020)	The MSED dataset is manually created and used to evaluate multi-modal sentiment, emotion, and desire understanding by incorporating textual and visual utterances. It enables researchers to assess models' capabilities in interpreting complex human expressions across both text and images, focusing on the integration and analysis of these multi-modal data types.		
cited_context | citing_context	Msvd	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1646396.1646452 (2009)	The MSVD dataset is primarily used for image and video captioning, enhancing the generation of descriptive captions through multi-modal data. It provides a diverse set of multimedia content, enabling researchers to train and evaluate models that integrate visual and textual information. This dataset supports the development of more accurate and contextually rich captioning systems.; The MSVD dataset is primarily used for image captioning, enhancing the reasoning capabilities by integrating textual descriptions with visual content through graph-structured information. This approach helps in generating more accurate and contextually relevant captions, leveraging the dataset's multimodal nature to improve the alignment between images and their textual descriptions.	MSVD	
cited_context	multi-modal variants of the OpenEA benchmarks	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.14778/3407790.3407828 (2020)	The multi-modal variants of the OpenEA benchmarks are used to evaluate multi-modal entity alignment methods by incorporating entity images obtained via Google search. This dataset focuses on enhancing knowledge graph reasoning through the integration of visual data, enabling researchers to assess the effectiveness of these methods in aligning entities across different modalities.		
cited_context | citing_context	Multi-Openea	https://doi.org/10.48550/arXiv.2307.16210 (2023), https://doi.org/10.1145/3627673.3679126 (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024) (+1), https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020), https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The Multi-OpenEA dataset is used to evaluate and benchmark models in multi-modal knowledge graph reasoning, particularly focusing on vision encoders like CLIP. It assesses model stability and performance in cross-lingual and cross-modal entity alignment, with a vision feature dimension of 512. The dataset's large scale and high ratio of image-equipped entities make it suitable for these tasks, providing a robust benchmark for multi-modal entity alignment.; The Multi-OpenEA dataset is used to evaluate and demonstrate model stability with the CLIP vision encoder, specifically focusing on the vision feature dimension of 512. This dataset enables researchers to assess how changes in vision feature dimensions impact model performance, providing insights into the robustness and reliability of models in multi-modal settings.	Multi-OpenEA	
citing_context	MultiModalQA	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2407.09413 (2024)	The MultiModalQA dataset is used for multimodal question answering tasks, where it integrates textual and visual data to enhance reasoning capabilities. This dataset enables researchers to develop and evaluate models that can effectively combine information from both modalities, improving the accuracy and robustness of question answering systems.		
cited_context	MusicBrainz	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The MusicBrainz dataset is used as a domain-specific knowledge graph to provide structured information about music, including details about artists, albums, and tracks. It is employed to enhance the accuracy and richness of music-related data in research, supporting applications that require detailed and organized musical metadata. This dataset enables researchers to integrate comprehensive music information into their studies, improving the depth and reliability of their analyses.		
cited_context	Nation	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/1273496.1273551 (2007)	The 'Nation' dataset is used for representing and reasoning about relations among nations, specifically within the domain of multi-modal knowledge graph reasoning. It facilitates statistical predicate invention, enabling researchers to explore complex relationships and patterns between nations using a combination of textual and numerical data. This dataset supports the development and evaluation of models that can infer new predicates and relationships from existing data, enhancing the understanding of international dynamics.		
cited_context | citing_context	Nell	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.24963/ijcai.2024/236 (2024), https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v24i1.7519 (2010)	The NELL dataset is primarily used for never-ending language learning, where it continuously extracts and represents knowledge in a graph structure. It serves as a benchmark for First-Order Knowledge Graph Completion, evaluating model performance in completing knowledge graphs. Additionally, NELL is utilized as a knowledge base for multi-modal reasoning, integrating textual and structured data to enhance the depth and breadth of the knowledge graph through continuous web learning.; The NELL dataset is used as a knowledge base for multi-modal reasoning, specifically integrating textual and structured data. This integration enhances understanding and inference capabilities, allowing researchers to explore complex relationships and improve reasoning systems. The dataset's rich combination of text and structured information supports advanced research in multi-modal knowledge graph reasoning.	NELL	
cited_context	NELL-995	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.3115/v1/P15-1009 (2015)	The NELL-995 dataset is utilized to evaluate the performance of knowledge graph reasoning models, focusing on their ability to handle smaller, more focused subsets of entities and relations. This dataset enables researchers to test and refine models specifically designed for these constrained environments, ensuring they can effectively reason and infer relationships within limited data contexts.		
cited_context	NELL23k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.3115/v1/P15-1009 (2015)	The NELL23k dataset is used to evaluate knowledge graph embedding methods, specifically focusing on semantic smoothness and reasoning accuracy. It enables researchers to assess how well these methods perform across a large set of entities and relations, providing insights into the effectiveness of different embedding techniques in maintaining and utilizing semantic relationships within the graph.		
citing_context	NELL995	https://doi.org/10.1145/3589334.3645569 (2023)	https://doi.org/10.18653/v1/D17-1060 (2017)	The NELL995 dataset is used to evaluate the Query2GMM model, focusing on reasoning over a diverse set of relations and entities within a knowledge graph. This dataset enables researchers to assess the model's performance in handling complex queries and reasoning tasks, thereby advancing the field of knowledge graph reasoning.		
cited_context | citing_context	Nombank	https://doi.org/10.1145/3573201 (2022)	https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e (2004)	NomBank is used to enhance event knowledge graphs by providing an annotated corpus of nominal predicate argument structures, specifically adding 114,576 noun-based events. This dataset extends the scale and diversity of event-related information, improving the representation of noun-based events in knowledge graphs. It is not used for multi-modal Knowledge Graph Reasoning but rather for enriching the structural and semantic content of event data.; NomBank is used to extend the size of event knowledge graphs by providing an annotated corpus of nominal predicate argument structures, specifically enhancing the representation of noun-based events. It contributes 114,576 events to the dataset, enriching the graph's content and improving the modeling of complex event relationships.	NomBank	
citing_context	NTU-RGB+D 120	https://doi.org/10.1109/ICME52920.2022.9859787 (2022)	https://doi.org/10.1109/CVPR46437.2021.01600 (2021)	The NTU-RGB+D 120 dataset is primarily used for action recognition research, focusing on multi-modal data integration from RGB, depth, and skeleton sensors. It enables researchers to preprocess and analyze these diverse data types, enhancing the accuracy and robustness of action recognition systems. The dataset's comprehensive multi-modal features support the development and evaluation of algorithms designed to recognize human actions in various contexts.		
citing_context	NTU-RGB+D 60	https://doi.org/10.1109/ICME52920.2022.9859787 (2022)	https://doi.org/10.1109/CVPR.2016.115 (2016)	The NTU-RGB+D 60 dataset is primarily used for skeleton-based action recognition, providing a large-scale resource for 3D human activity analysis. It enhances multi-modal reasoning capabilities by integrating RGB, depth, and skeleton data across 60 distinct actions. Researchers use this dataset to assess and improve action recognition performance, leveraging its comprehensive multi-modal inputs to develop and evaluate algorithms.		
cited_context	NUS-81	https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The NUS-81 dataset is primarily used for multi-label classification experiments, particularly in image tagging, where it evaluates performance across multiple time steps. It is also employed to compare performance with MS-COCO in multi-modal reasoning tasks involving images and text. This dataset facilitates research by providing a benchmark for these specific tasks, enabling the assessment of models' capabilities in handling complex, multi-modal data.		
cited_context | citing_context	Nus-Wide	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1145/1646396.1646452 (2009), https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The NUS-WIDE dataset is primarily used for enhancing image tagging and multi-modal reasoning tasks. It helps construct scene graphs and disambiguate concepts in web images, improving the accuracy of image-tag relationships. This dataset enables researchers to develop and test methods that integrate visual and textual data, thereby enhancing the reasoning capabilities in image tagging applications.; The NUS-WIDE dataset is primarily used for evaluating models in multi-label image annotation and retrieval, enhancing reasoning capabilities through scene graph construction, and disambiguating concepts in multi-modal data. It focuses on a large-scale web image dataset with multiple labels per image, enabling researchers to assess model performance in complex, real-world scenarios.	NUS-WIDE	
citing_context	NUS-WIDE-10K	https://doi.org/10.1145/3474085.3475567 (2021)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The NUS-WIDE-10K dataset is primarily used for cross-modal multimedia retrieval experiments, focusing on the role of correlation and abstraction in retrieval performance. It employs 1,000-dimensional Bag-of-Words (BoW) features and is utilized to assess the effectiveness of models like GCR in enhancing cross-modal retrieval by leveraging the correlation and abstraction of multimodal data.		
citing_context	NYTimes800k	https://doi.org/10.1109/TMM.2023.3301279 (2021)	https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd (2015)	The NYTimes800k dataset is used to collect and annotate images, captions, and news articles from the New York Times. It focuses on providing ground-truth captions for images, enabling research in image captioning and multimodal content analysis. The dataset's rich annotations facilitate the development and evaluation of models that generate accurate and contextually relevant captions for images.		
cited_context | citing_context	Ok-Vqa	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.3390/electronics12061390 (2023), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.18653/v1/2020.findings-emnlp.44 (2020)	https://doi.org/10.1109/CVPR.2019.00331 (2019)	The OK-VQA dataset is used for evaluating visual-retriever-reader models in knowledge-based question answering, focusing on open-ended questions that require external textual knowledge beyond the image content. It tests the reasoning capabilities of VQA models by integrating visual and textual information, assessing the need for external knowledge to answer questions and capturing relationships among mentions and entities.; The OK-VQA dataset is used to evaluate multi-modal reasoning in visual question answering, particularly for open-ended and complex queries that require external knowledge. It focuses on integrating visual and textual information to test the reasoning capabilities of VQA models, emphasizing the need for external knowledge to answer questions. The dataset benchmarks systems on their ability to integrate and reason about both visual and non-visual information, capturing relationships among entities and their mentions in images.	OK-VQA	
cited_context | citing_context	Open Images	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Open Images dataset is used to train detectors on a diverse set of labeled images, enhancing multi-modal reasoning across various domains. This dataset enables researchers to expand the capabilities of detection systems by providing a wide range of labeled data, which is crucial for improving the accuracy and robustness of these systems in real-world applications.; The Open Images dataset is used to train detectors with supervised data, specifically focusing on image-text pairs for multi-modal reasoning tasks. This enables researchers to develop models that can effectively understand and reason about visual and textual information simultaneously, enhancing the performance of multi-modal systems.	Open Images	
citing_context	OpenEA benchmarks	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.14778/3407790.3407828 (2020)	The OpenEA benchmarks dataset is used to evaluate multi-modal entity alignment methods by incorporating entity images obtained via Google search alongside traditional textual data. This approach enhances the evaluation of entity alignment techniques, focusing on the integration of visual and textual information to improve alignment accuracy. The dataset enables researchers to test and compare different multi-modal methods, addressing the challenge of aligning entities across heterogeneous data sources.		
citing_context	Oracle Bone Inscriptions	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/J.COMPELECENG.2021.107173 (2021)	The Oracle Bone Inscriptions dataset is used to detect and recognize oracle bones by integrating visual features with relevant literature, location, and institutional data. This integration enhances decision-making processes, enabling researchers to more accurately identify and contextualize these ancient artifacts. The dataset's multi-modal nature, combining visual and textual information, supports robust recognition and analysis.		
citing_context	Pascal Sentence	https://doi.org/10.1145/3474085.3475567 (2021)	https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146 (2010)	The Pascal Sentence dataset is used to collect and evaluate image annotations, providing five English sentences per image that focus on descriptive content. It is employed to assess models like GCR in cross-modal retrieval tasks, specifically aligning images with descriptive sentences. This dataset enables research in multi-modal reasoning by offering rich, descriptive textual data paired with visual content.		
citing_context	PASCAL VOC	https://doi.org/10.1145/3627673.3679175 (2024)	https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086 (2009)	The PASCAL VOC dataset is primarily used for object detection and segmentation in visual scenes. Researchers extract bounding boxes of objects to construct VisionKG, a knowledge graph focused on visual data. This dataset enables precise localization and categorization of objects, facilitating advancements in computer vision and scene understanding.		
citing_context	pFoodREQ	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.1109/TPAMI.2019.2927476 (2021)	The pFoodREQ dataset is utilized for personalized food recommendation by employing constrained question answering over a large-scale food knowledge graph. This approach enables researchers to address specific queries related to dietary preferences and restrictions, enhancing the accuracy and relevance of food recommendations. The dataset's extensive coverage of food items and their attributes supports sophisticated reasoning tasks, facilitating more tailored and context-aware recommendations.		
citing_context	PHEME	https://doi.org/10.48550/arXiv.2306.15946 (2023), https://doi.org/10.1145/3451215 (2021)	https://doi.org/10.1007/978-3-319-67217-5_8 (2017)	The PHEME dataset is primarily used for rumor detection in social media, particularly focusing on microblogs and multimedia posts from platforms like Twitter. It is employed to evaluate and compare the performance of various models, including multi-modal approaches, in detecting rumors during breaking news events. The dataset facilitates research by providing a benchmark for testing the robustness and effectiveness of different components, such as knowledge concept integration, entity consistency, and contextual information, in enhancing rumor detection accuracy.		
citing_context	PHOENIX14T	https://doi.org/10.48550/arXiv.2211.00526 (2022)	https://doi.org/10.1109/CVPR.2018.00812 (2018)	The PHOENIX14T dataset is primarily used to evaluate the Sign2Text task, which involves translating continuous sign language videos into spoken language. Researchers employ a multi-modal approach, leveraging both visual and textual data to improve translation accuracy. This dataset enables the development and testing of models that can effectively interpret sign language, addressing the challenge of converting visual gestures into coherent spoken language text.		
citing_context	PKG	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.18653/v1/2022.findings-emnlp.171 (2022)	The PKG dataset is used as a multi-modal knowledge graph for classical Chinese poetry, integrating textual and visual information. It enhances understanding and reasoning about poetic content by combining these modalities. Researchers use it to explore how textual and visual elements interact to convey meaning, supporting studies in cultural heritage and literary analysis.		
citing_context	PKU XMedia	https://doi.org/10.1145/3474085.3475567 (2021)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The PKU XMedia dataset is used to evaluate the GCR model's performance in cross-modal retrieval, focusing on its capabilities with diverse multimedia data. This dataset enables researchers to test and refine models designed for handling and integrating different types of media, enhancing the accuracy and efficiency of cross-modal search and retrieval systems.		
citing_context	PMR	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/2022.acl-long.66 (2021)	The PMR dataset is used to evaluate premise-based multimodal reasoning, focusing on conditional inference from joint textual and visual clues. It is employed to assess models like ModCR and to conduct ablation studies, verifying the effectiveness of high-quality, human-constructed samples and the impact of manual annotation on reasoning performance.		
citing_context	PMR (Dong et al.	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/2022.acl-long.66 (2021)	The PMR dataset (Dong et al., 2022) is used for visual question answering, specifically focusing on premise-based multimodal reasoning that integrates textual and visual clues. It enables researchers to explore how joint textual and visual information can be effectively utilized to answer complex questions, enhancing the understanding of multimodal data integration and reasoning.		
citing_context	PolitiFact	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/N16-1174 (2016)	The PolitiFact dataset is used to evaluate the performance of KGF and KGT models in detecting fake news, specifically focusing on the impact of different embedding dimensions. This dataset enables researchers to assess how effectively these models can identify false information, contributing to the development of more robust fake news detection systems.		
citing_context	ProMQA	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2407.09413 (2024)	The ProMQA dataset is used for multimodal procedural activity understanding, specifically in question answering tasks that integrate textual and visual information. It enables researchers to develop and evaluate models that can reason about complex activities by combining insights from both modalities, enhancing the accuracy and context-awareness of procedural understanding.		
cited_context | citing_context	Propbank	https://doi.org/10.1145/3573201 (2022)	https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e (2004)	PropBank is used to enhance event knowledge graphs by providing an annotated corpus of semantic roles, which significantly increases the scale and diversity of event-related information. This dataset contributes 112,917 events, enriching the representation of verb-based events and expanding the overall size of the knowledge graph.; The PropBank dataset is used to enhance event knowledge graphs by providing an annotated corpus of semantic roles, which enriches the representation of verb-based events. It contributes 112,917 events to the dataset, extending its size and improving the granularity and detail of event-related information. This enables more comprehensive and nuanced research in areas such as natural language processing and semantic analysis.	PropBank	
citing_context	PROTEIN	https://doi.org/10.48550/arXiv.2304.11116 (2023)	https://www.semanticscholar.org/paper/9b720e749e71960f323779ec63261250871ffb66 (2020)	The PROTEIN dataset is used to evaluate Graph-ToolFormer on protein interaction graphs, specifically for node classification tasks. This involves applying graph-based machine learning methods to classify nodes within the protein interaction network. The dataset's focus on protein interactions enables researchers to assess the performance of Graph-ToolFormer in predicting and understanding protein functions and interactions.		
citing_context	PTC	https://doi.org/10.48550/arXiv.2304.11116 (2023)	https://www.semanticscholar.org/paper/9b720e749e71960f323779ec63261250871ffb66 (2020)	The PTC dataset is used to evaluate Graph-ToolFormer's performance on chemical compound graphs, focusing on graph classification tasks. This involves assessing the model's ability to classify chemical compounds based on their structural properties. The dataset's relevance lies in its application to chemical graph analysis, enabling researchers to test and improve graph-based machine learning models in this domain.		
citing_context	PubMed	https://doi.org/10.48550/arXiv.2506.05766 (2025)		The PubMed dataset is used to source abstracts for multi-modal knowledge graph reasoning, specifically for integrating textual information into knowledge graphs. This approach leverages the rich textual data in PubMed to enhance the reasoning capabilities of knowledge graphs, addressing research questions related to the integration and interpretation of biomedical literature.		
cited_context	qaVG	https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019)	https://doi.org/10.18653/v1/N18-1040 (2017)	The qaVG dataset is used to train and evaluate visual question answering models. It augments triplets with auto-generated decoys and is split into training, validation, and testing sets. This methodology helps in assessing model performance in distinguishing correct answers from decoys, enhancing the robustness of visual reasoning tasks.		
citing_context	Recipe1M+	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.1109/TPAMI.2019.2927476 (2021)	The Recipe1M+ dataset is used to learn cross-modal embeddings for cooking recipes and food images, integrating textual and visual information. Research focuses on the relationship between textual recipe instructions and visual food representations, enabling the development of models that can understand and generate content across these modalities.		
citing_context	Rich-pedia	https://doi.org/10.1109/TKDE.2025.3546686 (2023), https://doi.org/10.1145/3581783.3612151 (2023)	https://doi.org/10.1016/j.bdr.2020.100159 (2020)	The Rich-pedia dataset is used to construct a large-scale, comprehensive multi-modal knowledge graph by integrating various types of data, including images, to enhance reasoning capabilities. It is employed to improve KG-based applications through the incorporation of visual modalities, demonstrating significant potential in multi-modal knowledge graph reasoning.		
cited_context	Richpedia	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1145/3581783.3612151 (2023)	https://doi.org/10.1007/978-3-030-41407-8_9 (2019)	The Richpedia dataset is used to construct and enhance comprehensive multi-modal knowledge graphs by integrating triplets, textual descriptions, and images. This integration supports reasoning tasks and demonstrates significant potential in multi-modal knowledge graph reasoning, enabling more robust KG-based applications through the incorporation of visual modalities.		
citing_context	RWTH-PHOENIX-Weather 2014T	https://doi.org/10.48550/arXiv.2211.00526 (2022)	https://doi.org/10.1109/CVPR.2018.00812 (2018)	The RWTH-PHOENIX-Weather 2014T dataset is primarily used to evaluate network performance in translating German sign language, with a focus on multi-modal reasoning. It enables researchers to assess how effectively models can integrate visual and linguistic data, addressing specific challenges in sign language recognition and translation. The dataset's multi-modal nature, combining video and text, supports robust testing of these capabilities.		
citing_context	ScanNet	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.48550/arXiv.2210.07474 (2022)	The ScanNet dataset is used to collect situated question-answer pairs in 3D scenes, enhancing situated understanding in real-world environments through multi-modal reasoning. It supports research in understanding and interacting with 3D environments by providing rich, annotated data that captures the spatial and semantic relationships within these scenes. This enables the development and evaluation of models that can reason about objects and their contexts in complex, real-world settings.		
citing_context	Scene Graph	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.1109/CVPR.2015.7298990 (2015)	The Scene Graph dataset is used to represent action and spatial relations in images, which facilitates image retrieval and reasoning tasks. It enables researchers to analyze and understand complex visual scenes by encoding relationships between objects, enhancing the accuracy and efficiency of image-based queries and reasoning processes.		
citing_context	ScienceQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The ScienceQA dataset is used to evaluate scientific question answering systems, particularly focusing on the integration of visual and textual information to solve complex problems. It sources data for assessing multi-modal reasoning in scientific contexts, enabling researchers to test how effectively models can combine different types of information to provide accurate answers.		
citing_context	SeedBench	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The SeedBench dataset is used to test and benchmark multi-modal models, specifically focusing on seed-based reasoning. It evaluates the models' ability to generate coherent responses from given seeds and provides a large-scale dataset for assessing performance across various tasks. This enables researchers to compare and improve the reasoning capabilities of multi-modal models.		
citing_context	Semantic MEDLINE	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1145/3442381.3449860 (2021)	The Semantic MEDLINE dataset is used to extract personalized graphs to enhance Electronic Health Record (EHR) representation learning. By focusing on medical knowledge paths, it improves health risk prediction. This involves leveraging the dataset's structured medical information to create more informative and contextually rich EHR representations, which are then used to address specific research questions related to patient health outcomes and risk assessment.		
cited_context	SIDER	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The SIDER dataset is used as a domain-specific knowledge graph to link drugs to their side effects, enhancing medical reasoning. It is employed in research to improve understanding and prediction of drug side effects, leveraging its structured data on adverse reactions to support more informed medical decisions and drug safety assessments.		
cited_context | citing_context	Situnet	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	SituNet is used to train models for situation recognition tasks, particularly focusing on visual semantic role labeling and defining visual event schemas. This dataset enables the development of models that enhance image understanding by identifying and labeling roles within visual scenes, contributing to more accurate and context-aware image analysis.; SituNet is used to train models for situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It aids in defining visual event schemas, which are crucial for developing robust situation recognition models. This dataset enables researchers to improve the accuracy and context-awareness of image interpretation systems.	SituNet	
citing_context	SpatialVOC2K	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.18653/v1/W18-6516 (2018)	The SpatialVOC2K dataset is used to investigate multilingual spatial relations by analyzing annotated images. It focuses on understanding spatial configurations across multiple languages, employing a methodology that leverages visual annotations to explore how spatial relationships are represented and understood in different linguistic contexts. This dataset enables researchers to address specific questions related to cross-linguistic spatial cognition and representation.		
citing_context	SPIQA	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2407.09413 (2024)	The SPIQA dataset is utilized for multimodal question answering on scientific papers, integrating textual content with figures and tables to enhance comprehension. This approach leverages the dataset's combination of visual and textual data to address research questions related to improving the understanding and accessibility of scientific information.		
citing_context	SQA3D	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.48550/arXiv.2210.07474 (2022)	The SQA3D dataset is used to facilitate situated question answering in real-world 3D scenes, focusing on spatial reasoning and scene understanding. It enables researchers to develop and evaluate models that can interpret and reason about complex 3D environments, addressing specific challenges in spatial awareness and context comprehension.		
citing_context	STaRK	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2406.04744 (2024)	The STaRK dataset is used to support RAG (Retrieval-Augmented Generation) models, specifically for enhancing reasoning tasks and integrating knowledge graphs. It enables researchers to focus on specific reasoning challenges, leveraging the dataset's structured information to improve model performance in knowledge graph integration tasks.		
cited_context | citing_context	Swig	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	The SWiG dataset is primarily used for training models in situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It is also utilized to define visual event schemas, which aids in developing more robust situation recognition models. These applications leverage the dataset's rich annotations to improve the accuracy and context-awareness of image interpretation systems.; The SWiG dataset is primarily used for training models in situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It is also utilized to define visual event schemas, which aids in developing more robust situation recognition models. These applications leverage the dataset's rich annotations to improve the accuracy and context-awareness of visual understanding systems.	SWiG	
citing_context	TabMWP	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2211.16492 (2022)	The TabMWP dataset is used for data augmentation in multi-modal reasoning tasks, particularly in ScienceQA. It addresses sparse data issues by constructing multi-step multi-modal samples, enhancing the robustness and diversity of training data. This dataset enables researchers to improve model performance in scenarios where data is limited, facilitating more effective multi-modal reasoning.		
citing_context	TACRED	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/D17-1004 (2017)	The TACRED dataset is used to train models like OpenNRE with a BERT encoder for relation extraction, specifically to enhance the construction of knowledge graphs from news articles. This application focuses on improving the accuracy and efficiency of extracting relationships between entities, leveraging the dataset's annotated relations to train and evaluate models.		
cited_context	textual description	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The 'textual description' dataset is used to provide contextual information alongside imaging data, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This dataset integrates textual and imaging data, enabling more robust diagnostic models by leveraging the complementary information from both modalities.		
citing_context	TQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The TQA dataset is used to evaluate multimodal question answering systems, particularly in educational contexts. It assesses models' abilities to reason over both visual and textual information, such as diagrams and images, and to integrate these modalities for accurate answers. The dataset emphasizes multimodal machine comprehension and problem-solving, enabling researchers to test and improve the performance of models in complex, real-world scenarios.		
citing_context	TWITTER	https://doi.org/10.1145/3451215 (2021), https://doi.org/10.48550/arXiv.2304.11116 (2023)	https://doi.org/10.1145/3123266.3123454 (2017)	The TWITTER dataset is primarily used for rumor detection on microblogs, integrating textual, visual, and social context information through multimodal fusion techniques, such as recurrent neural networks. It is employed to compare the performance of different models, particularly focusing on the effectiveness of multimodal information and knowledge concept components in enhancing accuracy. The dataset facilitates research by providing a rich, multi-faceted source of data for evaluating and refining rumor detection algorithms.		
citing_context	Twitter dataset	https://doi.org/10.1109/TMM.2023.3330296 (2024)		The Twitter dataset is used to automatically detect fake multi-media content on Twitter. Research focuses on developing and evaluating detection algorithms, leveraging the dataset's multi-modal content (text and images). This enables researchers to enhance the accuracy of fake content identification, contributing to the broader field of misinformation detection.		
citing_context	UAV-Human	https://doi.org/10.1109/ICME52920.2022.9859787 (2022)	https://doi.org/10.1109/CVPR46437.2021.01600 (2021)	The UAV-Human dataset is used for human behavior understanding with unmanned aerial vehicles, focusing on multi-modal video sequences captured from three sensors. It is employed for data pre-processing, multi-modal data integration, annotation, and action recognition. This dataset enables researchers to analyze and evaluate complex human behaviors in various contexts, enhancing the accuracy and robustness of behavior understanding systems.		
citing_context	UCY	https://doi.org/10.1109/ACCESS.2020.2991435 (2020)	https://doi.org/10.1109/ICCV.2009.5459260 (2009)	The UCY dataset is used to evaluate model performance in multi-modal reasoning, particularly focusing on multi-agent interactions and social behavior. It is applied in the context of multi-target tracking, enabling researchers to assess how models handle complex social dynamics and interactions in real-world scenarios.		
cited_context	ultrasound	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The ultrasound dataset is used to integrate visual information with textual descriptions, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This approach combines imaging data with clinical notes, improving diagnostic precision and reliability.		
citing_context	UMLS	https://doi.org/10.18653/v1/2024.findings-acl.319 (2024)	https://doi.org/10.1093/nar/gkh061 (2004)	The UMLS dataset is used to construct a comprehensive medical knowledge graph with over 15,000 entity nodes and 130,000 edges, encompassing 20 semantic types and 50 relation types. It is also utilized for entity disambiguation by linking textual entities to UMLS concepts during pre-processing. These applications enable robust representation and integration of medical knowledge in research.		
cited_context | citing_context	Unearthed Oracle Bones Photos	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/J.COMPELECENG.2021.107173 (2021)	The 'unearthed oracle bones photos' dataset is used to construct a multi-modal knowledge graph for oracle bone recognition. This involves integrating visual and textual data to enhance information processing. The dataset enables researchers to develop and refine methods for recognizing and interpreting oracle bone inscriptions, leveraging both image and text data to improve accuracy and understanding.; The 'unearthed oracle bones photos' dataset is used to construct a multi-modal knowledge graph for oracle bone recognition. This involves integrating visual and textual data to enhance information processing. The dataset enables researchers to develop and refine methods for recognizing and interpreting oracle bone inscriptions by leveraging both image and text data, thereby improving the accuracy and depth of historical and linguistic analysis.	unearthed oracle bones photos	
citing_context	Unified Medical Language System (UMLS)	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1093/nar/gkh061 (2004)	The Unified Medical Language System (UMLS) is used to integrate biomedical terminology, serving as a comprehensive knowledge base for multi-modal reasoning in medical data. It enables researchers to link and harmonize diverse medical terminologies, facilitating more robust and accurate data integration and analysis in biomedical research.		
citing_context	UUKG	https://doi.org/10.1145/3664647.3681705 (2024)	https://doi.org/10.48550/arXiv.2306.11443 (2023)	The UUKG dataset is used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities. It integrates diverse urban data sources, enabling researchers to analyze and predict urban dynamics by leveraging the structured representation of urban entities and their relationships. This approach supports the development of more efficient and responsive smart city systems.		
cited_context | citing_context	Vcr	https://doi.org/10.48550/arXiv.2405.16473 (2024), https://doi.org/10.48550/arXiv.2305.04530 (2023), https://doi.org/10.1109/TMM.2023.3279691 (2024), https://www.semanticscholar.org/paper/ef318e7ff0883e72d853c75736d20cc123b556d5 (2019), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.48550/arXiv.2205.11501 (2022)	https://doi.org/10.18653/v1/P19-1472 (2019), https://doi.org/10.1109/CVPR.2019.00688 (2018)	The VCR dataset is primarily used to assess and evaluate visual commonsense reasoning, focusing on the ability to understand and explain visual scenes through complex interactions between images and text. It is employed to fine-tune models like BERT for cross-modal alignment, enhancing their capability to integrate visual-linguistic data and reason about relationships within images. This dataset enables researchers to test and improve models' performance in understanding and explaining complex visual scenes.; The VCR dataset is primarily used to evaluate and fine-tune models for visual commonsense reasoning, focusing on understanding and explaining visual scenes. It supports extensive experiments and validation of state-of-the-art models on multiple-choice QA tasks derived from 110k movie scenes, emphasizing implicit visual-linguistic representations and downstream applications.	VCR	
citing_context	VCR (QR  A)	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The VCR (QR  A) dataset is used to assess visual commonsense reasoning, focusing on the evaluation of systems' abilities to answer questions based on both visual and textual inputs. It enables researchers to test and improve models that integrate visual and linguistic understanding, specifically addressing the challenge of reasoning about complex scenes and contexts.		
citing_context	VCR data set	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The VCR dataset is used to reorganize large-scale datasets, focusing on visual commonsense reasoning tasks. It evaluates models' ability to understand complex visual scenes, employing methodologies that test and enhance the reasoning capabilities of AI systems in interpreting visual content. This dataset enables researchers to assess and improve the performance of models in comprehending and reasoning about visual information.		
cited_context | citing_context	Vg	https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1007/978-3-030-58592-1_36 (2020)	https://doi.org/10.1109/CVPR.2017.330 (2017)	The VG dataset is primarily used for training and evaluating models in scene graph generation, focusing on extracting visual entities and relationships. It supports the development of models like Faster RCNN and SGG pipelines, enhancing their ability to understand and generate scene graphs by connecting visual data with common sense knowledge. The dataset includes 50 predicate classes and 150 object classes, enabling researchers to benchmark their methods against state-of-the-art techniques using standard evaluation metrics and splits.; The VG dataset is used to evaluate the performance of methods on scene graph generation tasks, specifically focusing on mean and overall triplet recall. It compares results with and without graph constraints, enabling researchers to assess the effectiveness of their approaches in generating accurate and contextually relevant scene graphs.	VG	
citing_context	VG dataset	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-030-77385-4_41 (2020)	The VG dataset is used to substitute predicates in new triplets for the CSKG, focusing on the most common predicate between nodes in the original dataset. This approach aids in enhancing the knowledge graph by incorporating more frequent and relevant relationships, thereby improving the accuracy and utility of the graph for various applications.		
citing_context	VG-Attribution	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99 (2022)	The VG-Attribution dataset is used to evaluate structured representations of visual attributes, specifically to compare the performance of proposed methods against state-of-the-art models. This dataset enables researchers to assess and enhance the accuracy and effectiveness of visual attribute representation techniques in a structured format.		
citing_context	VG-Relation	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99 (2022)	The VG-Relation dataset is used to evaluate the performance of methods in capturing visual relationships, specifically by comparing the proposed method against state-of-the-art models. This dataset enables researchers to assess the effectiveness of their approaches in understanding and reasoning about visual relationships, contributing to advancements in visual relationship detection and multi-modal reasoning.		
citing_context	VGGFace2	https://doi.org/10.1145/3583690 (2023)	https://doi.org/10.1016/j.neunet.2014.09.005 (2013)	The VGGFace2 dataset is primarily used to train deep learning models, such as FaceNet, for extracting facial identity features from detected faces in video frames. This enables researchers to develop more accurate face recognition systems, focusing on the robust extraction of facial features in various conditions. The dataset's large scale and diverse set of images facilitate the training of models that can generalize well across different environments and scenarios.		
citing_context	Visual Common-sense Reasoning	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The Visual Common-sense Reasoning dataset is used to evaluate models' ability to reason about complex scenarios involving both images and text, with a focus on common-sense understanding and inference. This dataset enables researchers to assess how well models can integrate visual and textual information to make logical deductions, addressing research questions related to multi-modal reasoning and common-sense inference.		
citing_context	Visual Dialog	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The Visual Dialog dataset is used to explore dialog-based interactions with images, focusing on generating and evaluating responses that require understanding both visual and textual content. This involves methodologies that integrate image recognition and natural language processing to enhance interactive systems. The dataset enables researchers to test and improve models that can effectively communicate about visual data, addressing research questions related to multimodal interaction and response generation.		
citing_context	Visual Entailment	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The Visual Entailment dataset is used to evaluate the ability to reason about the relationship between images and textual statements, focusing on determining whether the image entails, contradicts, or is neutral to the statement. This involves assessing multi-modal reasoning capabilities, where the dataset provides pairs of images and statements to test these relationships. The dataset enables researchers to develop and benchmark models that can understand and correlate visual and textual information effectively.		
cited_context | citing_context	Visual Genome	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1145/3579051.3579073 (2022), https://doi.org/10.3233/sw-233510 (2023) (+6), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019), https://doi.org/10.48550/arXiv.2406.02030 (2024) (+2)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The Visual Genome dataset is primarily used for training models in visual relationship detection and scene graph generation, enhancing multi-modal reasoning by connecting language and vision through dense image annotations. It supports tasks such as zero-shot phrase grounding, object detection, and relationship prediction, leveraging detailed object and attribute annotations to improve model performance in understanding complex visual scenes.; The Visual Genome dataset is extensively used for multi-modal reasoning, particularly in connecting language and vision through dense image annotations. It provides location triples of visual objects and supports scene graph parsing, which enhances visual relationship detection and reasoning. The dataset is crucial for constructing and evaluating scene graphs, improving visual question answering, and pretraining models for multi-modal knowledge graph reasoning. Its rich annotations enable researchers to integrate visual and textual information effectively, advancing the field of multi-modal reasoning.	Visual Genome	
cited_context	Visual Genome scene graph	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2019.00686 (2019)	The Visual Genome scene graph dataset is primarily used to generate question-answer pairs for visual reasoning and compositional question answering. It focuses on creating unbiased Visual Question Answering (VQA) datasets, enhancing the diversity and complexity of questions to improve model performance and reduce biases. This dataset enables researchers to develop and evaluate models that can understand and reason about complex scenes in images.		
cited_context | citing_context	Visual7W	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The Visual7w dataset is used to assess detailed visual understanding through a subset of VQA with wh-questions. It focuses on evaluating models' ability to comprehend complex visual scenes by answering specific, detailed questions. This dataset enables researchers to test and improve visual reasoning capabilities in AI systems, ensuring they can accurately interpret and respond to visual content.; The Visual7w dataset is primarily used for visual question answering, focusing on the model's ability to understand complex scenes. It is designed to test and improve systems' comprehension of visual content through questions that require reasoning about objects, actions, and relationships within images. This dataset enables researchers to evaluate and enhance the performance of visual question answering models in interpreting intricate visual scenes.	Visual7w	
cited_context | citing_context	Visual7W-Kb	https://doi.org/10.24963/ijcai.2020/153 (2020)	https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce (2018)	The Visual7W-KB dataset is used to test the performance of the Out of the Box method, specifically for factual visual question answering. This involves employing graph convolutional networks to process and answer questions that require understanding both visual and textual information. The dataset enables researchers to evaluate how effectively these networks can integrate and reason over multi-modal data.; The Visual7W-KB dataset is used to evaluate the performance of the Out of the Box method in factual visual question answering tasks. This involves employing graph convolution networks to integrate visual and textual information. The dataset's multi-modal nature and structured knowledge graph enable researchers to assess how effectively these models can reason about complex visual scenes and answer questions based on both visual and textual inputs.	Visual7W-KB	
cited_context	VisualGenome	https://doi.org/10.48550/arXiv.2205.11501 (2022), https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The VisualGenome dataset is used to connect language and vision through dense image annotations, supporting the integration of knowledge graphs into VQA systems and enabling the retrieval of relevant visual subgraphs. It is employed to train models on scene graphs, facilitating multi-modal reasoning and enhancing understanding of visual content by linking linguistic and visual elements.		
cited_context	VisualSem	https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.18653/v1/2021.mrl-1.13 (2020)	The VisualSem dataset is used to train models on a multi-modal graph, integrating visual and linguistic information. It enhances multi-modal reasoning in knowledge graphs by exploring vision and language concepts, including multilingual glosses and illustrative images. This dataset supports research in integrating diverse data types to improve understanding and reasoning capabilities in complex, multi-modal environments.		
cited_context | citing_context	Vqa	https://doi.org/10.48550/arXiv.2501.04173 (2025), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2504.10074 (2025), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2406.02030 (2024), https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-016-0966-6 (2015), https://doi.org/10.1007/s11263-016-0981-7 (2016)	The VQA dataset is primarily used for developing and evaluating visual question answering models, which integrate image and text modalities to answer questions about images. It includes 5,000 validation samples and employs the VQA score metric to assess performance. The dataset enhances diversity and complexity by balancing image-question pairs, and it captures relationships among mentions and entities, providing knowledge about named entities and their relations in images.; The VQA dataset is primarily used for visual question answering, focusing on image-text pairs to test and enhance multimodal reasoning capabilities. It is employed to train and evaluate models on tasks that require reasoning about images and textual questions. The dataset is also used to construct and pretrain models on MMKG-grounded datasets by matching VQA instances with corresponding MMKGs derived from scene graphs, which helps in capturing relationships among mentions and entities in images.	VQA	
cited_context | citing_context	Vqa 2.0	https://doi.org/10.48550/arXiv.2501.04173 (2025), https://doi.org/10.18653/v1/2020.findings-emnlp.44 (2020)	https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628 (2015), https://doi.org/10.1109/CVPR.2019.00331 (2019)	The VQA 2.0 dataset is primarily used for fine-tuning models on visual question answering tasks, which enhances the models' ability to accurately answer questions about images. This dataset enables researchers to improve the performance of AI systems in understanding and interpreting visual content, focusing on the integration of image and text data.; The VQA 2.0 dataset is used to evaluate visual question answering models, specifically focusing on their ability to reason about images and questions by integrating external knowledge. This dataset enables researchers to assess model performance in understanding complex visual scenes and answering questions that require reasoning beyond simple pattern recognition.	VQA 2.0	
cited_context	VQA v2	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-016-0966-6 (2015)	The VQA v2 dataset is used to enhance and assess visual question answering models, specifically addressing and mitigating biases present in the original VQA dataset. It balances question types and answers, enabling researchers to develop more robust and fair models. This dataset facilitates the evaluation of model performance across diverse and balanced visual and textual inputs, ensuring that advancements in visual question answering are more reliable and generalizable.		
cited_context	VQA-abstract	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-016-0966-6 (2015)	The VQA-abstract dataset is used to train and evaluate visual question answering models, particularly focusing on abstract scenes and clipart images. This dataset tests the reasoning capabilities of these models by presenting them with complex visual and textual queries. It enables researchers to assess how well models can understand and reason about abstract visual content, enhancing the development of more sophisticated visual reasoning systems.		
citing_context	VQA2	https://doi.org/10.1145/3579051.3579073 (2022)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The VQA2 dataset is primarily used to evaluate visual question answering systems, focusing on complex reasoning tasks that integrate both images and text. Researchers employ this dataset to assess system performance in understanding and answering questions that require interpreting visual content alongside textual information. This enables the development and refinement of models capable of multi-modal reasoning, enhancing their ability to handle real-world, complex queries.		
cited_context	VQAv2	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The VQAv2 dataset is primarily used to evaluate and assess visual question answering (VQA) models, with a focus on compositional reasoning and visual understanding in complex scenes. It emphasizes real-world visual reasoning and complex question answering, enabling researchers to test the performance of VQA models in challenging and realistic scenarios.		
citing_context	VrR-VG	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.1109/CVPR.2015.7298990 (2015)	The VrR-VG dataset is used to enhance the representation of action and spatial relations in images, focusing on visually-relevant relationships. It supports research in improving the understanding and interpretation of visual scenes, particularly in capturing and analyzing actions and spatial configurations. This dataset enables more accurate and contextually rich visual reasoning by providing detailed annotations of these relationships.		
cited_context | citing_context	Vtkb	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TMM.2019.2937181 (2020)	The VTKB dataset is used to construct a visio-textual knowledge base that links hierarchical concepts to images through embedding similarities. This enhances image tagging quality by leveraging both visual and textual data, improving the accuracy and relevance of tags assigned to images.	VTKB	
cited_context | citing_context	Wd-Singer	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1162/tacl_a_00360 (2019)	The WD-singer dataset is used to derive a subset of Wikidata for evaluating knowledge graph reasoning tasks, specifically focusing on singer-related entities and relationships. It enables researchers to assess reasoning algorithms by providing a specialized knowledge graph with detailed singer data, facilitating the evaluation of entity and relationship inference methods.; The WD-singer dataset is used to derive a subset of Wikidata, focusing on singer-related entities for entity linking and knowledge graph reasoning. It enables researchers to enhance the accuracy and depth of entity linking within the domain of singers, contributing to more robust knowledge graph reasoning systems.	WD-singer	
cited_context | citing_context	Webchild	https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1145/3626772.3657790 (2024), https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	The WebChild dataset is used to construct a knowledge graph with types of relations, distinct entities, and knowledge triples, which contributes to multi-modal reasoning tasks. It is also utilized as a subset for visual reasoning in Visual Question Answering (VQA), providing child-friendly knowledge for enhancing visual concept understanding. This dataset enables researchers to integrate textual and visual data, improving the performance of reasoning systems in both structured and unstructured environments.; The WebChild dataset is used to collect and organize commonsense knowledge triplets from the web, contributing to a knowledge base with 193,449 facts. It focuses on harvesting and structuring web-derived information, enabling the construction of a multi-modal knowledge base. This dataset supports research in organizing and utilizing large-scale commonsense knowledge for various applications.	WebChild	
citing_context	WEIBO	https://doi.org/10.48550/arXiv.2306.15946 (2023), https://doi.org/10.1145/3451215 (2021)	https://doi.org/10.1145/3123266.3123454 (2017)	The WEIBO dataset is primarily used for rumor detection in Chinese microblogging platforms, focusing on multimodal data integration and analysis. It is employed to train and evaluate models that fuse textual and visual information, comparing the performance of different methods, including KMAGCN and KhiCL, to enhance the accuracy of rumor detection in social media contexts.		
citing_context	Wiki	https://doi.org/10.24963/ijcai.2024/236 (2024)	https://doi.org/10.1145/3191513 (2015)	The Wiki dataset is used as a benchmark for First-Order Knowledge Graph Completion, specifically to evaluate model performance in completing knowledge graphs. It focuses on assessing how well models can infer missing links within the graph, providing a standardized testbed for comparing different approaches in knowledge graph completion tasks.		
citing_context	Wiki80	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/D17-1004 (2017)	The Wiki80 dataset is used to train OpenNRE models with BERT and CNN encoders for relation extraction, specifically to enhance knowledge graph construction from news items. This dataset facilitates the development and evaluation of models that can accurately identify and extract relationships between entities, thereby improving the quality and comprehensiveness of knowledge graphs derived from textual data.		
cited_context | citing_context	Wikidata	https://doi.org/10.1145/3626246.3654757 (2024), https://doi.org/10.1109/ICDE60146.2024.00061 (2024), https://doi.org/10.24963/ijcai.2024/281 (2024) (+5), https://doi.org/10.1609/aaai.v33i01.33018876 (2019), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022) (+2)	https://doi.org/10.1145/1376616.1376746 (2008)	Wikidata is used as a structured and linked open data resource, providing a comprehensive snapshot of diverse data types including images, text, tables, and audio files. It enhances multi-modal knowledge graph reasoning by linking news entities and obtaining contextual descriptions, thereby constructing rich news background knowledge. The dataset's rich semantic information and temporal capabilities enable enhanced reasoning and integration of structured information.; Wikidata is used as a large-scale, collaboratively edited Knowledge Graph containing 50 million items across various domains. It supports multi-modal reasoning and knowledge integration by providing structured data and entities. Researchers use it to retrieve structured information via SPARQL queries, answer templated questions about people in images, and enhance multi-modal reasoning with rich semantic information. It serves as a source of relational facts for tasks such as question answering, recommendation systems, and multimedia reasoning.	Wikidata	
citing_context	Wikidata SPARQL query log	https://doi.org/10.1145/3626246.3654757 (2024)		The Wikidata SPARQL query log is used to study query complexity and structure in multi-modal knowledge graph reasoning. Researchers extract real-world SPARQL queries and group them by the number of triple patterns. This allows for an analysis of how queries are structured and the complexity they exhibit, providing insights into the practical use and optimization of SPARQL queries in knowledge graphs.		
cited_context | citing_context	Wikidata5M	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1162/tacl_a_00360 (2019)	The wikidata5m dataset is used for knowledge graph reasoning, particularly in inductive settings. It contains 5 million entities and their relationships from Wikidata, enabling research on embedding and pre-trained language representations. This dataset facilitates the development and evaluation of models for inductive knowledge graph reasoning, enhancing the ability to infer relationships in unseen data.; The wikidata5m dataset is used to evaluate multi-modal reasoning capabilities, specifically focusing on a subset of Wikidata that includes multimedia content. Researchers employ this dataset to assess how well models can integrate and reason across textual and visual data, enhancing the understanding of complex relationships within knowledge graphs. This evaluation helps in developing more robust multi-modal reasoning systems.	wikidata5m	
cited_context | citing_context	Wikimedia Commons	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://www.semanticscholar.org/paper/a3a3d374a13e3cf4c69730e5c52138c0be57f6f2 (2017), https://doi.org/10.1007/978-3-319-68204-4_8 (2017)	The Wikimedia Commons dataset is used to enhance IMGpedia, a knowledge graph that supports visuo-semantic queries over images. It incorporates visual information, thereby improving multimodal reasoning capabilities. This application leverages the dataset's rich visual content to integrate and reason about both textual and image data, enabling more sophisticated and context-aware query responses.; The Wikimedia Commons dataset is used to integrate visual information into IMGpedia, a knowledge graph designed to support visuo-semantic queries over images. This integration enhances multi-modal reasoning capabilities, allowing researchers to explore complex relationships between textual and visual data. The dataset's rich multimedia content and structured metadata enable advanced querying and reasoning tasks, facilitating research in multi-modal knowledge representation and retrieval.	Wikimedia Commons	
cited_context | citing_context	Wikipedia	https://doi.org/10.1145/3474085.3475567 (2021), https://doi.org/10.1145/3583780.3614782 (2023), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1109/TPAMI.2013.142 (2014), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The Wikipedia dataset is primarily used for cross-modal multimedia retrieval experiments, generating image-text pairs from featured articles to explore correlation and abstraction in multimodal data. It is also utilized for entity aspect linking by treating section names as aspect labels and for event classification and relation extraction, contributing to the construction of a multi-modal event knowledge graph. These applications focus on integrating textual and visual information to enhance retrieval performance and knowledge graph construction.; The Wikipedia dataset is used for event classification and relation extraction, integrating textual and visual data into a multi-modal event knowledge graph. It also serves to extract textual content for constructing knowledge graphs, emphasizing the integration of multi-modal information from web sources. This enables researchers to build comprehensive, interconnected datasets that enhance understanding of events and their relationships.	Wikipedia	
cited_context | citing_context	Wikipedia Articles And Images	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TMM.2023.3301279 (2021)	The Wikipedia articles and images dataset is used to pre-train a cross-modal entity matching module, which focuses on aligning textual and visual scene graphs extracted from the articles and images. This alignment process enhances the model's ability to understand and match entities across different modalities, supporting research in cross-modal entity recognition and alignment.; The Wikipedia articles and images dataset is used to pre-train a cross-modal entity matching module, which aligns textual and visual scene graphs extracted from the input articles and images. This methodology supports research in cross-modal alignment, enhancing the integration of textual and visual information for more robust entity matching.	Wikipedia articles and images	
cited_context | citing_context	Wn-9	https://doi.org/10.1109/ICDE55515.2023.00015 (2022)	https://doi.org/10.18653/v1/D18-1359 (2018)	The WN-9 dataset is used to enhance multimodal reasoning capabilities by expanding knowledge graphs through the addition of images to entities. This approach integrates visual data with existing textual information, enabling more comprehensive and contextually rich representations. The dataset facilitates research focused on improving the multimodal reasoning abilities of knowledge graphs, specifically by incorporating visual elements to enrich entity descriptions and relationships.; The WN-9 dataset is used to enhance multimodal reasoning capabilities by expanding knowledge graphs through the addition of images to entities. This approach integrates visual data with existing textual information, enabling more comprehensive and contextually rich representations. The dataset facilitates research focused on improving the integration and reasoning over multimodal data in knowledge graphs.	WN-9	
cited_context | citing_context	Wn18	https://doi.org/10.48550/arXiv.2307.03591 (2023), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.1145/3308558.3313612 (2019), https://doi.org/10.1145/3474085.3475470 (2021)	https://doi.org/10.1145/1376616.1376746 (2008), https://doi.org/10.1609/aaai.v32i1.11573 (2017)	The WN18 dataset is primarily used to integrate visual information into lexical relationships by extending entities with images, enhancing multi-modal reasoning in knowledge graphs. It is also utilized to evaluate the performance of translational embeddings in knowledge graph completion tasks, focusing on multi-relational data modeling. This integration of visual and textual data supports advanced representation learning and reasoning capabilities.; The WN18 dataset is used for evaluating knowledge graph embedding models, particularly in relation prediction tasks. It models multi-relational data and assesses the models' ability to predict missing links. This dataset enables researchers to test and compare various embedding methods in a multi-modal context, focusing on the accuracy and effectiveness of these models in handling complex relational structures.	WN18	
cited_context | citing_context	Wn18-Img	https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1145/3474085.3475470 (2021)	https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248 (2013), https://doi.org/10.1145/3477495.3531992 (2022)	The WN18-IMG dataset is used to enhance multi-modal knowledge graph reasoning by extending WN18 with 10 images per entity. This incorporation of visual information allows researchers to explore how visual data can improve reasoning tasks in knowledge graphs, specifically addressing the integration of textual and visual modalities to enhance understanding and inference capabilities.; The WN18-IMG dataset is used to enhance multimodal knowledge graph completion by integrating image data with textual information. It extends WN18 with 10 images per entity, enabling researchers to explore the integration of visual information into relational data modeling. This dataset supports the development of models that can reason over both textual and visual data, thereby extending the scope of triplets in multimodal knowledge graphs.	WN18-IMG	
cited_context	WN18-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The WN18-sparse dataset is used to evaluate and train knowledge graph embedding methods, specifically focusing on sparse versions of the WordNet lexical knowledge graph. It is employed for hyperparameter tuning, including embedding dimensions and learning rate, with a maximum of 50 training iterations. This dataset enables researchers to assess the performance of embedding models in handling sparse data, contributing to the development of more efficient and accurate knowledge graph representations.		
cited_context | citing_context	Wn18Rr	https://doi.org/10.1145/3545573 (2022), https://doi.org/10.1109/TKDE.2022.3220625 (2023), https://doi.org/10.1145/3308558.3313612 (2019), https://doi.org/10.1145/3545573 (2022)	https://doi.org/10.18653/v1/W15-4007 (2015), https://doi.org/10.1609/aaai.v32i1.11573 (2017)	The WN18RR dataset is primarily used to evaluate and validate models for multi-modal knowledge graph reasoning, particularly focusing on wordnet relations and link prediction tasks. It is employed to assess the performance of various neural network architectures, such as CMGNN and HRGAT, in handling hierarchical and semantic relationships, entity ranking, and graph construction. The dataset also supports the integration of textual information to enhance knowledge graph reasoning, demonstrating improvements in convergence speed and state-of-the-art results.; The WN18RR dataset is primarily used for evaluating knowledge graph embedding models, particularly in relation prediction tasks. It focuses on a reduced set of relations, enabling researchers to assess model performance in predicting relationships within knowledge graphs. Additionally, it is used to collect textual information for entities, enhancing the accuracy of relationship predictions. This dataset facilitates the development and testing of models designed to improve knowledge graph reasoning.	WN18RR	
citing_context	WN18RR-IMG	https://doi.org/10.3390/s24237605 (2024)	https://doi.org/10.1145/3474085.3475470 (2021)	The WN18RR-IMG dataset is used to evaluate models' performance in handling relational reasoning with visual context within the WordNet hierarchy. It specifically assesses how well models can integrate visual information to reason about relationships between entities. This dataset enables researchers to test and improve multi-modal reasoning capabilities in knowledge graph models.		
cited_context	WN18RR-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The WN18RR-sparse dataset is used to evaluate and train knowledge graph embedding methods, particularly focusing on sparse versions of the refined WordNet lexical knowledge graph. It is employed for hyperparameter tuning, including embedding dimensions and learning rate, with a maximum of 10 training iterations. This dataset enables researchers to assess the performance and efficiency of embedding models in handling sparse data.		
cited_context	WN9-IMG	https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb (2022)	https://doi.org/10.24963/ijcai.2017/438 (2016)	The WN9-IMG dataset is used to evaluate and develop multi-modal knowledge graph representation learning, particularly focusing on integrating image and WordNet data. It employs translation-based approaches and visual embeddings learned with the VGG19 model to enhance image-embodied knowledge representation and reasoning. This dataset enables researchers to explore how visual and textual information can be effectively combined to improve multi-modal reasoning tasks.		
cited_context | citing_context	Wn9-Img-Txt	https://doi.org/10.1109/ICDE55515.2023.00015 (2022), https://doi.org/10.1109/TKDE.2025.3546686 (2023), https://doi.org/10.1109/ICDE55515.2023.00015 (2022)	https://doi.org/10.18653/v1/S18-2027 (2018)	The WN9-IMG-TXT dataset is used to enhance the data diversity of multi-modal knowledge graphs by integrating textual descriptions and images into entities. It supports research in multi-modal knowledge graph reasoning, particularly in improving the richness of auxiliary data and verifying reasoning performance in a transductive setting. The dataset is also utilized to set embedding dimensions for image features and extract image features for entities, typically using 10 images per entity. It serves as a benchmark to compare with other datasets like FB-IMG-TXT, highlighting differences in sparsity and complexity in multi-modal knowledge graph representation learning.; The WN9-IMG-TXT dataset is used in multimodal knowledge graph representation learning, specifically to integrate visual and textual information. It sets the embedding dimension for image features to 4096 and provides 10 images per entity. The dataset enhances data diversity by adding textual descriptions and images, and serves as a benchmark to compare with FB-IMG-TXT, highlighting differences in sparsity and complexity.	WN9-IMG-TXT	
cited_context | citing_context	Wordnet	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022), https://doi.org/10.1007/978-3-030-58592-1_36 (2020), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/219717.219748 (1995), https://www.semanticscholar.org/paper/d53bcbac7ea19173e95d3bd855b998fab765737d (1998)	WordNet is used as a lexical database for English, providing structured representations of words and their meanings. It enhances scene graphs generated by models like Faster R-CNN, enriching lexical and semantic relationships in multi-modal reasoning tasks. Specifically, it improves the semantic representation of events and their relations in knowledge graphs, aiding in scene graph generation and common sense graph enhancement.; WordNet is used to enhance semantic understanding and reasoning in multi-modal knowledge graphs by providing a rich lexical database. It initializes node representations and ontology, compiles SimilarTo edges, and enhances scene graph generation and event classification through its hierarchical structure and external commonsense knowledge. This enriches the complexity and performance of models in these tasks.	WordNet	
cited_context	WordNetGraph	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://www.semanticscholar.org/paper/25cd421e0f999ef8151b02fe6c7660e059a898ce (2018)	The WordNetGraph dataset is used to construct knowledge graphs by enhancing semantic relationships between words and automatically labeling terms defined by natural language. This enhances interpretability in text entailment recognition. The dataset's focus on semantic relationships and natural language labels supports research in improving the accuracy and understanding of textual data.		
cited_context	X-rays	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The X-rays dataset is used to integrate visual information with textual descriptions, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This approach combines imaging data with clinical notes, improving diagnostic precision and reliability. The dataset's multi-modal nature is crucial for linking visual and textual data, enabling more robust and contextually informed diagnoses.		
citing_context	XMedia	https://doi.org/10.1145/3474085.3475567 (2021)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The XMedia dataset is used for cross-modal multimedia retrieval experiments, employing 3,000-dimensional Bag-of-Words (BoW) features. Research focuses on evaluating the impact of correlation and abstraction on retrieval performance. This dataset enables researchers to explore how different feature representations and their interrelations affect the effectiveness of retrieving multimedia content across modalities.		
cited_context | citing_context	Yago	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1109/ICDE60146.2024.00061 (2024), https://doi.org/10.1145/3581783.3612151 (2023) (+6), https://doi.org/10.1145/3581783.3612151 (2023), https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022) (+4)	https://doi.org/10.1145/1242572.1242667 (2007)	YAGO is used as a core semantic knowledge base that integrates WordNet and Wikipedia, providing structured data about entities and their relationships. It enhances the representation of entities and supports various applications such as entity matching, image tagging, social event analysis, video recognition, recommendation systems, information retrieval, and machine learning. YAGO's multilingual and structured nature makes it valuable for enriching semantic web applications and multi-modal reasoning tasks.; YAGO is primarily used as a knowledge graph to enhance semantic understanding and reasoning over entities, often serving as a core of semantic knowledge for unifying different knowledge sources. It is utilized in various applications, including temporal Knowledge Graph Reasoning, where it incorporates time information to improve data representation and querying. Additionally, YAGO is employed in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, leveraging its multilingual knowledge from Wikipedias. It is also used to evaluate entity matching approaches, focusing on linking entities across different knowledge bases.	YAGO	
citing_context	YAGO Multi-Modal (YAGO15k)	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The YAGO Multi-Modal (YAGO15k) dataset is used to extend traditional knowledge graphs by integrating multi-modal data, specifically visual and textual information. This integration enhances reasoning capabilities, allowing researchers to explore more complex and nuanced relationships within the data. The dataset's multi-modal nature supports advanced reasoning tasks, enabling more comprehensive and context-aware knowledge graph applications.		
cited_context | citing_context	Yago15K	https://doi.org/10.3390/app13116747 (2023), https://doi.org/10.1145/3545573 (2022), https://doi.org/10.1145/3637528.3671769 (2024) (+2), https://doi.org/10.1145/3545573 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019), https://doi.org/10.18653/v1/W15-4007 (2015)	The YAGO15K dataset is primarily used for evaluating multi-modal knowledge graph reasoning, with a focus on entity and relation prediction, particularly incorporating temporal and geographical information. It is also utilized for cross-KG reasoning and entity alignment, including cross-lingual and cross-KG tasks such as linking entities between YAGO and DBpedia. The dataset supports the evaluation of various models, such as MEAFE, CMGNN, and HRGAT, by providing a rich, multi-modal context that enhances the assessment of their performance in reasoning and alignment tasks.; The YAGO15K dataset is used in research for multi-modal knowledge graph reasoning, specifically to test methods for relationship prediction and to evaluate their performance in terms of state-of-the-art results and convergence speed. It provides textual information for entities, enhancing the ability to predict relationships within knowledge graphs.	YAGO15K	
cited_context | citing_context	Yago15K Mmkgs	https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The YAGO15K MMKGs dataset is used alongside FB15K for entity alignment, focusing on multi-modal aspects. It is split into training and testing sets to evaluate the performance of entity alignment methods. This dataset enables researchers to assess and improve the accuracy of aligning entities across different knowledge graphs, leveraging its multi-modal features.	YAGO15K MMKGs	
cited_context	Yago15k-IMG-TXT	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The Yago15k-IMG-TXT dataset is used to integrate specific knowledge graphs with images and text, facilitating comprehensive multi-modal reasoning. This integration supports research methodologies that require the synthesis of textual and visual data within a structured knowledge framework. The dataset enables researchers to explore and develop advanced multi-modal reasoning systems, enhancing the ability to process and understand complex, multi-faceted information.		
cited_context	YAGO2	https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019)	https://doi.org/10.1162/tacl_a_00220 (2013)	The YAGO2 dataset is used as a curated knowledge base to extract entity-relationship-entity triples, which supports the construction and reasoning of knowledge graphs. It facilitates multi-modal reasoning by providing a structured representation of entities and their relationships, enabling researchers to build and analyze complex knowledge graphs.		
cited_context	YAGO3-10	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1222 (2018)	The YAGO3-10 dataset is used as a subset of a larger knowledge graph to evaluate relation prediction, focusing on a specific scope of relations. It is employed in methodologies that assess the accuracy and effectiveness of relation prediction models, enabling researchers to test and refine algorithms within a well-defined relational context.		
cited_context	YAGO37	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1222 (2018)	The YAGO37 dataset is used as a subset of a larger knowledge graph to evaluate relation prediction, focusing on a specific scope of relations. It enables researchers to assess the accuracy and effectiveness of relation prediction models within a defined relational context, contributing to the development and refinement of knowledge graph reasoning techniques.		
cited_context	YAGO39k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1222 (2018)	The YAGO39k dataset is used as a subset of a larger knowledge graph to evaluate relation prediction, focusing on a specific scope of relations. It enables researchers to assess the accuracy and effectiveness of relation prediction models within a defined relational context, providing a benchmark for comparing different methodologies.		
cited_context	YOGA-3SP	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA-3SP dataset is used to generate temporal knowledge graphs, enabling researchers to focus on different periods for temporal reasoning and embedding evaluation. This dataset facilitates the study of how entities and relationships evolve over time, providing a structured way to assess temporal dynamics in knowledge graphs.		
cited_context	YOGA11k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA11k dataset is used to generate temporal knowledge graphs, focusing on different periods for temporal reasoning and embedding evaluation. It enables researchers to assess the effectiveness of temporal embeddings and reasoning methods over varying time spans, providing insights into how well these models capture and predict temporal relationships in data.		
cited_context	YOGA15k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA15k dataset is used to generate temporal knowledge graphs, enabling researchers to focus on different periods for temporal reasoning and embedding evaluation. This dataset facilitates the study of how entities and relationships evolve over time, providing a robust framework for evaluating temporal reasoning models.		
cited_context	YOGA1830	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA1830 dataset is used to generate temporal knowledge graphs, focusing on different periods for temporal reasoning and embedding evaluation. It enables researchers to assess the effectiveness of temporal embeddings and reasoning methods over varying time spans, providing insights into how well these models capture and predict temporal relationships in data.		
