Name (extracted)	Citing Article	Citied Article	Features
ConceptNet	https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.48550/arXiv.2205.11501 (2022), https://doi.org/10.1145/3573201 (2022) (+7)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	ConceptNet is used to gather and enrich knowledge bases with general and commonsense knowledge, particularly through multilingual and open-source data. It contributes to constructing knowledge graphs by providing a large number of facts and commonsense triples, which enhance reasoning capabilities in various applications. Specifically, it is utilized to improve Visual Question Answering (VQA) systems by supplying relevant linguistic subgraphs and general knowledge, thereby enhancing the system's ability to reason about everyday concepts and integrate background knowledge. Additionally, ConceptNet aids in retrieving 1-hop neighbor nodes and pruning irrelevant nodes, improving the relevance of concept nodes in knowledge graph settings.
DBpedia	https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1145/3474085.3475470 (2021) (+4)	https://doi.org/10.1145/2556195.2556245 (2014)	DBpedia is used to extract structured information from Wikipedia, constructing a large-scale, linked data resource that enhances knowledge graphs with relational facts. It provides hasPart and isA triples, contributing over 193,449 facts. The dataset supports knowledge-driven applications like recommendation systems, information retrieval, and machine learning, and is utilized to provide categorical knowledge for Visual Question Answering (VQA), enhancing the system's ability to understand and answer questions about visual content. DBpedia's structured content from Wikimedia projects enriches multi-modal knowledge graph reasoning, offering diverse and interconnected data.
Freebase	https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.1145/3581783.3612151 (2023) (+7)	https://doi.org/10.1145/1376616.1376746 (2008)	Freebase is used as a collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia and NNDB. It supports knowledge-driven applications such as recommendation systems, information retrieval, and machine learning. Freebase is also utilized for tasks like question answering and evaluating entity matching approaches, focusing on linking entities across different knowledge bases. Its extensive structured data enables researchers to enhance and evaluate various knowledge-driven systems.
Visual Genome	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019), https://doi.org/10.48550/arXiv.2406.02030 (2024) (+2)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The Visual Genome dataset is extensively used for multi-modal reasoning, particularly in connecting language and vision through dense image annotations. It provides location triples of visual objects and supports scene graph parsing, which enhances visual relationship detection and reasoning. The dataset is crucial for constructing and evaluating scene graphs, improving visual question answering, and pretraining models for multi-modal knowledge graph reasoning. Its rich annotations enable researchers to integrate visual and textual information effectively, advancing the field of multi-modal reasoning.
YAGO	https://doi.org/10.1145/3581783.3612151 (2023), https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022) (+4)	https://doi.org/10.1145/1242572.1242667 (2007)	YAGO is primarily used as a knowledge graph to enhance semantic understanding and reasoning over entities, often serving as a core of semantic knowledge for unifying different knowledge sources. It is utilized in various applications, including temporal Knowledge Graph Reasoning, where it incorporates time information to improve data representation and querying. Additionally, YAGO is employed in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, leveraging its multilingual knowledge from Wikipedias. It is also used to evaluate entity matching approaches, focusing on linking entities across different knowledge bases.
DBP15K	https://doi.org/10.1145/3581783.3611786 (2022), https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.48550/arXiv.2209.00891 (2022)	The DBP15K dataset is primarily used to evaluate cross-lingual entity alignment methods, particularly those involving attribute-preserving embeddings and multi-modal knowledge graph alignment. It focuses on entities with attached images and is often used to assess the performance and stability of ResNet-152 vision encoders with 2048-dimensional vision features. This dataset enables researchers to test and refine models that integrate visual and textual data for improved entity alignment across multiple language pairs.
MSCOCO	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/AAAI.V34I07.6731 (2020), https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The MSCOCO dataset is extensively used in research for image captioning, object detection, and visual recognition, often to evaluate and compare the performance of different models. It contains over 100K images with multiple captions each, enabling detailed assessments of image-text alignment and multi-modal reasoning capabilities. Researchers utilize it to enhance the precision of visual object extraction, generate textual descriptions from images, and conduct multi-label classification experiments, leveraging its rich annotations and diverse visual contexts.
Wikidata	https://doi.org/10.1609/aaai.v33i01.33018876 (2019), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022) (+2)	https://doi.org/10.1145/1376616.1376746 (2008)	Wikidata is used as a large-scale, collaboratively edited Knowledge Graph containing 50 million items across various domains. It supports multi-modal reasoning and knowledge integration by providing structured data and entities. Researchers use it to retrieve structured information via SPARQL queries, answer templated questions about people in images, and enhance multi-modal reasoning with rich semantic information. It serves as a source of relational facts for tasks such as question answering, recommendation systems, and multimedia reasoning.
FVQA	https://doi.org/10.1609/aaai.v33i01.33018876 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TNNLS.2020.3045034 (2020) (+1)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The FVQA dataset is used to evaluate methods that integrate knowledge graphs with image representations for factual visual question answering. It focuses on the accuracy of answers generated from combined visual and textual inputs, emphasizing detailed visual reasoning, commonsense reasoning, and the integration of external knowledge sources. This dataset captures relationships among mentions and entities, providing knowledge about named entities and their relations in images, and is used to obtain questions directly from existing knowledge bases. It enables research by facilitating the evaluation of models that combine textual and visual information to answer questions accurately.
FB15k-237	https://doi.org/10.1145/3545573 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022) (+1)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	FB15k-237 is primarily used to train and evaluate knowledge graph completion models, focusing on entity linking, relation prediction, and entity prediction tasks. It is employed to reduce redundancy and improve evaluation, often serving as a baseline for comparing different methods, including graph convolutional networks and multimodal approaches that integrate visual and textual data. The dataset enables researchers to assess model performance and convergence speeds in various knowledge graph reasoning tasks.
KB-VQA	https://doi.org/10.1609/aaai.v33i01.33018876 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.18653/v1/2020.findings-emnlp.44 (2020)	https://doi.org/10.24963/IJCAI.2017/179 (2015)	The KB-VQA dataset is used for knowledge-based visual question answering, focusing on integrating external knowledge bases with image understanding. It is employed to compare against datasets with limited images and external knowledge, emphasizing the need for comprehensive data in multi-modal knowledge graph reasoning. The dataset supports explicit knowledge-based reasoning, generating template-based questions and capturing relationships among mentions and entities in images. This enables researchers to enhance visual question answering by leveraging structured knowledge graphs.
WebChild	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	The WebChild dataset is used to collect and organize commonsense knowledge triplets from the web, contributing to a knowledge base with 193,449 facts. It focuses on harvesting and structuring web-derived information, enabling the construction of a multi-modal knowledge base. This dataset supports research in organizing and utilizing large-scale commonsense knowledge for various applications.
FB15K	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1145/3308558.3313612 (2019), https://doi.org/10.1109/TPAMI.2024.3417451 (2022) (+4)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB15K dataset is primarily used for evaluating and training knowledge graph embedding models, focusing on relation prediction and link prediction tasks. It is employed to assess models' ability to predict missing links in multi-relational data, often using a medium-sized subset of Freebase entities and relations. The dataset supports research in knowledge graph completion, entity linking, and entity alignment, with some studies incorporating multimodal reasoning by adding images to entities. It is frequently split into training and testing sets to evaluate model performance, particularly in tasks such as TransE modeling.
FB15K-DB15K	https://doi.org/10.1145/3534678.3539244 (2022), https://doi.org/10.1145/3581783.3611786 (2022), https://doi.org/10.1609/aaai.v38i8.28762 (2024) (+1)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-DB15K dataset is primarily used for evaluating and training entity alignment methods in multi-modal knowledge graphs. It focuses on aligning entities across different modalities and datasets, such as FB15K and DB15K. Research employs this dataset to compare various alignment techniques (e.g., MSNEA, PoE, MMEA, EVA) using metrics like Hits@1, Hits@10, and MRR. Studies often use different training-testing splits (2:8, 5:5, 8:2) and explore the impact of multi-modal knowledge on alignment performance, particularly in monolingual settings.
VisualGenome	https://doi.org/10.48550/arXiv.2205.11501 (2022), https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The VisualGenome dataset is used to connect language and vision through dense image annotations, supporting the integration of knowledge graphs into VQA systems and enabling the retrieval of relevant visual subgraphs. It is employed to train models on scene graphs, facilitating multi-modal reasoning and enhancing understanding of visual content by linking linguistic and visual elements.
kgbench	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1007/978-3-030-77385-4_37 (2021)	The kgbench dataset is used to evaluate and benchmark relational and multimodal machine learning methods across a collection of knowledge graph datasets. It focuses on assessing performance across different modalities, enabling researchers to compare and analyze the effectiveness of various models in handling complex, multimodal data.
OK-VQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.18653/v1/2020.findings-emnlp.44 (2020)	https://doi.org/10.1109/CVPR.2019.00331 (2019)	The OK-VQA dataset is used to evaluate multi-modal reasoning in visual question answering, particularly for open-ended and complex queries that require external knowledge. It focuses on integrating visual and textual information to test the reasoning capabilities of VQA models, emphasizing the need for external knowledge to answer questions. The dataset benchmarks systems on their ability to integrate and reason about both visual and non-visual information, capturing relationships among entities and their mentions in images.
DB15K	https://doi.org/10.1609/aaai.v39i12.33454 (2024), https://doi.org/10.1145/3545573 (2022), https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The DB15K dataset is primarily used in multi-modal knowledge graph reasoning research, specifically for evaluating and enhancing model performance in tasks such as cross-modal entity alignment and relation prediction. It is often used alongside other datasets like FB15K and YAGO15K, with splits of 2:8, 5:5, and 8:2 for training and testing. The dataset facilitates the assessment of state-of-the-art methods, focusing on metrics like MRR, Hit@1, Hit@3, and Hit@10, and is also used to measure computational efficiency on a single GPU. It supports the selection of diverse entity categories (e.g., artist, city, organization, country) for constructing robust multi-modal knowledge graphs.
KVQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The KVQA dataset is primarily used for visual question answering (VQA) research, focusing on integrating visual data with structured knowledge graphs. It evaluates and trains models on their ability to reason about complex visual content using external knowledge, specifically designed to challenge state-of-the-art methods in multi-modal reasoning. The dataset captures relationships among mentions and entities, enabling researchers to study VQA over knowledge graphs and assess the integration of world knowledge with visual data.
FBDB15K	https://doi.org/10.1145/3581783.3611786 (2022)	https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108 (2014)	The FBDB15K dataset is used for multi-modal knowledge graph reasoning, particularly focusing on entity alignment with vision features. It employs the VGG-16 vision encoder and 4096-dimensional vision features to evaluate and demonstrate model stability in integrating visual and textual data within knowledge graphs. This dataset enables researchers to assess the effectiveness of vision encoders in enhancing multi-modal reasoning tasks.
FBYG15K	https://doi.org/10.1145/3581783.3611786 (2022)	https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108 (2014)	The FBYG15K dataset is used for multi-modal knowledge graph reasoning, particularly focusing on entity alignment with vision features. It employs the VGG-16 vision encoder and 4096-dimensional vision features to evaluate and demonstrate model stability in integrating visual and textual data. This dataset enables researchers to assess the effectiveness of vision encoders in enhancing multi-modal knowledge graph reasoning tasks.
MMEKG	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The MMEKG dataset is used to represent and integrate multi-modal events into a knowledge graph, facilitating universal representation across diverse data types and modalities. It advances the field of event knowledge graphs by providing a large-scale ontology with 990,000 concept events and 644 relation types, enabling comprehensive coverage of real-world happenings. This dataset supports research in creating more robust and versatile knowledge graphs.
WordNet	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022), https://doi.org/10.1007/978-3-030-58592-1_36 (2020), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://www.semanticscholar.org/paper/d53bcbac7ea19173e95d3bd855b998fab765737d (1998)	WordNet is used to enhance semantic understanding and reasoning in multi-modal knowledge graphs by providing a rich lexical database. It initializes node representations and ontology, compiles SimilarTo edges, and enhances scene graph generation and event classification through its hierarchical structure and external commonsense knowledge. This enriches the complexity and performance of models in these tasks.
VQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2406.02030 (2024), https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The VQA dataset is primarily used for visual question answering, focusing on image-text pairs to test and enhance multimodal reasoning capabilities. It is employed to train and evaluate models on tasks that require reasoning about images and textual questions. The dataset is also used to construct and pretrain models on MMKG-grounded datasets by matching VQA instances with corresponding MMKGs derived from scene graphs, which helps in capturing relationships among mentions and entities in images.
CLEVR	https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The CLEVR dataset is primarily used to evaluate compositional reasoning abilities, focusing on visual and linguistic understanding. It assesses models' capabilities in visual question answering, particularly in understanding complex queries involving multiple objects and attributes. The dataset is designed to test elementary visual reasoning and compositional language skills in a controlled environment, enabling researchers to evaluate and improve knowledge graph reasoning models.
GQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2019.00331 (2019)	The GQA dataset is used to test and enhance models' abilities in multi-modal reasoning, specifically focusing on visual scene understanding and compositional question answering. It evaluates how well models can integrate visual and textual information to reason about complex queries and ground answers in context. The dataset captures relationships among entities in images, providing rich knowledge about named entities and their interactions, which is crucial for grounded question answering and visual reasoning tasks.
FB-IMG-TXT	https://doi.org/10.1109/ICDE55515.2023.00015 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/S18-2027 (2018)	The FB-IMG-TXT dataset is used for multimodal knowledge graph representation learning, combining textual descriptions and images. It supports tasks such as multimodal translation-based representation learning and enhancing data diversity by integrating visual and textual information. The dataset features 100 images per entity and sets the embedding dimension for image features to 128, aiding in the comparison of sparsity and complexity in multimodal knowledge graphs.
Flickr30k	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Flickr30k dataset is primarily used for evaluating and training multi-modal systems, specifically in visual grounding and image-text matching. It supports fine-grained multimedia knowledge extraction and aligns textual descriptions with corresponding images. The dataset's image-text pairs enable researchers to assess the accuracy of these alignments and train detectors for multi-modal reasoning tasks.
MMKG	https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TKDE.2024.3352100 (2023)	https://doi.org/10.1145/3340531.3417439 (2020)	The MMKG dataset is used to enhance retrieval capabilities for deep learning papers and codes, generate a metallic materials knowledge graph by integrating structured data from DBpedia and Wikipedia, and incorporate both text and image information into knowledge graphs for multi-modal reasoning tasks. This dataset supports research in improving search functionalities and integrating diverse data types for enhanced knowledge representation and reasoning.
hasPart KB	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The 'hasPart KB' dataset is used to provide hasPart triples, which contribute to the construction of knowledge graphs for open-domain Visual Question Answering (VQA). It offers part-whole relationships, aiding in the understanding of complex objects and their components. This dataset enhances VQA systems by integrating explicit knowledge, improving the accuracy and depth of answers.
VCR	https://www.semanticscholar.org/paper/ef318e7ff0883e72d853c75736d20cc123b556d5 (2019), https://doi.org/10.1109/TNNLS.2020.3045034 (2020), https://doi.org/10.48550/arXiv.2205.11501 (2022)	https://doi.org/10.1109/CVPR.2019.00688 (2018)	The VCR dataset is primarily used to evaluate and fine-tune models for visual commonsense reasoning, focusing on understanding and explaining visual scenes. It supports extensive experiments and validation of state-of-the-art models on multiple-choice QA tasks derived from 110k movie scenes, emphasizing implicit visual-linguistic representations and downstream applications.
AWA2-KG	https://doi.org/10.48550/arXiv.2207.01328 (2022)	https://doi.org/10.1145/3442381.3450042 (2021)	The AWA2-KG dataset is primarily used for evaluating and validating models in zero-shot learning tasks, particularly focusing on knowledge graph-based approaches. It provides triples for prompting in ontology-enhanced zero-shot learning, emphasizing relations between classes, entities, and their attributes. This dataset facilitates the assessment of model performance in handling ontology-enhanced attributes and supports the development of flexible systems capable of reasoning with limited data.
DBpedia50	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DBpedia50 dataset is used for knowledge graph completion, specifically focusing on entity linking and relation prediction within a smaller subset of DBpedia. It is employed to evaluate and validate knowledge graph completion methods, enabling efficient testing and validation on smaller-scale entity sets. This dataset facilitates research by providing a manageable yet representative sample for these tasks.
DBpedia500	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DBpedia500 dataset is used for evaluating and assessing the performance and robustness of knowledge graph completion models. It focuses on scalability and model performance on a moderately sized dataset, enabling researchers to test the effectiveness of knowledge graph embedding techniques in a controlled yet challenging environment.
DB100K	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DB100K dataset is used for testing the scalability and robustness of knowledge graph embedding and reasoning algorithms. It provides a larger dataset to evaluate how well these models generalize and perform under increased data volume, focusing on the capabilities of algorithms to handle and reason over extensive knowledge graphs.
MMEA datasets	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The MMEA datasets are used to evaluate multi-modal entity alignment methods, addressing the challenges of aligning entities across different modalities in knowledge graphs. They are also employed to assess model performance across bilingual, monolingual, and high-degree categories using pre-trained visual encoders such as ResNet-152 and CLIP, in both standard and iterative settings. This enables researchers to compare and refine alignment techniques and model effectiveness in multi-modal contexts.
Multi-OpenEA	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The Multi-OpenEA dataset is used to evaluate and demonstrate model stability with the CLIP vision encoder, specifically focusing on the vision feature dimension of 512. This dataset enables researchers to assess how changes in vision feature dimensions impact model performance, providing insights into the robustness and reliability of models in multi-modal settings.
WN9-IMG-TXT	https://doi.org/10.1109/ICDE55515.2023.00015 (2022)	https://doi.org/10.18653/v1/S18-2027 (2018)	The WN9-IMG-TXT dataset is used in multimodal knowledge graph representation learning, specifically to integrate visual and textual information. It sets the embedding dimension for image features to 4096 and provides 10 images per entity. The dataset enhances data diversity by adding textual descriptions and images, and serves as a benchmark to compare with FB-IMG-TXT, highlighting differences in sparsity and complexity.
IMDB	https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The IMDB dataset is used as a domain-specific knowledge graph to provide structured information about movies, actors, and directors. It is employed to evaluate entity matching approaches, focusing on linking entities across different knowledge bases. This enables researchers to enhance the accuracy and efficiency of data integration and knowledge base alignment methods.
NUS-WIDE	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The NUS-WIDE dataset is primarily used for evaluating models in multi-label image annotation and retrieval, enhancing reasoning capabilities through scene graph construction, and disambiguating concepts in multi-modal data. It focuses on a large-scale web image dataset with multiple labels per image, enabling researchers to assess model performance in complex, real-world scenarios.
Babel-Net	https://doi.org/10.1109/ACCESS.2019.2933370 (2019), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	Babel-Net is used to build a multilingual knowledge graph, serving as a large-scale lexicalized semantic network. It provides lexical knowledge and supports the creation of multilingual resources. While listed as an example of a knowledge graph, it is not specifically used for multi-modal Knowledge Graph Reasoning research. Its extensive lexical coverage and multilingual capabilities enable the development of comprehensive semantic networks.
FB15k-237-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The FB15k-237-sparse dataset is used to evaluate and train multi-modal knowledge graph reasoning methods, particularly focusing on the sparse subset of the filtered Freebase knowledge graph. It is utilized to assess the performance of models like IterE+axioms, emphasizing the impact of deductive capabilities. The dataset supports hyperparameter tuning, including embedding dimensions and learning rate, and is used for training and validation with a maximum of 10 iterations.
FB15K-YG15K	https://doi.org/10.1145/3534678.3539244 (2022)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-YG15K dataset is used for evaluating entity alignment methods in multi-modal knowledge graph reasoning. It focuses on aligning entities across different modalities, using 20% alignment seeds to assess the effectiveness of these methods. This dataset enables researchers to test and improve algorithms designed for cross-modal entity alignment in knowledge graphs.
FB13	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB13 dataset is used to train and evaluate knowledge graph completion models, focusing on a smaller subset of Freebase entities and relations. It is employed in evaluating knowledge graph reasoning methods, particularly for entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in these specific areas, providing a benchmark for comparing different approaches.
FB122	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB122 dataset is used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase entities and relations. It is also employed for evaluating knowledge graph reasoning methods, particularly in entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting missing links and relationships within complex knowledge graphs.
FB24k	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB24k dataset is used to train and evaluate knowledge graph completion models, specifically focusing on entity linking and relation prediction tasks within a subset of the Freebase dataset. It enables researchers to assess the performance of knowledge graph reasoning methods by providing a structured set of entities and relations, facilitating the development and testing of algorithms designed to predict missing links and relationships.
FB20k	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB20k dataset is used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase entities and relations. It is also employed for evaluating knowledge graph reasoning methods, particularly in entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting missing links and relationships within complex knowledge graphs.
FB5M	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB5M dataset is used to train and evaluate knowledge graph completion models, focusing on a large subset of Freebase entities and relations. It is employed in evaluating knowledge graph reasoning methods, particularly for entity linking and relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting missing links and relationships within extensive knowledge graphs.
FB60k-NYT10	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB60k-NYT10 dataset is used to train and evaluate knowledge graph completion models, specifically focusing on entity linking and relation prediction tasks. It combines Freebase entities with New York Times articles to create a multimodal dataset, enabling researchers to assess the performance of knowledge graph reasoning methods in a rich, real-world context.
FB15K-YAGO15K	https://doi.org/10.1145/3581783.3611786 (2022), https://doi.org/10.1609/aaai.v38i8.28762 (2024)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-YAGO15K dataset is used for evaluating and training entity alignment in multi-modal knowledge graphs, particularly focusing on cross-KG entity matching and alignment accuracy. It supports monolingual experiments within the MMKG framework, enabling researchers to assess the impact of varying proportions of reference entity alignment pairs on model performance. This dataset facilitates the development and testing of methods for improving the accuracy and robustness of entity alignment in multi-modal settings.
imSitu	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://www.semanticscholar.org/paper/d53bcbac7ea19173e95d3bd855b998fab765737d (1998)	The imSitu dataset is used to enhance multi-modal knowledge graphs by incorporating situation recognition and visual semantic role labeling. It enriches the visual ontology through the alignment of WordNet synsets to annotated frames, thereby improving the semantic representation of visual scenes. This methodology supports research in extending and refining visual ontologies, enabling more nuanced understanding and interpretation of visual data.
ASER	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022), https://doi.org/10.1145/3573201 (2022), https://doi.org/10.1109/TKDE.2024.3352100 (2023)	https://doi.org/10.1145/3366423.3380107 (2019)	The ASER dataset is used to construct large-scale, multi-modal knowledge graphs by leveraging defined patterns and an automatic pipeline, focusing on scalable graph construction. It is also utilized to study causal relationships between events, enhancing commonsense reasoning through graph-based methods. ASER is noted for being the largest knowledge graph with eventualities as semantic units and relations as edges, though specific research applications beyond these areas are not detailed.
WordNetGraph	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://www.semanticscholar.org/paper/25cd421e0f999ef8151b02fe6c7660e059a898ce (2018)	The WordNetGraph dataset is used to construct knowledge graphs by enhancing semantic relationships between words and automatically labeling terms defined by natural language. This enhances interpretability in text entailment recognition. The dataset's focus on semantic relationships and natural language labels supports research in improving the accuracy and understanding of textual data.
PropBank	https://doi.org/10.1145/3573201 (2022)	https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e (2004)	The PropBank dataset is used to enhance event knowledge graphs by providing an annotated corpus of semantic roles, which enriches the representation of verb-based events. It contributes 112,917 events to the dataset, extending its size and improving the granularity and detail of event-related information. This enables more comprehensive and nuanced research in areas such as natural language processing and semantic analysis.
NomBank	https://doi.org/10.1145/3573201 (2022)	https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e (2004)	NomBank is used to extend the size of event knowledge graphs by providing an annotated corpus of nominal predicate argument structures, specifically enhancing the representation of noun-based events. It contributes 114,576 events to the dataset, enriching the graph's content and improving the modeling of complex event relationships.
Knowledge Vault	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The Knowledge Vault dataset is used as a large-scale knowledge graph to integrate diverse sources of information, supporting comprehensive knowledge representation. It enables researchers to aggregate and unify data from various sources, facilitating the creation of a more holistic and interconnected knowledge base. This integration supports research requiring extensive and multifaceted information, enhancing the depth and breadth of knowledge available for analysis.
Microsoft Concept Graph	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The Microsoft Concept Graph is used as a general knowledge graph to represent and categorize concepts and their relationships. It aids in text understanding and classification by providing structured information that enhances the interpretation and categorization of textual data. This dataset enables researchers to improve the accuracy and depth of text analysis through its comprehensive representation of concept relationships.
MusicBrainz	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The MusicBrainz dataset is used as a domain-specific knowledge graph to provide structured information about music, including details about artists, albums, and tracks. It is employed to enhance the accuracy and richness of music-related data in research, supporting applications that require detailed and organized musical metadata. This dataset enables researchers to integrate comprehensive music information into their studies, improving the depth and reliability of their analyses.
VisualSem	https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.18653/v1/2021.mrl-1.13 (2020)	The VisualSem dataset is used to train models on a multi-modal graph, integrating visual and linguistic information. It enhances multi-modal reasoning in knowledge graphs by exploring vision and language concepts, including multilingual glosses and illustrative images. This dataset supports research in integrating diverse data types to improve understanding and reasoning capabilities in complex, multi-modal environments.
WN9-IMG	https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb (2022)	https://doi.org/10.24963/ijcai.2017/438 (2016)	The WN9-IMG dataset is used to evaluate and develop multi-modal knowledge graph representation learning, particularly focusing on integrating image and WordNet data. It employs translation-based approaches and visual embeddings learned with the VGG19 model to enhance image-embodied knowledge representation and reasoning. This dataset enables researchers to explore how visual and textual information can be effectively combined to improve multi-modal reasoning tasks.
Open Images	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Open Images dataset is used to train detectors with supervised data, specifically focusing on image-text pairs for multi-modal reasoning tasks. This enables researchers to develop models that can effectively understand and reason about visual and textual information simultaneously, enhancing the performance of multi-modal systems.
VQAv2	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The VQAv2 dataset is primarily used to evaluate and assess visual question answering (VQA) models, with a focus on compositional reasoning and visual understanding in complex scenes. It emphasizes real-world visual reasoning and complex question answering, enabling researchers to test the performance of VQA models in challenging and realistic scenarios.
Wikipedia	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The Wikipedia dataset is used for event classification and relation extraction, integrating textual and visual data into a multi-modal event knowledge graph. It also serves to extract textual content for constructing knowledge graphs, emphasizing the integration of multi-modal information from web sources. This enables researchers to build comprehensive, interconnected datasets that enhance understanding of events and their relationships.
CC3M	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The CC3M dataset is used to enhance multi-modal data in research, specifically for object recognition and event relation extraction, contributing to the enrichment of multi-modal event knowledge graphs. It is also utilized to augment training sets, improving the performance of image captioning models by providing additional visual data.
Visual7W-KB	https://doi.org/10.24963/ijcai.2020/153 (2020)	https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce (2018)	The Visual7W-KB dataset is used to evaluate the performance of the Out of the Box method in factual visual question answering tasks. This involves employing graph convolution networks to integrate visual and textual information. The dataset's multi-modal nature and structured knowledge graph enable researchers to assess how effectively these models can reason about complex visual scenes and answer questions based on both visual and textual inputs.
Compositional ZSL	https://doi.org/10.48550/arXiv.2207.01328 (2022)	https://doi.org/10.1109/CVPR46437.2021.00101 (2021)	The Compositional ZSL dataset is used to explore zero-shot learning scenarios, specifically focusing on the composition of visual primitives to enhance feature-to-sequence transformation techniques. It enables researchers to investigate how class descriptions composed of visual primitives can improve model performance in predicting unseen classes. This dataset facilitates the development and evaluation of methods that leverage compositional structures in visual data.
WN18	https://doi.org/10.1145/3308558.3313612 (2019), https://doi.org/10.1145/3474085.3475470 (2021)	https://doi.org/10.1609/aaai.v32i1.11573 (2017)	The WN18 dataset is used for evaluating knowledge graph embedding models, particularly in relation prediction tasks. It models multi-relational data and assesses the models' ability to predict missing links. This dataset enables researchers to test and compare various embedding methods in a multi-modal context, focusing on the accuracy and effectiveness of these models in handling complex relational structures.
WN18RR	https://doi.org/10.1145/3308558.3313612 (2019), https://doi.org/10.1145/3545573 (2022)	https://doi.org/10.1609/aaai.v32i1.11573 (2017)	The WN18RR dataset is primarily used for evaluating knowledge graph embedding models, particularly in relation prediction tasks. It focuses on a reduced set of relations, enabling researchers to assess model performance in predicting relationships within knowledge graphs. Additionally, it is used to collect textual information for entities, enhancing the accuracy of relationship predictions. This dataset facilitates the development and testing of models designed to improve knowledge graph reasoning.
WN18-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The WN18-sparse dataset is used to evaluate and train knowledge graph embedding methods, specifically focusing on sparse versions of the WordNet lexical knowledge graph. It is employed for hyperparameter tuning, including embedding dimensions and learning rate, with a maximum of 50 training iterations. This dataset enables researchers to assess the performance of embedding models in handling sparse data, contributing to the development of more efficient and accurate knowledge graph representations.
WN18RR-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The WN18RR-sparse dataset is used to evaluate and train knowledge graph embedding methods, particularly focusing on sparse versions of the refined WordNet lexical knowledge graph. It is employed for hyperparameter tuning, including embedding dimensions and learning rate, with a maximum of 10 training iterations. This dataset enables researchers to assess the performance and efficiency of embedding models in handling sparse data.
FB15k-sparse	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.18653/v1/W15-4007 (2015)	The FB15k-sparse dataset is used to evaluate and train multi-modal knowledge graph reasoning methods, specifically focusing on the sparse subset of the Freebase knowledge graph. It is utilized for hyperparameter tuning, such as adjusting embedding dimensions and learning rates, with a maximum of 50 training iterations. This dataset enables researchers to assess the performance of their models in handling sparse data, enhancing the robustness of knowledge graph reasoning techniques.
Wikimedia Commons	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1007/978-3-319-68204-4_8 (2017)	The Wikimedia Commons dataset is used to integrate visual information into IMGpedia, a knowledge graph designed to support visuo-semantic queries over images. This integration enhances multi-modal reasoning capabilities, allowing researchers to explore complex relationships between textual and visual data. The dataset's rich multimedia content and structured metadata enable advanced querying and reasoning tasks, facilitating research in multi-modal knowledge representation and retrieval.
YAGO15K	https://doi.org/10.1145/3545573 (2022)	https://doi.org/10.18653/v1/W15-4007 (2015)	The YAGO15K dataset is used in research for multi-modal knowledge graph reasoning, specifically to test methods for relationship prediction and to evaluate their performance in terms of state-of-the-art results and convergence speed. It provides textual information for entities, enhancing the ability to predict relationships within knowledge graphs.
YAGO3-10	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1222 (2018)	The YAGO3-10 dataset is used as a subset of a larger knowledge graph to evaluate relation prediction, focusing on a specific scope of relations. It is employed in methodologies that assess the accuracy and effectiveness of relation prediction models, enabling researchers to test and refine algorithms within a well-defined relational context.
YAGO37	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1222 (2018)	The YAGO37 dataset is used as a subset of a larger knowledge graph to evaluate relation prediction, focusing on a specific scope of relations. It enables researchers to assess the accuracy and effectiveness of relation prediction models within a defined relational context, contributing to the development and refinement of knowledge graph reasoning techniques.
YAGO39k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1222 (2018)	The YAGO39k dataset is used as a subset of a larger knowledge graph to evaluate relation prediction, focusing on a specific scope of relations. It enables researchers to assess the accuracy and effectiveness of relation prediction models within a defined relational context, providing a benchmark for comparing different methodologies.
Richpedia	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1145/3581783.3612151 (2023)	https://doi.org/10.1007/978-3-030-41407-8_9 (2019)	The Richpedia dataset is used to construct and enhance comprehensive multi-modal knowledge graphs by integrating triplets, textual descriptions, and images. This integration supports reasoning tasks and demonstrates significant potential in multi-modal knowledge graph reasoning, enabling more robust KG-based applications through the incorporation of visual modalities.
MKG-Wikipedia	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.14778/3407790.3407828 (2020)	The MKG-Wikipedia dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. Researchers employ this dataset to assess the performance of algorithms in aligning entities and predicting relations within a multimodal context, leveraging the rich textual and structural information from Wikipedia. This enables the development and refinement of techniques for enhancing knowledge graph reasoning.
MKG-YAGO	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/3503161.3548388 (2022)	The MKG-YAGO dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. Researchers employ this dataset to assess the performance of algorithms in these tasks, leveraging the YAGO data's rich structure and multimodal features. This enables the development and refinement of techniques for enhancing knowledge graph reasoning.
WN18-IMG	https://doi.org/10.48550/arXiv.2212.05767 (2022), https://doi.org/10.1145/3474085.3475470 (2021)	https://doi.org/10.1145/3477495.3531992 (2022)	The WN18-IMG dataset is used to enhance multimodal knowledge graph completion by integrating image data with textual information. It extends WN18 with 10 images per entity, enabling researchers to explore the integration of visual information into relational data modeling. This dataset supports the development of models that can reason over both textual and visual data, thereby extending the scope of triplets in multimodal knowledge graphs.
FrameNet	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	FrameNet is integrated into ontologies to provide frame-based semantic structures, which support the reasoning process in multi-modal knowledge graphs. This integration enhances the representation and processing of complex, multi-modal data by leveraging FrameNet's detailed semantic frames, enabling more sophisticated and context-aware reasoning capabilities.
CM3KG	https://doi.org/10.18653/v1/2022.emnlp-demos.15 (2022)	https://doi.org/10.18653/v1/2020.acl-main.703 (2019)	The CM3KG dataset is used as an open-sourced multi-modal knowledge graph to support research in multi-modal knowledge graph reasoning. It provides a rich resource for integrating and reasoning across different modalities, enabling researchers to explore complex relationships and interactions within and between various data types. This dataset facilitates the development and evaluation of methods that can effectively combine textual, visual, and other forms of data to enhance reasoning capabilities.
ImageNet	https://doi.org/10.1109/IJCNN55064.2022.9892382 (2022)	https://doi.org/10.1109/CVPR.2009.5206848 (2009)	The ImageNet dataset is primarily used to train deep learning models like ResNet50, focusing on hierarchical image classification. It enables researchers to extract robust visual features from video frames, enhancing the accuracy of image and video analysis tasks. The large-scale, hierarchically organized nature of ImageNet supports the development and evaluation of advanced visual recognition systems.
BioRel	https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126 (2023)	https://doi.org/10.1186/s12859-020-03889-5 (2020)	The BioRel dataset is used for empirical evaluation and large-scale biomedical relation extraction, focusing on the performance of models in extracting biomedical relations from text. It leverages distant supervision to compile a comprehensive dataset for training and testing, enabling researchers to assess and improve relation extraction methodologies in the biomedical domain.
FB-IMG	https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb (2022)	https://doi.org/10.24963/ijcai.2017/438 (2016)	The FB-IMG dataset is used to evaluate multi-modal knowledge graph representation learning, specifically focusing on image-embodied knowledge representation and translation-based approaches. It enables researchers to assess how effectively models can integrate visual and textual information within knowledge graphs, addressing research questions related to the accuracy and robustness of these representations.
SituNet	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	SituNet is used to train models for situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It aids in defining visual event schemas, which are crucial for developing robust situation recognition models. This dataset enables researchers to improve the accuracy and context-awareness of image interpretation systems.
SWiG	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	The SWiG dataset is primarily used for training models in situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It is also utilized to define visual event schemas, which aids in developing more robust situation recognition models. These applications leverage the dataset's rich annotations to improve the accuracy and context-awareness of visual understanding systems.
FB400k	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.18653/v1/N18-1059 (2018)	The FB400k dataset is used for validating and testing SPARQL annotations and evaluating knowledge graph reasoning, particularly focusing on complex questions and their answers within the knowledge graph. It serves as a subset of Freebase, enabling researchers to assess the performance of reasoning systems on intricate web queries.
Freebase KG	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.1145/3397271.3401172 (2020)	The Freebase KG dataset is used to conduct multi-hop reasoning experiments and train knowledge graph embeddings, focusing on the scalability and efficiency of these processes in large-scale graphs with 86M nodes and 338M edges. This enables researchers to evaluate and improve embedding algorithms for handling extensive and complex knowledge graphs.
Microsoft COCO	https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The Microsoft COCO dataset is primarily used to evaluate model performance in identifying a diverse set of common objects within complex scenes. It serves as a benchmark for object detection and image recognition tasks, enabling researchers to assess the accuracy and robustness of their models in real-world scenarios. The dataset's rich annotations and varied imagery facilitate rigorous testing and comparison across different methodologies.
Visual7w	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The Visual7w dataset is primarily used for visual question answering, focusing on the model's ability to understand complex scenes. It is designed to test and improve systems' comprehension of visual content through questions that require reasoning about objects, actions, and relationships within images. This dataset enables researchers to evaluate and enhance the performance of visual question answering models in interpreting intricate visual scenes.
NUS-81	https://doi.org/10.1109/CVPR.2018.00170 (2017)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The NUS-81 dataset is primarily used for multi-label classification experiments, particularly in image tagging, where it evaluates performance across multiple time steps. It is also employed to compare performance with MS-COCO in multi-modal reasoning tasks involving images and text. This dataset facilitates research by providing a benchmark for these specific tasks, enabling the assessment of models' capabilities in handling complex, multi-modal data.
MS-Celebs	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-018-1116-0 (2016)	The MS-Celebs dataset is used to link visual data with structured knowledge, specifically by associating cropped celebrity faces with Freebase IDs to enhance multi-modal reasoning. It is also employed to merge entities from MS-Celebs and KVQA, focusing on entity recognition and integration, thereby facilitating the creation of comprehensive person lists. These applications leverage the dataset's rich visual and structured data to improve entity linking and multi-modal reasoning tasks.
Flick30k Entities	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Flick30k Entities dataset is used to train detectors with supervised data, focusing on image-text pairs annotated with entities. This supports multi-modal reasoning tasks by leveraging the aligned visual and textual information. The dataset's entity annotations enable researchers to develop models that can reason across both modalities, enhancing the understanding and integration of visual and textual data.
CT	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The CT dataset is used to integrate visual information with textual descriptions, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This approach leverages the dataset's ability to combine imaging data with clinical text, improving diagnostic outcomes.
X-rays	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The X-rays dataset is used to integrate visual information with textual descriptions, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This approach combines imaging data with clinical notes, improving diagnostic precision and reliability. The dataset's multi-modal nature is crucial for linking visual and textual data, enabling more robust and contextually informed diagnoses.
textual description	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The 'textual description' dataset is used to provide contextual information alongside imaging data, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This dataset integrates textual and imaging data, enabling more robust diagnostic models by leveraging the complementary information from both modalities.
ultrasound	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/j.inffus.2021.05.015 (2021)	The ultrasound dataset is used to integrate visual information with textual descriptions, specifically to enhance the accuracy of COVID-19 diagnosis through multi-modal knowledge graph reasoning. This approach combines imaging data with clinical notes, improving diagnostic precision and reliability.
unearthed oracle bones’ photos	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/J.COMPELECENG.2021.107173 (2021)	The 'unearthed oracle bones’ photos' dataset is used to construct a multi-modal knowledge graph for oracle bone recognition. This involves integrating visual and textual data to enhance information processing. The dataset enables researchers to develop and refine methods for recognizing and interpreting oracle bone inscriptions by leveraging both image and text data, thereby improving the accuracy and depth of historical and linguistic analysis.
Wikipedia articles and images	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TMM.2023.3301279 (2021)	The Wikipedia articles and images dataset is used to pre-train a cross-modal entity matching module, which aligns textual and visual scene graphs extracted from the input articles and images. This methodology supports research in cross-modal alignment, enhancing the integration of textual and visual information for more robust entity matching.
BookCorpus	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The BookCorpus dataset is used to provide textual data for event classification and relation extraction, which enhances the construction and enrichment of multi-modal event knowledge graphs. This dataset enables researchers to improve the accuracy and comprehensiveness of event-related information in knowledge graphs by leveraging its extensive textual content.
external knowledge base	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2019.00686 (2019)	The 'external knowledge base' dataset is used alongside the Visual Genome scene graph to generate question-answer pairs, enhancing the creation of unbiased Visual Question Answering (VQA) datasets. This methodology leverages the dataset to improve the diversity and fairness of VQA datasets, addressing research questions related to bias reduction in machine learning models.
Visual Genome scene graph	https://doi.org/10.1109/TNNLS.2020.3045034 (2020)	https://doi.org/10.1109/CVPR.2019.00686 (2019)	The Visual Genome scene graph dataset is primarily used to generate question-answer pairs for visual reasoning and compositional question answering. It focuses on creating unbiased Visual Question Answering (VQA) datasets, enhancing the diversity and complexity of questions to improve model performance and reduce biases. This dataset enables researchers to develop and evaluate models that can understand and reason about complex scenes in images.
CC12M	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The CC12M dataset is used to supply additional visual data for object recognition and event relation extraction, enhancing the construction and expansion of multi-modal event knowledge graphs. This dataset provides rich visual content that complements textual data, enabling more comprehensive and accurate event relation extraction and object recognition in research.
C4(news)	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The C4(news) dataset provides news-related textual data primarily used for event classification and relation extraction. It supports the construction and enhancement of multi-modal event knowledge graphs by offering rich, structured textual content. This dataset enables researchers to accurately classify events and extract relationships, contributing to more robust and detailed knowledge graphs.
Cyc	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The Cyc dataset is primarily listed as an example of a knowledge graph that contains common sense knowledge. It is referenced in research contexts but not utilized for specific empirical studies or methodologies. Its role is more illustrative, highlighting the availability of common sense knowledge in knowledge graphs, rather than being directly employed in multi-modal Knowledge Graph Reasoning or other specific research applications.
AWA2	https://doi.org/10.48550/arXiv.2207.01328 (2022)	https://www.semanticscholar.org/paper/ad7ddcc14984caae308c397f1a589aae75d4ab71 (2020)	The AWA2 dataset is primarily used to evaluate vision transformer encoders, particularly in zero-shot learning tasks for animal classification. It enables researchers to assess model performance in recognizing unseen animal categories, leveraging the dataset's rich visual and semantic attributes. This focus supports advancements in zero-shot learning methodologies within the domain of computer vision.
CN-DBpedia	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The CN-DBpedia dataset is primarily listed as an example of a knowledge graph containing encyclopedia knowledge. It is referenced in research contexts but not utilized for specific methodologies or research questions. Its role is more illustrative, highlighting the availability of structured encyclopedia data, rather than being directly employed in empirical studies or multi-modal Knowledge Graph Reasoning.
FB15-237	https://doi.org/10.1145/3308558.3313612 (2019)	https://doi.org/10.1609/aaai.v32i1.11573 (2017)	The FB15-237 dataset is used to train and evaluate multi-modal knowledge graph embedding models, specifically for relation prediction tasks. It employs a filtered set of relations to enhance model performance and accuracy. This dataset enables researchers to focus on improving the predictive capabilities of models in complex knowledge graphs.
NELL23k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.3115/v1/P15-1009 (2015)	The NELL23k dataset is used to evaluate knowledge graph embedding methods, specifically focusing on semantic smoothness and reasoning accuracy. It enables researchers to assess how well these methods perform across a large set of entities and relations, providing insights into the effectiveness of different embedding techniques in maintaining and utilizing semantic relationships within the graph.
NELL-995	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.3115/v1/P15-1009 (2015)	The NELL-995 dataset is utilized to evaluate the performance of knowledge graph reasoning models, focusing on their ability to handle smaller, more focused subsets of entities and relations. This dataset enables researchers to test and refine models specifically designed for these constrained environments, ensuring they can effectively reason and infer relationships within limited data contexts.
electronic medical database	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1038/s41598-017-05778-z (2017)	The electronic medical database is used to construct a health knowledge graph, enabling reasoning tasks by extracting meaningful information from patient records. This involves methodologies focused on data integration and semantic extraction to enhance understanding and utility of medical data in research settings.
Nation	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/1273496.1273551 (2007)	The 'Nation' dataset is used for representing and reasoning about relations among nations, specifically within the domain of multi-modal knowledge graph reasoning. It facilitates statistical predicate invention, enabling researchers to explore complex relationships and patterns between nations using a combination of textual and numerical data. This dataset supports the development and evaluation of models that can infer new predicates and relationships from existing data, enhancing the understanding of international dynamics.
NELL	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v24i1.7519 (2010)	The NELL dataset is used as a knowledge base for multi-modal reasoning, specifically integrating textual and structured data. This integration enhances understanding and inference capabilities, allowing researchers to explore complex relationships and improve reasoning systems. The dataset's rich combination of text and structured information supports advanced research in multi-modal knowledge graph reasoning.
Countries	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://www.semanticscholar.org/paper/03dfada96b88c741bb26bd4ce7b5ae4232157d37 (2015)	The 'Countries' dataset is used to represent and reason about geographical relations among countries, employing low-rank vector spaces for approximate reasoning. This approach facilitates the analysis of complex geographical relationships, enabling researchers to explore and model interactions between countries in a computationally efficient manner.
WD-singer	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1162/tacl_a_00360 (2019)	The WD-singer dataset is used to derive a subset of Wikidata, focusing on singer-related entities for entity linking and knowledge graph reasoning. It enables researchers to enhance the accuracy and depth of entity linking within the domain of singers, contributing to more robust knowledge graph reasoning systems.
wikidata5m	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1162/tacl_a_00360 (2019)	The wikidata5m dataset is used to evaluate multi-modal reasoning capabilities, specifically focusing on a subset of Wikidata that includes multimedia content. Researchers employ this dataset to assess how well models can integrate and reason across textual and visual data, enhancing the understanding of complex relationships within knowledge graphs. This evaluation helps in developing more robust multi-modal reasoning systems.
YOGA11k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA11k dataset is used to generate temporal knowledge graphs, focusing on different periods for temporal reasoning and embedding evaluation. It enables researchers to assess the effectiveness of temporal embeddings and reasoning methods over varying time spans, providing insights into how well these models capture and predict temporal relationships in data.
DBpedia15K	https://doi.org/10.1145/3545573 (2022)	https://doi.org/10.18653/v1/N19-1423 (2019)	The DBpedia15K dataset is used to collect textual information for entities, primarily focusing on relationship prediction in knowledge graphs. Researchers employ this dataset to enhance the accuracy of predicting relationships between entities by leveraging the rich textual data it provides. This approach supports the development of more robust and contextually informed knowledge graph models.
YOGA15k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA15k dataset is used to generate temporal knowledge graphs, enabling researchers to focus on different periods for temporal reasoning and embedding evaluation. This dataset facilitates the study of how entities and relationships evolve over time, providing a robust framework for evaluating temporal reasoning models.
YOGA-3SP	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA-3SP dataset is used to generate temporal knowledge graphs, enabling researchers to focus on different periods for temporal reasoning and embedding evaluation. This dataset facilitates the study of how entities and relationships evolve over time, providing a structured way to assess temporal dynamics in knowledge graphs.
MMKG-FB15k-IMG	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MMKG-FB15k-IMG dataset is used to integrate specific knowledge graphs with images, enhancing multi-modal reasoning capabilities in research. This integration supports the development of models that can effectively combine textual and visual information, improving the accuracy and robustness of multi-modal reasoning tasks. The dataset's unique combination of structured knowledge and image data enables researchers to explore and develop advanced multi-modal reasoning techniques.
YOGA1830	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.18653/v1/D18-1225 (2018)	The YOGA1830 dataset is used to generate temporal knowledge graphs, focusing on different periods for temporal reasoning and embedding evaluation. It enables researchers to assess the effectiveness of temporal embeddings and reasoning methods over varying time spans, providing insights into how well these models capture and predict temporal relationships in data.
Yago15k-IMG-TXT	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The Yago15k-IMG-TXT dataset is used to integrate specific knowledge graphs with images and text, facilitating comprehensive multi-modal reasoning. This integration supports research methodologies that require the synthesis of textual and visual data within a structured knowledge framework. The dataset enables researchers to explore and develop advanced multi-modal reasoning systems, enhancing the ability to process and understand complex, multi-faceted information.
MMKG-DB15k	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MMKG-DB15k dataset is used to integrate specific knowledge graphs with numeric literals, facilitating multi-modal reasoning in research. This integration supports the development and evaluation of methods that can handle both symbolic and numeric data, enhancing the capability to reason across diverse data types. The dataset's unique feature of incorporating numeric literals enables more robust and nuanced reasoning tasks in multi-modal knowledge graph research.
FB15K-237-IMG	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/3477495.3531992 (2022)	The FB15K-237-IMG dataset is used to extend the scope of triplets in multimodal knowledge graph completion by integrating image data with textual information. This integration enhances the representation and reasoning capabilities of knowledge graphs, enabling more comprehensive and contextually rich completions. The dataset's unique combination of images and textual data supports advanced multimodal reasoning tasks, facilitating research in areas such as enhanced entity linking and semantic understanding.
Global Database of Events, Language, and Tone	https://doi.org/10.48550/arXiv.2212.05767 (2022)	https://doi.org/10.1145/3511808.3557233 (2022)	The Global Database of Events, Language, and Tone is used to construct a dense knowledge graph for temporal knowledge graph completion. Researchers focus on analyzing events and tones over time, employing methodologies that enable the derivation of temporal relationships and patterns. This dataset facilitates research into how events and their associated tones evolve, providing insights into dynamic social and political phenomena.
ConceptNet KG	https://doi.org/10.48550/arXiv.2205.11501 (2022)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	The ConceptNet KG dataset is used to enhance multi-modal reasoning by retrieving concept-level knowledge from a general-domain knowledge graph. It integrates textual and visual information, enabling researchers to improve the understanding and processing of complex, multi-faceted data in various applications. This dataset facilitates the development of more robust and context-aware reasoning systems.
VG	https://doi.org/10.1007/978-3-030-58592-1_36 (2020)	https://doi.org/10.1109/CVPR.2017.330 (2017)	The VG dataset is used to evaluate the performance of methods on scene graph generation tasks, specifically focusing on mean and overall triplet recall. It compares results with and without graph constraints, enabling researchers to assess the effectiveness of their approaches in generating accurate and contextually relevant scene graphs.
multi-modal variants of the OpenEA benchmarks	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.14778/3407790.3407828 (2020)	The multi-modal variants of the OpenEA benchmarks are used to evaluate multi-modal entity alignment methods by incorporating entity images obtained via Google search. This dataset focuses on enhancing knowledge graph reasoning through the integration of visual data, enabling researchers to assess the effectiveness of these methods in aligning entities across different modalities.
CC3M&12M	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	https://doi.org/10.18653/v1/P18-1238 (2018)	The CC3M&12M dataset is used to train and evaluate multi-modal knowledge graph reasoning systems, specifically focusing on image-caption pairs. It enhances event extraction and representation by leveraging these pairs, enabling researchers to improve the accuracy and depth of event understanding in multi-modal contexts.
ICEWS-WIKI	https://doi.org/10.18653/v1/2024.findings-emnlp.148 (2024)	https://doi.org/10.48550/arXiv.2304.03468 (2023)	The ICEWS-WIKI dataset is extended to a multi-modal version to enhance multi-modal Knowledge Graph Reasoning. It integrates textual and visual information, enabling researchers to explore the integration of these modalities in knowledge graphs. This approach focuses on improving reasoning capabilities by leveraging both text and images, addressing research questions related to multi-modal data fusion and representation.
ICEWS-YAGO	https://doi.org/10.18653/v1/2024.findings-emnlp.148 (2024)	https://doi.org/10.48550/arXiv.2304.03468 (2023)	The ICEWS-YAGO dataset is extended to a multi-modal version to enhance multi-modal Knowledge Graph Reasoning, specifically by integrating textual and visual information. This extension supports research focused on improving the reasoning capabilities of knowledge graphs through the incorporation of diverse data types, enabling more comprehensive and contextually rich analyses.
WN-9	https://doi.org/10.1109/ICDE55515.2023.00015 (2022)	https://doi.org/10.18653/v1/D18-1359 (2018)	The WN-9 dataset is used to enhance multimodal reasoning capabilities by expanding knowledge graphs through the addition of images to entities. This approach integrates visual data with existing textual information, enabling more comprehensive and contextually rich representations. The dataset facilitates research focused on improving the integration and reasoning over multimodal data in knowledge graphs.
DB15K MMKGs	https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The DB15K MMKGs dataset is used alongside FB15K for entity alignment, specifically focusing on multi-modal aspects. It is split into training and testing sets to evaluate the performance of entity alignment methods. This dataset enables researchers to assess and improve the accuracy of aligning entities across different knowledge graphs by incorporating multi-modal data.
YAGO15K MMKGs	https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The YAGO15K MMKGs dataset is used alongside FB15K for entity alignment, focusing on multi-modal aspects. It is split into training and testing sets to evaluate the performance of entity alignment methods. This dataset enables researchers to assess and improve the accuracy of aligning entities across different knowledge graphs, leveraging its multi-modal features.
IMGpedia	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1007/978-3-319-68204-4_8 (2017)	The IMGpedia dataset is used to explore multi-modal relationships within a large knowledge graph, specifically focusing on image-to-image and image-to-text connections. This enhances the understanding of multimedia content by integrating visual and textual data, enabling researchers to analyze and reason about complex multi-modal interactions.
SIDER	https://doi.org/10.1109/ACCESS.2019.2933370 (2019)	https://doi.org/10.1093/nar/gkv1075 (2015)	The SIDER dataset is used as a domain-specific knowledge graph to link drugs to their side effects, enhancing medical reasoning. It is employed in research to improve understanding and prediction of drug side effects, leveraging its structured data on adverse reactions to support more informed medical decisions and drug safety assessments.
COCO Caption	https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The COCO Caption dataset is primarily used to assess image-text retrieval capabilities, focusing on caption generation and the alignment between images and textual descriptions. Researchers employ this dataset to evaluate models' ability to generate accurate captions and align them with corresponding images, enhancing the understanding of visual and textual data integration.
400 million image-text pairs	https://doi.org/10.48550/arXiv.2210.08901 (2022)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The 400 million image-text pairs dataset is primarily used to pre-train CLIP, a model designed to learn the correspondence between raw text and images. This enhances multi-modal representation learning, enabling the model to effectively match textual descriptions with visual content. The dataset's large scale and diverse content are crucial for training robust and versatile multi-modal models.
HacRED	https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126 (2023)	https://doi.org/10.1186/s12859-020-03889-5 (2020)	The HacRED dataset is used for empirical evaluation in relation extraction, specifically to assess model performance in extracting relations from text. It enables researchers to test and compare various relation extraction models, focusing on their accuracy and effectiveness in identifying relationships within textual data.
DocRED	https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126 (2023)	https://doi.org/10.1186/s12859-020-03889-5 (2020)	The DocRED dataset is primarily used for empirical evaluation in relation extraction, specifically to assess model performance in identifying and extracting relationships from textual data. It enables researchers to test and compare various relation extraction models, focusing on their accuracy and effectiveness in handling complex document-level relations.
YAGO2	https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019)	https://doi.org/10.1162/tacl_a_00220 (2013)	The YAGO2 dataset is used as a curated knowledge base to extract entity-relationship-entity triples, which supports the construction and reasoning of knowledge graphs. It facilitates multi-modal reasoning by providing a structured representation of entities and their relationships, enabling researchers to build and analyze complex knowledge graphs.
qaVG	https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47 (2019)	https://doi.org/10.18653/v1/N18-1040 (2017)	The qaVG dataset is used to train and evaluate visual question answering models. It augments triplets with auto-generated decoys and is split into training, validation, and testing sets. This methodology helps in assessing model performance in distinguishing correct answers from decoys, enhancing the robustness of visual reasoning tasks.
ATOMIC	https://doi.org/10.1109/TKDE.2024.3352100 (2023)	https://doi.org/10.1609/aaai.v35i7.16792 (2020)	The ATOMIC dataset is used to explore causal effects between events for commonsense reasoning, employing both symbolic and neural representations. It focuses on understanding how these representations can capture and reason about the relationships and effects between different events, enabling research into more nuanced and context-aware AI systems.
MKG-W	https://doi.org/10.1609/aaai.v39i12.33454 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MKG-W dataset is used to evaluate the effectiveness of models in multi-modal knowledge graph completion, focusing on the integration of textual and visual information. This dataset enables researchers to assess how well models can complete knowledge graphs by leveraging both text and images, addressing the specific challenge of combining these modalities in reasoning tasks.
MKG-Y	https://doi.org/10.1609/aaai.v39i12.33454 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MKG-Y dataset is used to evaluate the robustness of models in handling multi-modal data, focusing on scenarios with complex relationships and diverse modalities. It enables researchers to test how well models can integrate and reason across different types of data, ensuring they perform reliably in challenging, real-world conditions.
CWQ	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.18653/v1/N18-1059 (2018)	The CWQ dataset is used for evaluating complex query answering in knowledge graphs. It provides SPARQL annotations for validation and test questions, enabling researchers to assess the performance of query answering systems. This supports the development and refinement of methods for handling complex queries in knowledge graphs, focusing on accuracy and effectiveness in retrieving information.
MSVD	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1646396.1646452 (2009)	The MSVD dataset is primarily used for image captioning, enhancing the reasoning capabilities by integrating textual descriptions with visual content through graph-structured information. This approach helps in generating more accurate and contextually relevant captions, leveraging the dataset's multimodal nature to improve the alignment between images and their textual descriptions.
ComplexWebQuestion (CWQ)	https://doi.org/10.1145/3534678.3539405 (2021)	https://doi.org/10.18653/v1/N18-1059 (2018)	The ComplexWebQuestion (CWQ) dataset is used to derive FB400k, focusing on complex web questions and their answers. It enhances multi-modal knowledge graph reasoning by providing a rich set of complex queries and corresponding answers, which are utilized to improve the reasoning capabilities of knowledge graphs in handling intricate web-based questions.
GoodNews	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1646396.1646452 (2009)	The GoodNews dataset is used for enhancing image captioning by integrating textual descriptions with visual content through graph-structured information. This approach improves reasoning capabilities, allowing for more accurate and contextually rich captions. The dataset's multi-modal nature, combining text and images, is crucial for training models that can effectively relate and generate descriptions based on visual inputs.
MIRFlickr	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1460096.1460104 (2008)	The MIRFlickr dataset is used for multi-label image annotation and retrieval, focusing on disambiguating concepts and improving their relation to images. It enables researchers to enhance the accuracy and relevance of image annotations, which is crucial for effective image retrieval systems. The dataset's large collection of images and associated tags supports these research objectives.
Movie-QA	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1109/CVPR.2016.501 (2015)	The Movie-QA dataset is used to evaluate the ability to reason about visual and textual information in movie clips through question-answering tasks. It focuses on understanding stories in movies by integrating both visual and textual data, enabling researchers to assess systems' capabilities in comprehending complex narratives. This dataset supports research in natural language processing and computer vision, particularly in the context of multi-modal reasoning.
VQA v2	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-016-0966-6 (2015)	The VQA v2 dataset is used to enhance and assess visual question answering models, specifically addressing and mitigating biases present in the original VQA dataset. It balances question types and answers, enabling researchers to develop more robust and fair models. This dataset facilitates the evaluation of model performance across diverse and balanced visual and textual inputs, ensuring that advancements in visual question answering are more reliable and generalizable.
VQA-abstract	https://doi.org/10.1609/aaai.v33i01.33018876 (2019)	https://doi.org/10.1007/s11263-016-0966-6 (2015)	The VQA-abstract dataset is used to train and evaluate visual question answering models, particularly focusing on abstract scenes and clipart images. This dataset tests the reasoning capabilities of these models by presenting them with complex visual and textual queries. It enables researchers to assess how well models can understand and reason about abstract visual content, enhancing the development of more sophisticated visual reasoning systems.
VTKB	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TMM.2019.2937181 (2020)	The VTKB dataset is used to construct a visio-textual knowledge base that links hierarchical concepts to images through embedding similarities. This enhances image tagging quality by leveraging both visual and textual data, improving the accuracy and relevance of tags assigned to images.
VQA 2.0	https://doi.org/10.18653/v1/2020.findings-emnlp.44 (2020)	https://doi.org/10.1109/CVPR.2019.00331 (2019)	The VQA 2.0 dataset is used to evaluate visual question answering models, specifically focusing on their ability to reason about images and questions by integrating external knowledge. This dataset enables researchers to assess model performance in understanding complex visual scenes and answering questions that require reasoning beyond simple pattern recognition.
Chunyu	https://doi.org/10.1145/3343031.3351033 (2019)	https://doi.org/10.1145/3209978.3210081 (2018)	The Chunyu dataset is used to evaluate the performance of KABLSTM in medical question answering by comparing it with SMatrix and K-NRM. It demonstrates the utility of incorporating knowledge graphs in this context, focusing on enhancing the accuracy and relevance of medical answers. The dataset's multi-modal nature supports the integration of textual and structured data, enabling more effective reasoning and response generation in medical inquiries.
Dingxiang	https://doi.org/10.1145/3343031.3351033 (2019)	https://doi.org/10.1145/3209978.3210081 (2018)	The Dingxiang dataset is used to evaluate the precision of KABLSTM, a model that integrates knowledge graphs in medical domain applications. It specifically verifies the effectiveness of knowledge graph integration, focusing on high-precision outcomes in medical reasoning tasks. This dataset enables researchers to test and validate models designed for medical knowledge representation and inference.
