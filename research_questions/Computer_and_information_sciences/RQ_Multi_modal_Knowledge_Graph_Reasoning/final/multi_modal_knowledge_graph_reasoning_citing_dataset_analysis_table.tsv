Name (extracted)	Citing Article	Citied Article	Features
ConceptNet	https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1145/3573201 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022) (+5)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	ConceptNet is used to enhance knowledge graphs and multi-modal reasoning by representing commonsense knowledge in a graph format, where nodes are concepts and edges are relations. It is leveraged as an external knowledge corpus to enrich relation learning, object graphs, and scene graphs, particularly in visual question answering and event information granularity. Its multilingual support and general knowledge base enable researchers to integrate external knowledge, improving multi-modal reasoning capabilities and system understanding of everyday concepts and relationships.
DBpedia	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1145/3474085.3475470 (2021) (+8)	https://doi.org/10.3233/SW-140134 (2015)	DBpedia is used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing structured information for constructing and enhancing knowledge graphs. It is utilized to extract raw triplet facts, including hasPart/isA relationships, and to offer categorical knowledge for Visual Question Answering. The dataset highlights the incompleteness of entity-image associations, focusing on the integration of structured information and the lack of visual data for many entities.
DB15K	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://doi.org/10.1609/aaai.v39i12.33454 (2024), https://doi.org/10.1145/3545573 (2022) (+7)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The DB15K dataset is primarily used for evaluating and extending multi-modal knowledge graph reasoning, particularly in cross-lingual and cross-modal entity and relation prediction tasks. It enhances the original DBpedia dataset by incorporating images, enabling researchers to study diverse entity categories and improve entity representation and reasoning. The dataset is utilized to train and evaluate models, focusing on metrics like MRR, Hit@1, Hit@3, and Hit@10, and to assess computational efficiency and performance improvements in link prediction and entity alignment.
Freebase	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1145/3474085.3475470 (2021), https://doi.org/10.1109/ICDE60146.2024.00061 (2024) (+7)	https://doi.org/10.1145/1376616.1376746 (2008)	Freebase is used as a collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia. It supports various research tasks, including entity matching, recommendation systems, and multimedia reasoning. Researchers use it to evaluate entity matching approaches, provide pre-trained entity embeddings for multi-relational data modeling, and represent entities and relations in a graph structure. Its large, structured format enables the evaluation of multi-modal reasoning and the integration of diverse information sources.
Visual Genome	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1145/3579051.3579073 (2022), https://doi.org/10.3233/sw-233510 (2023) (+6)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The Visual Genome dataset is primarily used for training models in visual relationship detection and scene graph generation, enhancing multi-modal reasoning by connecting language and vision through dense image annotations. It supports tasks such as zero-shot phrase grounding, object detection, and relationship prediction, leveraging detailed object and attribute annotations to improve model performance in understanding complex visual scenes.
YAGO	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.1109/ICDE60146.2024.00061 (2024), https://doi.org/10.1145/3581783.3612151 (2023) (+6)	https://doi.org/10.1145/1242572.1242667 (2007)	YAGO is used as a core semantic knowledge base that integrates WordNet and Wikipedia, providing structured data about entities and their relationships. It enhances the representation of entities and supports various applications such as entity matching, image tagging, social event analysis, video recognition, recommendation systems, information retrieval, and machine learning. YAGO's multilingual and structured nature makes it valuable for enriching semantic web applications and multi-modal reasoning tasks.
MSCOCO	https://doi.org/10.1609/aaai.v38i12.29280 (2024), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1609/aaai.v36i2.20123 (2022) (+1)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The MSCOCO dataset is primarily used for training and evaluating models in multi-modal tasks, including object recognition, cross-modal hashing, and image-text matching. It provides bounding boxes and categories for object detection, enabling the construction of multi-modal knowledge graphs and vision-based knowledge graphs. The dataset supports the evaluation of various algorithms and methods, such as cross-modal hashing, image-to-text and text-to-image retrieval, and multi-modal reasoning tasks, often using metrics like Mean Average Precision (MAP). It also facilitates the investigation of learning components and data augmentation techniques in intra-modal contrastive learning.
DBP15K	https://doi.org/10.48550/arXiv.2307.16210 (2023), https://doi.org/10.3390/app13116747 (2023), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024) (+3)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The DBP15K dataset is primarily used for evaluating cross-lingual entity alignment methods, focusing on attribute-preserving embeddings and the integration of visual features. It is employed in experiments using pre-trained ResNet-152 and VGG vision encoders, assessing performance metrics like Hits@1, Hits@10, and MRR. The dataset supports research on model stability, robustness, and efficiency in multi-modal knowledge graph alignment across various language pairs, including Chinese-English, Japanese-English, and French-English.
YAGO15K	https://doi.org/10.3390/app13116747 (2023), https://doi.org/10.1145/3545573 (2022), https://doi.org/10.1145/3637528.3671769 (2024) (+2)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The YAGO15K dataset is primarily used for evaluating multi-modal knowledge graph reasoning, with a focus on entity and relation prediction, particularly incorporating temporal and geographical information. It is also utilized for cross-KG reasoning and entity alignment, including cross-lingual and cross-KG tasks such as linking entities between YAGO and DBpedia. The dataset supports the evaluation of various models, such as MEAFE, CMGNN, and HRGAT, by providing a rich, multi-modal context that enhances the assessment of their performance in reasoning and alignment tasks.
Wikidata	https://doi.org/10.1145/3626246.3654757 (2024), https://doi.org/10.1109/ICDE60146.2024.00061 (2024), https://doi.org/10.24963/ijcai.2024/281 (2024) (+5)	https://doi.org/10.1145/1376616.1376746 (2008)	Wikidata is used as a structured and linked open data resource, providing a comprehensive snapshot of diverse data types including images, text, tables, and audio files. It enhances multi-modal knowledge graph reasoning by linking news entities and obtaining contextual descriptions, thereby constructing rich news background knowledge. The dataset's rich semantic information and temporal capabilities enable enhanced reasoning and integration of structured information.
FB15K-DB15K	https://doi.org/10.3390/app13116747 (2023), https://doi.org/10.48550/arXiv.2407.19625 (2024), https://doi.org/10.1145/3637528.3671769 (2024) (+3)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The FB15K-DB15K dataset is primarily used for cross-KG entity alignment and reasoning, focusing on the alignment between Freebase and DBpedia subsets. It is employed in evaluating the performance of pre-trained models like VGG-16 for visual pivoting and assessing multi-modal reasoning in cross-KG settings. The dataset supports ablation experiments to verify correction schemes and cross-lingual alignment, and it is split into training and testing sets in various proportions (2:8, 5:5, 8:2) to evaluate model performance and computational challenges.
PHEME	https://doi.org/10.48550/arXiv.2306.15946 (2023), https://doi.org/10.1145/3451215 (2021)	https://doi.org/10.1007/978-3-319-67217-5_8 (2017)	The PHEME dataset is primarily used for rumor detection in social media, particularly focusing on microblogs and multimedia posts from platforms like Twitter. It is employed to evaluate and compare the performance of various models, including multi-modal approaches, in detecting rumors during breaking news events. The dataset facilitates research by providing a benchmark for testing the robustness and effectiveness of different components, such as knowledge concept integration, entity consistency, and contextual information, in enhancing rumor detection accuracy.
Flickr30k	https://doi.org/10.1609/aaai.v38i12.29280 (2024), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2501.04173 (2025) (+2)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The Flickr30k dataset is primarily used for evaluating and training models in multi-modal tasks, such as image captioning, visual grounding, and image-text matching. It provides a diverse set of images with multiple captions, enabling researchers to fine-tune and evaluate models on tasks like generating textual descriptions from images, aligning visual and textual modalities, and cross-modal hashing retrieval. The dataset supports methodologies including contrastive knowledge distillation, weakly supervised learning, and feature extraction with mechanisms like Faster RCNN and BUTD. It is also used to set hyperparameters for momentum encoders and to improve detectors on image-caption pairs, enhancing the alignment and reasoning capabilities between visual and textual data.
FB15K	https://doi.org/10.48550/arXiv.2408.11526 (2024), https://doi.org/10.1145/3637528.3671769 (2024), https://doi.org/10.1145/3656579 (2024) (+7)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	FB15K is primarily used for evaluating and benchmarking knowledge graph embedding models, focusing on link prediction and relational reasoning tasks. It is often extended to include images and additional relation types, enhancing its utility for multi-modal knowledge graph reasoning. The dataset supports entity and relation prediction, entity alignment, and cross-KG reasoning experiments, typically split into training and testing sets to assess model performance.
CSKG	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-030-77385-4_41 (2020)	The CSKG dataset is used to consolidate and integrate heterogeneous common sense knowledge from multiple sources, enhancing multi-modal reasoning capabilities. It is employed to enrich language models and scene graphs with relevant common sense information, improving tasks such as common sense question answering and visual understanding. The dataset's comprehensive and systematically consolidated nature allows researchers to compute node similarity, extract relevant triplets, and refine graph structures, thereby enhancing the reasoning capabilities of multi-modal systems.
WordNet	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/219717.219748 (1995)	WordNet is used as a lexical database for English, providing structured representations of words and their meanings. It enhances scene graphs generated by models like Faster R-CNN, enriching lexical and semantic relationships in multi-modal reasoning tasks. Specifically, it improves the semantic representation of events and their relations in knowledge graphs, aiding in scene graph generation and common sense graph enhancement.
WEIBO	https://doi.org/10.48550/arXiv.2306.15946 (2023), https://doi.org/10.1145/3451215 (2021)	https://doi.org/10.1145/3123266.3123454 (2017)	The WEIBO dataset is primarily used for rumor detection in Chinese microblogging platforms, focusing on multimodal data integration and analysis. It is employed to train and evaluate models that fuse textual and visual information, comparing the performance of different methods, including KMAGCN and KhiCL, to enhance the accuracy of rumor detection in social media contexts.
MKG-W	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024), https://doi.org/10.1109/KSE63888.2024.11063656 (2024) (+1)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MKG-W dataset is primarily used to evaluate and enhance multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information. It is employed in assessing model performance in tasks such as link prediction, using metrics like MRR and Hit@1. The dataset supports ablation studies to analyze the impact of different components and is used to configure vision encoders like BEiT. It facilitates the enhancement of representation learning with multimodal data, specifically for reasoning tasks.
FB15k-237	https://doi.org/10.1145/3545573 (2022), https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.1145/3394171.3413736 (2020) (+3)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	FB15k-237 is primarily used to evaluate and enhance knowledge graph completion models, focusing on link prediction and reducing redundancy. It supports multi-modal knowledge graph reasoning by integrating visual and textual data with relational triples. The dataset is utilized to train and evaluate models like Query2GMM, CMGNN, and HRGAT, addressing complex relations, entity linking, and bias reduction. It also facilitates comparisons of convergence speeds and performance improvements in knowledge base completion tasks using methods such as relational graph convolutional networks.
Multi-OpenEA	https://doi.org/10.48550/arXiv.2307.16210 (2023), https://doi.org/10.1145/3627673.3679126 (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024) (+1)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The Multi-OpenEA dataset is used to evaluate and benchmark models in multi-modal knowledge graph reasoning, particularly focusing on vision encoders like CLIP. It assesses model stability and performance in cross-lingual and cross-modal entity alignment, with a vision feature dimension of 512. The dataset's large scale and high ratio of image-equipped entities make it suitable for these tasks, providing a robust benchmark for multi-modal entity alignment.
MathVista	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The MathVista dataset is used to test mathematical problem-solving skills, particularly in multimodal settings. It integrates visual and textual data to comprehensively evaluate and assess the capabilities of models in solving complex mathematical problems. The dataset emphasizes the combination of text and images, enabling researchers to focus on multi-modal reasoning and the effective integration of diverse information types.
VCR	https://doi.org/10.48550/arXiv.2405.16473 (2024), https://doi.org/10.48550/arXiv.2305.04530 (2023), https://doi.org/10.1109/TMM.2023.3279691 (2024)	https://doi.org/10.18653/v1/P19-1472 (2019)	The VCR dataset is primarily used to assess and evaluate visual commonsense reasoning, focusing on the ability to understand and explain visual scenes through complex interactions between images and text. It is employed to fine-tune models like BERT for cross-modal alignment, enhancing their capability to integrate visual-linguistic data and reason about relationships within images. This dataset enables researchers to test and improve models' performance in understanding and explaining complex visual scenes.
Wikipedia	https://doi.org/10.1145/3474085.3475567 (2021), https://doi.org/10.1145/3583780.3614782 (2023), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The Wikipedia dataset is primarily used for cross-modal multimedia retrieval experiments, generating image-text pairs from featured articles to explore correlation and abstraction in multimodal data. It is also utilized for entity aspect linking by treating section names as aspect labels and for event classification and relation extraction, contributing to the construction of a multi-modal event knowledge graph. These applications focus on integrating textual and visual information to enhance retrieval performance and knowledge graph construction.
ImageNet	https://doi.org/10.1609/aaai.v36i2.20123 (2022), https://doi.org/10.1145/3637528.3671769 (2024), https://doi.org/10.1145/3394171.3413711 (2020) (+2)	https://doi.org/10.1109/cvpr.2016.90 (2015)	ImageNet is primarily used to pre-train deep learning models such as ResNet-50, ResNet-152, and VGG-16, which extract high-dimensional feature vectors from images. These pre-trained features are then utilized to initialize the backbone image encoders in models like SSD and RetinaNet, enhancing their performance in multi-modal reasoning tasks. The dataset's large-scale hierarchical structure and rich semantic information enable effective transfer learning, improving the models' ability to learn and generalize from visual data.
GQA	https://doi.org/10.1145/3579051.3579073 (2022), https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The GQA dataset is utilized for grounded question answering and visual reasoning, providing rich annotations and scene graphs that integrate visual and textual information. It supports the evaluation of various visual question answering methods, focusing on relational reasoning and complex question answering tasks that require multi-step reasoning over images and knowledge graphs. The dataset captures relationships among entities, enabling research that combines external knowledge with visual data.
TQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The TQA dataset is used to evaluate multimodal question answering systems, particularly in educational contexts. It assesses models' abilities to reason over both visual and textual information, such as diagrams and images, and to integrate these modalities for accurate answers. The dataset emphasizes multimodal machine comprehension and problem-solving, enabling researchers to test and improve the performance of models in complex, real-world scenarios.
MME	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The MME dataset is utilized to benchmark and evaluate multimodal large language models across various reasoning tasks, including image-text alignment and reasoning. It provides a comprehensive evaluation framework, enabling researchers to assess model performance in educational settings, particularly in integrating visual and textual data for problem-solving.
NELL	https://doi.org/10.48550/arXiv.2506.11012 (2025), https://doi.org/10.24963/ijcai.2024/236 (2024), https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v24i1.7519 (2010)	The NELL dataset is primarily used for never-ending language learning, where it continuously extracts and represents knowledge in a graph structure. It serves as a benchmark for First-Order Knowledge Graph Completion, evaluating model performance in completing knowledge graphs. Additionally, NELL is utilized as a knowledge base for multi-modal reasoning, integrating textual and structured data to enhance the depth and breadth of the knowledge graph through continuous web learning.
VQA	https://doi.org/10.48550/arXiv.2501.04173 (2025), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2504.10074 (2025)	https://doi.org/10.1007/s11263-016-0966-6 (2015)	The VQA dataset is primarily used for developing and evaluating visual question answering models, which integrate image and text modalities to answer questions about images. It includes 5,000 validation samples and employs the VQA score metric to assess performance. The dataset enhances diversity and complexity by balancing image-question pairs, and it captures relationships among mentions and entities, providing knowledge about named entities and their relations in images.
MKG-Y	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024), https://doi.org/10.1109/KSE63888.2024.11063656 (2024) (+1)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MKG-Y dataset is used in multi-modal knowledge graph reasoning research, specifically to evaluate and enhance models' capabilities in handling diverse modalities such as textual and visual information. It is employed to test robustness in entity linking, relation extraction, and cross-modal entity alignment. The dataset facilitates the integration of structured and unstructured data, often using vision encoders like BEiT, to improve multimodal representation learning and reasoning.
FB15K-YAGO15K	https://doi.org/10.1145/3637528.3671769 (2024), https://doi.org/10.48550/arXiv.2407.19625 (2024), https://doi.org/10.1609/aaai.v38i8.28762 (2024)	https://doi.org/10.1609/aaai.v35i5.16550 (2020)	The FB15K-YAGO15K dataset is primarily used for evaluating entity alignment in multi-modal knowledge graphs, particularly focusing on cross-KG entity alignment tasks. Researchers use it to assess the performance of methods like pre-trained VGG-16 for visual pivoting and the impact of seed entities or pre-aligned entities on alignment accuracy. This dataset facilitates the evaluation of cross-KG reasoning and alignment between subsets of Freebase and YAGO.
MM-Vet	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The MM-Vet dataset is used for verifying the correctness and consistency of multimodal reasoning outputs in veterinary question answering. It tests models' ability to accurately interpret and respond to complex inputs combining visual and textual information, ensuring that predictions are reliable and consistent across modalities. This dataset enables researchers to evaluate and improve the performance of multimodal reasoning systems in a specialized domain.
WN9-IMG-TXT	https://doi.org/10.1109/ICDE55515.2023.00015 (2022), https://doi.org/10.1109/TKDE.2025.3546686 (2023)	https://doi.org/10.18653/v1/S18-2027 (2018)	The WN9-IMG-TXT dataset is used to enhance the data diversity of multi-modal knowledge graphs by integrating textual descriptions and images into entities. It supports research in multi-modal knowledge graph reasoning, particularly in improving the richness of auxiliary data and verifying reasoning performance in a transductive setting. The dataset is also utilized to set embedding dimensions for image features and extract image features for entities, typically using 10 images per entity. It serves as a benchmark to compare with other datasets like FB-IMG-TXT, highlighting differences in sparsity and complexity in multi-modal knowledge graph representation learning.
FB-IMG-TXT	https://doi.org/10.1109/ICDE55515.2023.00015 (2022), https://doi.org/10.1109/TKDE.2025.3546686 (2023)	https://doi.org/10.18653/v1/S18-2027 (2018)	The FB-IMG-TXT dataset is used to enhance the data diversity of multi-modal knowledge graphs by integrating textual descriptions and images into entities, improving the richness of auxiliary data. It supports multi-modal knowledge graph reasoning, particularly in a transductive setting, by verifying reasoning performance and setting embedding dimensions for image features. The dataset includes 100 images per entity, aiding in representation learning and addressing challenges related to sparsity and complexity in multi-modal data.
PMR	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/2022.acl-long.66 (2021)	The PMR dataset is used to evaluate premise-based multimodal reasoning, focusing on conditional inference from joint textual and visual clues. It is employed to assess models like ModCR and to conduct ablation studies, verifying the effectiveness of high-quality, human-constructed samples and the impact of manual annotation on reasoning performance.
OK-VQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.3390/electronics12061390 (2023)	https://doi.org/10.1109/CVPR.2019.00331 (2019)	The OK-VQA dataset is used for evaluating visual-retriever-reader models in knowledge-based question answering, focusing on open-ended questions that require external textual knowledge beyond the image content. It tests the reasoning capabilities of VQA models by integrating visual and textual information, assessing the need for external knowledge to answer questions and capturing relationships among mentions and entities.
TWITTER	https://doi.org/10.1145/3451215 (2021), https://doi.org/10.48550/arXiv.2304.11116 (2023)	https://doi.org/10.1145/3123266.3123454 (2017)	The TWITTER dataset is primarily used for rumor detection on microblogs, integrating textual, visual, and social context information through multimodal fusion techniques, such as recurrent neural networks. It is employed to compare the performance of different models, particularly focusing on the effectiveness of multimodal information and knowledge concept components in enhancing accuracy. The dataset facilitates research by providing a rich, multi-faceted source of data for evaluating and refining rumor detection algorithms.
MMKG	https://doi.org/10.1145/3627673.3679126 (2024), https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The MMKG dataset is used to evaluate and enhance multi-modal knowledge graph reasoning, integrating visual, textual, and numerical information. It facilitates the incorporation of visual data into entities, though with low image coverage, which affects comprehensive entity representation. The dataset supports the integration of symbolic knowledge with pre-trained visual-language models, improving performance in open-domain VQA tasks. Additionally, it aids in retrieving deep learning paper implementations, enhancing access to associated code in academic research.
WN18RR	https://doi.org/10.1145/3545573 (2022), https://doi.org/10.1109/TKDE.2022.3220625 (2023)	https://doi.org/10.18653/v1/W15-4007 (2015)	The WN18RR dataset is primarily used to evaluate and validate models for multi-modal knowledge graph reasoning, particularly focusing on wordnet relations and link prediction tasks. It is employed to assess the performance of various neural network architectures, such as CMGNN and HRGAT, in handling hierarchical and semantic relationships, entity ranking, and graph construction. The dataset also supports the integration of textual information to enhance knowledge graph reasoning, demonstrating improvements in convergence speed and state-of-the-art results.
ScienceQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The ScienceQA dataset is used to evaluate scientific question answering systems, particularly focusing on the integration of visual and textual information to solve complex problems. It sources data for assessing multi-modal reasoning in scientific contexts, enabling researchers to test how effectively models can combine different types of information to provide accurate answers.
Geometry3K	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.18653/v1/2021.acl-long.528 (2021)	The Geometry3K dataset is used to evaluate geometry problem-solving capabilities, particularly focusing on formal language and symbolic reasoning in multimodal contexts. It enables researchers to assess and improve multimodal reasoning methods, specifically in handling geometry problems that require both visual and textual understanding.
Karpathy	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://doi.org/10.1109/CVPR.2015.7298932 (2014)	The Karpathy dataset is primarily used for training and evaluating image captioning and vision-language models, focusing on the alignment of visual and semantic information. It supports research in generating accurate image descriptions and aligning visual and textual data, enabling the development and assessment of deep visual-semantic alignment models.
KVC16K	https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a (2024), https://doi.org/10.1145/3626772.3657800 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The KVC16K dataset is used in multi-modal Knowledge Graph Reasoning research to evaluate and validate trends, particularly in knowledge graph completion tasks. It focuses on predicting missing links using both textual and visual data, assessing models like M O M O K through performance metrics such as MRR and Hit@1. This dataset enables researchers to test the consistency and effectiveness of multi-modal reasoning approaches across different datasets.
Recipe1M+	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.1109/TPAMI.2019.2927476 (2021)	The Recipe1M+ dataset is used to learn cross-modal embeddings for cooking recipes and food images, integrating textual and visual information. Research focuses on the relationship between textual recipe instructions and visual food representations, enabling the development of models that can understand and generate content across these modalities.
WN18	https://doi.org/10.48550/arXiv.2307.03591 (2023), https://doi.org/10.1145/3474085.3475470 (2021)	https://doi.org/10.1145/1376616.1376746 (2008)	The WN18 dataset is primarily used to integrate visual information into lexical relationships by extending entities with images, enhancing multi-modal reasoning in knowledge graphs. It is also utilized to evaluate the performance of translational embeddings in knowledge graph completion tasks, focusing on multi-relational data modeling. This integration of visual and textual data supports advanced representation learning and reasoning capabilities.
FB15k-237-IMG	https://doi.org/10.48550/arXiv.2307.03591 (2023), https://doi.org/10.3390/s24237605 (2024)	https://doi.org/10.1145/1376616.1376746 (2008)	The FB15k-237-IMG dataset is used to enhance multi-modal knowledge graph representation learning by extending entities in FB15k-237 with images. This extension supports visual context integration and entity linking, enabling researchers to evaluate models' performance on multi-modal knowledge graph reasoning tasks. The dataset's inclusion of images for each entity improves the visual aspect of entity relationships, facilitating more comprehensive and contextually rich reasoning.
IMGpedia	https://doi.org/10.1109/TKDE.2025.3546686 (2023), https://doi.org/10.1609/aaai.v38i17.29828 (2024), https://doi.org/10.1145/3627673.3679175 (2024)	https://doi.org/10.1007/978-3-319-68204-4_8 (2017)	The IMGpedia dataset is used to enhance multi-modal knowledge graph reasoning by providing content-based analysis of Wikimedia images. It links image metadata and annotations to structured data in DBpedia, grounding visual content and enriching linked data. This enables researchers to explore and reason about the relationships between visual and textual information in a more integrated manner.
FVQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The FVQA dataset is used for fact-based visual question answering, integrating factual knowledge with visual information. It captures relationships among mentions and entities, providing knowledge about named entities and their relations in images. This integration allows researchers to enhance the accuracy and depth of answers by leveraging supporting fact subgraphs, making it suitable for tasks that require both visual and textual understanding.
MMEA datasets	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The MMEA datasets are used to evaluate multi-modal entity alignment methods, addressing the challenges of aligning entities across different modalities in knowledge graphs. Research focuses on assessing model performance across bilingual, monolingual, and high-degree categories using pre-trained visual encoders like ResNet-152 and CLIP, under both standard and iterative settings. This enables researchers to compare and refine alignment techniques effectively.
hasPart KB	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The 'hasPart KB' dataset is used to provide part-whole relationships, specifically through has-Part triples, which contribute to the construction of knowledge graphs. It aids in the comprehension of complex objects and their components, particularly in the context of Visual Question Answering. This dataset enables researchers to enhance the reasoning capabilities of systems by integrating detailed part-whole relationships into knowledge graphs.
WebChild	https://doi.org/10.3233/sw-233510 (2023), https://doi.org/10.1145/3626772.3657790 (2024)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	The WebChild dataset is used to construct a knowledge graph with types of relations, distinct entities, and knowledge triples, which contributes to multi-modal reasoning tasks. It is also utilized as a subset for visual reasoning in Visual Question Answering (VQA), providing child-friendly knowledge for enhancing visual concept understanding. This dataset enables researchers to integrate textual and visual data, improving the performance of reasoning systems in both structured and unstructured environments.
KVQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2019.00331 (2019)	The KVQA dataset is used to test and evaluate models' ability to reason about visual content using external knowledge, incorporating knowledge graphs to answer questions that require structured data reasoning. It captures relationships among mentions and entities, providing knowledge about named entities and their relations in images, specifically designed to challenge models with complex reasoning tasks.
VG	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1109/CVPR.2017.330 (2017)	The VG dataset is primarily used for training and evaluating models in scene graph generation, focusing on extracting visual entities and relationships. It supports the development of models like Faster RCNN and SGG pipelines, enhancing their ability to understand and generate scene graphs by connecting visual data with common sense knowledge. The dataset includes 50 predicate classes and 150 object classes, enabling researchers to benchmark their methods against state-of-the-art techniques using standard evaluation metrics and splits.
kgbench	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1007/978-3-030-77385-4_37 (2021)	The kgbench dataset is used to evaluate relational and multimodal machine learning models by providing a collection of knowledge graph datasets for benchmarking. It enables researchers to assess model performance across various knowledge graphs, focusing on tasks such as entity prediction and relation classification. The dataset's diverse and multimodal nature supports the development and comparison of advanced machine learning algorithms in knowledge graph reasoning.
wikidata5m	https://doi.org/10.1109/TPAMI.2024.3417451 (2022), https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1162/tacl_a_00360 (2019)	The wikidata5m dataset is used for knowledge graph reasoning, particularly in inductive settings. It contains 5 million entities and their relationships from Wikidata, enabling research on embedding and pre-trained language representations. This dataset facilitates the development and evaluation of models for inductive knowledge graph reasoning, enhancing the ability to infer relationships in unseen data.
pFoodREQ	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.1109/TPAMI.2019.2927476 (2021)	The pFoodREQ dataset is utilized for personalized food recommendation by employing constrained question answering over a large-scale food knowledge graph. This approach enables researchers to address specific queries related to dietary preferences and restrictions, enhancing the accuracy and relevance of food recommendations. The dataset's extensive coverage of food items and their attributes supports sophisticated reasoning tasks, facilitating more tailored and context-aware recommendations.
A-OKVQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.18653/v1/2021.acl-long.528 (2021)	The A-OKVQA dataset is primarily used to evaluate open-ended visual question answering systems, focusing on generating accurate and coherent responses. It is employed to assess models' ability to understand complex visual scenes and provide contextually appropriate answers. This dataset emphasizes the importance of coherence and accuracy in responses, making it valuable for improving and benchmarking visual question answering methodologies.
KI-VQA	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.18653/v1/2021.acl-long.528 (2021)	The KI-VQA dataset is used to test knowledge-intensive visual question answering, specifically focusing on integrating external knowledge with visual data. This dataset enables researchers to evaluate models that combine textual and visual information, addressing research questions related to the effective fusion of these modalities for more accurate and contextually rich answers.
PropBank	https://doi.org/10.1145/3573201 (2022)	https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e (2004)	PropBank is used to enhance event knowledge graphs by providing an annotated corpus of semantic roles, which significantly increases the scale and diversity of event-related information. This dataset contributes 112,917 events, enriching the representation of verb-based events and expanding the overall size of the knowledge graph.
NomBank	https://doi.org/10.1145/3573201 (2022)	https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e (2004)	NomBank is used to enhance event knowledge graphs by providing an annotated corpus of nominal predicate argument structures, specifically adding 114,576 noun-based events. This dataset extends the scale and diversity of event-related information, improving the representation of noun-based events in knowledge graphs. It is not used for multi-modal Knowledge Graph Reasoning but rather for enriching the structural and semantic content of event data.
Rich-pedia	https://doi.org/10.1109/TKDE.2025.3546686 (2023), https://doi.org/10.1145/3581783.3612151 (2023)	https://doi.org/10.1016/j.bdr.2020.100159 (2020)	The Rich-pedia dataset is used to construct a large-scale, comprehensive multi-modal knowledge graph by integrating various types of data, including images, to enhance reasoning capabilities. It is employed to improve KG-based applications through the incorporation of visual modalities, demonstrating significant potential in multi-modal knowledge graph reasoning.
UAV-Human	https://doi.org/10.1109/ICME52920.2022.9859787 (2022)	https://doi.org/10.1109/CVPR46437.2021.01600 (2021)	The UAV-Human dataset is used for human behavior understanding with unmanned aerial vehicles, focusing on multi-modal video sequences captured from three sensors. It is employed for data pre-processing, multi-modal data integration, annotation, and action recognition. This dataset enables researchers to analyze and evaluate complex human behaviors in various contexts, enhancing the accuracy and robustness of behavior understanding systems.
Flickr30K Entities	https://doi.org/10.1609/aaai.v36i2.20123 (2022), https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The Flickr30K Entities dataset is used for pre-training models like the Oscar-based chunk-aware semantic interactor for phrase-level semantic alignment between text and images, particularly in multi-modal knowledge graph reasoning. It is also utilized for zero-shot phrase grounding experiments, providing region-to-phrase correspondences to enhance image-to-sentence models. Researchers often use sub-sampled splits to focus on specific research needs, enriching the models with detailed region-phrase mappings.
InfoSeek	https://doi.org/10.48550/arXiv.2504.10074 (2025)	https://doi.org/10.1109/ICCV51070.2023.00289 (2023)	The InfoSeek dataset is used to evaluate multi-modal knowledge retrieval systems, integrating Wikipedia-scale knowledge with 1.3 million image-question pairs linked to 11,000 Wikipedia pages. It assesses models' abilities to connect visual and textual information, emphasizing comprehensive entity understanding and fine-grained details. Research focuses on validating these capabilities using 73,000 validation samples, highlighting the dataset's role in advancing multi-modal reasoning.
Kinetics-TPS	https://doi.org/10.1145/3503161.3548257 (2022)	https://doi.org/10.1609/AAAI.V34I07.6836 (2019)	The Kinetics-TPS dataset is used to enhance multi-modal reasoning in video recognition tasks by providing over 15 million part-level annotations for detailed human action understanding. It is employed to construct and enrich visual knowledge graphs, focusing on action parsing and temporal segment annotation, which improves the accuracy and detail of human action recognition in videos.
NUS-WIDE-10K	https://doi.org/10.1145/3474085.3475567 (2021)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The NUS-WIDE-10K dataset is primarily used for cross-modal multimedia retrieval experiments, focusing on the role of correlation and abstraction in retrieval performance. It employs 1,000-dimensional Bag-of-Words (BoW) features and is utilized to assess the effectiveness of models like GCR in enhancing cross-modal retrieval by leveraging the correlation and abstraction of multimodal data.
DBP15K JA - EN	https://doi.org/10.48550/arXiv.2407.19625 (2024), https://www.semanticscholar.org/paper/fd43bd0f6a83b3a9c0debf6f0fa10abdf6ad394a (2024)	https://doi.org/10.1609/aaai.v35i5.16550 (2020)	The DBP15K JA - EN dataset is used for bilingual entity alignment between Japanese and English DBpedia versions, focusing on cross-lingual knowledge graph reasoning. It is employed to evaluate methods like EVA, measuring their performance in aligning entities across languages. This dataset enables researchers to assess the effectiveness of entity alignment techniques in a cross-lingual context.
Fakeddit	https://doi.org/10.48550/arXiv.2505.14714 (2025), https://doi.org/10.1145/3555178 (2022)	https://doi.org/10.1089/big.2020.0062 (2018)	The Fakeddit dataset is used for evaluating fine-grained fake news detection, particularly focusing on multimodal content and social context. It is also utilized to analyze millions of fauxtography posts from Reddit, examining visual and textual elements to understand the spread of misleading images. This dataset enables researchers to develop and test methods for detecting and analyzing misinformation by integrating both visual and textual data.
EVA-Dataset	https://doi.org/10.1145/3627673.3679126 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The EVA-Dataset is used to evaluate the performance of multi-modal reasoning models, particularly in entity linking and alignment tasks. It equips entities with visual information, though it has low image coverage, which affects the comprehensive representation of entities in multi-modal knowledge graphs. This dataset enables researchers to assess model effectiveness in integrating textual and visual data.
MSCOCO captions	https://doi.org/10.48550/arXiv.2501.04173 (2025)	https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628 (2015)	The MSCOCO captions dataset is primarily used for fine-tuning models on image captioning tasks. It provides a large set of images paired with descriptive captions, enabling researchers to train and evaluate models that generate natural language descriptions of visual content. This dataset supports the development and improvement of image-to-text generation models, enhancing their ability to produce accurate and contextually relevant captions.
VQA 2.0	https://doi.org/10.48550/arXiv.2501.04173 (2025)	https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628 (2015)	The VQA 2.0 dataset is primarily used for fine-tuning models on visual question answering tasks, which enhances the models' ability to accurately answer questions about images. This dataset enables researchers to improve the performance of AI systems in understanding and interpreting visual content, focusing on the integration of image and text data.
GoodNews	https://doi.org/10.1109/TKDE.2022.3224228 (2022), https://doi.org/10.1109/TMM.2023.3301279 (2021)	https://doi.org/10.1145/1646396.1646452 (2009)	The GoodNews dataset is primarily used for image captioning in news contexts, leveraging multi-modal data and graph-structured information to enhance context understanding. It supports entity-aware captioning, ensuring descriptions are context-driven and entity-relevant. The dataset includes images, captions, and news articles from the New York Times, annotated with ground-truth captions, enabling researchers to generate more accurate and contextually rich image descriptions.
KB-VQA	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The KB-VQA dataset is used to enhance visual question answering by integrating external knowledge from knowledge bases. It captures relationships among mentions and entities, providing detailed knowledge about named entities and their relations in images. This integration allows researchers to improve the accuracy and context-awareness of visual question answering systems, leveraging structured external information to better understand and answer complex queries.
Common Sense Knowledge Graph (CSKG)	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-030-77385-4_41 (2020)	The Common Sense Knowledge Graph (CSKG) is used to integrate and explore common sense knowledge across various dimensions, supporting the development of rich, well-connected, and heterogeneous knowledge graphs. It facilitates the integration of diverse data types and enhances multi-modal reasoning, enabling researchers to explore the breadth of common sense knowledge effectively.
TabMWP	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2211.16492 (2022)	The TabMWP dataset is used for data augmentation in multi-modal reasoning tasks, particularly in ScienceQA. It addresses sparse data issues by constructing multi-step multi-modal samples, enhancing the robustness and diversity of training data. This dataset enables researchers to improve model performance in scenarios where data is limited, facilitating more effective multi-modal reasoning.
KiloGram	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2211.16492 (2022)	The KiloGram dataset is used for data augmentation in multi-modal reasoning tasks, particularly in ScienceQA. It addresses sparse data issues and facilitates the construction of multi-step, multi-modal samples, enhancing the robustness and complexity of reasoning tasks. This dataset enables researchers to improve model performance by providing diverse and rich data samples.
SeedBench	https://doi.org/10.48550/arXiv.2405.16473 (2024)	https://doi.org/10.48550/arXiv.2306.13394 (2023)	The SeedBench dataset is used to test and benchmark multi-modal models, specifically focusing on seed-based reasoning. It evaluates the models' ability to generate coherent responses from given seeds and provides a large-scale dataset for assessing performance across various tasks. This enables researchers to compare and improve the reasoning capabilities of multi-modal models.
MELD	https://doi.org/10.1145/3593583 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.393 (2020)	The MELD dataset is used to evaluate multi-modal emotion recognition and sentiment analysis, specifically focusing on textual and visual utterances annotated with sentiment and emotion labels. Researchers employ this dataset to develop and test models that can accurately recognize emotions and sentiments from both text and visual data, enhancing the understanding of human communication in multi-modal contexts.
NTU-RGB+D 60	https://doi.org/10.1109/ICME52920.2022.9859787 (2022)	https://doi.org/10.1109/CVPR.2016.115 (2016)	The NTU-RGB+D 60 dataset is primarily used for skeleton-based action recognition, providing a large-scale resource for 3D human activity analysis. It enhances multi-modal reasoning capabilities by integrating RGB, depth, and skeleton data across 60 distinct actions. Researchers use this dataset to assess and improve action recognition performance, leveraging its comprehensive multi-modal inputs to develop and evaluate algorithms.
MEISD	https://doi.org/10.1145/3593583 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.393 (2020)	The MEISD dataset is used for assessing multi-modal emotion recognition and sentiment analysis. It includes multi-label emotion, intensity, and sentiment dialogue data, enabling researchers to evaluate models that can recognize and analyze emotions and sentiments across multiple modalities. This dataset supports the development and testing of algorithms designed to handle complex emotional and sentimental content in dialogues.
MSED	https://doi.org/10.1145/3593583 (2023)	https://doi.org/10.18653/V1/2020.COLING-MAIN.393 (2020)	The MSED dataset is manually created and used to evaluate multi-modal sentiment, emotion, and desire understanding by incorporating textual and visual utterances. It enables researchers to assess models' capabilities in interpreting complex human expressions across both text and images, focusing on the integration and analysis of these multi-modal data types.
VCR (QR → A)	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The VCR (QR → A) dataset is used to assess visual commonsense reasoning, focusing on the evaluation of systems' abilities to answer questions based on both visual and textual inputs. It enables researchers to test and improve models that integrate visual and linguistic understanding, specifically addressing the challenge of reasoning about complex scenes and contexts.
VGGFace2	https://doi.org/10.1145/3583690 (2023)	https://doi.org/10.1016/j.neunet.2014.09.005 (2013)	The VGGFace2 dataset is primarily used to train deep learning models, such as FaceNet, for extracting facial identity features from detected faces in video frames. This enables researchers to develop more accurate face recognition systems, focusing on the robust extraction of facial features in various conditions. The dataset's large scale and diverse set of images facilitate the training of models that can generalize well across different environments and scenarios.
FER2013	https://doi.org/10.1145/3583690 (2023)	https://doi.org/10.1016/j.neunet.2014.09.005 (2013)	The FER2013 dataset is used to train a FaceNet model for extracting facial emotion features from detected faces in video frames. This involves employing deep learning techniques to recognize and classify emotions, enabling research focused on facial emotion recognition in various applications such as human-computer interaction and affective computing. The dataset's large-scale and diverse set of labeled facial images facilitates robust model training and validation.
PMR (Dong et al., 2022)	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/2022.acl-long.66 (2021)	The PMR dataset (Dong et al., 2022) is used for visual question answering, specifically focusing on premise-based multimodal reasoning that integrates textual and visual clues. It enables researchers to explore how joint textual and visual information can be effectively utilized to answer complex questions, enhancing the understanding of multimodal data integration and reasoning.
LVIS	https://doi.org/10.1145/3583690 (2023)	https://doi.org/10.1109/CVPR.2019.00550 (2019)	The LVIS dataset is used to pretrain the ResNet152 model for large vocabulary instance segmentation, enhancing multi-modal reasoning capabilities. It is also utilized to detect objects and extract features, which contribute to the object component in multi-modal Knowledge Graph Reasoning. This dataset enables researchers to improve the accuracy and richness of feature extraction and object detection, crucial for advanced multi-modal reasoning tasks.
3RScan	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.1109/ICCV.2019.00775 (2019)	The 3RScan dataset is used to source complex real-world scenes, providing a range of 3D indoor scenarios for generating diverse spatial locations and viewpoints. It enables researchers to study and model various indoor environments, enhancing the realism and diversity of spatial data in computer vision and robotics applications. The dataset's rich 3D content supports the development and evaluation of algorithms that require detailed and varied indoor scene representations.
ARKitScenes	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.1109/ICCV.2019.00775 (2019)	The ARKitScenes dataset is used to generate diverse real-world scenes by leveraging mobile RGB-D data, enabling 3D indoor scene understanding. It provides spatial locations and viewpoints, facilitating research in 3D reconstruction and scene analysis. This dataset supports methodologies that require detailed, realistic indoor environments for testing and validating algorithms.
E-VQA	https://doi.org/10.48550/arXiv.2504.10074 (2025)	https://doi.org/10.1109/ICCV51070.2023.00289 (2023)	The E-VQA dataset, comprising 221,000 question-answer pairs linked to 16,700 fine-grained Wikipedia entities, is used to enhance entity understanding and integrate detailed properties of fine-grained categories. It supports both single-hop and two-hop reasoning, pushing research towards comprehensive entity understanding and the integration of fine-grained details from Wikipedia.
CM3KG	https://doi.org/10.18653/v1/2022.emnlp-demos.15 (2022)	https://doi.org/10.18653/v1/2020.acl-main.703 (2019)	The CM3KG dataset is used as an open-sourced multi-modal knowledge graph to support research in multi-modal knowledge graph reasoning. It provides a rich resource for integrating and reasoning across different modalities, enabling researchers to explore complex relationships and interactions within and between various data types. This dataset facilitates the development and evaluation of methods that can effectively combine textual, visual, and other forms of data to enhance reasoning capabilities.
Visual7W-KB	https://doi.org/10.24963/ijcai.2020/153 (2020)	https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce (2018)	The Visual7W-KB dataset is used to test the performance of the Out of the Box method, specifically for factual visual question answering. This involves employing graph convolutional networks to process and answer questions that require understanding both visual and textual information. The dataset enables researchers to evaluate how effectively these networks can integrate and reason over multi-modal data.
ProMQA	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2407.09413 (2024)	The ProMQA dataset is used for multimodal procedural activity understanding, specifically in question answering tasks that integrate textual and visual information. It enables researchers to develop and evaluate models that can reason about complex activities by combining insights from both modalities, enhancing the accuracy and context-awareness of procedural understanding.
MultiModalQA	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2407.09413 (2024)	The MultiModalQA dataset is used for multimodal question answering tasks, where it integrates textual and visual data to enhance reasoning capabilities. This dataset enables researchers to develop and evaluate models that can effectively combine information from both modalities, improving the accuracy and robustness of question answering systems.
SPIQA	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2407.09413 (2024)	The SPIQA dataset is utilized for multimodal question answering on scientific papers, integrating textual content with figures and tables to enhance comprehension. This approach leverages the dataset's combination of visual and textual data to address research questions related to improving the understanding and accessibility of scientific information.
UMLS	https://doi.org/10.18653/v1/2024.findings-acl.319 (2024)	https://doi.org/10.1093/nar/gkh061 (2004)	The UMLS dataset is used to construct a comprehensive medical knowledge graph with over 15,000 entity nodes and 130,000 edges, encompassing 20 semantic types and 50 relation types. It is also utilized for entity disambiguation by linking textual entities to UMLS concepts during pre-processing. These applications enable robust representation and integration of medical knowledge in research.
Pascal Sentence	https://doi.org/10.1145/3474085.3475567 (2021)	https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146 (2010)	The Pascal Sentence dataset is used to collect and evaluate image annotations, providing five English sentences per image that focus on descriptive content. It is employed to assess models like GCR in cross-modal retrieval tasks, specifically aligning images with descriptive sentences. This dataset enables research in multi-modal reasoning by offering rich, descriptive textual data paired with visual content.
XMedia	https://doi.org/10.1145/3474085.3475567 (2021)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The XMedia dataset is used for cross-modal multimedia retrieval experiments, employing 3,000-dimensional Bag-of-Words (BoW) features. Research focuses on evaluating the impact of correlation and abstraction on retrieval performance. This dataset enables researchers to explore how different feature representations and their interrelations affect the effectiveness of retrieving multimedia content across modalities.
DBP15K ZH - EN	https://doi.org/10.48550/arXiv.2407.19625 (2024)	https://doi.org/10.1007/978-3-319-68288-4_37 (2017)	The DBP15K ZH - EN dataset is used for bilingual entity alignment between Chinese and English DBpedia versions, focusing on cross-lingual knowledge graph reasoning. It enables researchers to develop and evaluate methods for aligning entities across different language versions, enhancing the interoperability and integration of multilingual knowledge graphs. This dataset supports research in improving cross-lingual information retrieval and semantic web technologies.
DBP15K FR - EN	https://doi.org/10.48550/arXiv.2407.19625 (2024)	https://doi.org/10.1007/978-3-319-68288-4_37 (2017)	The DBP15K FR - EN dataset is used for bilingual entity alignment between French and English DBpedia versions, focusing on cross-lingual knowledge graph reasoning. It enables researchers to develop and evaluate methods for aligning entities across different language versions of DBpedia, enhancing the interoperability and integration of multilingual knowledge graphs.
DBpedia50	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DBpedia50 dataset is used for knowledge graph completion, specifically focusing on entity linking and relation prediction within a smaller subset of DBpedia. Researchers employ this dataset to enhance the accuracy of these tasks by leveraging its structured data, enabling more precise and contextually relevant predictions in knowledge graphs.
DBpedia500	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DBpedia500 dataset is used for knowledge graph completion, specifically focusing on entity linking and relation prediction within a larger subset of DBpedia. Researchers employ this dataset to enhance the accuracy and robustness of knowledge graphs by improving the linking of entities and predicting relationships between them. This enables more effective and comprehensive knowledge representation and reasoning tasks.
DB100K	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The DB100K dataset is primarily used for knowledge graph embedding, where the focus is on learning representations of entities and relations within a large-scale knowledge graph. This methodology involves converting graph data into vector spaces to capture semantic relationships, enabling more effective reasoning and prediction tasks. The dataset's scale and structure support advanced embedding techniques, facilitating research in areas such as entity linking and relation prediction.
CN-DBpedia	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	CN-DBpedia is used as a structured data resource derived from Chinese Wikipedia, serving semantic web applications. It provides a Chinese-language knowledge graph, enabling researchers to leverage encyclopedia knowledge for various semantic web tasks. The dataset's structured format facilitates integration and querying, supporting applications that require rich, interconnected data.
Scene Graph	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.1109/CVPR.2015.7298990 (2015)	The Scene Graph dataset is used to represent action and spatial relations in images, which facilitates image retrieval and reasoning tasks. It enables researchers to analyze and understand complex visual scenes by encoding relationships between objects, enhancing the accuracy and efficiency of image-based queries and reasoning processes.
UUKG	https://doi.org/10.1145/3664647.3681705 (2024)	https://doi.org/10.48550/arXiv.2306.11443 (2023)	The UUKG dataset is used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities. It integrates diverse urban data sources, enabling researchers to analyze and predict urban dynamics by leveraging the structured representation of urban entities and their relationships. This approach supports the development of more efficient and responsive smart city systems.
VrR-VG	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.1109/CVPR.2015.7298990 (2015)	The VrR-VG dataset is used to enhance the representation of action and spatial relations in images, focusing on visually-relevant relationships. It supports research in improving the understanding and interpretation of visual scenes, particularly in capturing and analyzing actions and spatial configurations. This dataset enables more accurate and contextually rich visual reasoning by providing detailed annotations of these relationships.
Wiki80	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/D17-1004 (2017)	The Wiki80 dataset is used to train OpenNRE models with BERT and CNN encoders for relation extraction, specifically to enhance knowledge graph construction from news items. This dataset facilitates the development and evaluation of models that can accurately identify and extract relationships between entities, thereby improving the quality and comprehensiveness of knowledge graphs derived from textual data.
TACRED	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/D17-1004 (2017)	The TACRED dataset is used to train models like OpenNRE with a BERT encoder for relation extraction, specifically to enhance the construction of knowledge graphs from news articles. This application focuses on improving the accuracy and efficiency of extracting relationships between entities, leveraging the dataset's annotated relations to train and evaluate models.
DocRED	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/D17-1004 (2017)	The DocRED dataset is primarily used for training models, such as the ATLOP model, to perform document-level relation extraction. This involves identifying and extracting relationships between entities within complex documents, which contributes to the construction of comprehensive knowledge graphs. The dataset's focus on document-level relations enables researchers to develop more sophisticated and context-aware models for knowledge graph construction.
CI-FAR	https://doi.org/10.1145/3627673.3679175 (2024)	https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086 (2009)	The CI-FAR dataset is primarily used for small-scale image recognition and object localization. Researchers extract bounding boxes of objects from the images to construct VisionKG, a knowledge graph focused on visual data. This methodology enables the creation of structured visual information, facilitating research in object detection and image understanding.
PASCAL VOC	https://doi.org/10.1145/3627673.3679175 (2024)	https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086 (2009)	The PASCAL VOC dataset is primarily used for object detection and segmentation in visual scenes. Researchers extract bounding boxes of objects to construct VisionKG, a knowledge graph focused on visual data. This dataset enables precise localization and categorization of objects, facilitating advancements in computer vision and scene understanding.
SituNet	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	SituNet is used to train models for situation recognition tasks, particularly focusing on visual semantic role labeling and defining visual event schemas. This dataset enables the development of models that enhance image understanding by identifying and labeling roles within visual scenes, contributing to more accurate and context-aware image analysis.
SWiG	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/CVPR.2016.597 (2016)	The SWiG dataset is primarily used for training models in situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It is also utilized to define visual event schemas, which aids in developing more robust situation recognition models. These applications leverage the dataset's rich annotations to improve the accuracy and context-awareness of image interpretation systems.
PolitiFact	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/N16-1174 (2016)	The PolitiFact dataset is used to evaluate the performance of KGF and KGT models in detecting fake news, specifically focusing on the impact of different embedding dimensions. This dataset enables researchers to assess how effectively these models can identify false information, contributing to the development of more robust fake news detection systems.
GossipCop	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.18653/v1/N16-1174 (2016)	The GossipCop dataset is used to evaluate the performance of KGF and KGT models in detecting fake news, specifically focusing on the impact of different embedding dimensions. This dataset enables researchers to assess how varying embedding sizes affect the accuracy and efficiency of these models in distinguishing between real and fake news.
NUS-WIDE	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1646396.1646452 (2009)	The NUS-WIDE dataset is primarily used for enhancing image tagging and multi-modal reasoning tasks. It helps construct scene graphs and disambiguate concepts in web images, improving the accuracy of image-tag relationships. This dataset enables researchers to develop and test methods that integrate visual and textual data, thereby enhancing the reasoning capabilities in image tagging applications.
MSVD	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1646396.1646452 (2009)	The MSVD dataset is primarily used for image and video captioning, enhancing the generation of descriptive captions through multi-modal data. It provides a diverse set of multimedia content, enabling researchers to train and evaluate models that integrate visual and textual information. This dataset supports the development of more accurate and contextually rich captioning systems.
EAL dataset	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1145/3340531.3412875 (2020)	The EAL dataset is used in research for entity aspect linking, specifically to calculate similarity between context and aspects and to evaluate the performance of methods in linking entities to their aspects. It provides aspect content essential for these calculations, enabling detailed experimental analysis in this domain.
Visual7w	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TPAMI.2017.2754246 (2016)	The Visual7w dataset is used to assess detailed visual understanding through a subset of VQA with wh-questions. It focuses on evaluating models' ability to comprehend complex visual scenes by answering specific, detailed questions. This dataset enables researchers to test and improve visual reasoning capabilities in AI systems, ensuring they can accurately interpret and respond to visual content.
Flick30k Entities	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Flick30k Entities dataset is used to enhance entity recognition in images and their associated text, facilitating fine-grained multimedia knowledge extraction. Researchers apply this dataset to improve the accuracy of entity detection and linking in visual and textual data, supporting more robust and detailed multimedia analysis. This dataset enables the development and evaluation of models that integrate visual and textual information for enhanced knowledge extraction.
FB-Img-Few	https://doi.org/10.1109/ICDE60146.2024.00061 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The FB-Img-Few dataset is used to provide image embeddings for multi-modal knowledge graph reasoning, particularly in few-shot learning scenarios. It evaluates the performance of methods like MULTIFORM, focusing on enhancing entity representations through multi-modal data. The dataset is utilized to compare these methods against baselines, enabling researchers to assess improvements in entity representation and reasoning capabilities.
Open Images	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2020.acl-demos.11 (2020)	The Open Images dataset is used to train detectors on a diverse set of labeled images, enhancing multi-modal reasoning across various domains. This dataset enables researchers to expand the capabilities of detection systems by providing a wide range of labeled data, which is crucial for improving the accuracy and robustness of these systems in real-world applications.
Microsoft COCO	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-319-46448-0_51 (2016)	The Microsoft COCO dataset is primarily used to evaluate visual context models, focusing on metrics such as recall at K and mean R@K. It is employed to assess the performance of proposed approaches in visual recognition tasks, enabling researchers to benchmark their methods against established standards. The dataset's rich annotations and diverse images facilitate robust evaluation in these areas.
Conceptual Captions	https://doi.org/10.18653/v1/2023.acl-industry.16 (2023)	https://doi.org/10.1162/tacl_a_00166 (2014)	The Conceptual Captions dataset is used to analyze and compare the unique characteristics of image-text pairs, particularly in the product domain. Researchers employ this dataset to highlight differences in data distribution and content, focusing on how these pairs differ from other domains. This analysis aids in understanding the specific nuances and patterns within product-related image-text data, which can inform the development of more tailored multimodal models.
food.com	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.18653/v1/N19-1423 (2019)	The food.com dataset is used to analyze recipes from both home cooks and celebrity chefs, with a focus on integrating food news and pop culture into multi-modal reasoning in culinary contexts. This involves enhancing the understanding of culinary practices and trends through the analysis of textual and visual data, enabling more nuanced reasoning about food and cooking.
WD-singer	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1162/tacl_a_00360 (2019)	The WD-singer dataset is used to derive a subset of Wikidata for evaluating knowledge graph reasoning tasks, specifically focusing on singer-related entities and relationships. It enables researchers to assess reasoning algorithms by providing a specialized knowledge graph with detailed singer data, facilitating the evaluation of entity and relationship inference methods.
allrecipes.com	https://doi.org/10.48550/arXiv.2308.04579 (2023)	https://doi.org/10.18653/v1/N19-1423 (2019)	The allrecipes.com dataset is used to study a wide range of recipes, contributing to the development of multi-modal reasoning systems that understand diverse culinary content. It enables researchers to analyze and reason about various cooking techniques, ingredients, and preparation methods, enhancing the system's ability to process and generate culinary-related information.
WN18RR-IMG	https://doi.org/10.3390/s24237605 (2024)	https://doi.org/10.1145/3474085.3475470 (2021)	The WN18RR-IMG dataset is used to evaluate models' performance in handling relational reasoning with visual context within the WordNet hierarchy. It specifically assesses how well models can integrate visual information to reason about relationships between entities. This dataset enables researchers to test and improve multi-modal reasoning capabilities in knowledge graph models.
DB15K-IMG	https://doi.org/10.3390/s24237605 (2024)	https://doi.org/10.1145/3474085.3475470 (2021)	The DB15K-IMG dataset is used to evaluate models in cross-lingual knowledge graph reasoning with visual context, specifically focusing on entity alignment. It enables researchers to test the effectiveness of their models in aligning entities across different languages while incorporating visual information. This dataset is crucial for advancing multi-modal reasoning techniques in knowledge graphs.
VG-Attribution	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99 (2022)	The VG-Attribution dataset is used to evaluate structured representations of visual attributes, specifically to compare the performance of proposed methods against state-of-the-art models. This dataset enables researchers to assess and enhance the accuracy and effectiveness of visual attribute representation techniques in a structured format.
VG-Relation	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99 (2022)	The VG-Relation dataset is used to evaluate the performance of methods in capturing visual relationships, specifically by comparing the proposed method against state-of-the-art models. This dataset enables researchers to assess the effectiveness of their approaches in understanding and reasoning about visual relationships, contributing to advancements in visual relationship detection and multi-modal reasoning.
large-scale noisy image-text pairs	https://doi.org/10.1609/aaai.v38i3.28017 (2023)	https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99 (2022)	The 'large-scale noisy image-text pairs' dataset is used to pre-train a multi-modal mixture of the encoder-decoder model. Researchers focus on enhancing model robustness by injecting diverse synthetic captions and removing noisy captions. This approach aims to improve the quality and reliability of the model's outputs in multi-modal tasks.
ImageGraph	https://doi.org/10.1145/3656579 (2024)	https://doi.org/10.24432/C56P45 (2017)	ImageGraph is used to support visual-relational queries in web-extracted knowledge graphs by extending FB15K with 829,931 images and 1,330 relation types. Researchers employ this dataset to enhance multi-modal reasoning capabilities, enabling more complex and contextually rich queries that integrate both textual and visual data. This extension facilitates the exploration of how visual information can complement and refine relational knowledge in large-scale graphs.
Freebase-15k Multi-Modal (FB15k)	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The Freebase-15k Multi-Modal (FB15k) dataset is used to enhance knowledge graph reasoning by integrating visual and textual information, extending the original FB15k dataset. This integration supports research in multi-modal data fusion, enabling more comprehensive and contextually rich reasoning tasks. The dataset's multi-modal features facilitate the development and evaluation of models that can leverage both visual and textual inputs for improved reasoning capabilities.
FB15k-(237)	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The FB15k-(237) dataset is used for evaluating multi-modal knowledge graph reasoning, specifically focusing on more challenging and less noisy relation prediction tasks. This dataset enables researchers to assess the performance of models in predicting relationships within knowledge graphs, enhancing the accuracy and robustness of these predictions.
ASER	https://doi.org/10.1145/3573201 (2022)	https://doi.org/10.1145/3366423.3380107 (2019)	The ASER dataset is primarily recognized for being the largest knowledge graph with eventualities as semantic units and relations as edges. However, specific methodologies or research questions directly employing ASER are not detailed in the provided descriptions. Its scale and structure suggest potential utility in large-scale knowledge representation and reasoning tasks, though explicit research applications are not specified.
YAGO Multi-Modal (YAGO15k)	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The YAGO Multi-Modal (YAGO15k) dataset is used to extend traditional knowledge graphs by integrating multi-modal data, specifically visual and textual information. This integration enhances reasoning capabilities, allowing researchers to explore more complex and nuanced relationships within the data. The dataset's multi-modal nature supports advanced reasoning tasks, enabling more comprehensive and context-aware knowledge graph applications.
Wikidata SPARQL query log	https://doi.org/10.1145/3626246.3654757 (2024)		The Wikidata SPARQL query log is used to study query complexity and structure in multi-modal knowledge graph reasoning. Researchers extract real-world SPARQL queries and group them by the number of triple patterns. This allows for an analysis of how queries are structured and the complexity they exhibit, providing insights into the practical use and optimization of SPARQL queries in knowledge graphs.
Freebase-15k (237) Multi-Modal (FB15k-(237))	https://doi.org/10.48550/arXiv.2408.11526 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The Freebase-15k (237) Multi-Modal (FB15k-(237)) dataset is used to enhance multi-modal knowledge graphs by integrating visual and textual data. It focuses on 237 relations to improve entity linking and reasoning, enabling more accurate and contextually rich knowledge graph constructions. This dataset supports research in multi-modal knowledge graph enhancement, specifically addressing the integration of diverse data types to refine entity relationships and reasoning capabilities.
NTU-RGB+D 120	https://doi.org/10.1109/ICME52920.2022.9859787 (2022)	https://doi.org/10.1109/CVPR46437.2021.01600 (2021)	The NTU-RGB+D 120 dataset is primarily used for action recognition research, focusing on multi-modal data integration from RGB, depth, and skeleton sensors. It enables researchers to preprocess and analyze these diverse data types, enhancing the accuracy and robustness of action recognition systems. The dataset's comprehensive multi-modal features support the development and evaluation of algorithms designed to recognize human actions in various contexts.
VCR data set	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The VCR dataset is used to reorganize large-scale datasets, focusing on visual commonsense reasoning tasks. It evaluates models' ability to understand complex visual scenes, employing methodologies that test and enhance the reasoning capabilities of AI systems in interpreting visual content. This dataset enables researchers to assess and improve the performance of models in comprehending and reasoning about visual information.
Unified Medical Language System (UMLS)	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1093/nar/gkh061 (2004)	The Unified Medical Language System (UMLS) is used to integrate biomedical terminology, serving as a comprehensive knowledge base for multi-modal reasoning in medical data. It enables researchers to link and harmonize diverse medical terminologies, facilitating more robust and accurate data integration and analysis in biomedical research.
Visual Entailment	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The Visual Entailment dataset is used to evaluate the ability to reason about the relationship between images and textual statements, focusing on determining whether the image entails, contradicts, or is neutral to the statement. This involves assessing multi-modal reasoning capabilities, where the dataset provides pairs of images and statements to test these relationships. The dataset enables researchers to develop and benchmark models that can understand and correlate visual and textual information effectively.
Visual Dialog	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The Visual Dialog dataset is used to explore dialog-based interactions with images, focusing on generating and evaluating responses that require understanding both visual and textual content. This involves methodologies that integrate image recognition and natural language processing to enhance interactive systems. The dataset enables researchers to test and improve models that can effectively communicate about visual data, addressing research questions related to multimodal interaction and response generation.
Semantic MEDLINE	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1145/3442381.3449860 (2021)	The Semantic MEDLINE dataset is used to extract personalized graphs to enhance Electronic Health Record (EHR) representation learning. By focusing on medical knowledge paths, it improves health risk prediction. This involves leveraging the dataset's structured medical information to create more informative and contextually rich EHR representations, which are then used to address specific research questions related to patient health outcomes and risk assessment.
Visual Common-sense Reasoning	https://doi.org/10.48550/arXiv.2305.04530 (2023)	https://doi.org/10.18653/v1/P19-1472 (2019)	The Visual Common-sense Reasoning dataset is used to evaluate models' ability to reason about complex scenarios involving both images and text, with a focus on common-sense understanding and inference. This dataset enables researchers to assess how well models can integrate visual and textual information to make logical deductions, addressing research questions related to multi-modal reasoning and common-sense inference.
ICD/ATC	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1109/TCYB.2021.3109881 (2021)	The ICD/ATC dataset is used as a hierarchical dataset for self-supervised graph learning in healthcare, specifically to explore the relationships between International Classification of Diseases (ICD) and Anatomical Therapeutic Chemical (ATC) codes. This approach leverages the structured nature of the dataset to enhance understanding and prediction in medical coding and classification systems.
ICD/ATC − CO	https://doi.org/10.48550/arXiv.2307.04461 (2023)	https://doi.org/10.1109/TCYB.2021.3109881 (2021)	The ICD/ATC − CO dataset is used to enhance the representation of medical events in graph learning models by incorporating co-occurrence information. This extended version of ICD/ATC is specifically employed to improve the accuracy and richness of medical event representations in knowledge graphs, enabling more effective graph learning and reasoning tasks.
DBPedia50k	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1162/tacl_a_00360 (2019)	The DBPedia50k dataset is used for inductive knowledge graph reasoning, particularly focusing on embedding and pre-trained language representation. It enables researchers to explore how pre-trained models can be effectively utilized in inductive settings, enhancing the reasoning capabilities over knowledge graphs. This dataset supports the development and evaluation of methods that integrate language understanding with graph embeddings, facilitating more robust and scalable knowledge graph reasoning systems.
electronic medical records	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1016/j.bdr.2020.100174 (2020)	The electronic medical records dataset is used to integrate with a medical knowledge graph for safe medication recommendation, focusing on patient safety and drug interactions. This integration employs a knowledge graph reasoning approach to enhance the accuracy and safety of medication recommendations by leveraging detailed patient data and existing medical knowledge.
Global Database of Events, Language, and Tone (GDELT)	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1145/2187980.2188242 (2012)	The Global Database of Events, Language, and Tone (GDELT) is used for dynamic Knowledge Graph Reasoning (KGR) tasks, providing event data and linguistic tone analysis for global events. This dataset enables researchers to analyze and reason over temporal and spatial dynamics of world events, enhancing understanding of global phenomena through structured event data and sentiment analysis.
5,800 test samples	https://doi.org/10.48550/arXiv.2504.10074 (2025)		The dataset of 5,800 test samples is used to evaluate the performance of models in multi-modal knowledge graph reasoning, specifically focusing on single-hop and two-hop reasoning questions involving Wikipedia entities. This dataset enables researchers to assess how effectively models can reason across different types of data and relationships within a knowledge graph.
ScanNet	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.48550/arXiv.2210.07474 (2022)	The ScanNet dataset is used to collect situated question-answer pairs in 3D scenes, enhancing situated understanding in real-world environments through multi-modal reasoning. It supports research in understanding and interacting with 3D environments by providing rich, annotated data that captures the spatial and semantic relationships within these scenes. This enables the development and evaluation of models that can reason about objects and their contexts in complex, real-world settings.
Integrated Crisis Early Warning System (ICEWS)	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1145/2187980.2188242 (2012)	The Integrated Crisis Early Warning System (ICEWS) dataset is used for dynamic Knowledge Graph Reasoning (KGR) tasks, providing structured data on political events and crisis situations. It enables researchers to analyze temporal patterns and relationships within these events, facilitating the development of predictive models for crisis early warning systems. The dataset's structured nature supports the integration of event data into knowledge graphs, enhancing the accuracy and robustness of KGR methodologies.
SQA3D	https://doi.org/10.48550/arXiv.2409.02389 (2024)	https://doi.org/10.48550/arXiv.2210.07474 (2022)	The SQA3D dataset is used to facilitate situated question answering in real-world 3D scenes, focusing on spatial reasoning and scene understanding. It enables researchers to develop and evaluate models that can interpret and reason about complex 3D environments, addressing specific challenges in spatial awareness and context comprehension.
M2KR	https://doi.org/10.48550/arXiv.2504.10074 (2025)		The M2KR dataset is used to evaluate multi-modal knowledge retrieval systems, specifically focusing on tasks that involve retrieving information from multi-modal knowledge graphs. The dataset enables researchers to assess the performance of these systems in handling and integrating different types of data, though details about its content and structure are not provided. This evaluation helps address research questions related to the effectiveness and efficiency of multi-modal knowledge retrieval methods.
ICEWS	https://doi.org/10.48550/arXiv.2506.11012 (2025)	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	The ICEWS dataset is used to provide event data with daily temporal granularity, focusing on military and political events. It is employed in multi-modal knowledge graph reasoning research, enabling the analysis and reasoning over complex, time-sensitive event data. This dataset facilitates the exploration of relationships and patterns in military and political activities, enhancing the understanding of dynamic socio-political phenomena.
MIMIC-CXR	https://doi.org/10.1109/BIBM58861.2023.10386013 (2023)	https://doi.org/10.1093/jamia/ocv080 (2015)	The MIMIC-CXR dataset is primarily used for generating radiology reports, focusing on chest X-ray images and their associated reports. It is part of the MIMIC series and is widely utilized in medical imaging research. The dataset enables researchers to develop and evaluate algorithms for automated report generation, enhancing the accuracy and efficiency of radiological interpretations.
Twitter dataset	https://doi.org/10.1109/TMM.2023.3330296 (2024)		The Twitter dataset is used to automatically detect fake multi-media content on Twitter. Research focuses on developing and evaluating detection algorithms, leveraging the dataset's multi-modal content (text and images). This enables researchers to enhance the accuracy of fake content identification, contributing to the broader field of misinformation detection.
PROTEIN	https://doi.org/10.48550/arXiv.2304.11116 (2023)	https://www.semanticscholar.org/paper/9b720e749e71960f323779ec63261250871ffb66 (2020)	The PROTEIN dataset is used to evaluate Graph-ToolFormer on protein interaction graphs, specifically for node classification tasks. This involves applying graph-based machine learning methods to classify nodes within the protein interaction network. The dataset's focus on protein interactions enables researchers to assess the performance of Graph-ToolFormer in predicting and understanding protein functions and interactions.
PTC	https://doi.org/10.48550/arXiv.2304.11116 (2023)	https://www.semanticscholar.org/paper/9b720e749e71960f323779ec63261250871ffb66 (2020)	The PTC dataset is used to evaluate Graph-ToolFormer's performance on chemical compound graphs, focusing on graph classification tasks. This involves assessing the model's ability to classify chemical compounds based on their structural properties. The dataset's relevance lies in its application to chemical graph analysis, enabling researchers to test and improve graph-based machine learning models in this domain.
Wiki	https://doi.org/10.24963/ijcai.2024/236 (2024)	https://doi.org/10.1145/3191513 (2015)	The Wiki dataset is used as a benchmark for First-Order Knowledge Graph Completion, specifically to evaluate model performance in completing knowledge graphs. It focuses on assessing how well models can infer missing links within the graph, providing a standardized testbed for comparing different approaches in knowledge graph completion tasks.
IMDB	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	https://doi.org/10.1007/978-3-642-13489-0_23 (2010)	The IMDB dataset is used to evaluate entity matching approaches, particularly in the context of the OAEI, by focusing on linking entities across different knowledge bases. This involves comparing and aligning entities from various sources to enhance data integration and interoperability. The dataset's rich entity relationships and attributes facilitate robust testing and validation of entity matching algorithms.
CRAG	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2406.04744 (2024)	The CRAG dataset is used to benchmark RAG models, specifically focusing on comprehensive evaluation across various reasoning tasks and modalities. It enables researchers to assess model performance in multi-modal knowledge graph reasoning, ensuring robustness and versatility in handling diverse data types and reasoning challenges.
STaRK	https://doi.org/10.48550/arXiv.2506.05766 (2025)	https://doi.org/10.48550/arXiv.2406.04744 (2024)	The STaRK dataset is used to support RAG (Retrieval-Augmented Generation) models, specifically for enhancing reasoning tasks and integrating knowledge graphs. It enables researchers to focus on specific reasoning challenges, leveraging the dataset's structured information to improve model performance in knowledge graph integration tasks.
PubMed	https://doi.org/10.48550/arXiv.2506.05766 (2025)		The PubMed dataset is used to source abstracts for multi-modal knowledge graph reasoning, specifically for integrating textual information into knowledge graphs. This approach leverages the rich textual data in PubMed to enhance the reasoning capabilities of knowledge graphs, addressing research questions related to the integration and interpretation of biomedical literature.
PKU XMedia	https://doi.org/10.1145/3474085.3475567 (2021)	https://doi.org/10.1109/TPAMI.2013.142 (2014)	The PKU XMedia dataset is used to evaluate the GCR model's performance in cross-modal retrieval, focusing on its capabilities with diverse multimedia data. This dataset enables researchers to test and refine models designed for handling and integrating different types of media, enhancing the accuracy and efficiency of cross-modal search and retrieval systems.
WN-9	https://doi.org/10.1109/ICDE55515.2023.00015 (2022)	https://doi.org/10.18653/v1/D18-1359 (2018)	The WN-9 dataset is used to enhance multimodal reasoning capabilities by expanding knowledge graphs through the addition of images to entities. This approach integrates visual data with existing textual information, enabling more comprehensive and contextually rich representations. The dataset facilitates research focused on improving the multimodal reasoning abilities of knowledge graphs, specifically by incorporating visual elements to enrich entity descriptions and relationships.
FB13	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB13 dataset is used to evaluate knowledge graph completion methods, particularly focusing on smaller-scale entity sets and relation types. It enables researchers to assess the performance of these methods in predicting missing links within the graph, facilitating advancements in knowledge graph reasoning techniques.
AudioSet	https://doi.org/10.1609/aaai.v35i2.16231 (2021)	https://doi.org/10.1109/ICASSP.2017.7952261 (2017)	The AudioSet dataset is used for extracting 96-second Mel Spectrogram patches to classify audio events, focusing on the representation of audio features in multi-modal reasoning. This methodology supports research in audio event classification, leveraging the dataset's extensive audio content to enhance the accuracy and robustness of audio feature representations.
FB122	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB122 dataset is used to evaluate the performance of knowledge graph completion models. It serves as a moderate-sized dataset with a diverse set of relations, enabling researchers to assess model accuracy and effectiveness in completing knowledge graphs. This dataset facilitates the comparison of different models and approaches in knowledge graph reasoning tasks.
FB20k	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB20k dataset is used to evaluate the scalability and performance of knowledge graph completion algorithms. Researchers employ this dataset to test how well these algorithms handle larger-scale data, focusing on their efficiency and effectiveness in completing knowledge graphs. This enables the assessment of algorithmic capabilities in managing extensive and complex graph structures.
Flickr-0	https://doi.org/10.1609/aaai.v36i2.20123 (2022)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The Flickr-0 dataset, a sub-sampled split of Flickr30K Entities, is used for training and evaluating models that focus on region-phrase correspondences. This involves aligning textual phrases with specific regions in images, enabling research in visual grounding and multimodal understanding. The dataset's region-phrase annotations facilitate the development and assessment of algorithms that can accurately map textual descriptions to corresponding image segments.
Flickr-1	https://doi.org/10.1609/aaai.v36i2.20123 (2022)	https://doi.org/10.1007/s11263-016-0965-7 (2015)	The Flickr-1 dataset, a sub-sampled split of Flickr30K Entities, is used for training and evaluating models that focus on region-phrase correspondences. This involves aligning textual phrases with specific regions in images, enabling research in visual grounding and multimodal understanding. The dataset's region-phrase annotations facilitate the development and assessment of algorithms that can accurately map textual descriptions to corresponding image segments.
Clotho	https://doi.org/10.1609/aaai.v35i2.16231 (2021)	https://doi.org/10.1109/ICASSP40776.2020.9052990 (2019)	The Clotho dataset is used for generating natural language captions for audio/visual content, integrating audio and visual modalities in multi-modal reasoning. It enables researchers to develop and evaluate models that can effectively combine these modalities to produce descriptive captions, addressing challenges in multi-modal understanding and synthesis.
FB24k	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB24k dataset is used to evaluate the effectiveness of knowledge graph completion methods, particularly on a dataset with a slightly larger entity set. It enables researchers to assess the performance and scalability of these methods, focusing on the accuracy and efficiency of completing knowledge graphs.
FB5M	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB5M dataset is used to evaluate the performance of knowledge graph completion models, particularly focusing on their scalability and efficiency when handling very large datasets. This dataset enables researchers to test and compare different methodologies in knowledge graph completion, ensuring they can effectively manage extensive data volumes.
FB60k-NYT10	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://doi.org/10.1609/aaai.v32i1.11535 (2017)	The FB60k-NYT10 dataset is used to test the integration of knowledge graphs with textual data, specifically focusing on linking entities from New York Times articles. This involves methodologies that combine structured knowledge graph data with unstructured textual information to enhance entity recognition and linking. The dataset enables researchers to evaluate and improve the accuracy of entity linking in multi-source information systems.
VQA2	https://doi.org/10.1145/3579051.3579073 (2022)	https://doi.org/10.1007/s11263-016-0981-7 (2016)	The VQA2 dataset is primarily used to evaluate visual question answering systems, focusing on complex reasoning tasks that integrate both images and text. Researchers employ this dataset to assess system performance in understanding and answering questions that require interpreting visual content alongside textual information. This enables the development and refinement of models capable of multi-modal reasoning, enhancing their ability to handle real-world, complex queries.
Wikimedia Commons	https://doi.org/10.1109/TPAMI.2024.3417451 (2022)	https://www.semanticscholar.org/paper/a3a3d374a13e3cf4c69730e5c52138c0be57f6f2 (2017)	The Wikimedia Commons dataset is used to enhance IMGpedia, a knowledge graph that supports visuo-semantic queries over images. It incorporates visual information, thereby improving multimodal reasoning capabilities. This application leverages the dataset's rich visual content to integrate and reason about both textual and image data, enabling more sophisticated and context-aware query responses.
OpenEA benchmarks	https://doi.org/10.48550/arXiv.2307.16210 (2023)	https://doi.org/10.14778/3407790.3407828 (2020)	The OpenEA benchmarks dataset is used to evaluate multi-modal entity alignment methods by incorporating entity images obtained via Google search alongside traditional textual data. This approach enhances the evaluation of entity alignment techniques, focusing on the integration of visual and textual information to improve alignment accuracy. The dataset enables researchers to test and compare different multi-modal methods, addressing the challenge of aligning entities across heterogeneous data sources.
HICO-DET	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.1109/WACV.2018.00048 (2017)	The HICO-DET dataset is primarily used to detect human-object interactions in visual scenes. Researchers employ manually labeled object bounding boxes to enhance multi-modal reasoning, focusing on the accurate identification and interaction analysis between humans and objects. This dataset enables detailed studies on visual scene understanding and interaction detection, leveraging its rich annotations to improve the performance of multi-modal reasoning models.
SpatialVOC2K	https://doi.org/10.1609/aaai.v38i17.29828 (2024)	https://doi.org/10.18653/v1/W18-6516 (2018)	The SpatialVOC2K dataset is used to investigate multilingual spatial relations by analyzing annotated images. It focuses on understanding spatial configurations across multiple languages, employing a methodology that leverages visual annotations to explore how spatial relationships are represented and understood in different linguistic contexts. This dataset enables researchers to address specific questions related to cross-linguistic spatial cognition and representation.
HotPotQA	https://doi.org/10.48550/arXiv.2501.04173 (2025)	https://doi.org/10.1109/CVPR.2019.01094 (2018)	The HotPotQA dataset is used to develop and evaluate text-based question answering (QA) systems that require reasoning over multiple documents. It enhances multi-modal knowledge graph reasoning capabilities, focusing on complex queries that necessitate information integration from various sources. This dataset supports research in improving the accuracy and reasoning abilities of QA systems, particularly in handling multi-document contexts.
CVE	https://doi.org/10.1109/ISADS56919.2023.10092024 (2023)		The CVE dataset is used to enhance the construction of the ICS-Het-KG, a heterogeneous knowledge graph for industrial control systems. It provides comprehensive and accurate vulnerability data, which enriches the graph's structure and content. This dataset supports research focused on improving the security and resilience of industrial control systems by integrating detailed vulnerability information into the knowledge graph.
CNVD	https://doi.org/10.1109/ISADS56919.2023.10092024 (2023)		The CNVD dataset is used to enrich the ICS-Het-KG by providing additional vulnerability data for industrial control systems. This enhances the graph's information, contributing to more comprehensive knowledge representation. The dataset's specific role is to augment the graph with detailed vulnerability data, which supports research focused on improving the security and resilience of industrial control systems.
ICS Vulnerability Database	https://doi.org/10.1109/ISADS56919.2023.10092024 (2023)		The ICS Vulnerability Database is used to construct the ICS-Het-KG, a specialized knowledge graph for industrial control systems. It provides detailed vulnerability data, enhancing the graph's relevance and specificity. This dataset improves the accuracy and detail of the knowledge graph, supporting research focused on enhancing security in industrial control systems.
CLEVR	https://doi.org/10.18653/v1/P19-1347 (2018)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The CLEVR dataset is used to address the visual grounding problem by focusing on compositional language and elementary visual reasoning. It employs diagnostic tasks to evaluate models' ability to understand and reason about visual scenes described in natural language. This dataset enables researchers to test and improve algorithms that integrate linguistic and visual information, enhancing the interpretability and accuracy of visual reasoning systems.
AI2D	https://doi.org/10.18653/v1/P19-1347 (2018)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The AI2D dataset is used for knowledge extraction from diagrams, enhancing multimodal reasoning capabilities. It provides a unique type of data that supports the development and evaluation of models capable of integrating visual and textual information. This dataset enables researchers to address challenges in understanding and interpreting complex diagrams, contributing to advancements in multimodal reasoning systems.
NELL995	https://doi.org/10.1145/3589334.3645569 (2023)	https://doi.org/10.18653/v1/D17-1060 (2017)	The NELL995 dataset is used to evaluate the Query2GMM model, focusing on reasoning over a diverse set of relations and entities within a knowledge graph. This dataset enables researchers to assess the model's performance in handling complex queries and reasoning tasks, thereby advancing the field of knowledge graph reasoning.
cross kg datasets	https://doi.org/10.3390/app13116747 (2023)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The 'cross kg datasets' are used for evaluating the robustness of multi-modal reasoning systems by varying the percentage of aligned entity pairs (20%, 50%, 80%). This assessment helps researchers understand how these systems perform under different levels of alignment, providing insights into their reliability and effectiveness in multi-modal knowledge graph reasoning tasks.
DB15K MMKGs	https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The DB15K MMKGs dataset is used alongside FB15K for entity alignment, specifically focusing on multi-modal aspects. It is split into training and testing sets to evaluate the performance of entity alignment methods. This dataset enables researchers to assess and improve the accuracy of aligning entities across different knowledge graphs by incorporating multi-modal data.
YAGO15K MMKGs	https://doi.org/10.48550/arXiv.2310.06365 (2023)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The YAGO15K MMKGs dataset is used alongside FB15K for entity alignment, focusing on multi-modal aspects. It is split into training and testing sets to evaluate the performance of entity alignment methods. This dataset enables researchers to assess and improve the accuracy of aligning entities across different knowledge graphs, leveraging its multi-modal features.
WN18-IMG	https://doi.org/10.1145/3474085.3475470 (2021)	https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248 (2013)	The WN18-IMG dataset is used to enhance multi-modal knowledge graph reasoning by extending WN18 with 10 images per entity. This incorporation of visual information allows researchers to explore how visual data can improve reasoning tasks in knowledge graphs, specifically addressing the integration of textual and visual modalities to enhance understanding and inference capabilities.
aspect catalog	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1145/3340531.3412875 (2020)	The 'aspect catalog' dataset is used to define aspects for entity instances, enhancing the representation of entities in a knowledge graph. It specifically supports entity aspect linking by providing detailed attributes that enrich entity representations, enabling more accurate and nuanced connections within the knowledge graph. This dataset facilitates research in improving the granularity and specificity of entity data, which is crucial for advanced knowledge graph applications.
KGT	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.1007/978-3-030-00671-6_39 (2018)	The KGT dataset is used to construct a knowledge graph of real news, focusing on the structure and content of authentic news articles. This dataset enables researchers to analyze and detect patterns in news articles by leveraging the structured representation of news data, enhancing the accuracy of news detection methodologies.
KGF	https://www.semanticscholar.org/paper/04db62a14f78f693d6bd14a4803b9b73325b36bb (2021)	https://doi.org/10.1007/978-3-030-00671-6_39 (2018)	The KGF dataset is used to construct a knowledge graph focused on fake news, specifically analyzing the structure and content of misleading news articles. This dataset enables researchers to develop and test methods for detecting fake news by leveraging the detailed structural and textual features of the articles.
eal-dataset-2020	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.1145/3340531.3412875 (2020)	The eal-dataset-2020 is used to evaluate the AspectMMKG model, specifically for entity aspect linking tasks. This involves linking entities to their relevant aspects in multi-modal knowledge graphs, aiming to achieve state-of-the-art performance. The dataset's focus on entity aspects and its integration with multi-modal data enables researchers to enhance the precision and effectiveness of knowledge graph reasoning models.
PKG	https://doi.org/10.1145/3583780.3614782 (2023)	https://doi.org/10.18653/v1/2022.findings-emnlp.171 (2022)	The PKG dataset is used as a multi-modal knowledge graph for classical Chinese poetry, integrating textual and visual information. It enhances understanding and reasoning about poetic content by combining these modalities. Researchers use it to explore how textual and visual elements interact to convey meaning, supporting studies in cultural heritage and literary analysis.
MMKG1	https://doi.org/10.1109/ICDE60146.2024.00274 (2024)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The MMKG1 dataset is used to identify and analyze semantic inconsistencies in the Multi-Modal Entity Alignment (MMEA) task. It focuses on aligning entities across different modalities, enabling researchers to evaluate and improve the consistency and accuracy of multi-modal knowledge graph reasoning systems. This dataset highlights specific challenges in entity alignment, facilitating the development of more robust alignment methodologies.
MMKG2	https://doi.org/10.1109/ICDE60146.2024.00274 (2024)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The MMKG2 dataset is used to identify and analyze semantic inconsistencies in the Multi-Modal Entity Alignment (MMEA) task. It focuses on aligning entities across different modalities, enabling researchers to evaluate and improve the consistency and accuracy of entity relationships in multi-modal knowledge graphs. This dataset highlights specific challenges in cross-modal alignment, facilitating the development of more robust alignment methods.
DiaKG	https://doi.org/10.3390/app13127115 (2023)	https://www.semanticscholar.org/paper/bc411487f305e451d7485e53202ec241fcc97d3b (2020)	The DiaKG dataset is utilized as a medical knowledge graph to evaluate the performance of reasoning models in healthcare applications. It enables researchers to assess how effectively these models can reason within a medical context, focusing on the accuracy and reliability of their outputs in practical healthcare scenarios.
CORD-19	https://doi.org/10.3390/app13127115 (2023)	https://www.semanticscholar.org/paper/bc411487f305e451d7485e53202ec241fcc97d3b (2020)	The CORD-19 dataset is used to integrate textual and structured data in COVID-19 research, enhancing multi-modal reasoning capabilities. Researchers apply this dataset to study how combining different data types can improve understanding and analysis in the context of the pandemic. This integration supports more comprehensive and nuanced research questions and methodologies, leveraging the dataset's rich textual and structured content.
RWTH-PHOENIX-Weather 2014T	https://doi.org/10.48550/arXiv.2211.00526 (2022)	https://doi.org/10.1109/CVPR.2018.00812 (2018)	The RWTH-PHOENIX-Weather 2014T dataset is primarily used to evaluate network performance in translating German sign language, with a focus on multi-modal reasoning. It enables researchers to assess how effectively models can integrate visual and linguistic data, addressing specific challenges in sign language recognition and translation. The dataset's multi-modal nature, combining video and text, supports robust testing of these capabilities.
PHOENIX14T	https://doi.org/10.48550/arXiv.2211.00526 (2022)	https://doi.org/10.1109/CVPR.2018.00812 (2018)	The PHOENIX14T dataset is primarily used to evaluate the Sign2Text task, which involves translating continuous sign language videos into spoken language. Researchers employ a multi-modal approach, leveraging both visual and textual data to improve translation accuracy. This dataset enables the development and testing of models that can effectively interpret sign language, addressing the challenge of converting visual gestures into coherent spoken language text.
ETH	https://doi.org/10.1109/ACCESS.2020.2991435 (2020)	https://doi.org/10.1109/ICCV.2009.5459260 (2009)	The ETH dataset is used to evaluate model performance in multi-modal reasoning, particularly focusing on multi-agent interactions and social behavior. It is applied in multi-target tracking scenarios, enabling researchers to assess how models reason about complex social dynamics and interactions in real-world settings.
UCY	https://doi.org/10.1109/ACCESS.2020.2991435 (2020)	https://doi.org/10.1109/ICCV.2009.5459260 (2009)	The UCY dataset is used to evaluate model performance in multi-modal reasoning, particularly focusing on multi-agent interactions and social behavior. It is applied in the context of multi-target tracking, enabling researchers to assess how models handle complex social dynamics and interactions in real-world scenarios.
FB15K-YG15K	https://doi.org/10.1145/3534678.3539244 (2022)	https://doi.org/10.1007/978-3-030-55130-8_12 (2020)	The FB15K-YG15K dataset is used to evaluate entity alignment methods in multi-modal knowledge graph reasoning. It specifically focuses on aligning entities with 20% alignment seeds, enabling researchers to assess the effectiveness of these methods in integrating and reasoning across different knowledge graphs. This dataset facilitates the development and testing of algorithms designed to improve entity alignment accuracy.
MIRFlickr	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1145/1460096.1460104 (2008)	The MIRFlickr dataset is used to enhance multi-modal reasoning in image tagging tasks by disambiguating concepts and improving image-tag relationships. This involves employing methodologies that integrate image and tag data to refine the accuracy and relevance of tags associated with images, thereby supporting more effective and contextually appropriate image annotation and retrieval.
FB15M	https://doi.org/10.1145/3534678.3539244 (2022)	https://doi.org/10.1007/978-3-030-75762-5_40 (2021)	The dataset 'FB15M' is mentioned in the citation context but lacks detailed descriptions of its usage in research. Therefore, there is no specific evidence to describe its application, methodology, research questions, or enabling features in any particular research area.
LAION-5B	https://doi.org/10.1145/3592573.3593102 (2023)	https://doi.org/10.48550/arXiv.2210.08402 (2022)	The LAION-5B dataset is primarily used to train the OpenCLIP model, focusing on capturing semantic relationships between images and text. This supports the development of advanced multi-modal reasoning models, enabling more sophisticated image-text understanding. The dataset's large scale and diverse content are crucial for training robust models that can handle complex reasoning tasks.
VTKB	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TMM.2019.2937181 (2020)	The VTKB dataset is used to construct a visio-textual knowledge base that links hierarchical concepts to images through embedding similarities. This enhances image tagging quality by leveraging both visual and textual data, improving the accuracy and relevance of tags assigned to images.
NYTimes800k	https://doi.org/10.1109/TMM.2023.3301279 (2021)	https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd (2015)	The NYTimes800k dataset is used to collect and annotate images, captions, and news articles from the New York Times. It focuses on providing ground-truth captions for images, enabling research in image captioning and multimodal content analysis. The dataset's rich annotations facilitate the development and evaluation of models that generate accurate and contextually relevant captions for images.
AWA2	https://doi.org/10.1109/ICTAI56018.2022.00064 (2022)	https://doi.org/10.1109/CVPR.2019.01175 (2018)	The AWA2 dataset is used to evaluate zero-shot learning models, particularly focusing on the performance of implicit and explicit correlation methods in knowledge graph propagation. It enables researchers to assess how well these models can predict unseen classes by leveraging existing knowledge. The dataset's characteristics support the evaluation of model accuracy in propagating knowledge across different categories.
DB-Img-Few	https://doi.org/10.1109/ICDE60146.2024.00061 (2024)	https://doi.org/10.1007/978-3-030-21348-0_30 (2019)	The DB-Img-Few dataset is used to provide image embeddings for multi-modal knowledge graph reasoning, specifically in few-shot learning scenarios. This dataset enables researchers to integrate visual information into knowledge graphs, enhancing reasoning capabilities with limited data. The focus on few-shot learning highlights its utility in scenarios where labeled data is scarce.
unearthed oracle bones’ photos	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/J.COMPELECENG.2021.107173 (2021)	The 'unearthed oracle bones’ photos' dataset is used to construct a multi-modal knowledge graph for oracle bone recognition. This involves integrating visual and textual data to enhance information processing. The dataset enables researchers to develop and refine methods for recognizing and interpreting oracle bone inscriptions, leveraging both image and text data to improve accuracy and understanding.
VG dataset	https://doi.org/10.3233/sw-233510 (2023)	https://doi.org/10.1007/978-3-030-77385-4_41 (2020)	The VG dataset is used to substitute predicates in new triplets for the CSKG, focusing on the most common predicate between nodes in the original dataset. This approach aids in enhancing the knowledge graph by incorporating more frequent and relevant relationships, thereby improving the accuracy and utility of the graph for various applications.
Oracle Bone Inscriptions	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1016/J.COMPELECENG.2021.107173 (2021)	The Oracle Bone Inscriptions dataset is used to detect and recognize oracle bones by integrating visual features with relevant literature, location, and institutional data. This integration enhances decision-making processes, enabling researchers to more accurately identify and contextualize these ancient artifacts. The dataset's multi-modal nature, combining visual and textual information, supports robust recognition and analysis.
Wikipedia articles and images	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1109/TMM.2023.3301279 (2021)	The Wikipedia articles and images dataset is used to pre-train a cross-modal entity matching module, which focuses on aligning textual and visual scene graphs extracted from the articles and images. This alignment process enhances the model's ability to understand and match entities across different modalities, supporting research in cross-modal entity recognition and alignment.
CC3M	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The CC3M dataset is used to support event recognition and relation extraction in multi-modal knowledge graphs. It provides visual data that enhances the ability to extract and recognize events and their relationships, contributing to more accurate and contextually rich knowledge graphs. This dataset enables researchers to integrate visual and textual information effectively, improving the overall performance of multi-modal reasoning systems.
BookCorpus	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The BookCorpus dataset is used to enhance event knowledge graphs by providing textual data that offers context for event relations. This textual information is integrated into the knowledge graph to enrich the representation and understanding of events, thereby improving the accuracy and depth of event-related research. The dataset's extensive textual content is crucial for contextualizing and linking events within the graph.
CC12M	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The CC12M dataset is used to support event recognition and relation extraction in multi-modal knowledge graphs. It provides visual data that enhances the accuracy and richness of these tasks, enabling researchers to build more comprehensive and contextually aware knowledge graphs. This dataset's visual content is crucial for integrating and reasoning about complex, multi-source information.
C4(news)	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.18653/v1/2022.acl-demo.23 (2022)	The C4(news) dataset is used to enrich event knowledge graphs with current textual news data, incorporating new events and relations. This methodology involves updating knowledge graphs dynamically to reflect recent developments, enhancing their relevance and accuracy. The dataset's timely and diverse content supports research in maintaining up-to-date knowledge bases, particularly in areas requiring real-time information.
Cyc	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	The Cyc dataset is primarily listed as an example of a knowledge graph that covers common sense knowledge. It is not utilized in specific research contexts or methodologies, nor is it directly involved in addressing particular research questions or applications. Its main relevance lies in its representation of common sense knowledge, which can inform the design and evaluation of other knowledge graphs and reasoning systems.
BabelNet	https://doi.org/10.1109/TKDE.2022.3224228 (2022)	https://doi.org/10.1023/B:BTTJ.0000047600.45421.6D (2004)	BabelNet is primarily listed as an example of a knowledge graph that covers lexical knowledge. It is not used in specific research contexts or methodologies related to multi-modal Knowledge Graph Reasoning. The dataset's comprehensive lexical coverage makes it a reference point in discussions about knowledge graphs, but its direct application in research is not specified in the provided descriptions.
Fashion-MMKG	https://doi.org/10.18653/v1/2023.acl-industry.16 (2023)	https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 (2021)	The Fashion-MMKG dataset is used to enhance e-commerce image-text retrieval by incorporating cross-modal fashion knowledge into the training of a CLIP-style model. This approach improves the model's understanding of fashion-related multimodal data, enabling more accurate and contextually relevant search results. The dataset's focus on fashion-specific multimodal information is crucial for this application.
Houston dataset	https://doi.org/10.1080/01431161.2023.2240032 (2023)	https://doi.org/10.1016/J.ISPRSJPRS.2021.05.011 (2021)	The Houston dataset is used for multimodal remote sensing, focusing on land cover classification by integrating Hyperspectral Imaging (HSI) and Multispectral Imaging (MSI) data. Researchers employ shared and specific feature learning models to enhance classification accuracy, leveraging the dataset's detailed spectral and spatial information. This approach addresses the challenge of accurately classifying diverse land cover types in urban environments.
