{
  "results": [
    {
      "context_text": "Among the models in the first type, there are five unimodal KGR state-of-the-art models, including TransE [4], DisMult [48], Com-plEX [41], ConvE [11], and RGCN [34], and three multi-modal state-of-the-art KGR models, including IKRL [46], TransAE [44] and RSME [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The citation intent is to reference existing models for comparison.",
      "processing_time": 18.993420362472534,
      "citing_paper_id": "273164062",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "Among the models in the first type, there are five unimodal KGR state-of-the-art models, including TransE [4], DisMult [48], Com-plEX [41], ConvE [11], and RGCN [34], and three multi-modal state-of-the-art KGR models, including IKRL [46], TransAE [44] and RSME [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The citation intent is to reference existing models for comparison.",
      "processing_time": 18.993420362472534,
      "citing_paper_id": "273164062",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "b l i s o r As an attempt at pre-trained transformer models for multi-modal knowledge graph reasoning, we are the first to work to integrate the structural information underlying the MKGs. h We emphasize the attributes of SGMPT to highlight our contributions and novelties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the integration of structural information in multi-modal knowledge graphs.",
      "processing_time": 16.024494409561157,
      "citing_paper_id": "273164062",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "b l i s o r As an attempt at pre-trained transformer models for multi-modal knowledge graph reasoning, we are the first to work to integrate the structural information underlying the MKGs. h We emphasize the attributes of SGMPT to highlight our contributions and novelties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the integration of structural information in multi-modal knowledge graphs.",
      "processing_time": 16.024494409561157,
      "citing_paper_id": "273164062",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "We first discuss the complexities of these parts, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the complexities of certain parts without providing any verifiable resources.",
      "processing_time": 21.383636474609375,
      "citing_paper_id": "273164062",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "‚Ä¶two commonly used transformers for encoding M ùë° and M ùë£ , and structure-guided fusion module (S-Guider) is a novel module designed in our SGMPT to leverage G . k i i o ùë° , M ùë£ , and G represent the textual information, visual information, and graph structural information, respectively. n g n .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of a model. The context is focused on describing the architecture and modules of a system rather than the datasets used.",
      "processing_time": 21.004755973815918,
      "citing_paper_id": "273164062",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "Specifically, compared to traditional KGs, extra multi-modal data are linked with entities in MKGs to provide more meaningful information, such as visual and textual attributes, which makes them closer to the real world.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of multi-modal knowledge graphs. No verifiable resources are named.",
      "processing_time": 18.62287163734436,
      "citing_paper_id": "273164062",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "Specifically, compared to traditional KGs, extra multi-modal data are linked with entities in MKGs to provide more meaningful information, such as visual and textual attributes, which makes them closer to the real world.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of multi-modal knowledge graphs. No verifiable resources are named.",
      "processing_time": 18.62287163734436,
      "citing_paper_id": "273164062",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "Specifically, compared to traditional KGs, extra multi-modal data are linked with entities in MKGs to provide more meaningful information, such as visual and textual attributes, which makes them closer to the real world.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of multi-modal knowledge graphs. No verifiable resources are named.",
      "processing_time": 18.62287163734436,
      "citing_paper_id": "273164062",
      "cited_paper_id": 201103729
    },
    {
      "context_text": "Specifically, compared to traditional KGs, extra multi-modal data are linked with entities in MKGs to provide more meaningful information, such as visual and textual attributes, which makes them closer to the real world.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of multi-modal knowledge graphs. No verifiable resources are named.",
      "processing_time": 18.62287163734436,
      "citing_paper_id": "273164062",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "‚Ä¶either independently or composed . n u o t f Inspired by multi-modal information fusion strategies for other tasks, we design two fusion strategies, i.e., weighted summation and alignment constraint, to fuse the extra structural information into the original pre-trained transformer models for MKGR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only fusion strategies and pre-trained transformer models. No verifiable resources are identified.",
      "processing_time": 17.85524320602417,
      "citing_paper_id": "273164062",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "‚Ä¶either independently or composed . n u o t f Inspired by multi-modal information fusion strategies for other tasks, we design two fusion strategies, i.e., weighted summation and alignment constraint, to fuse the extra structural information into the original pre-trained transformer models for MKGR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only fusion strategies and pre-trained transformer models. No verifiable resources are identified.",
      "processing_time": 17.85524320602417,
      "citing_paper_id": "273164062",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "‚Ä¶either independently or composed . n u o t f Inspired by multi-modal information fusion strategies for other tasks, we design two fusion strategies, i.e., weighted summation and alignment constraint, to fuse the extra structural information into the original pre-trained transformer models for MKGR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only fusion strategies and pre-trained transformer models. No verifiable resources are identified.",
      "processing_time": 17.85524320602417,
      "citing_paper_id": "273164062",
      "cited_paper_id": null
    },
    {
      "context_text": "M i Note that ùëí and ùëü denote entity and relation separately.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general notation for entities and relations in knowledge graphs.",
      "processing_time": 16.006173849105835,
      "citing_paper_id": "273164062",
      "cited_paper_id": 220496043
    },
    {
      "context_text": "M i Note that ùëí and ùëü denote entity and relation separately.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general notation for entities and relations in knowledge graphs.",
      "processing_time": 16.006173849105835,
      "citing_paper_id": "273164062",
      "cited_paper_id": 259145427
    },
    {
      "context_text": "M i Note that ùëí and ùëü denote entity and relation separately.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general notation for entities and relations in knowledge graphs.",
      "processing_time": 16.006173849105835,
      "citing_paper_id": "273164062",
      "cited_paper_id": 267334765
    },
    {
      "context_text": "M i Note that ùëí and ùëü denote entity and relation separately.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general notation for entities and relations in knowledge graphs.",
      "processing_time": 16.006173849105835,
      "citing_paper_id": "273164062",
      "cited_paper_id": null
    },
    {
      "context_text": "M i Note that ùëí and ùëü denote entity and relation separately.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general notation for entities and relations in knowledge graphs.",
      "processing_time": 16.006173849105835,
      "citing_paper_id": "273164062",
      "cited_paper_id": null
    },
    {
      "context_text": "M i Note that ùëí and ùëü denote entity and relation separately.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general notation for entities and relations in knowledge graphs.",
      "processing_time": 16.006173849105835,
      "citing_paper_id": "273164062",
      "cited_paper_id": null
    },
    {
      "context_text": ") , the main goal of the fine-tuning procedure is to predict the masked target entity ùëí ùë° .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a general fine-tuning procedure.",
      "processing_time": 30.993586540222168,
      "citing_paper_id": "273164062",
      "cited_paper_id": 220496043
    },
    {
      "context_text": "Multi-modal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit many practical down-stream tasks, such as recommendation systems [20, 52], information retrieval [13, 49], and visual question answering [21].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of multi-modal knowledge graphs. No verifiable resources are identified.",
      "processing_time": 17.568185806274414,
      "citing_paper_id": "273164062",
      "cited_paper_id": 235436185
    },
    {
      "context_text": "i s Multi-modal knowledge graph reasoning (MKGR) aims to infer the potential missing facts in multi-modal knowledge graphs, which can be roughly divided into two types, i.e., non-transformer models and transformer models, according to the model architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graph reasoning and model types.",
      "processing_time": 18.314183712005615,
      "citing_paper_id": "273164062",
      "cited_paper_id": 236957384
    },
    {
      "context_text": "i s Multi-modal knowledge graph reasoning (MKGR) aims to infer the potential missing facts in multi-modal knowledge graphs, which can be roughly divided into two types, i.e., non-transformer models and transformer models, according to the model architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graph reasoning and model types.",
      "processing_time": 18.314183712005615,
      "citing_paper_id": "273164062",
      "cited_paper_id": 259075619
    },
    {
      "context_text": "i s Multi-modal knowledge graph reasoning (MKGR) aims to infer the potential missing facts in multi-modal knowledge graphs, which can be roughly divided into two types, i.e., non-transformer models and transformer models, according to the model architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graph reasoning and model types.",
      "processing_time": 18.314183712005615,
      "citing_paper_id": "273164062",
      "cited_paper_id": 259129314
    },
    {
      "context_text": "i s Multi-modal knowledge graph reasoning (MKGR) aims to infer the potential missing facts in multi-modal knowledge graphs, which can be roughly divided into two types, i.e., non-transformer models and transformer models, according to the model architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graph reasoning and model types.",
      "processing_time": 18.314183712005615,
      "citing_paper_id": "273164062",
      "cited_paper_id": 272655172
    },
    {
      "context_text": "i s Multi-modal knowledge graph reasoning (MKGR) aims to infer the potential missing facts in multi-modal knowledge graphs, which can be roughly divided into two types, i.e., non-transformer models and transformer models, according to the model architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graph reasoning and model types.",
      "processing_time": 18.314183712005615,
      "citing_paper_id": "273164062",
      "cited_paper_id": null
    },
    {
      "context_text": "The MKGformer [7], which is the most representative MPT model for KGR, is selected as our MPT backbone. and outputs the textual feature H . n It takes images M as input, and outputs the visual feature H . p u Vision encoder ùëì ùë£ (¬∑) is composed of the first ùêø ùë£ layers of ViT [14], which aims to capture basic visual features from the patched images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the components of the MKGformer model and its vision encoder.",
      "processing_time": 20.929540634155273,
      "citing_paper_id": "273164062",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "Following [7], multi-modal information encoder ùëì ùëö (¬∑) aims to model multi-modal features of the entity across the last ùêø ùëö layers of ViT and BERT with multi-level fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodology of multi-modal feature modeling using ViT and BERT.",
      "processing_time": 32.87796425819397,
      "citing_paper_id": "273164062",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "Following it, we adopt the BERT base [12] and ViT-B/32 [14] as the text encoder and vision encoder in SGMPT.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (BERT and ViT-B/32). The context is about using these models as encoders in a system called SGMPT.",
      "processing_time": 22.609341859817505,
      "citing_paper_id": "273164062",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "o r b u t Besides, BERT [12] and ViT [14] are two commonly used transformers for encoding M ùë° and M ùë£ , and structure-guided fusion module (S-Guider) is a novel module designed in our SGMPT to leverage G . k i i o ùë° , M ùë£ , and G represent the textual information, visual information, and graph structural information, respectively. n g n .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of BERT and ViT for encoding textual and visual information, and the introduction of a structure-guided fusion module.",
      "processing_time": 22.793436765670776,
      "citing_paper_id": "273164062",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "o r b u t Besides, BERT [12] and ViT [14] are two commonly used transformers for encoding M ùë° and M ùë£ , and structure-guided fusion module (S-Guider) is a novel module designed in our SGMPT to leverage G . k i i o ùë° , M ùë£ , and G represent the textual information, visual information, and graph‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of BERT and ViT for encoding textual and visual information, and the introduction of a structure-guided fusion module.",
      "processing_time": 23.564851760864258,
      "citing_paper_id": "273164062",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "The incompleteness issues, as a common nature among different types of KGs, have been widely studied these years [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue in knowledge graphs.",
      "processing_time": 14.775416374206543,
      "citing_paper_id": "273164062",
      "cited_paper_id": null
    },
    {
      "context_text": "(2) Lack of reward balancing mechanism to prevent reasoning models from repeatedly grabbing local rewards while ignoring the ultimate goal [43], [51].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a lack of a reward balancing mechanism in reinforcement learning models.",
      "processing_time": 18.547488927841187,
      "citing_paper_id": "252089825",
      "cited_paper_id": 1219941
    },
    {
      "context_text": "(2) Lack of reward balancing mechanism to prevent reasoning models from repeatedly grabbing local rewards while ignoring the ultimate goal [43], [51].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a lack of a reward balancing mechanism in reinforcement learning models.",
      "processing_time": 18.547488927841187,
      "citing_paper_id": "252089825",
      "cited_paper_id": 235412627
    },
    {
      "context_text": "Note that, Wang et al. have proved that the performance of TransAE on MKGs is better than that of the most traditional KG reasoning methods, such as TransE, RESCAL [39], Com-plEx [52], HolE [38], and DistMult [69].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance of TransAE with other traditional KG reasoning methods.",
      "processing_time": 20.37222671508789,
      "citing_paper_id": "252089825",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "The limitation of this method is that the multi-modal noise affects the extraction of key features [14].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a limitation of a method but does not specify any dataset. The context does not provide enough information to identify a specific, verifiable dataset.",
      "processing_time": 17.809140920639038,
      "citing_paper_id": "252089825",
      "cited_paper_id": 2840197
    },
    {
      "context_text": "A group of KG reasoning studies on traditional KGs aim at inferring missing elements by embedding-based methods [48] [3] and deep learning [9] [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches for KG reasoning. No verifiable resources are identified.",
      "processing_time": 18.95497965812683,
      "citing_paper_id": "252089825",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "A group of KG reasoning studies on traditional KGs aim at inferring missing elements by embedding-based methods [48] [3] and deep learning [9] [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches for KG reasoning. No verifiable resources are identified.",
      "processing_time": 18.95497965812683,
      "citing_paper_id": "252089825",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "A group of KG reasoning studies on traditional KGs aim at inferring missing elements by embedding-based methods [48] [3] and deep learning [9] [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches for KG reasoning. No verifiable resources are identified.",
      "processing_time": 18.95497965812683,
      "citing_paper_id": "252089825",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "A group of KG reasoning studies on traditional KGs aim at inferring missing elements by embedding-based methods [48] [3] and deep learning [9] [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches for KG reasoning. No verifiable resources are identified.",
      "processing_time": 18.95497965812683,
      "citing_paper_id": "252089825",
      "cited_paper_id": 213318844
    },
    {
      "context_text": "To alleviate the problem, we use the reward shaping trick [27] to design the reward when e d is not reached, where l is a score function using ConvE [9] and is used to evaluate the probability over ( e s , r q , e T ).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ConvE, which is a method for evaluating probabilities in knowledge graphs, but does not refer to it as a dataset. No other datasets are mentioned.",
      "processing_time": 33.40303587913513,
      "citing_paper_id": "252089825",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "Since KGs are inherently incomplete, KG reasoning technology that can synthesize the original knowledge and infer the missing knowledge is particularly important [18] [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of KG reasoning technology for synthesizing and inferring knowledge in incomplete KGs.",
      "processing_time": 30.923519372940063,
      "citing_paper_id": "252089825",
      "cited_paper_id": 5471519
    },
    {
      "context_text": "Thus, some multi-modal studies [71] [30] [65] adopt conventional attention model to extract important features of auxiliary modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of conventional attention models in multi-modal studies. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 19.651158094406128,
      "citing_paper_id": "252089825",
      "cited_paper_id": 8849206
    },
    {
      "context_text": "Thus, some multi-modal studies [71] [30] [65] adopt conventional attention model to extract important features of auxiliary modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of conventional attention models in multi-modal studies. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 19.651158094406128,
      "citing_paper_id": "252089825",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Thus, some multi-modal studies [71] [30] [65] adopt conventional attention model to extract important features of auxiliary modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of conventional attention models in multi-modal studies. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 19.651158094406128,
      "citing_paper_id": "252089825",
      "cited_paper_id": 18347865
    },
    {
      "context_text": "‚Ä¶in Table I, our MMKGR Ô¨Ålls the gap for multi-hop reasoning on MKGs. Focusing on single-hop reasoning on MKGs, some studies employ the conventional attention model or concatenation to fuse multi-modal features and then adopt TransE to infer missing elements, such as IKRL [65] [63] and TransAE [61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the methodologies used for multi-modal knowledge graph reasoning.",
      "processing_time": 18.057352781295776,
      "citing_paper_id": "252089825",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "‚Ä¶completes intra-modal and inter-modal attention interactions with noise robustness, which is technically different from both existing KG multi-modal data modelling methods [64] [65] [61] [45] [50] and general multi-modal data modelling approaches [24] [54] that are not limited to KG area.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 18.51874279975891,
      "citing_paper_id": "252089825",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "‚Ä¶completes intra-modal and inter-modal attention interactions with noise robustness, which is technically different from both existing KG multi-modal data modelling methods [64] [65] [61] [45] [50] and general multi-modal data modelling approaches [24] [54] that are not limited to KG area.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 18.51874279975891,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Different from these methods, some reasoning models [64] [65] [61] [45] [53] [50] have been proposed to integrate the multi-modal knowledge on MKGs, but they are based on the model TransE [3] that focuses on completing the single-hop reasoning only.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions reasoning models that integrate multi-modal knowledge on MKGs, but does not specify any datasets. It refers to TransE, which is a method, not a dataset.",
      "processing_time": 22.570772647857666,
      "citing_paper_id": "252089825",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Different from these methods, some reasoning models [64] [65] [61] [45] [53] [50] have been proposed to integrate the multi-modal knowledge on MKGs, but they are based on the model TransE [3] that focuses on completing the single-hop reasoning only.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions reasoning models that integrate multi-modal knowledge on MKGs, but does not specify any datasets. It refers to TransE, which is a method, not a dataset.",
      "processing_time": 22.570772647857666,
      "citing_paper_id": "252089825",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Different from these methods, some reasoning models [64] [65] [61] [45] [53] [50] have been proposed to integrate the multi-modal knowledge on MKGs, but they are based on the model TransE [3] that focuses on completing the single-hop reasoning only.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions reasoning models that integrate multi-modal knowledge on MKGs, but does not specify any datasets. It refers to TransE, which is a method, not a dataset.",
      "processing_time": 22.570772647857666,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "This is because the introduction of multi-modal auxiliary data further exacerbates the sparse reward problem, which leads to the decision bias of reinforcement learning [5], [15], [35], [76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the issue of sparse rewards in reinforcement learning with multi-modal data.",
      "processing_time": 18.898146152496338,
      "citing_paper_id": "252089825",
      "cited_paper_id": 13694791
    },
    {
      "context_text": "This is because the introduction of multi-modal auxiliary data further exacerbates the sparse reward problem, which leads to the decision bias of reinforcement learning [5], [15], [35], [76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the issue of sparse rewards in reinforcement learning with multi-modal data.",
      "processing_time": 18.898146152496338,
      "citing_paper_id": "252089825",
      "cited_paper_id": 262380035
    },
    {
      "context_text": "‚Ä¶details: (1) redundant and irrelevant features can impair the generalization and robustness of the model [25]; (2) only self-attention or co-attention is considered in the same training stage, which limits the utilization of samples and the effective extraction of Ô¨Åne-grained features [54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of attention mechanisms in models.",
      "processing_time": 16.140631437301636,
      "citing_paper_id": "252089825",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Secondly, existing multi-modal reasoning methods ignore intra-modal interactions that have been shown to be the key of Ô¨Åne-grained learning for many unimodal learning tasks [54], [60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their limitations.",
      "processing_time": 15.046024322509766,
      "citing_paper_id": "252089825",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Notably, this network is mainly inspired by (1) the signiÔ¨Åcant impact of intra-modal attention interaction on Ô¨Åne-grained features [54], and (2) gate network which effectively Ô¨Ålters noise [77].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and concepts. The context focuses on the impact of intra-modal attention and gate networks, which are methodological aspects.",
      "processing_time": 21.726727724075317,
      "citing_paper_id": "252089825",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "To address the problem of insufÔ¨Åcient multi-modal interactions, MCAN [73] and PSAC [24] apply self-attention mechanism [54] and co-attention to complete the intra-modal and inter-modal attention interactions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the application of self-attention and co-attention mechanisms.",
      "processing_time": 18.496562957763672,
      "citing_paper_id": "252089825",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Relation triplets in knowledge graphs are structured [3], and the corresponding structural features are different from the features learned from auxiliary data (i.e., texts and images).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about knowledge graphs and their structural features.",
      "processing_time": 27.539056539535522,
      "citing_paper_id": "252089825",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "For example, the embedding-based TransE [3] learns vector representations of entities and relations by minimizing the heuristic self-supervised loss functions and then the learned vectors are used to predict the probability of correct triplets.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (TransE) for learning vector representations in multi-relational data.",
      "processing_time": 17.480862379074097,
      "citing_paper_id": "252089825",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "2) Evaluation Protocol: Entity link prediction [3] and relation link prediction [40] are used to evaluate the performance of MMKGR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation methods for entity and relation link prediction. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 22.296021938323975,
      "citing_paper_id": "252089825",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "2) Evaluation Protocol: Entity link prediction [3] and relation link prediction [40] are used to evaluate the performance of MMKGR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation methods for entity and relation link prediction. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 22.296021938323975,
      "citing_paper_id": "252089825",
      "cited_paper_id": 233407536
    },
    {
      "context_text": "M 1) Feature Extraction: (1) Structural features with d s dimensions are initialized from all entities and relations by using the TransE algorithm [3].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the TransE algorithm but does not refer to a specific dataset. The citation is used to describe a method for initializing structural features.",
      "processing_time": 18.46292996406555,
      "citing_paper_id": "252089825",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Here, we set the number of epochs E ‚àà [10, 30, 50, 70, 90, 110] and the size of batches N ‚àà [16, 32, 64, 128, 256, 512].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hyperparameters. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 20.306885242462158,
      "citing_paper_id": "252089825",
      "cited_paper_id": 18347865
    },
    {
      "context_text": "Here, we set the number of epochs E ‚àà [10, 30, 50, 70, 90, 110] and the size of batches N ‚àà [16, 32, 64, 128, 256, 512].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hyperparameters. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 20.306885242462158,
      "citing_paper_id": "252089825",
      "cited_paper_id": 201070367
    },
    {
      "context_text": "Here, we set the number of epochs E ‚àà [10, 30, 50, 70, 90, 110] and the size of batches N ‚àà [16, 32, 64, 128, 256, 512].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hyperparameters. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 20.306885242462158,
      "citing_paper_id": "252089825",
      "cited_paper_id": 201698166
    },
    {
      "context_text": "In addition, MTRL [45] with the state-of-the-art performance performs single-hop reasoning by concatenating the features of relation triplets and multi-modal auxiliary features that comprehensively contain textual and visual features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MTRL) and its performance. The context focuses on the methodology and performance rather than a specific dataset.",
      "processing_time": 33.3656702041626,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "MKGs are proposed to integrate multi-modal data in KGs [41] [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of integrating multi-modal data into knowledge graphs.",
      "processing_time": 16.64040756225586,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "MKGs are proposed to integrate multi-modal data in KGs [41] [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of integrating multi-modal data into knowledge graphs.",
      "processing_time": 16.64040756225586,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "The embedding dimension d s of entity, relation and history is set to 200, the embedding dimension d i of image feature is set to 128 and 4096 on FB-IMG-TXT and WN9-IMG-TXT respectively, and the embedding dimension d t of textual feature is 1000 [45].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-IMG-TXT",
        "WN9-IMG-TXT"
      ],
      "dataset_descriptions": {
        "FB-IMG-TXT": "Used to set the embedding dimension for image features to 128, focusing on multi-modal knowledge graph representation learning.",
        "WN9-IMG-TXT": "Used to set the embedding dimension for image features to 4096, focusing on multi-modal knowledge graph representation learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FB-IMG-TXT and WN9-IMG-TXT, which are used to set the embedding dimensions for image features. These datasets are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 36.52278995513916,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "To extract the image features of the entities, 10 images and 100 images are crawled for each entity in WN9-IMG-TXT and FB-IMG-TXT, respectively [45].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG-TXT",
        "FB-IMG-TXT"
      ],
      "dataset_descriptions": {
        "WN9-IMG-TXT": "Used to crawl 10 images per entity for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information.",
        "FB-IMG-TXT": "Used to crawl 100 images per entity for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, WN9-IMG-TXT and FB-IMG-TXT, which are used to crawl images for entities. These datasets are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 37.00657415390015,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "To investigate the performance of MMKGR, two categories of methods are compared: 1) single-hop MKG reasoning meth-ods MTRL [45]; 2) multi-hop reasoning methods MINERVA [8], FIRE [75], GAATs [59], NeuraILP [70] and RLH [56] in traditional KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.72816514968872,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "To investigate the performance of MMKGR, two categories of methods are compared: 1) single-hop MKG reasoning meth-ods MTRL [45]; 2) multi-hop reasoning methods MINERVA [8], FIRE [75], GAATs [59], NeuraILP [70] and RLH [56] in traditional KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.72816514968872,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "To investigate the performance of MMKGR, two categories of methods are compared: 1) single-hop MKG reasoning meth-ods MTRL [45]; 2) multi-hop reasoning methods MINERVA [8], FIRE [75], GAATs [59], NeuraILP [70] and RLH [56] in traditional KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 17.72816514968872,
      "citing_paper_id": "252089825",
      "cited_paper_id": 226283804
    },
    {
      "context_text": "The essential reason is that the dataset FB-IMG-TXT is more sparse and complex than the dataset WN9-IMG-TXT [45] as shown in Table II.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-IMG-TXT",
        "WN9-IMG-TXT"
      ],
      "dataset_descriptions": {
        "FB-IMG-TXT": "Used to compare sparsity and complexity in multimodal knowledge graph representation learning, highlighting challenges in handling sparse and complex data.",
        "WN9-IMG-TXT": "Used as a benchmark to compare against FB-IMG-TXT, demonstrating differences in sparsity and complexity in multimodal knowledge graph representation learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, FB-IMG-TXT and WN9-IMG-TXT, which are used to compare sparsity and complexity in the context of multimodal knowledge graph representation learning.",
      "processing_time": 35.62863516807556,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Both of them are MKGs widely adopted by existing reasoning studies [63] [61] [45].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide specific dataset names, only mentions 'MKGs' which are likely Multi-modal Knowledge Graphs. No clear, verifiable datasets are named.",
      "processing_time": 22.062157154083252,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Both of them are MKGs widely adopted by existing reasoning studies [63] [61] [45].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide specific dataset names, only mentions 'MKGs' which are likely Multi-modal Knowledge Graphs. No clear, verifiable datasets are named.",
      "processing_time": 22.062157154083252,
      "citing_paper_id": "252089825",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "To expand the auxiliary data with one modality, two datasets WN9-IMG-TXT and FB-IMG-TXT simultaneously add a number of textual descriptions and images to each entity, aiming to further enhance the data diversity of the MKGs [45].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG-TXT",
        "FB-IMG-TXT"
      ],
      "dataset_descriptions": {
        "WN9-IMG-TXT": "Used to enhance multimodal knowledge graphs by adding textual descriptions and images to entities, increasing data diversity.",
        "FB-IMG-TXT": "Used to enhance multimodal knowledge graphs by adding textual descriptions and images to entities, increasing data diversity."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, WN9-IMG-TXT and FB-IMG-TXT, which are used to enhance the data diversity of multimodal knowledge graphs by adding textual descriptions and images.",
      "processing_time": 33.88072752952576,
      "citing_paper_id": "252089825",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "The existing KG reasoning methods based on RL are not suitable to be directly applied to reasoning in MKGs due to the dilemma of sparse rewards [27].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the limitations of existing KG reasoning methods using RL, particularly the issue of sparse rewards in multi-hop reasoning. No specific dataset is mentioned.",
      "processing_time": 19.570865869522095,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52143467
    },
    {
      "context_text": "In fact, most RL-based KG reasoning methods have suffered from the problem of sparse reward on traditional KGs, which is manifested in the lack of feedback rewards and blind reasoning in most states [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses a common issue in reinforcement learning-based knowledge graph reasoning methods, specifically the problem of sparse rewards. No specific dataset is mentioned.",
      "processing_time": 17.707868099212646,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52143467
    },
    {
      "context_text": "It has been proven that RL-based multi-hop KG reasoning models have not only semantic explainability, but also higher reasoning performance than single-hop reasoning models [27] [18] [81], which mo-",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance of RL-based multi-hop KG reasoning models compared to single-hop models.",
      "processing_time": 20.816566705703735,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52143467
    },
    {
      "context_text": "It has been proven that RL-based multi-hop KG reasoning models have not only semantic explainability, but also higher reasoning performance than single-hop reasoning models [27] [18] [81], which mo-",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance of RL-based multi-hop KG reasoning models compared to single-hop models.",
      "processing_time": 20.816566705703735,
      "citing_paper_id": "252089825",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "To alleviate the problem, we employ a shaping method [27] to design our destination reward when ed is not reached,",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for reward shaping in knowledge graph reasoning.",
      "processing_time": 16.604185819625854,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52143467
    },
    {
      "context_text": "(1) Mean reciprocal rank (MRR) of all correct entities and (2) the proportion of correct entities that rank no larger than N (Hits@N) [27], [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics (MRR and Hits@N).",
      "processing_time": 15.84289002418518,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52143467
    },
    {
      "context_text": "(1) Mean reciprocal rank (MRR) of all correct entities and (2) the proportion of correct entities that rank no larger than N (Hits@N) [27], [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics (MRR and Hits@N).",
      "processing_time": 15.84289002418518,
      "citing_paper_id": "252089825",
      "cited_paper_id": 201698166
    },
    {
      "context_text": "Similar studies [28] [41] expand the existing KGs WN-9 and FB15K respectively, only adding images for each entity to further explain them.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN-9",
        "FB15K"
      ],
      "dataset_descriptions": {
        "WN-9": "Used to expand the knowledge graph by adding images to entities, enhancing multimodal reasoning capabilities.",
        "FB15K": "Used to expand the knowledge graph by adding images to entities, enhancing multimodal reasoning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions expanding existing KGs WN-9 and FB15K by adding images, which aligns with multi-modal knowledge graph reasoning.",
      "processing_time": 29.462594747543335,
      "citing_paper_id": "252089825",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "Sigmoid SpeciÔ¨Åcally, redundant features are either shifted versions of the features related to the triple query or very similar with little or no variations [1], which can amplify the negative effects of noise [22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses features and their effects on noise. No verifiable resources are identified.",
      "processing_time": 29.459181785583496,
      "citing_paper_id": "252089825",
      "cited_paper_id": 73425227
    },
    {
      "context_text": "The attended features VÃÇ obtained by attention-fusion module may contain irrelevant features [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model related to image captioning.",
      "processing_time": 18.174451112747192,
      "citing_paper_id": "252089825",
      "cited_paper_id": 201070367
    },
    {
      "context_text": "Secondly, although multi-modal features are not used, some RL-based methods (e.g., RLH) and the graph neural network-based reasoning method (i.e., GAATs) exceed MTRL that uses multi-modal features in overall performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance comparisons.",
      "processing_time": 16.33056879043579,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "Then, existing multi-hop reasoning methods (e.g. GAATs) are we combined to conduct multi-hop reasoning on the MKG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 18.410046339035034,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": ", rules-based NeuralLP [70] and graph attention networks-based GAATs [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 16.31423592567444,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "To investigate the performance of MMKGR, two categories of methods are compared: 1) single-hop MKG reasoning methods MTRL [45]; 2) multi-hop reasoning methods MINERVA [8], FIRE [75], GAATs [59], NeuraILP [70] and RLH [56] in traditional KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing different reasoning methods in knowledge graphs.",
      "processing_time": 18.133457899093628,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "Although non-RL methods (e.g., GAATs) are not affected by the sparse rewards, the Hits@1 still declines compared with the absence of multi-modal data.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between non-RL methods and the impact of multi-modal data on performance metrics.",
      "processing_time": 31.95953893661499,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "In addition, there are other multi-hop methods, e.g., rules-based NeuralLP [70] and graph attention networks-based GAATs [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about comparing different multi-hop reasoning methods in knowledge graphs.",
      "processing_time": 20.224817037582397,
      "citing_paper_id": "252089825",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "nology that can synthesize the original knowledge and infer the missing knowledge is particularly important [18] [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about knowledge graphs.",
      "processing_time": 15.576690435409546,
      "citing_paper_id": "252089825",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "At present, the actual internet data show multi-modal characteristics [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to 'internet data' in a general sense, which is too vague to be considered a specific dataset.",
      "processing_time": 20.474161386489868,
      "citing_paper_id": "252089825",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "Focusing on single-hop reasoning on MKGs, some studies employ the conventional attention model or concatenation to fuse multi-modal features and then adopt TransE to infer missing elements, such as IKRL [65] [63] and TransAE [61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the methodologies used for multi-modal knowledge graph reasoning.",
      "processing_time": 20.215539693832397,
      "citing_paper_id": "252089825",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "To integrate more diverse knowledge in KGs, multi-modal Knowledge graph (MKG) has been proposed [28] [63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of multi-modal Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 17.64970898628235,
      "citing_paper_id": "252089825",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "The above methods have sufficient interaction, but ignore the following details: (1) redundant and irrelevant features can impair the generalization and robustness of the model [25]; (2) only self-attention or co-attention is considered in the same training stage, which limits the utilization of samples and the effective extraction of fine-grained features [54].",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodological limitations and gaps in existing approaches.",
      "processing_time": 16.549378871917725,
      "citing_paper_id": "252089825",
      "cited_paper_id": 218563659
    },
    {
      "context_text": ", black background in images) and redundant noise (compared with the image of Rose Bukater, the image of Kate Winslet is highly similar and contains less useful information) impair the robustness and generalization of the model [4], [25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues with image data that could affect model performance. No clear, verifiable dataset names are provided.",
      "processing_time": 20.75468397140503,
      "citing_paper_id": "252089825",
      "cited_paper_id": 218563659
    },
    {
      "context_text": "1, a MKG not only contains the structural data, but also includes additional multi-modal auxiliary data (i.e., texts and images), and it is more in line with the characteristics of real-world data compared with traditional KGs [31] [49] [66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general characteristics of multi-modal knowledge graphs.",
      "processing_time": 14.913013696670532,
      "citing_paper_id": "252089825",
      "cited_paper_id": 235503675
    },
    {
      "context_text": "At present, large-scale KGs have achieved great success in assisting many applications, such as information retrieval [29], question answering [20], recommendation systems [62] [80] [72], etc.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of knowledge graphs. No dataset names are provided.",
      "processing_time": 16.273627042770386,
      "citing_paper_id": "252089825",
      "cited_paper_id": 260537889
    },
    {
      "context_text": "1) RNN-Based Model: Recurrent Neural Networks (RNNs) [219], [220], [221] are suitable for mining the changes over time, which can be generally divided into three variants, i.e., basic RNN, LSTM [222], and GRU [223].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide additional information about datasets.",
      "processing_time": 16.88738512992859,
      "citing_paper_id": "257220329",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "1) RNN-Based Model: Recurrent Neural Networks (RNNs) [219], [220], [221] are suitable for mining the changes over time, which can be generally divided into three variants, i.e., basic RNN, LSTM [222], and GRU [223].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide additional information about datasets.",
      "processing_time": 16.88738512992859,
      "citing_paper_id": "257220329",
      "cited_paper_id": 8492900
    },
    {
      "context_text": "1) RNN-Based Model: Recurrent Neural Networks (RNNs) [219], [220], [221] are suitable for mining the changes over time, which can be generally divided into three variants, i.e., basic RNN, LSTM [222], and GRU [223].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide additional information about datasets.",
      "processing_time": 16.88738512992859,
      "citing_paper_id": "257220329",
      "cited_paper_id": 254926959
    },
    {
      "context_text": "1) RNN-Based Model: Recurrent Neural Networks (RNNs) [219], [220], [221] are suitable for mining the changes over time, which can be generally divided into three variants, i.e., basic RNN, LSTM [222], and GRU [223].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide additional information about datasets.",
      "processing_time": 16.88738512992859,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "KBLRN [246] Ô¨Årst proposes an end-to-end reasoning framework, which combines neural network techniques with expert models for latent, relational, and numerical features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called KBLRN. The context focuses on the methodological aspects of combining neural networks with expert models.",
      "processing_time": 21.579713344573975,
      "citing_paper_id": "257220329",
      "cited_paper_id": 3875633
    },
    {
      "context_text": "KBLRN [172] first proposes an end-to-end reasoning framework, which combines neural network techniques with expert models for latent, relational, and numerical features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called KBLRN. The context focuses on the methodological aspects of combining neural networks with expert models.",
      "processing_time": 20.765331983566284,
      "citing_paper_id": "257220329",
      "cited_paper_id": 3875633
    },
    {
      "context_text": "2) CNN Model: To learn deeper features, convolutional neural networks (CNNs) [64], [65] are integrated with KGR models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of CNNs with KGR models. No verifiable resources are identified.",
      "processing_time": 16.876657009124756,
      "citing_paper_id": "257220329",
      "cited_paper_id": 3879949
    },
    {
      "context_text": "2) CNN Model: To learn deeper features, convolutional neural networks (CNNs) [64], [65] are integrated with KGR models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of CNNs with KGR models. No verifiable resources are identified.",
      "processing_time": 16.876657009124756,
      "citing_paper_id": "257220329",
      "cited_paper_id": 214803074
    },
    {
      "context_text": "Random walk [111], [112], [113], [114] inferences have been widely investigated in this type of model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method (random walk inferences). The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 19.10317897796631,
      "citing_paper_id": "257220329",
      "cited_paper_id": 3982237
    },
    {
      "context_text": "Random walk [111], [112], [113], [114] inferences have been widely investigated in this type of model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method (random walk inferences). The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 19.10317897796631,
      "citing_paper_id": "257220329",
      "cited_paper_id": 249626322
    },
    {
      "context_text": "Random walk [111], [112], [113], [114] inferences have been widely investigated in this type of model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method (random walk inferences). The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 19.10317897796631,
      "citing_paper_id": "254564635",
      "cited_paper_id": 249626322
    },
    {
      "context_text": "Based on the above reviews, we can further get the following observations, which may indicate the scope for different temporal KGR models and reveal the future trend in temporal KGR. (1) RNN-agnostic models, i.e., time-vector guided models and time-operation guided models, generally treat the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only discusses types of models and their general approaches. No verifiable resources are identified.",
      "processing_time": 30.502186059951782,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4090850
    },
    {
      "context_text": "Besides, NeuralLP [8] leverages attention mechanisms and auxiliary memory to optimize the gradients for mining the rules, and Neural-Num-LP [108] further integrates the cumulative sum operations and dynamic programming with NeuralLP to learn numerical rules.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 15.987108945846558,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4502993
    },
    {
      "context_text": "However, lots of rule-based models can still achieve promising performance on transductive scenarios, such as NeuralLP [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NeuralLP) and a general scenario (transductive scenarios).",
      "processing_time": 19.136436223983765,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4502993
    },
    {
      "context_text": "Therefore,variousadvancedKGRmodels[6],[7], [8],[9],[10],[11],[12],[13]havebeencontinuouslydeveloped andstudiedtheseyearsforbetterreasoningperformance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advanced KGR models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.063968896865845,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4502993
    },
    {
      "context_text": "Therefore,variousadvancedKGRmodels[6],[7], [8],[9],[10],[11],[12],[13]havebeencontinuouslydeveloped andstudiedtheseyearsforbetterreasoningperformance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advanced KGR models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.063968896865845,
      "citing_paper_id": "257220329",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Therefore,variousadvancedKGRmodels[6],[7], [8],[9],[10],[11],[12],[13]havebeencontinuouslydeveloped andstudiedtheseyearsforbetterreasoningperformance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advanced KGR models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.063968896865845,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "2021 DualE [43] Transductive Tensor Decompositional 2018 DIVA [97] Transductive Logic Rule",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The titles do not provide additional context to identify datasets.",
      "processing_time": 19.101826429367065,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4669223
    },
    {
      "context_text": "Then, a uniÔ¨Åed variational inference framework is proposed by DIVA [119], which separates multi-hop reasoning into two steps, i.e., path-Ô¨Ånding and path-reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DIVA) and its application to multi-hop reasoning in knowledge graphs.",
      "processing_time": 17.317947387695312,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4669223
    },
    {
      "context_text": "Then, a unified variational inference framework is proposed by DIVA [97], which separates multi-hop reasoning into two steps, i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DIVA) and a general approach to multi-hop reasoning. No verifiable resources are identified.",
      "processing_time": 20.427708625793457,
      "citing_paper_id": "257220329",
      "cited_paper_id": 4669223
    },
    {
      "context_text": "According to the results, we Ô¨Ånd that the prototype early models have promising time & space complexity, such as TransE [6], RGCN [74], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (TransE, RGCN). The context focuses on the performance of these models rather than the use of datasets.",
      "processing_time": 20.423248052597046,
      "citing_paper_id": "257220329",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "RGCN [74] Ô¨Årst uses the relation-speciÔ¨Åc transformation to aggregate neighborhood information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RGCN) and its application. There are no verifiable resources or datasets mentioned.",
      "processing_time": 21.102831602096558,
      "citing_paper_id": "257220329",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Thus,theexistingKGRmodels[74], [83]mainlyfocusonuni-relationalandbi-relationalfactsand evenusuallytreatmulti-relationalfactsasuni-relationalandbi-relationalfactsbyomittingsomeofthefacts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their limitations. There are no clear identifiers for datasets in the text.",
      "processing_time": 17.304243803024292,
      "citing_paper_id": "257220329",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Itbuildstherewardfunctionbasedontheembeddingsgenerated bytheRGCN[74],whichmakesthosepath-basedmodelsmore explainable.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RGCN) and a general reference to path-based models. No verifiable resources are identified.",
      "processing_time": 21.91121554374695,
      "citing_paper_id": "257220329",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Itbuildstherewardfunctionbasedontheembeddingsgenerated bytheRGCN[74],whichmakesthosepath-basedmodelsmore explainable.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RGCN) and a general reference to path-based models. No verifiable resources are identified.",
      "processing_time": 21.91121554374695,
      "citing_paper_id": "257220329",
      "cited_paper_id": 20667722
    },
    {
      "context_text": "KG2E [28] leverages Gaussian distribution covariance, and TransG [29] uses Bayesian techniques for one-to-many relational facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TransG and KG2E, which are methods/models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 18.671332359313965,
      "citing_paper_id": "257220329",
      "cited_paper_id": 6233645
    },
    {
      "context_text": "KG2E [28] leverages Gaussian distribution covariance, and TransG [29] uses Bayesian techniques for one-to-many relational facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TransG and KG2E, which are methods/models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 18.671332359313965,
      "citing_paper_id": "257220329",
      "cited_paper_id": 207228784
    },
    {
      "context_text": "Most of them have great expressive ability but suffer from explainability.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a general characteristic of certain systems.",
      "processing_time": 19.07264256477356,
      "citing_paper_id": "257220329",
      "cited_paper_id": 6911541
    },
    {
      "context_text": "Compared to them, rule-basedandpath-basedKGRmodelsaremoreexplainablebut computation-consumingandlessexpressive[305].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between rule-based and path-based KGR models. No verifiable resources are identified.",
      "processing_time": 18.684374809265137,
      "citing_paper_id": "257220329",
      "cited_paper_id": 6911541
    },
    {
      "context_text": "To name a few, Know-Evolve [213] is a classical and Ô¨Årst temporal KGR model that generates non-linear entity embeddings over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Know-Evolve' as a model, not a dataset. No specific dataset is referenced in the context.",
      "processing_time": 20.378677368164062,
      "citing_paper_id": "257220329",
      "cited_paper_id": 8040343
    },
    {
      "context_text": "‚Ä¢ NELL [229] is the knowledge base built based on NeverEnding Language Learner, which attempts to learn to read the web over time.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NELL"
      ],
      "dataset_descriptions": {
        "NELL": "Used as a knowledge base for multi-modal reasoning, focusing on integrating textual and structured data to enhance knowledge graph construction."
      },
      "confidence_score": 0.7,
      "reasoning": "NELL is mentioned as a knowledge base, but it is not a dataset in the traditional sense. It is a continuously evolving knowledge base built by a machine learning system.",
      "processing_time": 25.022725582122803,
      "citing_paper_id": "257220329",
      "cited_paper_id": 8423494
    },
    {
      "context_text": "Path-Ranking Algorithm (PRA) [145] derives the path-based logic rules under path constraints.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (Path-Ranking Algorithm). The citation is focused on describing the method rather than a dataset.",
      "processing_time": 17.830323934555054,
      "citing_paper_id": "257220329",
      "cited_paper_id": 10811631
    },
    {
      "context_text": "Additionally, [250], [251] executes plagiarism detection by conducting the KGR approach in a continuous learning manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for plagiarism detection using KGR in a continuous learning manner.",
      "processing_time": 16.806807279586792,
      "citing_paper_id": "257220329",
      "cited_paper_id": 10838787
    },
    {
      "context_text": "Additionally, [250], [251] executes plagiarism detection by conducting the KGR approach in a continuous learning manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for plagiarism detection using KGR in a continuous learning manner.",
      "processing_time": 16.806807279586792,
      "citing_paper_id": "257220329",
      "cited_paper_id": 257129854
    },
    {
      "context_text": "RE-GCN[210]focuseson theevolutionaldynamicsintemporalKGsandgeneratesentity embeddingsbymodelingtheKGsequenceofaÔ¨Åxedlengthat thelatestafewtimestamps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RE-GCN) and its application to temporal knowledge graphs. No verifiable datasets are referenced.",
      "processing_time": 19.05062222480774,
      "citing_paper_id": "257220329",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "Moreover,CENET[183]learnsboththehistoricaland non-historicaldependencyforinferringthemostpotentialfacts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CENET) for inferring potential facts using historical and non-historical dependencies.",
      "processing_time": 18.274155616760254,
      "citing_paper_id": "257220329",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "Then, NAM [62] proposes the relational-modulated neural network (RMNN), and NN models share variables in ProjE [63], which jointly learns embeddings of the entities and relations via the standard loss function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the models and their functionalities.",
      "processing_time": 17.249577522277832,
      "citing_paper_id": "257220329",
      "cited_paper_id": 18367155
    },
    {
      "context_text": "Facts # Test Facts ATOMIC [205] 304,388 9 610,536 87,700 87,701 Countries [206] 271 2 1,110 24 24 CoDEX-S [207] 2,034 42 32,888 3,654 3656 CoDEX-M [207] 17,050 51 185,584 20620 20622 CoDEX-L [207] 77,951 69 551,193 30,622 30622 ConceptNet [208] 28,370,083 50 27,259,933 3,407,492 3,407,492 ConceptNet100K [209] 78,334 34 100,000 1,200 1,200 DBpedia50 [210] 49,900 654 32,388 399 10,969 DBpedia500 [210] 517,475 654 3,102,677 10,000 1,155,937 DB100K [211] 99,604 470 597,482 49,997 50,000 FAMILY [212] 3,007 12 23,483 2,038 2,835 FB13 [213] 75,043 13 316,232 11,816 47,464 FB122 [214] 9,738 122 91,638 9,595 11,243 FB15k [215] 14,951 1,345 483,142 50,000 59,071 FB20k [210] 19,923 1,345 472,860 48,991 90,149 FB24k [216] 23,634 673 402,493 - 21,067 FB5M [19] 5,385,322 1,192 19,193,556 50,000 59,071 FB15k-237 [217] 14,505 237 272,115 17,535 20,466 FB60k-NYT10 [218] 69,514 1,327 268,280 8,765 8,918 Hetionet [219] 45,158 24 1,800,157 225,020 225,020 Kinship [212] 104 25 8,544 1,068 1,074 Location [220] 445 5 384 65 65 Nation [221] 14 55 1,592 199 201 NELL23k [222] 22,925 200 25,445 4,961 4,952 NELL-995 [223] 75,492 200 126,176 5,000 5,000 OpenBioLink [224] 180,992 28 4,192,002 188,394 183,011 Sport [220] 1,039 4 1,349 358 358 Toy [225] 280 112 4,565 109 152 UMLS [226] 135 46 5,216 652 661 UMLS-PubMed [218] 59,226 443 2,030,841 8,756 8,689 WD-singer [222] 10,282 135 16,142 2,163 2,203 WN11 [213] 38,588 11 110,361 5,212 21,035 WN18 [227] 40,943 18 141,442 5,000 5,000 WN18RR [217] 40,559 11 86,835 2,924 2,824 wikidata5m [228] 4,594,485 822 20,614,279 5,163 5,163 YAGO3-10 [229] 123,143 37 1,079,040 4,978 4,982 YAGO37 [230] 123,189 37 420,623 50,000 50,000 M-/YAGO39k [231] 85,484 39 354,997 9,341 9,364",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets with their statistics, which are like",
      "processing_time": 158.74308919906616,
      "citing_paper_id": "257220329",
      "cited_paper_id": 18938726
    },
    {
      "context_text": "Facts # Test Facts ATOMIC [205] 304,388 9 610,536 87,700 87,701 Countries [206] 271 2 1,110 24 24 CoDEX-S [207] 2,034 42 32,888 3,654 3656 CoDEX-M [207] 17,050 51 185,584 20620 20622 CoDEX-L [207] 77,951 69 551,193 30,622 30622 ConceptNet [208] 28,370,083 50 27,259,933 3,407,492 3,407,492 ConceptNet100K [209] 78,334 34 100,000 1,200 1,200 DBpedia50 [210] 49,900 654 32,388 399 10,969 DBpedia500 [210] 517,475 654 3,102,677 10,000 1,155,937 DB100K [211] 99,604 470 597,482 49,997 50,000 FAMILY [212] 3,007 12 23,483 2,038 2,835 FB13 [213] 75,043 13 316,232 11,816 47,464 FB122 [214] 9,738 122 91,638 9,595 11,243 FB15k [215] 14,951 1,345 483,142 50,000 59,071 FB20k [210] 19,923 1,345 472,860 48,991 90,149 FB24k [216] 23,634 673 402,493 - 21,067 FB5M [19] 5,385,322 1,192 19,193,556 50,000 59,071 FB15k-237 [217] 14,505 237 272,115 17,535 20,466 FB60k-NYT10 [218] 69,514 1,327 268,280 8,765 8,918 Hetionet [219] 45,158 24 1,800,157 225,020 225,020 Kinship [212] 104 25 8,544 1,068 1,074 Location [220] 445 5 384 65 65 Nation [221] 14 55 1,592 199 201 NELL23k [222] 22,925 200 25,445 4,961 4,952 NELL-995 [223] 75,492 200 126,176 5,000 5,000 OpenBioLink [224] 180,992 28 4,192,002 188,394 183,011 Sport [220] 1,039 4 1,349 358 358 Toy [225] 280 112 4,565 109 152 UMLS [226] 135 46 5,216 652 661 UMLS-PubMed [218] 59,226 443 2,030,841 8,756 8,689 WD-singer [222] 10,282 135 16,142 2,163 2,203 WN11 [213] 38,588 11 110,361 5,212 21,035 WN18 [227] 40,943 18 141,442 5,000 5,000 WN18RR [217] 40,559 11 86,835 2,924 2,824 wikidata5m [228] 4,594,485 822 20,614,279 5,163 5,163 YAGO3-10 [229] 123,143 37 1,079,040 4,978 4,982 YAGO37 [230] 123,189 37 420,623 50,000 50,000 M-/YAGO39k [231] 85,484 39 354,997 9,341 9,364",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets with their statistics, which are like",
      "processing_time": 158.74308919906616,
      "citing_paper_id": "257220329",
      "cited_paper_id": 19139252
    },
    {
      "context_text": "Facts # Test Facts ATOMIC [205] 304,388 9 610,536 87,700 87,701 Countries [206] 271 2 1,110 24 24 CoDEX-S [207] 2,034 42 32,888 3,654 3656 CoDEX-M [207] 17,050 51 185,584 20620 20622 CoDEX-L [207] 77,951 69 551,193 30,622 30622 ConceptNet [208] 28,370,083 50 27,259,933 3,407,492 3,407,492 ConceptNet100K [209] 78,334 34 100,000 1,200 1,200 DBpedia50 [210] 49,900 654 32,388 399 10,969 DBpedia500 [210] 517,475 654 3,102,677 10,000 1,155,937 DB100K [211] 99,604 470 597,482 49,997 50,000 FAMILY [212] 3,007 12 23,483 2,038 2,835 FB13 [213] 75,043 13 316,232 11,816 47,464 FB122 [214] 9,738 122 91,638 9,595 11,243 FB15k [215] 14,951 1,345 483,142 50,000 59,071 FB20k [210] 19,923 1,345 472,860 48,991 90,149 FB24k [216] 23,634 673 402,493 - 21,067 FB5M [19] 5,385,322 1,192 19,193,556 50,000 59,071 FB15k-237 [217] 14,505 237 272,115 17,535 20,466 FB60k-NYT10 [218] 69,514 1,327 268,280 8,765 8,918 Hetionet [219] 45,158 24 1,800,157 225,020 225,020 Kinship [212] 104 25 8,544 1,068 1,074 Location [220] 445 5 384 65 65 Nation [221] 14 55 1,592 199 201 NELL23k [222] 22,925 200 25,445 4,961 4,952 NELL-995 [223] 75,492 200 126,176 5,000 5,000 OpenBioLink [224] 180,992 28 4,192,002 188,394 183,011 Sport [220] 1,039 4 1,349 358 358 Toy [225] 280 112 4,565 109 152 UMLS [226] 135 46 5,216 652 661 UMLS-PubMed [218] 59,226 443 2,030,841 8,756 8,689 WD-singer [222] 10,282 135 16,142 2,163 2,203 WN11 [213] 38,588 11 110,361 5,212 21,035 WN18 [227] 40,943 18 141,442 5,000 5,000 WN18RR [217] 40,559 11 86,835 2,924 2,824 wikidata5m [228] 4,594,485 822 20,614,279 5,163 5,163 YAGO3-10 [229] 123,143 37 1,079,040 4,978 4,982 YAGO37 [230] 123,189 37 420,623 50,000 50,000 M-/YAGO39k [231] 85,484 39 354,997 9,341 9,364",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets with their statistics, which are like",
      "processing_time": 158.74308919906616,
      "citing_paper_id": "257220329",
      "cited_paper_id": 19187663
    },
    {
      "context_text": "Facts # Test Facts ATOMIC [205] 304,388 9 610,536 87,700 87,701 Countries [206] 271 2 1,110 24 24 CoDEX-S [207] 2,034 42 32,888 3,654 3656 CoDEX-M [207] 17,050 51 185,584 20620 20622 CoDEX-L [207] 77,951 69 551,193 30,622 30622 ConceptNet [208] 28,370,083 50 27,259,933 3,407,492 3,407,492 ConceptNet100K [209] 78,334 34 100,000 1,200 1,200 DBpedia50 [210] 49,900 654 32,388 399 10,969 DBpedia500 [210] 517,475 654 3,102,677 10,000 1,155,937 DB100K [211] 99,604 470 597,482 49,997 50,000 FAMILY [212] 3,007 12 23,483 2,038 2,835 FB13 [213] 75,043 13 316,232 11,816 47,464 FB122 [214] 9,738 122 91,638 9,595 11,243 FB15k [215] 14,951 1,345 483,142 50,000 59,071 FB20k [210] 19,923 1,345 472,860 48,991 90,149 FB24k [216] 23,634 673 402,493 - 21,067 FB5M [19] 5,385,322 1,192 19,193,556 50,000 59,071 FB15k-237 [217] 14,505 237 272,115 17,535 20,466 FB60k-NYT10 [218] 69,514 1,327 268,280 8,765 8,918 Hetionet [219] 45,158 24 1,800,157 225,020 225,020 Kinship [212] 104 25 8,544 1,068 1,074 Location [220] 445 5 384 65 65 Nation [221] 14 55 1,592 199 201 NELL23k [222] 22,925 200 25,445 4,961 4,952 NELL-995 [223] 75,492 200 126,176 5,000 5,000 OpenBioLink [224] 180,992 28 4,192,002 188,394 183,011 Sport [220] 1,039 4 1,349 358 358 Toy [225] 280 112 4,565 109 152 UMLS [226] 135 46 5,216 652 661 UMLS-PubMed [218] 59,226 443 2,030,841 8,756 8,689 WD-singer [222] 10,282 135 16,142 2,163 2,203 WN11 [213] 38,588 11 110,361 5,212 21,035 WN18 [227] 40,943 18 141,442 5,000 5,000 WN18RR [217] 40,559 11 86,835 2,924 2,824 wikidata5m [228] 4,594,485 822 20,614,279 5,163 5,163 YAGO3-10 [229] 123,143 37 1,079,040 4,978 4,982 YAGO37 [230] 123,189 37 420,623 50,000 50,000 M-/YAGO39k [231] 85,484 39 354,997 9,341 9,364",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets with their statistics, which are like",
      "processing_time": 158.74308919906616,
      "citing_paper_id": "257220329",
      "cited_paper_id": 60254729
    },
    {
      "context_text": "According to the entity set size, we can derive several subsets from it, including FB13 [213], FB122 [214], FB15k [215], FB20k [210], FB24k [216], FB5M [19], FB15k-237 [217], FB60k-NYT10 [218].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB13",
        "FB122",
        "FB15k",
        "FB20k",
        "FB24k",
        "FB5M",
        "FB15k-237",
        "FB60k-NYT10"
      ],
      "dataset_descriptions": {
        "FB13": "Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets and relation types.",
        "FB122": "Used to assess knowledge graph reasoning algorithms, emphasizing a moderate-sized entity set with diverse relations.",
        "FB15k": "Used to benchmark knowledge graph embedding models, focusing on a widely-used subset of Freebase with 15,000 entities.",
        "FB20k": "Used to test knowledge graph reasoning systems, providing a slightly larger entity set than FB15k for more comprehensive evaluation.",
        "FB24k": "Used to evaluate knowledge graph completion models, offering a larger entity set to challenge the scalability of algorithms.",
        "FB5M": "Used to assess large-scale knowledge graph reasoning, focusing on a significantly larger entity set to test system robustness.",
        "FB15k-237": "Used to evaluate knowledge graph completion methods, addressing issues in FB15k by filtering out inverse relations and improving dataset quality.",
        "FB60k-NYT10": "Used to evaluate multi-modal knowledge graph reasoning, combining textual data from New York Times articles with Freebase entities."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple subsets derived from a larger entity set, all of which are specific datasets used in knowledge graph research.",
      "processing_time": 68.08880114555359,
      "citing_paper_id": "257220329",
      "cited_paper_id": 19139252
    },
    {
      "context_text": ", DBpedia50 [210], DBpedia500 [210] and DB100K [211].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia50",
        "DBpedia500",
        "DB100K"
      ],
      "dataset_descriptions": {
        "DBpedia50": "Used for knowledge graph completion, focusing on entity linking and relation prediction in a smaller subset of DBpedia.",
        "DBpedia500": "Used for knowledge graph completion, focusing on entity linking and relation prediction in a larger subset of DBpedia.",
        "DB100K": "Used for knowledge graph embedding, focusing on large-scale entity and relation representation learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for knowledge graph completion and embedding, which are directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 36.68405508995056,
      "citing_paper_id": "257220329",
      "cited_paper_id": 19139252
    },
    {
      "context_text": ", DBpedia50 [210], DBpedia500 [210] and DB100K [211].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia50",
        "DBpedia500",
        "DB100K"
      ],
      "dataset_descriptions": {
        "DBpedia50": "Used for knowledge graph completion, focusing on entity linking and relation prediction in a smaller subset of DBpedia.",
        "DBpedia500": "Used for knowledge graph completion, focusing on entity linking and relation prediction in a larger subset of DBpedia.",
        "DB100K": "Used for knowledge graph embedding, focusing on large-scale entity and relation representation learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for knowledge graph completion and embedding, which are directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 36.68405508995056,
      "citing_paper_id": "257220329",
      "cited_paper_id": 19187663
    },
    {
      "context_text": "SpeciÔ¨Åcally, three taxonomy criteria are adopted to classify the reviewed KGR models, i.e., graph types, techniques, and scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only taxonomy criteria for classifying KGR models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 20.085887908935547,
      "citing_paper_id": "257220329",
      "cited_paper_id": 26996000
    },
    {
      "context_text": "To alleviate heterogeneity and imbalance issues, TranSparse [30] provides an efÔ¨Åcient solution by designing adaptive transfer sparser matrices.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TranSparse. The context focuses on the method's ability to address heterogeneity and imbalance issues in knowledge graphs.",
      "processing_time": 21.480005979537964,
      "citing_paper_id": "257220329",
      "cited_paper_id": 38485677
    },
    {
      "context_text": "Deep reinforcement learning (DRL) [147] techniques, such as the Markov decision process (MDP) [148], have recently been used to reformulate path-Ô¨Ånding between entities as a sequential decision-making task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 18.667609453201294,
      "citing_paper_id": "257220329",
      "cited_paper_id": 54434537
    },
    {
      "context_text": "3) GNN Model: Graph neural networks (GNN) [72], [73] are widely used for graph tasks, due to their promising capacity for structural information mining.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of graph neural networks (GNNs) for graph tasks. No verifiable resources are identified.",
      "processing_time": 21.03096866607666,
      "citing_paper_id": "257220329",
      "cited_paper_id": 56517517
    },
    {
      "context_text": "Besides, Analogy [49] makes use of analogical structures by setting speciÔ¨Åc constraints, and CrossE [50] considers crossover interactions between entities via a relation-speciÔ¨Åc interaction matrix, which both enhance the interactions between entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Analogy and CrossE).",
      "processing_time": 15.437563419342041,
      "citing_paper_id": "257220329",
      "cited_paper_id": 59516071
    },
    {
      "context_text": "‚Ä¢ Nation [212] contains relations among nations [221].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Nation"
      ],
      "dataset_descriptions": {
        "Nation": "Used to represent relations among nations, focusing on the structure and connections within the dataset. The specific research question involves analyzing the dimensionality of these relations."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Nation' as containing relations among nations, which could be a dataset. However, without more specific details or a clear identifier, it is ambiguous.",
      "processing_time": 26.665438175201416,
      "citing_paper_id": "257220329",
      "cited_paper_id": 60254729
    },
    {
      "context_text": "‚Ä¢ IMGpedia [223], [238], [239], [240] is the KG, which incorporates visual information of the images from the Wikimedia Commons dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia",
        "Wikimedia Commons"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used as a knowledge graph to incorporate visual information from Wikimedia Commons images, enabling visuo-semantic queries and content-based analysis.",
        "Wikimedia Commons": "Serves as the source of images for IMGpedia, providing a large-scale collection of multimedia files for content-based analysis and visuo-semantic queries."
      },
      "confidence_score": 1.0,
      "reasoning": "IMGpedia is identified as a knowledge graph incorporating visual information from the Wikimedia Commons dataset. The context indicates that IMGpedia is used as a reusable resource.",
      "processing_time": 32.05511426925659,
      "citing_paper_id": "257220329",
      "cited_paper_id": 67474824
    },
    {
      "context_text": "‚Ä¢ IMGpedia [223], [238], [239], [240] is the KG, which incorporates visual information of the images from the Wikimedia Commons dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia",
        "Wikimedia Commons"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used as a knowledge graph to incorporate visual information from Wikimedia Commons images, enabling visuo-semantic queries and content-based analysis.",
        "Wikimedia Commons": "Serves as the source of images for IMGpedia, providing a large-scale collection of multimedia files for content-based analysis and visuo-semantic queries."
      },
      "confidence_score": 1.0,
      "reasoning": "IMGpedia is identified as a knowledge graph incorporating visual information from the Wikimedia Commons dataset. The context indicates that IMGpedia is used as a reusable resource.",
      "processing_time": 32.05511426925659,
      "citing_paper_id": "254564635",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "Besides, RotatE [7] and RotatE3D [32] both model the relation into complex space as rotations to better infer the symmetry, anti-symmetry, inversion, and composition facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RotatE and RotatE3D but does not refer to them as datasets. They are described as methods for modeling relations in complex space.",
      "processing_time": 20.61011552810669,
      "citing_paper_id": "257220329",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "EvolveGCN [182] adopts the graph convolutional networks (GCNs) to model the graph structure in each static snapshot and utilizes the LSTM model (also can utilize GRU model) to evolvetheGCNparametersovertime.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodology of using GCNs and LSTMs/GRUs for dynamic graphs.",
      "processing_time": 22.283616542816162,
      "citing_paper_id": "257220329",
      "cited_paper_id": 67856459
    },
    {
      "context_text": "Furthermore, EvolveGCN [147] adopts the graph convolutional networks (GCNs) to model the graph structure in each static snapshot and utilizes the LSTM model (also can utilize GRU model) to evolve the GCN parameters over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the methodology of evolving GCN parameters over time using LSTM or GRU.",
      "processing_time": 22.055968523025513,
      "citing_paper_id": "257220329",
      "cited_paper_id": 67856459
    },
    {
      "context_text": "Furthermore, EvolveGCN [147] adopts the graph convolutional networks (GCNs) to model the graph structure in each static snapshot and utilizes the LSTM model (also can utilize GRU model) to evolve the GCN parameters over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the methodology of evolving GCN parameters over time using LSTM or GRU.",
      "processing_time": 22.055968523025513,
      "citing_paper_id": "254564635",
      "cited_paper_id": 67856459
    },
    {
      "context_text": "2022 ExKGR [146] Interpolation LSTM 2020 EvolveGCN [147] Extrapolation LSTM+GRU",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on describing different approaches to graph reasoning over time.",
      "processing_time": 20.598568201065063,
      "citing_paper_id": "257220329",
      "cited_paper_id": 67856459
    },
    {
      "context_text": "In the future, it is necessary to study how to leverage multi-relational facts to enhance reasoning ability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general direction for future research.",
      "processing_time": 16.75599980354309,
      "citing_paper_id": "257220329",
      "cited_paper_id": 85558018
    },
    {
      "context_text": "Random walk [146] inferences have been widely investigated.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (random walk). The context does not provide information about a dataset being used.",
      "processing_time": 20.99798560142517,
      "citing_paper_id": "257220329",
      "cited_paper_id": 117107657
    },
    {
      "context_text": "While, OTKGE [260] models the multi-modal fusion procedure as a transport plan moving different modal embeddings to a uniÔ¨Åed space by minimizing the Wasserstein distance [261].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (OTKGE) and a mathematical concept (Wasserstein distance).",
      "processing_time": 20.995604038238525,
      "citing_paper_id": "257220329",
      "cited_paper_id": 118675981
    },
    {
      "context_text": "The Poincar√© ball model of hyperbolic space is used in MuRP [31] for better performance in low dimensionality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MuRP) and a model (Poincar√© ball model).",
      "processing_time": 19.331223487854004,
      "citing_paper_id": "257220329",
      "cited_paper_id": 168633605
    },
    {
      "context_text": "‚Ä¶some general multi-modal pretrained transformer (MPT) models, such as VisualBERT [226], ViLBERT [227], mPLUG [228], UNITER [229], and LXMERT [230], can also be adopted for multi-modal KGR. Due to the variance between the multi-modal KGs and other multi-modal data, directly applying the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several multi-modal pretrained transformer models but does not refer to any specific datasets. The context is about using these models for multi-modal knowledge graph reasoning.",
      "processing_time": 22.257400035858154,
      "citing_paper_id": "257220329",
      "cited_paper_id": 201103729
    },
    {
      "context_text": "Although some general multi-modal pretrained transformer (MPT) models, such as VisualBERT [226], ViLBERT [227], mPLUG [228], UNITER [229], and LXMERT [230], can also be adopted for multi-modal KGR. Due to the variance between the multi-modal KGs and other multi-modal data, directly applying the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several multi-modal pretrained transformer models but does not specify any datasets. The context focuses on models and their potential application to multi-modal KGR, without referencing specific datasets.",
      "processing_time": 22.949129104614258,
      "citing_paper_id": "257220329",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "Meanwhile, an end-to-end differentiable rule-based model is proposed in DRUM [144].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DRUM, which is a method for mining rules on knowledge graphs, not a dataset. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 21.421242475509644,
      "citing_paper_id": "257220329",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "Nowadays, there are also some interesting attempts for translational models for more sufficient interactions, such as TripleRE [32] and InterHT [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on translational models for interactions in knowledge graphs.",
      "processing_time": 20.976635694503784,
      "citing_paper_id": "257220329",
      "cited_paper_id": 202777324
    },
    {
      "context_text": "Besides, to handle the complex relational facts with a uniÔ¨Åed model, PairRE [36] models each relation representation with paired vectors in a unit circle to adaptive adjustment for complex relations, and InterHT [37] further enhances the information interactions between the tail and head.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (PairRE and InterHT). These are not datasets but models or methods used for knowledge graph embedding and reasoning.",
      "processing_time": 22.77703285217285,
      "citing_paper_id": "257220329",
      "cited_paper_id": 202777324
    },
    {
      "context_text": "Besides, to handle the complex relational facts with a uniÔ¨Åed model, PairRE [36] models each relation representation with paired vectors in a unit circle to adaptive adjustment for complex relations, and InterHT [37] further enhances the information interactions between the tail and head.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (PairRE and InterHT). These are not datasets but models or methods used for knowledge graph embedding and reasoning.",
      "processing_time": 22.77703285217285,
      "citing_paper_id": "257220329",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "2022 InterHT [33] Transductive Translational 2019 CPL [87] Transductive Relation Path",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is too limited to infer any dataset usage.",
      "processing_time": 20.5923273563385,
      "citing_paper_id": "257220329",
      "cited_paper_id": 202777324
    },
    {
      "context_text": "Such attention mechanism is also leveraged by TransAE [171].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransAE) that leverages an attention mechanism. The context does not provide information about the usage of a dataset.",
      "processing_time": 22.935668468475342,
      "citing_paper_id": "257220329",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "Such attention mechanism is also leveraged by TransAE [241].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransAE) that leverages an attention mechanism. The context does not provide information about the use of datasets.",
      "processing_time": 29.68402361869812,
      "citing_paper_id": "257220329",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "After-ward, TransGCN [79] trains both relation and entity embeddings simultaneously with the transformation operator for relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransGCN) and its functionality. No verifiable resources are referenced.",
      "processing_time": 28.902695178985596,
      "citing_paper_id": "257220329",
      "cited_paper_id": 203626690
    },
    {
      "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "kgbench"
      ],
      "dataset_descriptions": {
        "kgbench": "Used to evaluate relational and multimodal machine learning models, providing a collection of knowledge graph datasets for benchmarking."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning. However, only 'kgbench' is clearly identified as a dataset in the cited paper titles.",
      "processing_time": 37.8231987953186,
      "citing_paper_id": "257220329",
      "cited_paper_id": 206477883
    },
    {
      "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "kgbench"
      ],
      "dataset_descriptions": {
        "kgbench": "Used to evaluate relational and multimodal machine learning models, providing a collection of knowledge graph datasets for benchmarking."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning. However, only 'kgbench' is clearly identified as a dataset in the cited paper titles.",
      "processing_time": 37.8231987953186,
      "citing_paper_id": "257220329",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "kgbench"
      ],
      "dataset_descriptions": {
        "kgbench": "Used to evaluate relational and multimodal machine learning models, providing a collection of knowledge graph datasets for benchmarking."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning. However, only 'kgbench' is clearly identified as a dataset in the cited paper titles.",
      "processing_time": 37.8231987953186,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235271284
    },
    {
      "context_text": "As a research field at an early stage, the relevant works are not systematic enough [164], [165], [166].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only indicates that the field is at an early stage and lacks systematic works.",
      "processing_time": 29.673014879226685,
      "citing_paper_id": "257220329",
      "cited_paper_id": 206710454
    },
    {
      "context_text": "Neural Network Model: Neural network (NN) models [58], [59] use neural networks to generate embeddings for triple representation, yielding remarkable performance for KG reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural network models and their applications in KG reasoning.",
      "processing_time": 28.320711374282837,
      "citing_paper_id": "257220329",
      "cited_paper_id": 207116476
    },
    {
      "context_text": "Neural Network Model: Neural network (NN) models [58], [59] use neural networks to generate embeddings for triple representation, yielding remarkable performance for KG reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural network models and their applications in KG reasoning.",
      "processing_time": 28.320711374282837,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¢ FreeBASE [227] is a large knowledge base generated from multiple sources, such as Wikipedia, NNDB, Fashion Model Directory, etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FreeBASE"
      ],
      "dataset_descriptions": {
        "FreeBASE": "Used as a large, collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia and NNDB."
      },
      "confidence_score": 1.0,
      "reasoning": "FreeBASE is identified as a large knowledge base used in the context of multi-modal knowledge graph reasoning, which aligns with the research topic.",
      "processing_time": 36.91426610946655,
      "citing_paper_id": "257220329",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "WD-singer [210] and wikidata5m [216] are derived from it according to different scopes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WD-singer",
        "wikidata5m"
      ],
      "dataset_descriptions": {
        "WD-singer": "Used to derive a subset of Wikidata for evaluating knowledge graph completion tasks, focusing on entities and relations within a specific scope.",
        "wikidata5m": "Used to derive a larger subset of Wikidata for evaluating knowledge graph completion tasks, covering a broader range of entities and relations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, WD-singer and wikidata5m, which are derived from a larger dataset. These are specific, named datasets relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 43.49901056289673,
      "citing_paper_id": "257220329",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "wikidata5m [216] 4,594,485 822 20,614,279 5,163 5,163",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "wikidata5m"
      ],
      "dataset_descriptions": {
        "wikidata5m": "Mentioned as a dataset, but no specific usage details are provided in the context. It is likely a large-scale knowledge graph dataset."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'wikidata5m' which appears to be a specific dataset. However, there is no additional context about how it is used or its relevance to multi-modal knowledge graph reasoning.",
      "processing_time": 39.408851861953735,
      "citing_paper_id": "257220329",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "After that, how to inject logical rules into embeddings for better reasoning performance has drawn increasing research attention [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in research.",
      "processing_time": 27.92968201637268,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "While, the construction of KGs is a dynamic and continuous procedure, thus most KGs suffer from incomplete issues, hindering their effectiveness in KG-assisted applications, such as question answering [4], recommendation system [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with KGs. No verifiable resources are identified.",
      "processing_time": 28.4602952003479,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "Meanwhile, MINERVA [124] takes path Ô¨Ånding between entities as a sequential optimization problem by maximizing the expected reward [4], which excludes the target answer entity for more capable reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MINERVA) and a general approach to knowledge graph reasoning. No verifiable datasets are referenced.",
      "processing_time": 31.128814458847046,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "The prototype GraIL [9], as the landmark inductive GNN-based model, Ô¨Årst leverages RGCN to perform the reasoning based on the local enclosingsubgraph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (GraIL) and its methodology (RGCN for subgraph reasoning) but does not reference any specific dataset.",
      "processing_time": 30.547545433044434,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "Similarly, many works perform reasoning on subgraphs instead of complete graphs, such as GraIL [9], CSR [101] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models for subgraph reasoning.",
      "processing_time": 28.453421592712402,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "Some recent attempts provide potential solutions for inferring unseen entities, which are known as inductive reasoning models, such as [9], [87], [88], [90].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to inductive reasoning models. No verifiable resources are identified.",
      "processing_time": 29.41813087463379,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "Some recent attempts provide potential solutions for inferring unseen entities, which are known as inductive reasoning models, such as [9], [87], [88], [90].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to inductive reasoning models. No verifiable resources are identified.",
      "processing_time": 29.41813087463379,
      "citing_paper_id": "257220329",
      "cited_paper_id": 250629390
    },
    {
      "context_text": "Afterthat,motivatedbythegreatsuccessof contrastivemechanisms[3],[95],contrastivelearningmodels havebeenincreasinglyproposed,e.g.,RPC-IR[91],SNRI[90] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 30.296886920928955,
      "citing_paper_id": "257220329",
      "cited_paper_id": 211096730
    },
    {
      "context_text": "Afterthat,motivatedbythegreatsuccessof contrastivemechanisms[3],[95],contrastivelearningmodels havebeenincreasinglyproposed,e.g.,RPC-IR[91],SNRI[90] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 30.296886920928955,
      "citing_paper_id": "257220329",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "Afterthat,motivatedbythegreatsuccessof contrastivemechanisms[3],[95],contrastivelearningmodels havebeenincreasinglyproposed,e.g.,RPC-IR[91],SNRI[90] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 30.296886920928955,
      "citing_paper_id": "257220329",
      "cited_paper_id": 250629390
    },
    {
      "context_text": "Afterthat,motivatedbythegreatsuccessof contrastivemechanisms[3],[95],contrastivelearningmodels havebeenincreasinglyproposed,e.g.,RPC-IR[91],SNRI[90] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 30.296886920928955,
      "citing_paper_id": "257220329",
      "cited_paper_id": 259949958
    },
    {
      "context_text": "Afterthat,motivatedbythegreatsuccessof contrastivemechanisms[3],[95],contrastivelearningmodels havebeenincreasinglyproposed,e.g.,RPC-IR[91],SNRI[90] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 30.296886920928955,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, TComplEx and TNTComplEx [167] both come from ComplEx, where the fourth-order tensor space with additional consideration of time information is modeled by them.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The citation is about tensor decompositions for temporal knowledge base completion, which is a methodological discussion.",
      "processing_time": 31.940488815307617,
      "citing_paper_id": "257220329",
      "cited_paper_id": 214390104
    },
    {
      "context_text": "Moreover, different advanced techniques are integrated within multi-modal KGR models for more complex situations, MKBE [243] and MRCGN [252] integrate different neural encoders anddecoders withrelational models for embeddinglearn-ing and multi-modal data for reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'multi-modal data for reasoning' but does not specify any particular dataset. The cited paper titles do not provide additional clarity on specific datasets.",
      "processing_time": 30.8632972240448,
      "citing_paper_id": "257220329",
      "cited_paper_id": 214693100
    },
    {
      "context_text": "Moreover, MKBE [174] and MRCGN [175] integrate different neural encoders and decoders with relational models for embedding learning and multimodal data for reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'multimodal data for reasoning' but does not specify any particular dataset. The context is about integrating neural encoders and decoders with relational models, which is more methodological than dataset-specific.",
      "processing_time": 34.25786542892456,
      "citing_paper_id": "257220329",
      "cited_paper_id": 214693100
    },
    {
      "context_text": "2023 RETIA [114] Extrapolation GRU 2021 xERTE [115] Extrapolation Time-Vector 2023 RPC [116] Extrapolation GRU 2021 CyGNet [117] Extrapolation Time-Vector 2022 CENET [118] Extrapolation Time-Operation 2021 TIE [119] Interpolation Time-Operation 2022 DA-Net [120] Extrapolation Time-Operation 2021 TeLM [121] Interpolation Time-Operation 2022 HiSMatch [122] Extrapolation GRU 2021 ChronoR [123] Interpolation Time-Vector 2022 rGalT [124] Extrapolation Time-Operation 2021 RE-GCN [125] Extrapolation GRU 2022 MetaTKGR [126] Extrapolation Time-Operation 2021 RTFE [127] Interpolation Time-Operation 2022 FILT [128] Interpolation Time-Operation 2021 HIP [129] Extrapolation GRU 2022 TKGC-AGP [130] Interpolation Time-Operation 2021 Tpath [131] Interpolation LSTM 2022 Tlogic [132] Extrapolation Time-Operation 2020 TIMEPLEX [133] Interpolation Time-Operation 2022 TLT-KGE [134] Interpolation Time-Vector 2020 DyERNIE [135] Interpolation Time-Operation 2022 CEN [136] Extrapolation Time-Operation 2020 DacKGR [137] Interpolation RNN 2022 BoxTE [138] Interpolation Time-Vector 2020 TNTComplEx [139] Interpolation Time-Vector 2022 TempoQR [140] Interpolation Time-Vector 2020 TComplEx [139] Interpolation Time-Vector 2022 TuckERTNT [141] Interpolation Time-Vector 2020 TDGNN [142] Extrapolation Time-Operation 2022 GHT [143] Extrapolation Time-Operation 2020 ATiSE [144] Interpolation Time-Operation 2022 DKGE [145] Interpolation Time-Operation 2020 Diachronic [146] Interpolation Time-Operation 2022 TiRGN [147] Extrapolation GRU 2020 DE-Simple [146] Interpolation Time-Operation 2022 RotateQVS [148] Interpolation Time-Vector 2020 TeRo [149] Interpolation Time-Operation 2022 ExKGR [150] Interpolation LSTM 2020 EvolveGCN [151] Extrapolation LSTM+GRU 2022 TRHyTE [152] Interpolation GRU 2020 TeMP [153] Interpolation GRU 2022 EvoKG [154] Extrapolation RNN 2020 RE-NET [155] Extrapolation RNN 2021 TPmod [156] Interpolation GRU 2019 DyRep [157] Extrapolation Time-Operation 2021 TimeTraveler [158] Extrapolation LSTM 2018 TTransE [159] Interpolation LSTM 2021 CluSTeR [160] Extrapolation LSTM+GRU 2018 HyTE [161] Interpolation Time-Operation 2021 TPRec [162] Extrapolation Time-Operation 2018 ChronoTranslate [163] Interpolation Time-Operation 2021 DBKGE [164] Interpolation Time-Vector 2018 TA-DISTMULT [165] Interpolation LSTM 2021 TANGO [166] Extrapolation Time-Operation 2018 TA-TransE [165] Interpolation LSTM 2021 T-GAP [167] Interpolation Time-Vector 2017 Know-Evolve [168] Extrapolation RNN",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and models for temporal knowledge graph reasoning. No clear, verifiable datasets are identified.",
      "processing_time": 30.522676467895508,
      "citing_paper_id": "257220329",
      "cited_paper_id": 218581196
    },
    {
      "context_text": "2023 RETIA [114] Extrapolation GRU 2021 xERTE [115] Extrapolation Time-Vector 2023 RPC [116] Extrapolation GRU 2021 CyGNet [117] Extrapolation Time-Vector 2022 CENET [118] Extrapolation Time-Operation 2021 TIE [119] Interpolation Time-Operation 2022 DA-Net [120] Extrapolation Time-Operation 2021 TeLM [121] Interpolation Time-Operation 2022 HiSMatch [122] Extrapolation GRU 2021 ChronoR [123] Interpolation Time-Vector 2022 rGalT [124] Extrapolation Time-Operation 2021 RE-GCN [125] Extrapolation GRU 2022 MetaTKGR [126] Extrapolation Time-Operation 2021 RTFE [127] Interpolation Time-Operation 2022 FILT [128] Interpolation Time-Operation 2021 HIP [129] Extrapolation GRU 2022 TKGC-AGP [130] Interpolation Time-Operation 2021 Tpath [131] Interpolation LSTM 2022 Tlogic [132] Extrapolation Time-Operation 2020 TIMEPLEX [133] Interpolation Time-Operation 2022 TLT-KGE [134] Interpolation Time-Vector 2020 DyERNIE [135] Interpolation Time-Operation 2022 CEN [136] Extrapolation Time-Operation 2020 DacKGR [137] Interpolation RNN 2022 BoxTE [138] Interpolation Time-Vector 2020 TNTComplEx [139] Interpolation Time-Vector 2022 TempoQR [140] Interpolation Time-Vector 2020 TComplEx [139] Interpolation Time-Vector 2022 TuckERTNT [141] Interpolation Time-Vector 2020 TDGNN [142] Extrapolation Time-Operation 2022 GHT [143] Extrapolation Time-Operation 2020 ATiSE [144] Interpolation Time-Operation 2022 DKGE [145] Interpolation Time-Operation 2020 Diachronic [146] Interpolation Time-Operation 2022 TiRGN [147] Extrapolation GRU 2020 DE-Simple [146] Interpolation Time-Operation 2022 RotateQVS [148] Interpolation Time-Vector 2020 TeRo [149] Interpolation Time-Operation 2022 ExKGR [150] Interpolation LSTM 2020 EvolveGCN [151] Extrapolation LSTM+GRU 2022 TRHyTE [152] Interpolation GRU 2020 TeMP [153] Interpolation GRU 2022 EvoKG [154] Extrapolation RNN 2020 RE-NET [155] Extrapolation RNN 2021 TPmod [156] Interpolation GRU 2019 DyRep [157] Extrapolation Time-Operation 2021 TimeTraveler [158] Extrapolation LSTM 2018 TTransE [159] Interpolation LSTM 2021 CluSTeR [160] Extrapolation LSTM+GRU 2018 HyTE [161] Interpolation Time-Operation 2021 TPRec [162] Extrapolation Time-Operation 2018 ChronoTranslate [163] Interpolation Time-Operation 2021 DBKGE [164] Interpolation Time-Vector 2018 TA-DISTMULT [165] Interpolation LSTM 2021 TANGO [166] Extrapolation Time-Operation 2018 TA-TransE [165] Interpolation LSTM 2021 T-GAP [167] Interpolation Time-Vector 2017 Know-Evolve [168] Extrapolation RNN",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and models for temporal knowledge graph reasoning. No clear, verifiable datasets are identified.",
      "processing_time": 30.522676467895508,
      "citing_paper_id": "257220329",
      "cited_paper_id": 259949958
    },
    {
      "context_text": "Due to the uncertainty of temporal information in the graph evolution over time, ATiSE [141] maps the entity and relation embeddings of Temporal KGs into the Gaussian spaces according to the time stamps.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (ATiSE) and its application to Temporal KGs, but does not name a specific dataset.",
      "processing_time": 32.71547603607178,
      "citing_paper_id": "257220329",
      "cited_paper_id": 218900866
    },
    {
      "context_text": "Due to the uncertainty of temporal information in the graph evolution over time, ATiSE [141] maps the entity and relation embeddings of Temporal KGs into the Gaussian spaces according to the time stamps.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (ATiSE) and its application to Temporal KGs, but does not name a specific dataset.",
      "processing_time": 32.71547603607178,
      "citing_paper_id": "254564635",
      "cited_paper_id": 218900866
    },
    {
      "context_text": "2022 DKGE [140] Interpolation Time-Operation 2020 ATiSE [141] Interpolation Time-Operation",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. No clear identifiers for datasets are present.",
      "processing_time": 29.392494916915894,
      "citing_paper_id": "257220329",
      "cited_paper_id": 218900866
    },
    {
      "context_text": "Due to the uncertainty of temporal information in the graph evolution over time, ATiSE [173] maps the entity and relation embeddings of Temporal KGs into the Gaussian spaces according to the time stamps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (ATiSE) and its approach to handling temporal information in knowledge graphs.",
      "processing_time": 31.656689405441284,
      "citing_paper_id": "257220329",
      "cited_paper_id": 218900866
    },
    {
      "context_text": "After that, motivated by the great success of contrastive mechanisms [77], [78], [79], [80], contrastive learning models have been increasingly proposed, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to contrastive learning models and mechanisms. No verifiable resources are identified.",
      "processing_time": 30.249977588653564,
      "citing_paper_id": "257220329",
      "cited_paper_id": 219531264
    },
    {
      "context_text": "After that, motivated by the great success of contrastive mechanisms [77], [78], [79], [80], contrastive learning models have been increasingly proposed, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to contrastive learning models and mechanisms. No verifiable resources are identified.",
      "processing_time": 30.249977588653564,
      "citing_paper_id": "257220329",
      "cited_paper_id": 245634466
    },
    {
      "context_text": "After that, motivated by the great success of contrastive mechanisms [77], [78], [79], [80], contrastive learning models have been increasingly proposed, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to contrastive learning models and mechanisms. No verifiable resources are identified.",
      "processing_time": 30.249977588653564,
      "citing_paper_id": "257220329",
      "cited_paper_id": 248834330
    },
    {
      "context_text": "Although a large amount of KGR methods have been proposed in recent years, demonstrating the great potential of KGR in theoretical fields, the applications of KGR still need to be studied more [257].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the need for more application studies in KGR.",
      "processing_time": 29.58831000328064,
      "citing_paper_id": "257220329",
      "cited_paper_id": 221819250
    },
    {
      "context_text": "Meanwhile, TeRo [180] introduces a novel temporal guided rotation operation for scoring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TeRo, which is a method for time-aware knowledge graph embedding, not a dataset. No specific dataset is mentioned.",
      "processing_time": 30.50003743171692,
      "citing_paper_id": "257220329",
      "cited_paper_id": 222124934
    },
    {
      "context_text": "RE-NET [151] applies the GCN and RNN models to capture the evolutional dynamics in temporal knowledge graphs to the query over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of GCN and RNN models on temporal knowledge graphs but does not specify a dataset. The focus is on the method rather than a specific dataset.",
      "processing_time": 32.69413423538208,
      "citing_paper_id": "257220329",
      "cited_paper_id": 222205878
    },
    {
      "context_text": "2022 EvoKG [150] Extrapolation RNN 2020 RE-NET [151] Extrapolation RNN",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on describing different approaches for extrapolation in temporal knowledge graphs.",
      "processing_time": 31.36045479774475,
      "citing_paper_id": "257220329",
      "cited_paper_id": 222205878
    },
    {
      "context_text": "RE-NET[188]appliesthe GCNandRNNmodelstocapturetheevolutionaldynamicsin temporalKGstothequeryovertime.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the application of GCN and RNN models to capture evolutionary dynamics in temporal knowledge graphs.",
      "processing_time": 32.96544337272644,
      "citing_paper_id": "257220329",
      "cited_paper_id": 222205878
    },
    {
      "context_text": ", question answering [3], recommendation system [4], information extraction [5], image classification [6], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas. No verifiable resources are identified.",
      "processing_time": 29.375645875930786,
      "citing_paper_id": "257220329",
      "cited_paper_id": 231925372
    },
    {
      "context_text": "2021 DBKGE [160] Interpolation Time-Vector 2018 TA-DISTMULT [161] Interpolation LSTM",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on describing the use of interpolation techniques and LSTM in temporal knowledge graphs.",
      "processing_time": 32.126078367233276,
      "citing_paper_id": "257220329",
      "cited_paper_id": 232126110
    },
    {
      "context_text": "Furthermore, DBKGE [187] proposed an online inference algorithm that smoothed the representation vector of nodes over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or algorithm. The context focuses on the proposed online inference algorithm for smoothing node representations over time.",
      "processing_time": 31.043333292007446,
      "citing_paper_id": "257220329",
      "cited_paper_id": 232126110
    },
    {
      "context_text": "Furthermore, DBKGE [160] proposed an online inference algorithm that smoothed the representation vector of nodes over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DBKGE' which is likely a method or model rather than a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 30.45491099357605,
      "citing_paper_id": "257220329",
      "cited_paper_id": 232126110
    },
    {
      "context_text": "Besides, BERTRL [99] tries to handle this case based on their textual semantics calculated by language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERTRL) that uses language models for relation prediction.",
      "processing_time": 30.213387727737427,
      "citing_paper_id": "257220329",
      "cited_paper_id": 232222958
    },
    {
      "context_text": "Moreover, BERTRL [99] and ConGLR [100] integrate the context for each entity to enhance the reasoning on the KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (BERTRL and ConGLR) that enhance reasoning on knowledge graphs.",
      "processing_time": 31.044153213500977,
      "citing_paper_id": "257220329",
      "cited_paper_id": 232222958
    },
    {
      "context_text": "Moreover, BERTRL [99] and ConGLR [100] integrate the context for each entity to enhance the reasoning on the KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (BERTRL and ConGLR) that enhance reasoning on knowledge graphs.",
      "processing_time": 31.044153213500977,
      "citing_paper_id": "257220329",
      "cited_paper_id": 250340389
    },
    {
      "context_text": "In particular, BERTRL [99] can handle unseen relational facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERTRL) and its capability to handle unseen relational facts.",
      "processing_time": 30.808300018310547,
      "citing_paper_id": "257220329",
      "cited_paper_id": 232222958
    },
    {
      "context_text": "Besides, TIE [201] combines the experience replay and time regularization into KGR to learn the time-aware incremental embedding.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TIE) that combines experience replay and time regularization for time-aware incremental embedding in knowledge graph reasoning.",
      "processing_time": 31.603620767593384,
      "citing_paper_id": "257220329",
      "cited_paper_id": 233295959
    },
    {
      "context_text": "TPmod [177] and HIP network [215] make more use of dynamic interactions of entities and relations with more Ô¨Åne-grained mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TPmod and HIP network but does not refer to any specific datasets. The focus is on the methodologies and models rather than the data used.",
      "processing_time": 32.354652643203735,
      "citing_paper_id": "257220329",
      "cited_paper_id": 233486383
    },
    {
      "context_text": "TPmod [177] and HIP network [215] make more use of dynamic interactions of entities and relations with more Ô¨Åne-grained mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TPmod and HIP network but does not refer to any specific datasets. The focus is on the methodologies and models rather than the data used.",
      "processing_time": 32.354652643203735,
      "citing_paper_id": "257220329",
      "cited_paper_id": 237100866
    },
    {
      "context_text": "Concretely, TACT [70] uses topologyaware correlations between relations to generate representations for triplet scoring, which also inspires RMPI [75] and TEMP [76].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 31.299646139144897,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235262529
    },
    {
      "context_text": "Concretely,TACT[87]uses topology-awarecorrelationsbetweenrelationstogeneraterep-resentationsfortripletscoring,whichalsoinspiresRMPI[92], TEMP[93]andInGram[94].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 31.593063592910767,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235262529
    },
    {
      "context_text": "Concretely,TACT[87]uses topology-awarecorrelationsbetweenrelationstogeneraterep-resentationsfortripletscoring,whichalsoinspiresRMPI[92], TEMP[93]andInGram[94].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 31.593063592910767,
      "citing_paper_id": "257220329",
      "cited_paper_id": 248496482
    },
    {
      "context_text": "Concretely,TACT[87]uses topology-awarecorrelationsbetweenrelationstogeneraterep-resentationsfortripletscoring,whichalsoinspiresRMPI[92], TEMP[93]andInGram[94].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 31.593063592910767,
      "citing_paper_id": "257220329",
      "cited_paper_id": 258987428
    },
    {
      "context_text": "2022 RMPI [75] Inductive GNN 2019 Neural-Num-LP [92] Inductive Logic Rule",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 29.912907600402832,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235262529
    },
    {
      "context_text": "2021 CluSTeR [156] Extrapolation LSTM+GRU 2018 HyTE [157] Interpolation Time-Operation",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on temporal knowledge graph reasoning but does not specify any datasets used.",
      "processing_time": 32.61227798461914,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235266233
    },
    {
      "context_text": "Forinstance, CluSTeR [181] adopts reinforcement learning to discover evolutional patterns with both LSTM and GRU models in temporal KGs over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the CluSTeR method and its application to temporal knowledge graphs.",
      "processing_time": 33.25770044326782,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235266233
    },
    {
      "context_text": "CluSTeR [156] adopts reinforcement learning to discover evolutional patterns with both LSTM and GRU models in Temporal KGs over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the approach used in the cited paper.",
      "processing_time": 31.281872272491455,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235266233
    },
    {
      "context_text": "In recent years, different strategies have been leveraged for more efÔ¨Åcient KGE models in recent years, which can be roughly categorized into three types, i.e., quantiÔ¨Åcation [52], knowledge distillation [154], and anchor-based sampling [155].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and strategies for improving KGE models.",
      "processing_time": 29.709140300750732,
      "citing_paper_id": "257220329",
      "cited_paper_id": 235606453
    },
    {
      "context_text": "In recent years, different strategies have been leveraged for more efÔ¨Åcient KGE models in recent years, which can be roughly categorized into three types, i.e., quantiÔ¨Åcation [52], knowledge distillation [154], and anchor-based sampling [155].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and strategies for improving KGE models.",
      "processing_time": 29.709140300750732,
      "citing_paper_id": "257220329",
      "cited_paper_id": 245124050
    },
    {
      "context_text": "Besides,Meta-iKG[89]veriÔ¨Åestheeffectivenessofmeta-learningintheKGRtask.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide specific details about a dataset. It mentions 'Meta-iKG' which appears to be a method or model rather than a dataset.",
      "processing_time": 32.90294551849365,
      "citing_paper_id": "257220329",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "As for the unseen relation inference, few-shot KGR models [89], [101], [195] tend to improve the generalization ability of models so that the trained model can scale well to the unseen relations with a small number of facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of few-shot KGR models. There are no clear identifiers for datasets in the text.",
      "processing_time": 32.934293270111084,
      "citing_paper_id": "257220329",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "Basedonit,manyincrementalworksarede-velopedbyintroducingdifferentsubgraphstructures,andprop-agationstrategies,includingTACT[87],CoMPILE[88],Meta-iKG[89],SNRI[90],RPC-IR[91],andetc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 32.29272198677063,
      "citing_paper_id": "257220329",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "Basedonit,manyincrementalworksarede-velopedbyintroducingdifferentsubgraphstructures,andprop-agationstrategies,includingTACT[87],CoMPILE[88],Meta-iKG[89],SNRI[90],RPC-IR[91],andetc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 32.29272198677063,
      "citing_paper_id": "257220329",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "Basedonit,manyincrementalworksarede-velopedbyintroducingdifferentsubgraphstructures,andprop-agationstrategies,includingTACT[87],CoMPILE[88],Meta-iKG[89],SNRI[90],RPC-IR[91],andetc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 32.29272198677063,
      "citing_paper_id": "257220329",
      "cited_paper_id": 250629390
    },
    {
      "context_text": "Basedonit,manyincrementalworksarede-velopedbyintroducingdifferentsubgraphstructures,andprop-agationstrategies,includingTACT[87],CoMPILE[88],Meta-iKG[89],SNRI[90],RPC-IR[91],andetc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 32.29272198677063,
      "citing_paper_id": "257220329",
      "cited_paper_id": 259949958
    },
    {
      "context_text": "2022 Tlogic [128] Extrapolation Time-Operation 2021 HIP [129] Extrapolation GRU",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is focused on extrapolation reasoning using temporal knowledge graphs.",
      "processing_time": 31.821474075317383,
      "citing_paper_id": "257220329",
      "cited_paper_id": 237100866
    },
    {
      "context_text": "HIP network [129] passes information from temporal, structural, and repetitive perspectives, which are used to mine the graph‚Äôs dynamic evolution, the interactions of events at the same time step, and the known events respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HIP network) and its application in reasoning over temporal knowledge graphs.",
      "processing_time": 30.412307500839233,
      "citing_paper_id": "257220329",
      "cited_paper_id": 237100866
    },
    {
      "context_text": "However, most existing artiÔ¨Åcial intelligence (AI) models only imitate the learning procedure from experiences while ignoring the former [1], [2], thus making them less explainable and worse performances.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to AI models in general terms without naming any particular resources.",
      "processing_time": 32.28246450424194,
      "citing_paper_id": "257220329",
      "cited_paper_id": 238583240
    },
    {
      "context_text": "Recently, RSME [244] designed a forget gate with an MRP metric to select valuable images for multi-modal KGR, which tries to avoid the inÔ¨Çuence caused by the noise from irrelevant images corresponding to entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MRP metric) used in multi-modal knowledge graph reasoning. No verifiable resources are identified.",
      "processing_time": 32.879393100738525,
      "citing_paper_id": "257220329",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "2021 RPC-IR [74] Inductive GNN 2016 TranSparse [23] Transductive Translational",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is too limited to infer the use of any datasets.",
      "processing_time": 32.04496455192566,
      "citing_paper_id": "257220329",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "Based on it, many incremental works are developed, including TACT [70], CoMPILE [71], Meta-iKG [72], SNRI [73], RPC-IR [74], and etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles also do not provide clear evidence of dataset usage.",
      "processing_time": 32.87424945831299,
      "citing_paper_id": "257220329",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "Based on it, many incremental works are developed, including TACT [70], CoMPILE [71], Meta-iKG [72], SNRI [73], RPC-IR [74], and etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles also do not provide clear evidence of dataset usage.",
      "processing_time": 32.87424945831299,
      "citing_paper_id": "254564635",
      "cited_paper_id": 229213013
    },
    {
      "context_text": "Based on it, many incremental works are developed, including TACT [70], CoMPILE [71], Meta-iKG [72], SNRI [73], RPC-IR [74], and etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles also do not provide clear evidence of dataset usage.",
      "processing_time": 32.87424945831299,
      "citing_paper_id": "254564635",
      "cited_paper_id": 232135385
    },
    {
      "context_text": "Based on it, many incremental works are developed, including TACT [70], CoMPILE [71], Meta-iKG [72], SNRI [73], RPC-IR [74], and etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles also do not provide clear evidence of dataset usage.",
      "processing_time": 32.87424945831299,
      "citing_paper_id": "254564635",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "Based on it, many incremental works are developed, including TACT [70], CoMPILE [71], Meta-iKG [72], SNRI [73], RPC-IR [74], and etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or models. The cited papers' titles also do not provide clear evidence of dataset usage.",
      "processing_time": 32.87424945831299,
      "citing_paper_id": "254564635",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "GEN[84]and HRFN[85]learnentityembeddingsbasedonmeta-learningfor bothseen-to-unseenandunseen-to-unseenfacts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on meta-learning and entity embeddings, which are methods, not datasets.",
      "processing_time": 33.212708473205566,
      "citing_paper_id": "257220329",
      "cited_paper_id": 240230810
    },
    {
      "context_text": "Moreover, TripleRE [41] creatively divides the relationship vector into three parts, takes advantage of the concept of residual, and achieves better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TripleRE. The context focuses on the methodology and performance improvements of TripleRE.",
      "processing_time": 32.54535436630249,
      "citing_paper_id": "257220329",
      "cited_paper_id": 245591477
    },
    {
      "context_text": "NBF-net[96]andRED-GNN[97] achievebetterefÔ¨Åciencybyleveragingthetraditionalalgo-rithm, i.e., bellman-ford algorithm and dynamic programming to optimize the propagation strategy in previous GNN models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and algorithms. There are no clear identifiers for datasets in the text.",
      "processing_time": 31.777798652648926,
      "citing_paper_id": "257220329",
      "cited_paper_id": 246210481
    },
    {
      "context_text": "Typically, there are two multi-modal KG [21], i.e., N-MMKG and A-MMKG (See Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [
        "N-MMKG",
        "A-MMKG"
      ],
      "dataset_descriptions": {
        "N-MMKG": "Mentioned as a type of multi-modal knowledge graph, but specific usage and research context are not provided in the citation.",
        "A-MMKG": "Mentioned as a type of multi-modal knowledge graph, but specific usage and research context are not provided in the citation."
      },
      "confidence_score": 0.3,
      "reasoning": "The citation mentions two multi-modal KGs, N-MMKG and A-MMKG, but does not provide enough context to determine their specific usage or characteristics. The names are plausible and specific, but the citation intent and resource type are unclear.",
      "processing_time": 48.145344972610474,
      "citing_paper_id": "257220329",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "Besides, to handle the complex relational facts with a unified model, PairRE [30] models each relation representation with paired vectors to adaptive adjustment for complex relations, and HousE [31] involves a novel parameterization based on the designed Householder transformations for rotation and projection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 32.25309085845947,
      "citing_paper_id": "257220329",
      "cited_paper_id": 246867470
    },
    {
      "context_text": "Based on it, HousE [33] further involves a novel Householder transformation for rotation and projection to guarantee the operations to high-dimensional space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called HousE which uses Householder transformations. No datasets are referenced for use or evaluation.",
      "processing_time": 32.01929044723511,
      "citing_paper_id": "257220329",
      "cited_paper_id": 246867470
    },
    {
      "context_text": "2022 HousE [31] Transductive Translational 2019 IterE [88] Inductive Logic Rule",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 32.015801668167114,
      "citing_paper_id": "257220329",
      "cited_paper_id": 246867470
    },
    {
      "context_text": "ExKGR [169] introduces LSTM for reasoning in temporal KGs and provides the reasoning paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ExKGR' but does not refer to it as a dataset. It is described as a method or system that introduces LSTM for reasoning in temporal KGs.",
      "processing_time": 34.72889852523804,
      "citing_paper_id": "257220329",
      "cited_paper_id": 248435922
    },
    {
      "context_text": "Moreover, A‚àóStar [242] Net further optimizes the aggregation procedure with the greedy algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (A‚àóStar Net) and an optimization technique (greedy algorithm).",
      "processing_time": 32.84013748168945,
      "citing_paper_id": "257220329",
      "cited_paper_id": 249210042
    },
    {
      "context_text": "To this end, some existing works try to optimize the propagation procedures in a progressive manner [242].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimizing propagation procedures in knowledge graph reasoning.",
      "processing_time": 29.85069441795349,
      "citing_paper_id": "257220329",
      "cited_paper_id": 249210042
    },
    {
      "context_text": "To this end, some existing works try to optimize the propagation procedures in a progressive manner [242].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimizing propagation procedures in knowledge graph reasoning.",
      "processing_time": 29.85069441795349,
      "citing_paper_id": "254564635",
      "cited_paper_id": 249210042
    },
    {
      "context_text": "2022 ConGLR [85] Inductive GNN 2019 KBGAT [59] Transductive GNN",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 32.86773204803467,
      "citing_paper_id": "257220329",
      "cited_paper_id": 250340389
    },
    {
      "context_text": "Moreover, BERTRL [84] and ConGLR [85] integrates the context for each entity to enhance the reasoning on the knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context is about integrating entity context to enhance reasoning on knowledge graphs.",
      "processing_time": 32.829649448394775,
      "citing_paper_id": "257220329",
      "cited_paper_id": 250340389
    },
    {
      "context_text": "However, most existing artificial intelligence (AI) models only imitate the learning procedure from experiences while ignoring the former [1], [2], [3], [4], thus making them less explainable and worse performances.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a general limitation of AI models.",
      "processing_time": 31.99294352531433,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252199918
    },
    {
      "context_text": "Besides, MM-RNS [262] and CKGC [263] leverage contrastive learning strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MM-RNS and CKGC but does not specify them as datasets. They are likely methods or models, not datasets.",
      "processing_time": 32.82084679603577,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252782076
    },
    {
      "context_text": "Moreover, recurrent neural networks (RNNs) are wildly integrated with GNN models for better expression of the time information for temporal KGR [8], [9], [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of RNNs with GNNs for temporal knowledge graph reasoning.",
      "processing_time": 32.51180577278137,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252905025
    },
    {
      "context_text": "Moreover, recurrent neural networks (RNNs) are wildly integrated with GNN models for better expression of the time information for temporal KGR [8], [9], [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of RNNs with GNNs for temporal knowledge graph reasoning.",
      "processing_time": 32.51180577278137,
      "citing_paper_id": "254564635",
      "cited_paper_id": 233324265
    },
    {
      "context_text": "Moreover, MoSE [245] exploits three ensemble inference techniques to combine the modality-split predictions by assessing modality importance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MoSE) and its approach to multimodal knowledge graph completion.",
      "processing_time": 32.53395652770996,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "MetaTKGR[195]dynamicallyadjuststhestrategiesofsampling andaggregatingneighborsfromrecentfactsfornewentities throughtemporallysupervisedsignalsonfuturefactsasinstant feedback.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MetaTKGR, which appears to be a method or model rather than a dataset. No specific dataset names are mentioned in the citation span.",
      "processing_time": 32.84483027458191,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252918814
    },
    {
      "context_text": "MetaTKGR [121] dynamically adjusts the strategies of sampling and aggregating neighbors from recent facts for new entities through temporally supervised signals on future facts as instant feedback.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for reasoning over temporal knowledge graphs.",
      "processing_time": 30.6625657081604,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252918814
    },
    {
      "context_text": "As for the unseen relation inference, few-shot KGR models [72], [86], [121] tend to improve the generalization ability of models so that the trained model can scale well to the unseen relations with a small amount of facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of few-shot KGR models and their ability to generalize to unseen relations. No verifiable resources are named.",
      "processing_time": 34.409796714782715,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252918814
    },
    {
      "context_text": "As for the unseen relation inference, few-shot KGR models [72], [86], [121] tend to improve the generalization ability of models so that the trained model can scale well to the unseen relations with a small amount of facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of few-shot KGR models and their ability to generalize to unseen relations. No verifiable resources are named.",
      "processing_time": 34.409796714782715,
      "citing_paper_id": "254564635",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "2022 MetaTKGR [121] Extrapolation Time-Operation 2021 TeLM [122] Interpolation Time-Operation",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers and their contributions to temporal knowledge graph reasoning.",
      "processing_time": 31.93378496170044,
      "citing_paper_id": "257220329",
      "cited_paper_id": 252918814
    },
    {
      "context_text": "After that, [16] and [17] propose more Ô¨Åne-grained categorizations for logic-based and embedding-based KGR models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categorizations of KGR models.",
      "processing_time": 30.31261110305786,
      "citing_paper_id": "257220329",
      "cited_paper_id": 253862877
    },
    {
      "context_text": "Besides, MSNEA [235] and IMF [12] makes use of contrastive learning for multi-modal alignment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'contrastive learning for multi-modal alignment' but does not specify any datasets. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 34.41394376754761,
      "citing_paper_id": "257220329",
      "cited_paper_id": 257631615
    },
    {
      "context_text": "Besides, MSNEA [178] and IMF [179] makes use of contrastive learning for multi-modal alignment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'contrastive learning for multi-modal alignment' but does not specify any datasets. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 34.644805908203125,
      "citing_paper_id": "257220329",
      "cited_paper_id": 257631615
    },
    {
      "context_text": "2022 LogCo [86] Inductive GNN 2019 M-GNN [59] Transductive GNN 2022 REPORT [87] Inductive GNN 2019 SACN [62] Transductive GNN 2022 RED-GNN [81] Inductive GNN 2019 KBGAT [60] Transductive GNN 2022 ConGLR [84] Inductive GNN 2019 LAN [61] Inductive GNN 2022 TripleRE [33] Transductive Translational 2019 CPL [88] Transductive Relation Path 2022 InterHT [34] Transductive Translational 2019 IterE [89] Inductive Logic Rule 2022 HousE [32] Transductive Translational 2019 pLogicNet [90] Inductive Logic Rule 2022 BERTRL [83] Inductive GNN 2019 DRUM [91] Inductive Logic Rule 2022 SNRI [75] Inductive GNN 2019 RLvLR [92] Inductive Logic Rule 2022 TEMP [78] Inductive GNN 2019 Neural-Num-LP [93] Inductive Logic Rule 2022 RMPI [77] Inductive GNN 2018 SimplE [40] Transductive Tensor Decompositional 2022 Meta-iKG [74] Inductive GNN 2018 ConvKB [53] Transductive CNN 2022 CSR [85] Inductive GNN 2018 ConvE [52] Transductive CNN 2022 CURL [94] Transductive Relation Path 2018 RGCN [58] Transductive GNN 2022 GCR [95] Inductive Logic Rule 2018 M-walk [96] Transductive Relation Path 2021 PairRE [31] Transductive Translational 2018 MultiHop [97] Transductive Relation Path 2021 HopfE [45] Transductive Tensor Decompositional 2018 DIVA [98] Transductive Logic Rule 2021 DualE [44] Transductive Tensor Decompositional 2018 RuleN [99] Inductive Logic Rule 2021 ConEx [57] Transductive CNN 2018 RUGE [100] Inductive Logic Rule 2021 KE-GCN [66] Transductive GNN 2017 ANALOGY [39] Transductive Tensor Decompositional 2021 HRFN [69] Inductive GNN 2017 ProjE [51] Transductive Traditional NN 2021 GEN [68] Inductive GNN 2017 MINERVA [101] Transductive Relation Path 2021 INDIGO [70] Inductive GNN 2017 DeepPath [102] Transductive Relation Path 2021 NBF-Net [80] Inductive GNN 2017 NTP [103] Inductive Logic Rule 2021 CoMPILE [73] Inductive GNN 2017 NeuralLP [104] Inductive Logic Rule 2021 TACT [72] Inductive GNN 2016 TranSparse [24] Transductive Translational 2021 RPC-IR [76] Inductive GNN 2016 TransG [23] Transductive Translational 2020 HAKE [29] Transductive Translational 2016 HolE [38] Transductive Tensor Decompositional 2020 TransRHS [30] Transductive Translational 2016 ComplEx [37] Transductive Tensor Decompositional 2020 LowFER [46] Transductive Tensor Decompositional 2016 NAM [50] Transductive Traditional NN 2020 InteractE [56] Transductive CNN 2016 LogSumExp [105] Transductive Relation Path 2020 DPMPN [64] Transductive GNN 2016 KALE [106] Inductive Logic Rule 2020 RGHAT [65] Transductive GNN 2015 TransD [21] Transductive Translational 2020 COMPGCN [67] Transductive GNN 2015 TransR [20] Transductive Translational 2020 GraIL [71] Inductive GNN 2015 KG2E [22] Transductive Translational 2020 ExpressGNN [107] Inductive Logic Rule 2015 DISTMULT [36] Transductive Tensor Decompositional 2020 pGAT [82] Inductive GNN 2015 RNNPRA [108] Transductive Relation Path 2019 RotatE [28] Transductive Translational 2014 TransH [19] Transductive Translational 2019 TransW [27] Inductive Translational 2014 ProPPR [109] Transductive Relation Path 2019 MuRP [26] Transductive Translational 2013 AMIE [110] Inductive Logic Rule 2019 QuatE [43] Transductive Tensor Decompositional 2013 SME [48] Transductive Traditional NN 2019 TuckER [41] Transductive Tensor Decompositional 2013 NTN [49] Transductive Traditional NN 2019 CrossE [42] Transductive Tensor Decompositional 2013 TransE [18] Transductive Translational 2019 ConvR [55] Transductive CNN 2011 RESCAL [35] Transductive Tensor Decompositional 2019 HypER [54] Transductive CNN 2010 PRA [111] Transductive Relation Path",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. No verifiable resources are identified.",
      "processing_time": 31.904029369354248,
      "citing_paper_id": "257220329",
      "cited_paper_id": 257912512
    },
    {
      "context_text": "Large language models (LLMs) [317], i.e., ChatGPT, GPT-4, are very popular this year, which have huge impacts due to their promising reasoning capacity and generalizability [318], [319].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only large language models. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 33.29900407791138,
      "citing_paper_id": "257220329",
      "cited_paper_id": 257921533
    },
    {
      "context_text": "Moreover, RPC [114] and RETIA [178] both considers the relational view-graph for better expressive capacity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RPC and RETIA, which are methods/models, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 33.294504165649414,
      "citing_paper_id": "257220329",
      "cited_paper_id": 259949958
    },
    {
      "context_text": "Moreover, RPC [114] and RETIA [178] both considers the relational view-graph for better expressive capacity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RPC and RETIA, which are methods/models, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 33.294504165649414,
      "citing_paper_id": "257220329",
      "cited_paper_id": 260171695
    },
    {
      "context_text": "As for the second problem, some graph-enhanced techniques, such as Chain-of-Thought (CoT) [321] and Graph-of-Thought (GoT) [322], can be treated asagoodchoice.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Chain-of-Thought and Graph-of-Thought).",
      "processing_time": 32.13625407218933,
      "citing_paper_id": "257220329",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "Besides, the idea of graph clustering [299], [300], [301] is also used to save the resource cost.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of graph clustering. No verifiable resources are identified.",
      "processing_time": 32.429360151290894,
      "citing_paper_id": "257220329",
      "cited_paper_id": 263886314
    },
    {
      "context_text": "Although a large amount of KGR methods have been proposed in recent years, demonstrating the great potential of KGR in theoretical Ô¨Åelds, the applications of KGR [308], [309], [310] stillneedtobestudiedmore[311].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about KGR methods and their applications. No verifiable resources are identified.",
      "processing_time": 33.985897064208984,
      "citing_paper_id": "257220329",
      "cited_paper_id": 263896114
    },
    {
      "context_text": "Besides, LSTK [123] makes use of the extra-textual information to generate soft rules to enhance the capacity of rule-based models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTK) that uses extra-textual information. There are no clear identifiers for datasets in the citation context.",
      "processing_time": 35.53973078727722,
      "citing_paper_id": "257220329",
      "cited_paper_id": 268042282
    },
    {
      "context_text": "(1) Embedding-based models generally have better expressive ability but lack explainability [150], [151], [152] Mean-while, more attention is currently focused on developing GNN-based models since the KGR models require a high-quality representation of the relational facts and the graph structure,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the general characteristics of embedding-based models and GNN-based models.",
      "processing_time": 33.05558443069458,
      "citing_paper_id": "257220329",
      "cited_paper_id": 269634664
    },
    {
      "context_text": "Although KGR models generally are more explainable, it is still worthwhile exploring this topic, especially for embedding-based KGR models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about KGR models.",
      "processing_time": 30.60078263282776,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¢ Kinship [200] contains relations of the kinships in Alyawarra tribes [228].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinship"
      ],
      "dataset_descriptions": {
        "Kinship": "Used to study kinship relations in Alyawarra tribes, specifically analyzing the structure and patterns of familial connections."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Kinship' as a dataset containing relations of kinships in Alyawarra tribes. This is a specific, verifiable dataset with a clear identifier.",
      "processing_time": 39.57249450683594,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¢ Kinship [200] contains relations of the kinships in Alyawarra tribes [228].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinship"
      ],
      "dataset_descriptions": {
        "Kinship": "Used to study kinship relations in Alyawarra tribes, specifically analyzing the structure and patterns of familial connections."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Kinship' as a dataset containing relations of kinships in Alyawarra tribes. This is a specific, verifiable dataset with a clear identifier.",
      "processing_time": 39.57249450683594,
      "citing_paper_id": "254564635",
      "cited_paper_id": 6911541
    },
    {
      "context_text": "For example, WAR [153] and KGE-SymCL [3] introduce more information with triplet augmentation and contrastive learning, beneÔ¨Åting various KGR backbones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on techniques for improving knowledge graph reasoning.",
      "processing_time": 32.71113109588623,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "Knowledge graphs (KGs), which store the human knowledge facts in intuitive graph structures [3], are treated as potential solutions these years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to knowledge graphs. No verifiable resources are identified.",
      "processing_time": 32.40270113945007,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "Instead of modeling the time in a discrete space, like a timestamp, leveraging some temporal processing mechanism, such as Hawkes processing [302], [303], may be a direction for it.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Hawkes processing) for temporal data. There are no verifiable resources or datasets mentioned.",
      "processing_time": 34.97941207885742,
      "citing_paper_id": "257220329",
      "cited_paper_id": null
    },
    {
      "context_text": "MTRL [38] offers three different strategies, i.e., simple summation, DeViSE [17], and Imagined [11] to integrate multimodal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (DeViSE, Imagined) but does not refer to any specific datasets. The context is about integrating multimodal information, which is relevant to the topic but does not mention datasets.",
      "processing_time": 36.74100637435913,
      "citing_paper_id": "259375740",
      "cited_paper_id": 261138
    },
    {
      "context_text": "MTRL [38] offers three different strategies, i.e., simple summation, DeViSE [17], and Imagined [11] to integrate multimodal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (DeViSE, Imagined) but does not refer to any specific datasets. The context is about integrating multimodal information, which is relevant to the topic but does not mention datasets.",
      "processing_time": 36.74100637435913,
      "citing_paper_id": "259375740",
      "cited_paper_id": 27263492
    },
    {
      "context_text": "MTRL [38] offers three different strategies, i.e., simple summation, DeViSE [17], and Imagined [11] to integrate multimodal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (DeViSE, Imagined) but does not refer to any specific datasets. The context is about integrating multimodal information, which is relevant to the topic but does not mention datasets.",
      "processing_time": 36.74100637435913,
      "citing_paper_id": "259375740",
      "cited_paper_id": 44145776
    },
    {
      "context_text": ", simple summation, DeViSE [17], and Imagined [11] to integrate multimodal information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DeViSE' but does not indicate it is a dataset. It is likely a method or model, and there are no other specific datasets mentioned.",
      "processing_time": 35.503939390182495,
      "citing_paper_id": "259375740",
      "cited_paper_id": 261138
    },
    {
      "context_text": "Among the models in the first type, there are five unimodal KGR state-of-the-art models, including TransE [5], DisMult [56], Com-plEX [49], ConvE [12], and RGCN [42], and three multimodal state-of-the-art KGR models, including IKRL [54], TransAE [52] and RSME [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on describing models used in knowledge graph reasoning, which are not datasets.",
      "processing_time": 35.6665940284729,
      "citing_paper_id": "259375740",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "To address the limitations, more and more recent attempts [8, 52, 53, 55, 62, 63] for knowledge graph reasoning (KGR) are taken in multimodal scenarios, and our work also lays in this scope.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to knowledge graph reasoning in multimodal scenarios.",
      "processing_time": 33.69644641876221,
      "citing_paper_id": "259375740",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "To address the limitations, more and more recent attempts [8, 52, 53, 55, 62, 63] for knowledge graph reasoning (KGR) are taken in multimodal scenarios, and our work also lays in this scope.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to knowledge graph reasoning in multimodal scenarios.",
      "processing_time": 33.69644641876221,
      "citing_paper_id": "259375740",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "Specifically, both FB15k-237-IMG [38] and WN18 [38] datasets are constructed by extending ten images for each entity based on FB15k-237 and WN18, which are the subset of the large-scale knowledge graph Freebase [4] and WordNet [37], separately.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237-IMG",
        "WN18"
      ],
      "dataset_descriptions": {
        "FB15k-237-IMG": "Used to extend entities in FB15k-237 with images for multi-modal knowledge graph representation learning, enhancing the visual aspect of entity relationships.",
        "WN18": "Used to extend entities in WN18 with images for multi-modal knowledge graph representation learning, integrating visual information into lexical semantic relationships."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FB15k-237-IMG and WN18, which are extensions of FB15k-237 and WN18, respectively. These datasets are used for multi-modal knowledge graph reasoning.",
      "processing_time": 52.661226749420166,
      "citing_paper_id": "259375740",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Specifically, both FB15k-237-IMG [38] and WN18 [38] datasets are constructed by extending ten images for each entity based on FB15k-237 and WN18, which are the subset of the large-scale knowledge graph Freebase [4] and WordNet [37], separately.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237-IMG",
        "WN18"
      ],
      "dataset_descriptions": {
        "FB15k-237-IMG": "Used to extend entities in FB15k-237 with images for multi-modal knowledge graph representation learning, enhancing the visual aspect of entity relationships.",
        "WN18": "Used to extend entities in WN18 with images for multi-modal knowledge graph representation learning, integrating visual information into lexical semantic relationships."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FB15k-237-IMG and WN18, which are extensions of FB15k-237 and WN18, respectively. These datasets are used for multi-modal knowledge graph reasoning.",
      "processing_time": 52.661226749420166,
      "citing_paper_id": "259375740",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "For example, IKRL [54] and MTRL [38] are both developed based on TransE [5] but with different fusion strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing different approaches to knowledge graph representation learning.",
      "processing_time": 35.494781255722046,
      "citing_paper_id": "259375740",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit many practical down-stream tasks, such as recommendation systems [21, 60], information retrieval [14, 57], and visual question answering [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of multimodal knowledge graphs. No dataset names are present in the text.",
      "processing_time": 35.65195083618164,
      "citing_paper_id": "259375740",
      "cited_paper_id": 49644765
    },
    {
      "context_text": "Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit many practical down-stream tasks, such as recommendation systems [21, 60], information retrieval [14, 57], and visual question answering [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of multimodal knowledge graphs. No dataset names are present in the text.",
      "processing_time": 35.65195083618164,
      "citing_paper_id": "259375740",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit many practical down-stream tasks, such as recommendation systems [21, 60], information retrieval [14, 57], and visual question answering [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of multimodal knowledge graphs. No dataset names are present in the text.",
      "processing_time": 35.65195083618164,
      "citing_paper_id": "259375740",
      "cited_paper_id": 195351633
    },
    {
      "context_text": "Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit many practical down-stream tasks, such as recommendation systems [21, 60], information retrieval [14, 57], and visual question answering [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of multimodal knowledge graphs. No dataset names are present in the text.",
      "processing_time": 35.65195083618164,
      "citing_paper_id": "259375740",
      "cited_paper_id": 220730157
    },
    {
      "context_text": "Due to their promising capabilities in multimodal scenarios, various general multimodal pretrained transformer (MPT) models [9, 19, 25, 27, 34, 43, 45] have been proposed these years.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 34.775399923324585,
      "citing_paper_id": "259375740",
      "cited_paper_id": 201058752
    },
    {
      "context_text": "Due to their promising capabilities in multimodal scenarios, various general multimodal pretrained transformer (MPT) models [9, 19, 25, 27, 34, 43, 45] have been proposed these years.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 34.775399923324585,
      "citing_paper_id": "259375740",
      "cited_paper_id": 201103729
    },
    {
      "context_text": "Specifically, both FB15k-237-IMG [38] and WN18 [38] datasets are constructed by extending ten images for each entity based on FB15k-237 and WN18, which are the subset of the largescale knowledge graph Freebase [4] and WordNet [37], separately.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237-IMG",
        "WN18"
      ],
      "dataset_descriptions": {
        "FB15k-237-IMG": "Used to extend FB15k-237 with images for each entity, enhancing multi-modal reasoning in knowledge graphs.",
        "WN18": "Used to extend WN18 with images for each entity, enhancing multi-modal reasoning in knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, FB15k-237-IMG and WN18, which are extensions of FB15k-237 and WN18, respectively. These datasets are used for multi-modal knowledge graph reasoning.",
      "processing_time": 53.22757625579834,
      "citing_paper_id": "259375740",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "Besides, the HAKE is the most effective choice among these three structure encoders.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HAKE).",
      "processing_time": 34.94919204711914,
      "citing_paper_id": "259375740",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "We also replace the structure encoder ùëî (¬∑) , i.e., HAKE [61], as three other typical structure encoders, including CompGCN [50], and Nodepiece [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on replacing a structure encoder with other encoders.",
      "processing_time": 37.55034375190735,
      "citing_paper_id": "259375740",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "This paper selects one of the state-of-the-art models, HAKE [61], as the structure encoder to generate the structural features for most of the experiments.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HAKE as a model, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 36.85031318664551,
      "citing_paper_id": "259375740",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "As for the structure encoder, most of our experiments are carried out based on the HAKE [61], but we also evaluate the performance of our model with other typical structure encoders, including HousE [28], and COMPGCN [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models (HAKE, HousE, COMPGCN) but does not refer to any specific datasets. The context is about evaluating different structure encoders, not using datasets.",
      "processing_time": 39.9754421710968,
      "citing_paper_id": "259375740",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "Different knowledge graph embedding models contribute to the candidate structure encoder ùëî (¬∑) , such as HAKE [61], ComPGCN [50], Nodepiece [18], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models but does not refer to any specific datasets. The context is focused on describing different knowledge graph embedding models rather than using a particular dataset.",
      "processing_time": 39.758334159851074,
      "citing_paper_id": "259375740",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "For example, the conventional KGR models [10, 29, 31, 32, 35, 36, 39, 46, 50, 59] focus on mining the structural information in static KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conventional KGR models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 39.6309278011322,
      "citing_paper_id": "259375740",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "For example, the conventional KGR models [10, 29, 31, 32, 35, 36, 39, 46, 50, 59] focus on mining the structural information in static KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conventional KGR models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 39.6309278011322,
      "citing_paper_id": "259375740",
      "cited_paper_id": 259145427
    },
    {
      "context_text": "For example, the conventional KGR models [10, 29, 31, 32, 35, 36, 39, 46, 50, 59] focus on mining the structural information in static KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conventional KGR models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 39.6309278011322,
      "citing_paper_id": "259375740",
      "cited_paper_id": 259165563
    },
    {
      "context_text": "Meanwhile, pretrained transformer models [7, 8, 20, 24, 26, 33, 44] for KGR are also springing up, such as KG-BERT [58], which is the first pretrained contextual language model for the KGR task, etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on pretrained transformer models and their application to Knowledge Graph Reasoning (KGR).",
      "processing_time": 40.515514612197876,
      "citing_paper_id": "259375740",
      "cited_paper_id": 221376974
    },
    {
      "context_text": "The transformer models originated form natural language processing [6, 13, 41], and quickly shifted the paradigm of image processing [3, 15] from fully supervised learning to pretraining and fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the shift in paradigms from fully supervised learning to pretraining and fine-tuning in image processing.",
      "processing_time": 40.297205448150635,
      "citing_paper_id": "259375740",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "The transformer models originated form natural language processing [6, 13, 41], and quickly shifted the paradigm of image processing [3, 15] from fully supervised learning to pretraining and fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the shift in paradigms from fully supervised learning to pretraining and fine-tuning in image processing.",
      "processing_time": 40.297205448150635,
      "citing_paper_id": "259375740",
      "cited_paper_id": 235436185
    },
    {
      "context_text": "Besides, BERT [13] and ViT [15] are two commonly used transformers for encoding M ùë° and M ùë£ , and structure-guided fusion module (S-Guider) is a novel module designed in our SGMPT to leverage G .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of BERT and ViT as transformers for encoding modalities, which are not datasets.",
      "processing_time": 41.43351650238037,
      "citing_paper_id": "259375740",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Following [8], multimodal information encoder ùëì ùëö (¬∑) aims to model the multimodal features of the entity across the last ùêø ùëö layers of ViT and BERT with multi-level fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the multimodal information encoder and the use of ViT and BERT, which are models, not datasets.",
      "processing_time": 42.45198202133179,
      "citing_paper_id": "259375740",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Following the MKGformer, we adopt the BERT base [13] and ViT-B/32 [15] as the text encoder and vision encoder in our method.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BERT and ViT-B/32 as encoders but does not refer to them as datasets. They are models or methods, not datasets.",
      "processing_time": 41.196227073669434,
      "citing_paper_id": "259375740",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Vision encoder ùëì ùë£ (¬∑) is composed of the first ùêø ùë£ layers of ViT [15], which aims to capture basic visual features from the patched images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ViT (Vision Transformer) but does not refer to a specific dataset. It describes the use of a model (ViT) for capturing visual features, which is not a dataset.",
      "processing_time": 42.95185565948486,
      "citing_paper_id": "259375740",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "and more recent attempts [8, 52, 53, 55, 62, 63] for knowledge graph reasoning (KGR) are taken in multimodal scenarios, and our work also lays in this scope.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to multimodal knowledge graph reasoning. No verifiable resources are identified.",
      "processing_time": 40.716495513916016,
      "citing_paper_id": "259375740",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "and more recent attempts [8, 52, 53, 55, 62, 63] for knowledge graph reasoning (KGR) are taken in multimodal scenarios, and our work also lays in this scope.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to multimodal knowledge graph reasoning. No verifiable resources are identified.",
      "processing_time": 40.716495513916016,
      "citing_paper_id": "259375740",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "Moreover, MoSE [62] exploits three ensemble inference",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called MoSE. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 40.8892617225647,
      "citing_paper_id": "259375740",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "Moreover, MoSE [62] exploits three ensemble inference techniques to combine the modality-split predictions by assessing modality importance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MoSE) and its approach to multimodal knowledge graph completion.",
      "processing_time": 40.70317983627319,
      "citing_paper_id": "259375740",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "This structural information will definitely benefit the expressive ability of the models, which has been proven in those multimodal non-transformer KGR models [52, 62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only refers to multimodal non-transformer KGR models. No verifiable resources are identified.",
      "processing_time": 41.00123572349548,
      "citing_paper_id": "259375740",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "However, these multi-modal data vary in structure [8], with representation forms that include text, tables, and spatial structure.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only discusses the variability in structure of multi-modal data, which is too generic.",
      "processing_time": 41.15460133552551,
      "citing_paper_id": "275176948",
      "cited_paper_id": 227151278
    },
    {
      "context_text": "[5] emphasizes the significance of alarm modality and its implementation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of alarm modality and its implementation. No verifiable resources are identified.",
      "processing_time": 40.998199701309204,
      "citing_paper_id": "275176948",
      "cited_paper_id": 244521828
    },
    {
      "context_text": "In [6], physical structure knowledge is integrated to enhance fault localization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating physical structure knowledge to enhance fault localization.",
      "processing_time": 40.04142689704895,
      "citing_paper_id": "275176948",
      "cited_paper_id": 264057354
    },
    {
      "context_text": "Machine learning (ML) techniques have emerged as the preferred choice for fault management, achieving high accuracy in tasks such as fault detection [2], fault localization [3], and fault diagnosis [4], typically utilizing monitored device performance data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'monitored device performance data' but does not specify a named dataset. The term is too generic and lacks a specific identifier.",
      "processing_time": 41.94572114944458,
      "citing_paper_id": "275176948",
      "cited_paper_id": 264325404
    },
    {
      "context_text": "Machine learning (ML) techniques have emerged as the preferred choice for fault management, achieving high accuracy in tasks such as fault detection [2], fault localization [3], and fault diagnosis [4], typically utilizing monitored device performance data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'monitored device performance data' but does not specify a named dataset. The term is too generic and lacks a specific identifier.",
      "processing_time": 41.94572114944458,
      "citing_paper_id": "275176948",
      "cited_paper_id": null
    },
    {
      "context_text": "Some researchers have considered introducing generative models such as GAN [39] and VAE [40] to assist in completing modality information or generating labeled data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models (GAN and VAE). These are methods, not datasets, and thus do not meet the criteria for inclusion.",
      "processing_time": 43.07494354248047,
      "citing_paper_id": "274397704",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "RESCAL [22] introduces a matrix-based tensor decomposition method that performs representation learning by minimizing the reconstruction error of the predicted triplets.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called RESCAL. The context focuses on the method's functionality and application.",
      "processing_time": 41.14321041107178,
      "citing_paper_id": "274397704",
      "cited_paper_id": 1157792
    },
    {
      "context_text": "Subsequently, Lin et al. [19] proposed TransR to project entities and relations into different semantic spaces, providing varying weights within different relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TransR. The context focuses on the methodology and its application in knowledge graph completion.",
      "processing_time": 41.37098979949951,
      "citing_paper_id": "274397704",
      "cited_paper_id": 2949428
    },
    {
      "context_text": "Inspired by TransE [4], DistMult [23], HolE [53], and CrossE [26], we propose four entity‚Äìrelation combining operations for the combination of entity and relation information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on the inspiration from various embedding models.",
      "processing_time": 41.566179037094116,
      "citing_paper_id": "274397704",
      "cited_paper_id": 6071257
    },
    {
      "context_text": "In contrast to the traditional methods that project entity embeddings into the Euclidean space, RotatE [20] defines each object entity as a rotation from the subject entity in the complex space, demonstrating good performance in inferring various relational patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called RotatE. The method is described as a way to embed knowledge graphs using rotations in complex space.",
      "processing_time": 43.05911421775818,
      "citing_paper_id": "274397704",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "‚Ä¢ RotatE [20]: RotatE adopts a rotation operation to encode the relational information between nodes and relationships in the complex space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions RotatE, which is a method for encoding relational information in knowledge graphs, not a dataset. No specific dataset is mentioned.",
      "processing_time": 42.04964900016785,
      "citing_paper_id": "274397704",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Han et al. [43] proposed a joint training model consisting of two modality-specific encoders and a shared classifier, which implicitly integrates information from different modalities for emotion recognition.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for joint training involving multiple modalities.",
      "processing_time": 40.664159536361694,
      "citing_paper_id": "274397704",
      "cited_paper_id": 145832359
    },
    {
      "context_text": "The textual descriptions of the first two datasets are collected by [56], while the textual descriptions of the latter two datasets are collected from DBpedia [57].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia"
      ],
      "dataset_descriptions": {
        "DBpedia": "Used to collect textual descriptions for the latter two datasets, providing structured information for multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions two sources for textual descriptions, but does not specify the names of the datasets. DBpedia is referenced, but it is not clear if it is used as a dataset or just a source of information.",
      "processing_time": 50.75798940658569,
      "citing_paper_id": "274397704",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "TransAE [13] further integrates image and description features of entities, extending TransE into a multi-modal scoring function to decode the predicted triplets.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions the integration of image and description features but does not specify a dataset. The cited paper title suggests multimodal data but does not provide a specific dataset name.",
      "processing_time": 43.9024920463562,
      "citing_paper_id": "274397704",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "‚Ä¢ TransAE [13]: TransAE is based on TransE by employing a multi-modal auto-encoder to integrate visual and textual information into a unified representation, regarding the fused embedding as the entity representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransAE) that integrates visual and textual information. No verifiable datasets are referenced.",
      "processing_time": 42.61831617355347,
      "citing_paper_id": "274397704",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "TransAE [13] uses a five-layer multi-modal encoder to automatically fuse visual and textual features, applying a TransE decoder for entity and relation prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransAE) and its components. The context focuses on the architecture and functionality of the model rather than the data used.",
      "processing_time": 44.43186020851135,
      "citing_paper_id": "274397704",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "InteractE [27] employs feature permutation, checkered reshaping, and circular convolution to capture diverse heterogeneous interactions, enhancing the model‚Äôs expression ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (InteractE) and its features. The context focuses on the technical aspects of the model rather than the use of a dataset.",
      "processing_time": 44.94050407409668,
      "citing_paper_id": "274397704",
      "cited_paper_id": 207775322
    },
    {
      "context_text": "We propose two decoders to validate the effectiveness of MERGE according to ConvE [25] and InteractE [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (ConvE and InteractE). The context focuses on validating the effectiveness of a method using other methods.",
      "processing_time": 43.62087559700012,
      "citing_paper_id": "274397704",
      "cited_paper_id": 207775322
    },
    {
      "context_text": "‚Ä¢ InteractE [27]: InteractE employs feature permutation, checkered reshaping, and circular convolution to capture diverse heterogeneous interactions between entities and relationships.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called InteractE. The context focuses on describing the method's features and operations.",
      "processing_time": 42.61536502838135,
      "citing_paper_id": "274397704",
      "cited_paper_id": 207775322
    },
    {
      "context_text": "As a textual-visual pre-trained model, CLIP [18] has demonstrated promising zero-shot performance on various image classification datasets.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions CLIP as a model, not a dataset. No specific datasets are named in the context.",
      "processing_time": 41.32527041435242,
      "citing_paper_id": "274397704",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "To accurately capture the multi-modal information of entities, we first utilize a visual filtering gate to filter the noise images, pre-training the visual and textual features on the CLIP [18] model to obtain semantically aligned modal-specific entity embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-training on the CLIP model, which is a method, not a dataset. No specific datasets are mentioned.",
      "processing_time": 41.33155155181885,
      "citing_paper_id": "274397704",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "To address this, RSME [34] proposes a filter gate and a forget gate to obtain valuable visual embeddings, modeling entity relationships through the ComplEx scoring function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RSME) and a scoring function (ComplEx).",
      "processing_time": 42.014673948287964,
      "citing_paper_id": "274397704",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "Inspired by the method of [34], we introduce a visual filtering gate before the image encoder to eliminate noisy images from the datasets.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for filtering noisy images. No verifiable datasets are named.",
      "processing_time": 41.67562508583069,
      "citing_paper_id": "274397704",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "‚Ä¢ RSME [34]: RSME is a method that proposes a filter gate and a forget gate to obtain valuable visual embeddings, applying the score function of ComplEx to model the structural information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called RSME. The context focuses on the method's functionality and application.",
      "processing_time": 42.1603467464447,
      "citing_paper_id": "274397704",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "‚Ä¶model is evaluated on four publicly available datasets: FB15k-237-IMG [55] (https://github.com/mniepert/mmkb, accessed on 11 May 2018), WN18RR-IMG [34] (https://github.com/wangmengsd/RSME, accessed on 14 January 2022), DB15K-IMG [55] (https://github.com/mniepert/mmkb, accessed on 11 May 2018),‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237-IMG",
        "WN18RR-IMG",
        "DB15K-IMG"
      ],
      "dataset_descriptions": {
        "FB15k-237-IMG": "Used to evaluate the model's performance on multi-modal knowledge graph reasoning, focusing on visual context integration and representation learning.",
        "WN18RR-IMG": "Used to assess the model's ability to handle relational reasoning with visual context, emphasizing the impact of image data on knowledge graph embeddings.",
        "DB15K-IMG": "Used to test the model's effectiveness in multi-modal reasoning across different knowledge bases, incorporating visual information to enhance entity linking."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions four specific datasets used for evaluating a model. These datasets are clearly named and have public access points, making them verifiable resources.",
      "processing_time": 66.20366430282593,
      "citing_paper_id": "274397704",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "Specifically, based on the perceptual hash algorithm (pHash) [49], we first convert the color image to grayscale and resize it to a fixed dimension of 32 √ó 32.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for converting images using pHash.",
      "processing_time": 41.67322850227356,
      "citing_paper_id": "274397704",
      "cited_paper_id": 240241582
    },
    {
      "context_text": "The real world consists of a variety of modal information [5], where entity description enhances the understanding of abstract concepts, and additional image information visually aids in distinguishing entity attributes and categories.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about multi-modal information and knowledge graphs.",
      "processing_time": 41.30976700782776,
      "citing_paper_id": "274397704",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "Nowadays, multi-modal knowledge graphs have been widely utilized in various knowledge-driven downstream tasks, such as object detection [6,7], fake detection [8,9], and sentiment analysis [10,11].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of multi-modal knowledge graphs. No verifiable resources are identified.",
      "processing_time": 42.145100831985474,
      "citing_paper_id": "274397704",
      "cited_paper_id": 249440365
    },
    {
      "context_text": "Nowadays, multi-modal knowledge graphs have been widely utilized in various knowledge-driven downstream tasks, such as object detection [6,7], fake detection [8,9], and sentiment analysis [10,11].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of multi-modal knowledge graphs. No verifiable resources are identified.",
      "processing_time": 42.145100831985474,
      "citing_paper_id": "274397704",
      "cited_paper_id": 266939061
    },
    {
      "context_text": "HRGAT [17] further incorporates numerical features into a graph attention network, considering the varying importance of neighboring nodes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HRGAT) and its enhancements. The context focuses on the model's architecture and feature incorporation.",
      "processing_time": 44.18025016784668,
      "citing_paper_id": "274397704",
      "cited_paper_id": 250118042
    },
    {
      "context_text": "‚Ä¢ HRGAT [17]: HRGAT is a GNN-based model that captures multi-source information and the graphical structure information in MMKGs. ‚Ä¢ CMGNN [36]: CMGNN achieves multi-modal and high-order structure modeling under the graph neural networks with a contrastive learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the models HRGAT and CMGNN and their capabilities in handling multi-modal knowledge graphs.",
      "processing_time": 45.36078906059265,
      "citing_paper_id": "274397704",
      "cited_paper_id": 250118042
    },
    {
      "context_text": "Among these efforts, HRGAT [17] stands out.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HRGAT). The context is too limited to infer the use of any datasets.",
      "processing_time": 42.557960987091064,
      "citing_paper_id": "274397704",
      "cited_paper_id": 250118042
    },
    {
      "context_text": "In contrast, decision-level fusion has been shown to offer greater interpretability and flexibility by combining modality-specific outputs explicitly [44,45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multimodal knowledge graph completion.",
      "processing_time": 41.06134390830994,
      "citing_paper_id": "274397704",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "OTKGE [37] models the multi-modal fusion process as a transportation process that moves different modal embeddings into a unified space, considering the optimal transport strategy for three modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (OTKGE) for multi-modal fusion in knowledge graphs, but does not mention any specific datasets.",
      "processing_time": 42.293946266174316,
      "citing_paper_id": "274397704",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "‚Ä¢ OTKGE [37]: OTKGE overcomes the spatial heterogeneity of different modalities by minimizing the Wasserstein distance between the multi-modal distributions, modeling the multi-modal fusion process as a transmission process to transport different modal embeddings into a unified space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (OTKGE) and its approach to multi-modal fusion. No verifiable datasets are referenced.",
      "processing_time": 43.86144423484802,
      "citing_paper_id": "274397704",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "Zhang et al. [41] proposed MACO to generate missing visual features, designing cross-modal contrastive loss to improve the quality of the generated features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MACO, which is a method for generating missing visual features in multi-modal knowledge graphs, not a dataset.",
      "processing_time": 41.46066904067993,
      "citing_paper_id": "274397704",
      "cited_paper_id": 260887576
    },
    {
      "context_text": "Then, a Discrete Cosine Transform (DCT) [50] is applied to extract the primary features of the image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Discrete Cosine Transform (DCT) but does not refer to any specific dataset. DCT is a method, not a dataset.",
      "processing_time": 43.82157850265503,
      "citing_paper_id": "274397704",
      "cited_paper_id": 267841518
    },
    {
      "context_text": "SR-GNN SR-GNN [31] develops two semantic aggregation modules to pay attention to the influence of semantic information among neighboring entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SR-GNN) and its components. No verifiable resources are identified.",
      "processing_time": 42.94087243080139,
      "citing_paper_id": "274397704",
      "cited_paper_id": 269684581
    },
    {
      "context_text": "‚Ä¢ SR-GNN [31]: SR-GNN is a GNN-based method, which uses two semantic aggregation modules to combine the semantic similarity information between neighboring entities and the relational features of knowledge graphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (SR-GNN) rather than a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 42.54536581039429,
      "citing_paper_id": "274397704",
      "cited_paper_id": 269684581
    },
    {
      "context_text": "‚Ä¶include the following: 1) TransE [26], a classic translation-based model that encodes entities and relationships into a linear space; 2) DistMult [53], which uses a linear neural network to encode a multi-relation graph for multi-relation learning; 3) ComplEx [54], which solves both symmetric‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions models (TransE, DistMult, ComplEx) but does not reference any specific datasets. The context is focused on describing the models used for multi-relational learning in knowledge graphs.",
      "processing_time": 46.27063488960266,
      "citing_paper_id": "259726922",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "ConvE learns deeper features using two-dimensional convolutional layers [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to a method (ConvE) and its use of 2D convolutional layers.",
      "processing_time": 43.21143960952759,
      "citing_paper_id": "259726922",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "DIVA proposed a uniÔ¨Åed reasoning framework that divides multi-hop reasoning into a path search and path inference steps [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework for reasoning over knowledge graphs.",
      "processing_time": 41.44695496559143,
      "citing_paper_id": "259726922",
      "cited_paper_id": 4669223
    },
    {
      "context_text": "RGCN encodes each entity into a vector, uses speciÔ¨Åc transformations to aggregate neighborhood information for di Ô¨Ä erent relationship categories, and then reproduces facts through a decoder [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RGCN) and its functionality. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 44.33615159988403,
      "citing_paper_id": "259726922",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "With the continuous improvement in neural networks (NN) in learning and expressing knowledge, additional embedding-based models choose to use neural network architectures to implement KGC. NTN uses neural tensor networks for relation reasoning in KG [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions NTN (Neural Tensor Networks) but does not refer to a specific dataset. It is used to describe a method for knowledge graph completion.",
      "processing_time": 43.79356145858765,
      "citing_paper_id": "259726922",
      "cited_paper_id": 8429835
    },
    {
      "context_text": "Translation-based models such as TransE, TransR, and TransH embed entities and their relations by projection, and use a distance function to score the factual triplets [26‚Äì28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 43.824711084365845,
      "citing_paper_id": "259726922",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Translation-based models such as TransE, TransR, and TransH embed entities and their relations by projection, and use a distance function to score the factual triplets [26‚Äì28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 43.824711084365845,
      "citing_paper_id": "259726922",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "The unimodal methods include the following: 1) TransE [26], a classic translation-based model that encodes entities and relationships into a linear space; 2) DistMult [53], which uses a linear neural network to encode a multi-relation graph for multi-relation learning; 3) ComplEx [54], which solves‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 43.215903997421265,
      "citing_paper_id": "259726922",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "‚Ä¶into a linear space; 2) DistMult [53], which uses a linear neural network to encode a multi-relation graph for multi-relation learning; 3) ComplEx [54], which solves both symmetric and asymmetric relations by introducing complex methods; and 4) RotatE [55], which deÔ¨Ånes relations as rotations‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is focused on describing different approaches to multi-relation learning and link prediction.",
      "processing_time": 44.32037281990051,
      "citing_paper_id": "259726922",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "‚Ä¶(i) IKRL (UNION) [44], which extends TransE to learn about visual representations of entities and structural features of KGs; (ii) TransAE [56], which combines multi-modal encoders with TransE to achieve uniÔ¨Åed representation of visual and textual features; (iii) RSME [57], which uses a‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods and models (IKRL, TransAE, RSME) but does not specify any datasets. The context focuses on describing different approaches to multi-modal knowledge graph reasoning.",
      "processing_time": 45.46346068382263,
      "citing_paper_id": "259726922",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "MKGs typically augment semantic KGs with additional modality data, such as visual and audio attributes, to provide more physically rich representations of the world [14‚Äì16], as illustrated in Figure 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of augmenting semantic knowledge graphs with multimodal data.",
      "processing_time": 42.22996377944946,
      "citing_paper_id": "259726922",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "MKBE and MRCGN integrated different neural encoders and decoders with relation models to embed learning and multi-modal data for inference [14, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the integration of neural encoders, decoders, and relation models for multi-modal data inference.",
      "processing_time": 45.44675803184509,
      "citing_paper_id": "259726922",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "‚Ä¶graph for multi-relation learning; 3) ComplEx [54], which solves both symmetric and asymmetric relations by introducing complex methods; and 4) RotatE [55], which deÔ¨Ånes relations as rotations from the head entity to the tail entity in a complex space to achieve multi-class reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited paper 'RotatE' is a method for knowledge graph embedding, not a dataset.",
      "processing_time": 45.06392216682434,
      "citing_paper_id": "259726922",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Rule-based models such as AMIE and RLvLR utilize symbolic features to perform reasoning through either rule mining or rule searching algorithms [19, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only rule-based models and their methodologies.",
      "processing_time": 40.97968506813049,
      "citing_paper_id": "259726922",
      "cited_paper_id": 125343003
    },
    {
      "context_text": "The graph-based approach to data processing has demonstrated a superior performance in tasks such as assisting information retrieval, question-answering systems, and recommendation systems, when compared to traditional structured data [12, 13].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to graph-based approaches and their performance in various tasks.",
      "processing_time": 42.238943338394165,
      "citing_paper_id": "259726922",
      "cited_paper_id": 135465817
    },
    {
      "context_text": "VisualBERT is considered to be the Ô¨Årst image-text pre-training model, which uses Faster R-CNN to extract visual features and connects them with text embeddings, which are then input into a transformer initialized by BERT [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the VisualBERT model and its components.",
      "processing_time": 43.16315960884094,
      "citing_paper_id": "259726922",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "TransAE introduced a KG representation learning method that integrates multi-channel (visual and language) information in a translation-based framework, and extended the deÔ¨Ånition of triple energy to consider new multi-channel representations [45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (TransAE) that integrates multi-channel information for KG representation learning, but does not explicitly mention a dataset.",
      "processing_time": 42.895490407943726,
      "citing_paper_id": "259726922",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "NBF-Net and RED-GNN improve on traditional algorithms, choosing Bellman-Ford algorithms and dynamic programming to optimize the propagation strategy in previous GNN models, and achieve e Ô¨É ciency improvements [37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on improvements in GNN models using Bellman-Ford algorithms and dynamic programming.",
      "processing_time": 44.56086540222168,
      "citing_paper_id": "259726922",
      "cited_paper_id": 246210481
    },
    {
      "context_text": "Knowledge-CLIP takes entities and relations in KGs as inputs and extracts the original features of these entities and relations [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (Knowledge-CLIP) that processes entities and relations in knowledge graphs.",
      "processing_time": 43.464707374572754,
      "citing_paper_id": "259726922",
      "cited_paper_id": 252917745
    },
    {
      "context_text": "DRAGON proposes a deep bidirectional, self-supervised pretraining method for language knowledge models from text and KGs [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for pretraining language-knowledge graph models.",
      "processing_time": 41.87816619873047,
      "citing_paper_id": "259726922",
      "cited_paper_id": 252968266
    },
    {
      "context_text": "Furthermore, deep learning has demonstrated a superior performance in scenarios with larger datasets, such as multi-view clustering [6‚Äì10].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to 'larger datasets' in general, which is too generic.",
      "processing_time": 43.47178816795349,
      "citing_paper_id": "259726922",
      "cited_paper_id": 257168851
    },
    {
      "context_text": "‚Ä¶namely ImageNet [6], AWA2 [7], and Attribute Pascal and Yahoo (aPY) [8] ImageNet:This dataset aims to use an average of 500-1000 clean and high-resolution images to form the majority of WordNet [9]‚Äôs 80000 synonyms, and to construct a semantic tree of millions of ImageNet images using WordNet.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet",
        "AWA2",
        "Attribute Pascal and Yahoo (aPY)"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to construct a semantic tree of millions of images, aligning with WordNet's 80,000 synonyms, with an average of 500-1000 clean, high-resolution images per synonym.",
        "AWA2": "Used in multi-modal reasoning experiments, though specific details about its usage are not provided in the context.",
        "Attribute Pascal and Yahoo (aPY)": "Used in multi-modal reasoning experiments, though specific details about its usage are not provided in the context."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions three datasets: ImageNet, AWA2, and aPY. ImageNet is described in detail, while AWA2 and aPY are mentioned without additional details.",
      "processing_time": 67.33637475967407,
      "citing_paper_id": "276928279",
      "cited_paper_id": 1671874
    },
    {
      "context_text": "4) Comparison method: ConSE [12]maps images to a semantic embedding space through convex combinations of class label embedding vectors, without the need for additional training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called ConSE. The context focuses on describing the method rather than using a dataset.",
      "processing_time": 43.46583914756775,
      "citing_paper_id": "276928279",
      "cited_paper_id": 1926319
    },
    {
      "context_text": "Gaussian Ort [17] set a simple learning objective inspired by linear discriminant analysis, kernel target alignment, and kernel polarization methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on describing a learning objective and related methods.",
      "processing_time": 43.13262915611267,
      "citing_paper_id": "276928279",
      "cited_paper_id": 3656231
    },
    {
      "context_text": "GCNZ [18] is based on graph convolutional networks and uses both semantic embedding and classification relationships to predict classifiers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GCNZ) and its components. The title of the cited paper suggests the use of semantic embeddings and knowledge graphs, but no specific dataset is named.",
      "processing_time": 46.94615840911865,
      "citing_paper_id": "276928279",
      "cited_paper_id": 4703853
    },
    {
      "context_text": "Our main experiment was tested on three datasets, namely ImageNet [6], AWA2 [7], and Attribute Pascal and Yahoo (aPY) [8] ImageNet:This dataset aims to use an average of 500-1000 clean and high-resolution images to form the majority of WordNet [9]‚Äôs 80000 synonyms, and to construct a semantic tree‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet",
        "AWA2",
        "Attribute Pascal and Yahoo (aPY)"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to test the main experiment, focusing on zero-shot learning by constructing a semantic tree from high-resolution images representing WordNet synonyms.",
        "AWA2": "Used to test the main experiment, focusing on zero-shot learning by leveraging attributes and class labels for animal categories.",
        "Attribute Pascal and Yahoo (aPY)": "Used to test the main experiment, focusing on zero-shot learning by combining attributes and class labels from Pascal VOC and Yahoo datasets."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for testing the main experiment. These datasets are clearly identified and are relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 63.70685577392578,
      "citing_paper_id": "276928279",
      "cited_paper_id": 4852047
    },
    {
      "context_text": "AWA2: This dataset collected 37322 images from 50 categories of the AWA1 dataset [7] from public network resources such as Flickr and Wikipedia.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AWA2"
      ],
      "dataset_descriptions": {
        "AWA2": "Used for zero-shot learning, specifically evaluating models' ability to recognize unseen categories using attributes and visual features."
      },
      "confidence_score": 1.0,
      "reasoning": "AWA2 is a specific dataset with a clear identifier and is used for zero-shot learning, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 49.1625816822052,
      "citing_paper_id": "276928279",
      "cited_paper_id": 4852047
    },
    {
      "context_text": "3) Set details: For the GCN used, in order to avoid over-fitting, we referred to [4] and used the dropout function [11], with a dropout rate of 0.5 for each layer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. The context is about avoiding overfitting in a GCN using dropout.",
      "processing_time": 43.446882486343384,
      "citing_paper_id": "276928279",
      "cited_paper_id": 258220916
    },
    {
      "context_text": "For instance, Zhang et al. [4] developed a zero-shot learning framework that introduced a semantic knowledge graph (KG) to capture correlations between classes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It focuses on a methodological development using a semantic knowledge graph.",
      "processing_time": 42.85043692588806,
      "citing_paper_id": "276928279",
      "cited_paper_id": 258220916
    },
    {
      "context_text": "A zero shot learning method based on a multi modal KG [4]introduces a semantic KG to capture the correlations between classes and learn the visual feature representations of all classes through this correlation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a multi-modal knowledge graph but does not specify a named dataset. The focus is on the method and the concept of using a knowledge graph for zero-shot learning.",
      "processing_time": 44.51731300354004,
      "citing_paper_id": "276928279",
      "cited_paper_id": 258220916
    },
    {
      "context_text": "For each o i , its position feature f p i and region-of-interest (RoI) feature f roi i are used to represent it [12] as where W p , W roi is weight parameter matrix, b p , b roi is bias parameter of f p i and f roi i , respectively, and LN is Layer Normalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model components and features. The context is focused on the technical details of the model architecture.",
      "processing_time": 43.70310306549072,
      "citing_paper_id": "268147816",
      "cited_paper_id": 201103729
    },
    {
      "context_text": "For instance, Mucko [10] constructed a multi-layer knowledge graph, while MuKEA [11] learned from higher-order complex relationships to model a triplet structure.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'multi-layer knowledge graph' and 'higher-order complex relationships', which are concepts rather than specific datasets. No clear, verifiable datasets are named.",
      "processing_time": 45.00538659095764,
      "citing_paper_id": "268147816",
      "cited_paper_id": 219708313
    },
    {
      "context_text": "The external knowledge is utilized to assist the text and image information fusion in [7], [8], [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not provide specific dataset names. It only mentions the use of external knowledge for text and image information fusion.",
      "processing_time": 43.42720127105713,
      "citing_paper_id": "268147816",
      "cited_paper_id": 252780775
    },
    {
      "context_text": "The external knowledge is utilized to assist the text and image information fusion in [7], [8], [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not provide specific dataset names. It only mentions the use of external knowledge for text and image information fusion.",
      "processing_time": 43.42720127105713,
      "citing_paper_id": "268147816",
      "cited_paper_id": 258352810
    },
    {
      "context_text": "The authors in [5] designed a method of directly fusing RGB images and infrared images in wireless channels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing RGB and infrared images. No verifiable resources are identified.",
      "processing_time": 43.672468185424805,
      "citing_paper_id": "268147816",
      "cited_paper_id": 253309006
    },
    {
      "context_text": "‚Ä¶method and a semantic similarity evaluation approach speciÔ¨Åcally for similar content in multi-modal semantics have been proposed in [3] The authors are with the School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and an evaluation approach. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 44.2260046005249,
      "citing_paper_id": "268147816",
      "cited_paper_id": 261562315
    },
    {
      "context_text": "STGNNs [20, 23, 39] aim to model both the spatial and temporal dependency of data and learn the representation of the spatial-temporal graph, which have advantages in the related tasks, such as human action recognition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.41666221618652,
      "citing_paper_id": "254097121",
      "cited_paper_id": 563473
    },
    {
      "context_text": "STGNNs [20, 23, 39] aim to model both the spatial and temporal dependency of data and learn the representation of the spatial-temporal graph, which have advantages in the related tasks, such as human action recognition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.41666221618652,
      "citing_paper_id": "254097121",
      "cited_paper_id": 3508727
    },
    {
      "context_text": "PropBank[31] and NomBank[27] further extend the size of event KG, with a number of 112,917",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PropBank",
        "NomBank"
      ],
      "dataset_descriptions": {
        "PropBank": "Used to extend the size of the event knowledge graph by providing a large annotated corpus of semantic roles, enhancing the representation of verb-based events.",
        "NomBank": "Used to extend the size of the event knowledge graph by providing an annotated corpus of nominal predicate argument structures, enhancing the representation of noun-based events."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions PropBank and NomBank, which are annotated corpora used for semantic role labeling and nominal predicate argument structure, respectively. These are specific, verifiable resources.",
      "processing_time": 57.98227119445801,
      "citing_paper_id": "254097121",
      "cited_paper_id": 2486369
    },
    {
      "context_text": "PropBank[31] and NomBank[27] further extend the size of event KG, with a number of 112,917",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PropBank",
        "NomBank"
      ],
      "dataset_descriptions": {
        "PropBank": "Used to extend the size of the event knowledge graph by providing a large annotated corpus of semantic roles, enhancing the representation of verb-based events.",
        "NomBank": "Used to extend the size of the event knowledge graph by providing an annotated corpus of nominal predicate argument structures, enhancing the representation of noun-based events."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions PropBank and NomBank, which are annotated corpora used for semantic role labeling and nominal predicate argument structure, respectively. These are specific, verifiable resources.",
      "processing_time": 57.98227119445801,
      "citing_paper_id": "254097121",
      "cited_paper_id": 16273722
    },
    {
      "context_text": "Then inspired by the success of CNN, convolution operation is also introduced to graph data in both spectral [8, 15, 21] and spatial ways [2, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 44.01047921180725,
      "citing_paper_id": "254097121",
      "cited_paper_id": 3016223
    },
    {
      "context_text": "Then inspired by the success of CNN, convolution operation is also introduced to graph data in both spectral [8, 15, 21] and spatial ways [2, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 44.01047921180725,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15483870
    },
    {
      "context_text": "For the word embedding, we use 300-dimensional skip-gram word embedding pre-trained on Google News [28].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using pre-trained word embeddings from Google News, but does not specify a dataset name. The reference is to a method or model rather than a dataset.",
      "processing_time": 45.35650968551636,
      "citing_paper_id": "254097121",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "For example, Lukovnikov et al. [25] and Huang et al. [18] use the fact in knowledge graphs to complete the question answering.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of facts in knowledge graphs for question answering.",
      "processing_time": 42.147342681884766,
      "citing_paper_id": "254097121",
      "cited_paper_id": 12983389
    },
    {
      "context_text": "Considering the advantage of self-attention[42] in modeling the long-time dependency and successful use in many NLP taks, Bosselut et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (self-attention) and its application in NLP tasks.",
      "processing_time": 43.06956434249878,
      "citing_paper_id": "254097121",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "MultiHead is the multi-head self-attention layer and FFN is the feed-forward network introduced in [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of a neural network architecture.",
      "processing_time": 41.47665357589722,
      "citing_paper_id": "254097121",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "and each block consists of masked multi-head self-attention layers, multi-head context-attention layers and feed-forward layers as in [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures and layers. The context is about the structure of the model, not the data used.",
      "processing_time": 44.705748319625854,
      "citing_paper_id": "254097121",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "‚Ä¢ Transformer[42] is the widely self-attention based encoder-decoder model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model (Transformer).",
      "processing_time": 41.46108555793762,
      "citing_paper_id": "254097121",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Their relations are also extracted with the external knowledge graph ConceptNet, such as \"cup-AtLocation-table\" and \"cup-RelatedTo-drinking\".",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to extract semantic relations for multi-modal knowledge graph reasoning, enhancing the understanding of object interactions and related concepts."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions the use of ConceptNet for extracting relations, which is a knowledge graph, not a traditional dataset. However, it is a reusable resource and fits the broader definition of a dataset in the context of multi-modal knowledge graph reasoning.",
      "processing_time": 51.9214403629303,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "ConceptNet [41] represents the commonsense knowledge with a graph where the nodes are concepts and edges are relations from a fixed type set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to represent commonsense knowledge in a graph format, focusing on nodes as concepts and edges as relations, enhancing multi-modal reasoning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions ConceptNet as a representation of commonsense knowledge in a graph format, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 49.75583839416504,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "To better understand the relations, we leverage an external knowledge corpus ConceptNet [41] which has abundant object and relation knowledge to conduct the relation learning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Leveraged as an external knowledge corpus to enhance relation learning, providing abundant object and relation knowledge for multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "ConceptNet is mentioned as an external knowledge corpus used for relation learning, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 49.358420610427856,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "ConceptNet [41] represents the commonsense knowledge with a graph where the nodes are concepts and edges are relations from a ixed type set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to represent commonsense knowledge in a graph format, focusing on nodes as concepts and edges as relations, enhancing multi-modal reasoning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions ConceptNet as a representation of commonsense knowledge in a graph format, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 50.32584309577942,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "Then the intra-event object relations are explored to get the ine-grained event information, where external knowledge graph ConceptNet is introduced to build the object graph.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to build the object graph by exploring intra-event object relations, enhancing fine-grained event information through external knowledge integration."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions the use of ConceptNet, which is a knowledge graph, but does not refer to it as a dataset. However, given the topic of multi-modal knowledge graph reasoning, it is included with a lower confidence.",
      "processing_time": 51.74528741836548,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "To better understand the relations, we leverage an external knowledge corpus ConceptNet [41] which has abundant After building the object graph, we propose to learn the object relations with graph neural networks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Leveraged as an external knowledge corpus to understand and enhance object relations in a graph neural network framework, focusing on general knowledge across multiple languages."
      },
      "confidence_score": 1.0,
      "reasoning": "ConceptNet is mentioned as an external knowledge corpus used to understand relations, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 50.04094076156616,
      "citing_paper_id": "254097121",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "PropBank [31] and NomBank [27] further extend the size of event KG, with a number of 112,917 and 114,576 events, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NomBank"
      ],
      "dataset_descriptions": {
        "NomBank": "Used to extend the size of an event knowledge graph, contributing 114,576 events to the dataset."
      },
      "confidence_score": 0.9,
      "reasoning": "NomBank is identified as a dataset extending the size of an event knowledge graph. PropBank is also mentioned but not as a dataset in this context.",
      "processing_time": 50.47895550727844,
      "citing_paper_id": "254097121",
      "cited_paper_id": 16273722
    },
    {
      "context_text": "As the early work of GNNs, RecGNNs [38][11] apply recurrent architectures to learn the node representation, where message passing is done constantly with nodes‚Äô neighborhoods until the node representations are stable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the application of recurrent architectures in graph neural networks.",
      "processing_time": 42.35565948486328,
      "citing_paper_id": "254097121",
      "cited_paper_id": 17011026
    },
    {
      "context_text": "[24] construct a narrative event evolutionary graph based on event",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a narrative event evolutionary graph.",
      "processing_time": 42.08253526687622,
      "citing_paper_id": "254097121",
      "cited_paper_id": 21723549
    },
    {
      "context_text": "Considering that there may be noises in the returned images, we follow [30] to ilter out the noise image by its noise score, which is calculated by summarizing the pairwise distances of all images retrieved for one event.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general process for filtering noise in images. No clear, verifiable dataset is referenced.",
      "processing_time": 43.35013556480408,
      "citing_paper_id": "254097121",
      "cited_paper_id": 27494872
    },
    {
      "context_text": "The temporal relation based inference tries to understand temporal relations between events and complete the related event prediction tasks, such as script event prediction which requires the model to choose a correct subsequent event among candidates[45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task related to script event prediction. No verifiable resources are identified.",
      "processing_time": 43.61347413063049,
      "citing_paper_id": "254097121",
      "cited_paper_id": 38934160
    },
    {
      "context_text": "‚Ä¢ GPT[34] is a Transformer based pre-trained language model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a pre-trained language model. No verifiable resources are identified.",
      "processing_time": 43.01419496536255,
      "citing_paper_id": "254097121",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "As for the pre-trained language model, GPT performs better than other previous methods.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison of performance between GPT and other methods. No verifiable resources are identified.",
      "processing_time": 44.404669761657715,
      "citing_paper_id": "254097121",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "However, our model still outperforms GPT on all metrics, which may because the data used to pre-train GPT has a relatively large discrepancy with the data of the commonsense inference task, and the three kinds of relation knowledge which have been proved to be helpful for the task is not utilized in GPT.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'data' in a generic sense and discusses the performance of GPT relative to other models.",
      "processing_time": 45.87230110168457,
      "citing_paper_id": "254097121",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "[7] introduced a Global Reasoning unit that implements the relation reasoning via graph convolution on a graph in the interaction space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the introduction of a Global Reasoning unit, which is a method for implementing relation reasoning.",
      "processing_time": 46.31120204925537,
      "citing_paper_id": "254097121",
      "cited_paper_id": 54206179
    },
    {
      "context_text": "Recently, Graph Neural Network has shown great advantages in processing the structural data [3, 22, 47, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing Graph Neural Networks. No verifiable resources are identified.",
      "processing_time": 43.628623247146606,
      "citing_paper_id": "254097121",
      "cited_paper_id": 54559476
    },
    {
      "context_text": "Then inspired by the success of Convolutional Neural Network(CNN), convolution operation is also introduced to graph data in both spectral[15][8][21] and spatial ways[2][12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 42.715229511260986,
      "citing_paper_id": "254097121",
      "cited_paper_id": 69481030
    },
    {
      "context_text": "Then inspired by the success of Convolutional Neural Network(CNN), convolution operation is also introduced to graph data in both spectral[15][8][21] and spatial ways[2][12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 42.715229511260986,
      "citing_paper_id": "254097121",
      "cited_paper_id": null
    },
    {
      "context_text": "Up to now, the largest event KG ASER [51] has reached the scale of 194 million events.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ASER"
      ],
      "dataset_descriptions": {
        "ASER": "ASER is used as a large-scale eventuality knowledge graph, providing a rich resource for multi-modal knowledge graph reasoning tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions ASER as a large-scale eventuality knowledge graph, which is relevant to multi-modal knowledge graph reasoning. However, it does not specify usage details.",
      "processing_time": 50.60153031349182,
      "citing_paper_id": "254097121",
      "cited_paper_id": 141406559
    },
    {
      "context_text": "As far as we know, ASER[51] is the largest knowledge graph whose primitive units of semantics are eventualities and edges are relations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ASER"
      ],
      "dataset_descriptions": {
        "ASER": "ASER is used as a large-scale eventuality knowledge graph, providing a rich resource for semantic relations and eventualities in multi-modal reasoning contexts."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions ASER as a large-scale knowledge graph, which is relevant to multi-modal knowledge graph reasoning. However, it does not specify usage details or research context.",
      "processing_time": 51.1454873085022,
      "citing_paper_id": "254097121",
      "cited_paper_id": 141406559
    },
    {
      "context_text": "[44] propose a knowledge graph network for the recommendation, which explicitly models the high-order connectivities in KG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KGAT) for recommendation using knowledge graphs.",
      "processing_time": 42.98503756523132,
      "citing_paper_id": "254097121",
      "cited_paper_id": 159042183
    },
    {
      "context_text": "Recently, Graph Neural Network has shown great advantages in processing the structural data[47] [52][3][22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general capability of Graph Neural Networks. No verifiable resources are identified.",
      "processing_time": 44.1141414642334,
      "citing_paper_id": "254097121",
      "cited_paper_id": 204972196
    },
    {
      "context_text": "Recently, Graph Neural Network has shown great advantages in processing the structural data[47] [52][3][22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general capability of Graph Neural Networks. No verifiable resources are identified.",
      "processing_time": 44.1141414642334,
      "citing_paper_id": "254097121",
      "cited_paper_id": 236428227
    },
    {
      "context_text": "The image features are extracted with the pre-trained ResNet [14] and the distance is computed by the Euclidean metric.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using pre-trained ResNet for extracting image features, but does not refer to any specific dataset. ResNet is a model, not a dataset.",
      "processing_time": 45.269848346710205,
      "citing_paper_id": "254097121",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "For the visual modality, images are input into the pre-trained ResNet to get the visual features firstly.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the use of a pre-trained ResNet model for extracting visual features.",
      "processing_time": 44.64002013206482,
      "citing_paper_id": "254097121",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "The image features are extracted with the pre-trained ResNet[14] and the distance is computed by the Euclidean metric.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (ResNet) and a metric (Euclidean).",
      "processing_time": 43.29879188537598,
      "citing_paper_id": "254097121",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Event-centered commonsense inference aims to understand the relations among events and conduct the event prediction based on commonsense[46][17][16][48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of event-centered commonsense inference. No verifiable resources are identified.",
      "processing_time": 44.099544048309326,
      "citing_paper_id": "254097121",
      "cited_paper_id": 213104909
    },
    {
      "context_text": "[33] input the event sequence into LSTM to predict the probability of the next event.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (LSTM) for predicting event sequences. No verifiable resource is identified.",
      "processing_time": 44.607271671295166,
      "citing_paper_id": "254097121",
      "cited_paper_id": 215717250
    },
    {
      "context_text": "[19] propose a Graph-based Temporal Reasoning Module to learn the relations among multiple action segments thus complete the action segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for action segmentation using graph-based temporal reasoning.",
      "processing_time": 42.032511472702026,
      "citing_paper_id": "254097121",
      "cited_paper_id": 219615799
    },
    {
      "context_text": "In order to evaluate the performance of this research method, two baseline methods are used here to compare this research method : ‚Ä¢ RGCN [12]: A GNN model is proposed for modeling relational data, particularly suitable for link prediction and entity classiÔ¨Åcation tasks in knowledge graphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RGCN) for evaluating the research method. The context focuses on the method's capabilities rather than a specific dataset.",
      "processing_time": 46.60635757446289,
      "citing_paper_id": "271723072",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "MHGRN [4] uses the Ô¨Ånal context embedding of the input text to guide the reasoning based on the knowledge graph embedding, but this kind of model only designs the one-way Ô¨Çow of information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MHGRN) and its approach to reasoning using knowledge graph embeddings.",
      "processing_time": 44.581626176834106,
      "citing_paper_id": "271723072",
      "cited_paper_id": 218486837
    },
    {
      "context_text": "‚Ä¢ QA-GNN [5]: A GNN model combined with a question-answering system is proposed, which utilizes BERT to compute relevance scores between given questions and nodes in the subgraph of the knowledge",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BERT) and a general approach (QA-GNN). No verifiable datasets are referenced.",
      "processing_time": 45.44644498825073,
      "citing_paper_id": "271723072",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "For example, QA-GNN [5] introduces the concept of correlation score to calculate the correlation between the input text and the knowledge graph entity text, and uses the correlation score to enhance the graph embedding of the corresponding entity in each layer of the graph neural network (GNN).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (QA-GNN) and its application. The context focuses on the methodology and the concept of correlation scores, not on any particular dataset.",
      "processing_time": 47.09343886375427,
      "citing_paper_id": "271723072",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "Common MKGs are IMGpedia [9], Rich-pedia [40], and FB-Des [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia",
        "Rich-pedia",
        "FB-Des"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used to provide content-based analysis of Wikimedia images, enhancing multi-modal reasoning capabilities in knowledge graphs.",
        "Rich-pedia": "Utilized as a large-scale, comprehensive multi-modal knowledge graph to support reasoning across various data types and modalities.",
        "FB-Des": "Serves as a foundational dataset for multi-modal knowledge graph reasoning, though specific details on its structure and use are not provided."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions three specific multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 61.11010813713074,
      "citing_paper_id": "259203022",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "Common MKGs are IMGpedia [9], Rich-pedia [40], and FB-Des [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia",
        "Rich-pedia",
        "FB-Des"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used to provide content-based analysis of Wikimedia images, enhancing multi-modal reasoning capabilities in knowledge graphs.",
        "Rich-pedia": "Utilized as a large-scale, comprehensive multi-modal knowledge graph to support reasoning across various data types and modalities.",
        "FB-Des": "Serves as a foundational dataset for multi-modal knowledge graph reasoning, though specific details on its structure and use are not provided."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions three specific multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 61.11010813713074,
      "citing_paper_id": "259203022",
      "cited_paper_id": 225115084
    },
    {
      "context_text": "(2) Multi-modal auxiliary features: To initialize image features f i , we extract a d i -dimensional vector of the last fully-connected layer before the soft-max in VGG model [4].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using VGG model for extracting image features but does not specify a dataset. The VGG model is a method, not a dataset.",
      "processing_time": 44.06898307800293,
      "citing_paper_id": "259203022",
      "cited_paper_id": 7204540
    },
    {
      "context_text": "Note that, the transductive reasoning meth-ods (i.e., TransE [3] or RLH [38]) on the traditional KG cannot be applied to MKGR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and limitations of applying them to multi-modal knowledge graph reasoning.",
      "processing_time": 43.52711367607117,
      "citing_paper_id": "259203022",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "By adding inverse relation, each triplet ( e s , r , e d ) is equivalent to the triplet ( e d , r ‚àí 1 , e s ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological detail about inverse relations in triplets.",
      "processing_time": 41.986802101135254,
      "citing_paper_id": "259203022",
      "cited_paper_id": 16153365
    },
    {
      "context_text": "‚Ä¶gate-attention network in MMKGR to complete multi-modal feature fusion, RARL conducts MKGR by dynamically adding actions and automatically generating rewards, which is mainly inspired by the fact that humans learn optimal policies by imitating demonstrations rather than predefined paradigms [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodology and inspiration behind the research.",
      "processing_time": 44.331417083740234,
      "citing_paper_id": "259203022",
      "cited_paper_id": 16771371
    },
    {
      "context_text": "This is because the combination of the shortest relation path can accurately represent the semantic link between two entities [7], [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for knowledge graph reasoning.",
      "processing_time": 42.24931740760803,
      "citing_paper_id": "259203022",
      "cited_paper_id": 20667722
    },
    {
      "context_text": "And longer paths are more likely to contain worthless or even noisy information [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about longer paths in knowledge graphs.",
      "processing_time": 41.991206407547,
      "citing_paper_id": "259203022",
      "cited_paper_id": 52143467
    },
    {
      "context_text": "MKGR typically refers to the link prediction task of the inferring triple query ( e s , r q , ?) and (?, r q , e d ), where r q is a query relation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes a typical task in multi-modal knowledge graph reasoning.",
      "processing_time": 42.241395235061646,
      "citing_paper_id": "259203022",
      "cited_paper_id": 57189444
    },
    {
      "context_text": "Specifically, redundant features are either shifted versions of the features related to the triple query or very similar with little or no variations, which can amplify the negative effects of noise [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses redundant features and their effects on noise. No verifiable resources are identified.",
      "processing_time": 44.554194688797,
      "citing_paper_id": "259203022",
      "cited_paper_id": 73425227
    },
    {
      "context_text": "1) Knowledge graph reasoning methods under the induction setting: DRUM [33], CoMPILE [29], MorsE [6], RED-GNN [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for knowledge graph reasoning but does not refer to any specific datasets. The context is focused on describing methods rather than datasets.",
      "processing_time": 45.415022134780884,
      "citing_paper_id": "259203022",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "Actually, the conclusion of inductive reasoning methods [33], [47] on traditional KGs is consistent with the above experimental findings: a transductive reasoning method that relies only on multi-modal auxiliary data and lacks generalizability to unseen entities is unsuitable for the inductive‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to inductive reasoning on knowledge graphs.",
      "processing_time": 43.53159976005554,
      "citing_paper_id": "259203022",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "Rule-based methods can leverage the logical rules of existing knowledge to infer new facts, because the rules are independent of specific entities [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for mining rules from knowledge graphs.",
      "processing_time": 42.63560199737549,
      "citing_paper_id": "259203022",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "To answer this question, we first construct datasets of the induction setting where the entities in the test set and the train set are disjoint [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions constructing datasets but does not specify any named, verifiable datasets. The reference to 'induction setting' and 'entities in the test set and the train set are disjoint' suggests a methodological approach rather than a specific dataset.",
      "processing_time": 47.867146730422974,
      "citing_paper_id": "259203022",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "Some studies employ the attention model or concatenation to fuse multi-modal auxiliary features and then adopt TransE to infer missing elements, such as IKRL [44] and TransAE [42], and MTRL [34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and methods (IKRL, TransAE, MTRL) but does not specify any datasets. The context focuses on the methodologies used for multi-modal knowledge graph reasoning.",
      "processing_time": 46.70915722846985,
      "citing_paper_id": "259203022",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "A reason is that the weak generalizability of manually-designed 3D rewards leads to unstable training on different datasets [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with training stability across different datasets.",
      "processing_time": 42.911842584609985,
      "citing_paper_id": "259203022",
      "cited_paper_id": 210839653
    },
    {
      "context_text": "As shown in Figure 1 (b), all entities are assumed to be seen during testing in the transductive setting [37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general setting (transductive setting) which is not a verifiable resource.",
      "processing_time": 44.52849745750427,
      "citing_paper_id": "259203022",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "Following [37], to generate the training graph, we first uniformly sample several entities as the root nodes, and then conduct the union of k-hop neighbor triplets around the roots.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for generating a training graph. No verifiable resources are identified.",
      "processing_time": 42.8810396194458,
      "citing_paper_id": "259203022",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "[25], [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not provide specific information about datasets used. The cited papers' titles suggest a focus on graph-based methods for relation prediction and knowledge graph completion, but do not mention specific datasets.",
      "processing_time": 47.026408672332764,
      "citing_paper_id": "259203022",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "[25], [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not provide specific information about datasets used. The cited papers' titles suggest a focus on graph-based methods for relation prediction and knowledge graph completion, but do not mention specific datasets.",
      "processing_time": 47.026408672332764,
      "citing_paper_id": "259203022",
      "cited_paper_id": 245119728
    },
    {
      "context_text": "In addition, GraIL [37] and CoMPILE [29] aim to generalize to unseen entities and improve reasoning performance by subgraph extraction, but the enclosing subgraphs cannot learn relational structures so as to weaken the inductive capability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses methods (GraIL and CoMPILE) rather than datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 44.279717445373535,
      "citing_paper_id": "259203022",
      "cited_paper_id": 211082667
    },
    {
      "context_text": "The absence of any relation in this path causes the reasoning path to be unavailable, which limits the reasoning performance [50], [52].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation in reasoning performance due to missing relations in paths.",
      "processing_time": 44.0019793510437,
      "citing_paper_id": "259203022",
      "cited_paper_id": 219437602
    },
    {
      "context_text": "Since MKGs inherently contain incomplete knowledge, MKGR technology that can synthesize the original knowledge and infer the missing knowledge is particularly important [34], [35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of MKGR technology for handling incomplete knowledge in multi-modal knowledge graphs.",
      "processing_time": 44.76775360107422,
      "citing_paper_id": "259203022",
      "cited_paper_id": 224814368
    },
    {
      "context_text": "Without loss of generality, MKGR methods can predict missing head entities by converting (?, r q , e d ) to ( e s , r ‚àí 1 q , ?).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach for predicting missing head entities in multi-modal knowledge graphs.",
      "processing_time": 44.76477599143982,
      "citing_paper_id": "259203022",
      "cited_paper_id": 231626386
    },
    {
      "context_text": "K NOWLEDGE graphs (KGs) store and manage huge amounts of data in reality and have been widely used in applications, including recommendation systems [43], information retrieval [28], and knowledge question answering [16].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of knowledge graphs. No verifiable resources are identified.",
      "processing_time": 43.99130058288574,
      "citing_paper_id": "259203022",
      "cited_paper_id": 234357689
    },
    {
      "context_text": "It comprises three manual sub-rewards, relying on the experience of domain experts and existing data distributions [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to 'existing data distributions' but does not provide a clear identifier or name.",
      "processing_time": 45.362547636032104,
      "citing_paper_id": "259203022",
      "cited_paper_id": 236477903
    },
    {
      "context_text": "‚Ä¶of inductive reasoning methods [33], [47] on traditional KGs is consistent with the above experimental findings: a transductive reasoning method that relies only on multi-modal auxiliary data and lacks generalizability to unseen entities is unsuitable for the inductive setting [11], [25], [41].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to inductive reasoning on knowledge graphs.",
      "processing_time": 43.789037227630615,
      "citing_paper_id": "259203022",
      "cited_paper_id": 245119728
    },
    {
      "context_text": "Existing methods are powerless to capture fine-grained entity-independent representation, thereby restricting the inductive capability of MKGR models [11], [25], [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to existing methods and their limitations.",
      "processing_time": 42.58452844619751,
      "citing_paper_id": "259203022",
      "cited_paper_id": 245119728
    },
    {
      "context_text": "To expand the auxiliary data with one modality, WN9-IMG-TXT and FB-IMG-TXT simultaneously add a number of textual descriptions and images to each entity, aiming to further enhance the data diversity of the MKGs [34], [48].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG-TXT",
        "FB-IMG-TXT"
      ],
      "dataset_descriptions": {
        "WN9-IMG-TXT": "Used to enhance the data diversity of multi-modal knowledge graphs by adding textual descriptions and images to entities, aiming to improve the richness of the graph.",
        "FB-IMG-TXT": "Used to enhance the data diversity of multi-modal knowledge graphs by adding textual descriptions and images to entities, aiming to improve the richness of the graph."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions WN9-IMG-TXT and FB-IMG-TXT as datasets used to enhance the data diversity of multi-modal knowledge graphs (MKGs). These names are specific and plausible, fitting the criteria for inclusion.",
      "processing_time": 63.85470414161682,
      "citing_paper_id": "259203022",
      "cited_paper_id": 247794106
    },
    {
      "context_text": "Specifically, inspired by the human inductive ability to generalize unseen tasks from existing relevant knowledge [19], TAIR generates fine-grained entity-independent features from the existing topological structure in an attentive manner to represent unseen entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach inspired by human inductive abilities.",
      "processing_time": 43.434762716293335,
      "citing_paper_id": "259203022",
      "cited_paper_id": 249017794
    },
    {
      "context_text": "MMKGR [53] stands out as the unique multi-hop MKGR model in existing ones, garnering significant attention for its state-of-the-art (SOTA) performance and interpretability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (MMKGR). The context focuses on the model's performance and interpretability.",
      "processing_time": 45.35796618461609,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "To address this problem, the multi-modal knowledge graph reasoning (MKGR) technique is proposed to infer missing triplets in MKGs [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique for multi-modal knowledge graph reasoning.",
      "processing_time": 42.81843662261963,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "In the literature, existing MKGR methods can be categorized into two types: single-hop reasoning and multi-hop reasoning [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categorizes methods into single-hop and multi-hop reasoning.",
      "processing_time": 43.42391324043274,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "The former focuses on modeling score functions for one-step relations that contain relatively less information [34], [45], while the latter represents the latest work that interpretably infers missing elements by combining multi-hop relations and fusing the corresponding multi-modal features [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on comparing different approaches to knowledge graph reasoning.",
      "processing_time": 44.942429304122925,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "To evaluate the reasoning performance of TMR over inductive and transductive settings, we adopt the mean reciprocal rank (MRR) and Hits@N to report experimental results, which are common metrics for MKGR [34], [53].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics commonly used in multi-modal knowledge graph reasoning.",
      "processing_time": 43.416927099227905,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "MMKGR, the version of our conference [53], is limited by manually designed reward functions and relation sparsity as well as poor performance under inductive settings, which motivates us to propose TMR in this paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMKGR) and its limitations. The context focuses on the motivation for proposing a new method (TMR) rather than using a dataset.",
      "processing_time": 47.66359210014343,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "2) SOTA MKGR method: MMKGR [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMKGR). The context is focused on describing the method rather than using a dataset.",
      "processing_time": 45.881704330444336,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "Following MMKGR [34], [53], we use WN9-IMG-TXT and FB-IMG-TXT to verify the reasoning performance under the transductive setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG-TXT",
        "FB-IMG-TXT"
      ],
      "dataset_descriptions": {
        "WN9-IMG-TXT": "Used to verify reasoning performance in a multi-modal knowledge graph under the transductive setting, focusing on image and text integration.",
        "FB-IMG-TXT": "Used to verify reasoning performance in a multi-modal knowledge graph under the transductive setting, focusing on image and text integration."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for verifying reasoning performance in a multi-modal knowledge graph setting.",
      "processing_time": 57.36459517478943,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "In summary, as an extension of our conference paper [53], this work makes the following contributions: ‚Ä¢ To the best of our knowledge, this is the first work to investigate how to conduct MKGR under both inductive and transductive settings.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only contributions to the field of multi-modal knowledge graph reasoning.",
      "processing_time": 43.74443173408508,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "Following [53], a group of context features Y is calculated as Y = [ y 1 , y 2 , ..., y m ] , where Y ‚àà R m √ó d y , m and d y are the number of entities and the dimension of the features, respectively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating context features. The context is too generic to infer any specific dataset usage.",
      "processing_time": 45.32489013671875,
      "citing_paper_id": "259203022",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "A MKG is an extension of KG by adding multi-modal auxiliary data, it is denoted as G m = {E m , R , U} , where R is a set of semantic relations, and E m denotes a set of entities associated with related multi-modal auxiliary data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only defines a multi-modal knowledge graph (MKG) and its components.",
      "processing_time": 45.09126114845276,
      "citing_paper_id": "259203022",
      "cited_paper_id": 258879983
    },
    {
      "context_text": "MKGR under the inductive setting requires the model to learn inductive capability in the training graph to infer a set of queries Q on the test graph, Q = { ( e s , r q , ?)",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It describes a requirement for inductive capability in multi-modal knowledge graph reasoning but does not reference any particular dataset.",
      "processing_time": 46.785234689712524,
      "citing_paper_id": "259203022",
      "cited_paper_id": 258879983
    },
    {
      "context_text": "However, this necessitates time-consuming redesign when adapting to new environments [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general challenge in adapting systems to new environments.",
      "processing_time": 44.45776581764221,
      "citing_paper_id": "259203022",
      "cited_paper_id": null
    },
    {
      "context_text": "Note that, the agent learns adaptive rewards from demonstrations without manually designing and tuning, which reduces manual intervention and subjective bias [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for learning adaptive rewards from demonstrations.",
      "processing_time": 42.81190514564514,
      "citing_paper_id": "259203022",
      "cited_paper_id": null
    },
    {
      "context_text": "Then, the joint representation B l of Q and K is learned based on the MLB pooling method [17], inspired by the recent successes of it in fine-grained multi-modal fusion, Similarly, we can generate the joint representation B r of V and Q with the following equation, where W lk , W lq , W rv , W rq ‚àà‚Ä¶",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MLB pooling) used for multi-modal fusion. No verifiable resources are identified.",
      "processing_time": 45.64932060241699,
      "citing_paper_id": "259203022",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, SSE [92] constrains entities of the same type to cluster together in the embedding space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SSE) for clustering entities in the embedding space of a knowledge graph.",
      "processing_time": 45.2888343334198,
      "citing_paper_id": "279392166",
      "cited_paper_id": 205692
    },
    {
      "context_text": "(b) Probability-based Negative Sampling : Bernoulli negative sampling [118] employs a sparse attention mechanism to discover relation concepts and replaces the head or tail entity of a positive triple with different probabilities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for negative sampling in the context of knowledge base completion.",
      "processing_time": 44.17118215560913,
      "citing_paper_id": "279392166",
      "cited_paper_id": 982761
    },
    {
      "context_text": "(c) Adversarial Generation-based Negative Sampling : KBGAN [121] integrates KGE into the adversarial training framework (GAN) [122] to select high-quality negative triples beneficial for learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the integration of KGE into GANs for negative sampling.",
      "processing_time": 46.25165510177612,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "(b) Tensor Decomposition-based Model : RESCAL [18] employs tensor decomposition by representing the entire KG as a large-scale third-order tensor, where each entry indicates whether the corresponding triple exists.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (RESCAL) for tensor decomposition in knowledge graphs.",
      "processing_time": 44.67154550552368,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1157792
    },
    {
      "context_text": "Currently, KGR techniques have been widely applied in various practical scenarios, including COVID-19 drug discovery [17], bond market risk supervision [18], product recommendation [19], voice assistants [20], and intelligent education [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGR techniques. No dataset names are provided in the context.",
      "processing_time": 45.494601249694824,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1157792
    },
    {
      "context_text": "Currently, KGR techniques have been widely applied in various practical scenarios, including COVID-19 drug discovery [17], bond market risk supervision [18], product recommendation [19], voice assistants [20], and intelligent education [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGR techniques. No dataset names are provided in the context.",
      "processing_time": 45.494601249694824,
      "citing_paper_id": "279392166",
      "cited_paper_id": 3806582
    },
    {
      "context_text": "Currently, KGR techniques have been widely applied in various practical scenarios, including COVID-19 drug discovery [17], bond market risk supervision [18], product recommendation [19], voice assistants [20], and intelligent education [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGR techniques. No dataset names are provided in the context.",
      "processing_time": 45.494601249694824,
      "citing_paper_id": "279392166",
      "cited_paper_id": 73729352
    },
    {
      "context_text": "Currently, KGR techniques have been widely applied in various practical scenarios, including COVID-19 drug discovery [17], bond market risk supervision [18], product recommendation [19], voice assistants [20], and intelligent education [21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGR techniques. No dataset names are provided in the context.",
      "processing_time": 45.494601249694824,
      "citing_paper_id": "279392166",
      "cited_paper_id": 218862816
    },
    {
      "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "WordNet",
        "NELL",
        "DBpedia"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure.",
        "YAGO": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations.",
        "WordNet": "Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction.",
        "NELL": "Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
        "DBpedia": "Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 73.75048041343689,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1181640
    },
    {
      "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "WordNet",
        "NELL",
        "DBpedia"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure.",
        "YAGO": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations.",
        "WordNet": "Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction.",
        "NELL": "Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
        "DBpedia": "Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 73.75048041343689,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1671874
    },
    {
      "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "WordNet",
        "NELL",
        "DBpedia"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure.",
        "YAGO": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations.",
        "WordNet": "Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction.",
        "NELL": "Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
        "DBpedia": "Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 73.75048041343689,
      "citing_paper_id": "279392166",
      "cited_paper_id": 8423494
    },
    {
      "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "WordNet",
        "NELL",
        "DBpedia"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure.",
        "YAGO": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations.",
        "WordNet": "Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction.",
        "NELL": "Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
        "DBpedia": "Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 73.75048041343689,
      "citing_paper_id": "279392166",
      "cited_paper_id": 207163173
    },
    {
      "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "WordNet",
        "NELL",
        "DBpedia"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure.",
        "YAGO": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations.",
        "WordNet": "Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction.",
        "NELL": "Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
        "DBpedia": "Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 73.75048041343689,
      "citing_paper_id": "279392166",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "In recent years, KGs have played crucial roles in various tasks, such as relation extraction [8], semantic search [9], dialogue systems [10], question answering systems [11], recommendation systems [12], and have also facilitated large language models such as GraphRAG [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGs. No verifiable resources are identified.",
      "processing_time": 45.03647017478943,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1336493
    },
    {
      "context_text": "In recent years, KGs have played crucial roles in various tasks, such as relation extraction [8], semantic search [9], dialogue systems [10], question answering systems [11], recommendation systems [12], and have also facilitated large language models such as GraphRAG [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGs. No verifiable resources are identified.",
      "processing_time": 45.03647017478943,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202540096
    },
    {
      "context_text": "Lao et al. initially improve their PRA by introducing a relation inference algorithm based on constrained and weighted random walks [182], which adjusts the weights of different paths generated by random walks to infer target relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving relation inference in a knowledge base.",
      "processing_time": 43.34908127784729,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1619841
    },
    {
      "context_text": "(a) Relational Path-based Model : The existing approaches Path-RNN [96], PTransE [97], and PRN [98] integrate the embeddings of all relations in a path using recurrent neural networks (RNNs), addition or multiplication operations to obtain path embedding and introduce it into the KGE model based on‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing different models and their approaches to integrating relation paths in knowledge graphs.",
      "processing_time": 46.557809829711914,
      "citing_paper_id": "279392166",
      "cited_paper_id": 1969092
    },
    {
      "context_text": "(a) Relational Path-based Model : The existing approaches Path-RNN [96], PTransE [97], and PRN [98] integrate the embeddings of all relations in a path using recurrent neural networks (RNNs), addition or multiplication operations to obtain path embedding and introduce it into the KGE model based on‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing different models and their approaches to integrating relation paths in knowledge graphs.",
      "processing_time": 46.557809829711914,
      "citing_paper_id": "279392166",
      "cited_paper_id": 236349787
    },
    {
      "context_text": "DistMult [65] simplifies this approach by representing each relation as a diagonal matrix, thereby reducing computational complexity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (DistMult) for representing relations in knowledge bases.",
      "processing_time": 44.63756775856018,
      "citing_paper_id": "279392166",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "TDistMult [223] and TComplEx [224] extend DistMult [65] and ComplEx [66] models via integrating a time scale component to compute event quadruple scores through four-way tensor decompositions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes extensions to existing models (DistMult and ComplEx) by integrating a time scale component. No specific datasets are mentioned.",
      "processing_time": 45.810850620269775,
      "citing_paper_id": "279392166",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "TDistMult [223] and TComplEx [224] extend DistMult [65] and ComplEx [66] models via integrating a time scale component to compute event quadruple scores through four-way tensor decompositions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes extensions to existing models (DistMult and ComplEx) by integrating a time scale component. No specific datasets are mentioned.",
      "processing_time": 45.810850620269775,
      "citing_paper_id": "279392166",
      "cited_paper_id": 54444869
    },
    {
      "context_text": "TransH [47] and TransR [49] extend TransE by assigning each relation its own hyperplane or projection space, onto which entities are projected.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (TransH, TransR, TransE) for knowledge graph embedding. These methods are not datasets and do not qualify as verifiable resources.",
      "processing_time": 47.71931982040405,
      "citing_paper_id": "279392166",
      "cited_paper_id": 2949428
    },
    {
      "context_text": "TransH [47] and TransR [49] extend TransE by assigning each relation its own hyperplane or projection space, onto which entities are projected.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (TransH, TransR, TransE) for knowledge graph embedding. These methods are not datasets and do not qualify as verifiable resources.",
      "processing_time": 47.71931982040405,
      "citing_paper_id": "279392166",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "‚Ä¶and effectively mine rules from KGs. Early rule learning methods treated the triples in KGs as facts described by binary predicates, enabling the use of inductive logic programming (ILP) [155] techniques to learn first-order logic rules from KGs, such as FOIL [156], MDIE [157], and Inspire [158].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses rule learning methods in KGs and mentions specific techniques but does not refer to any datasets. The cited papers are about ILP techniques, which are methods, not datasets.",
      "processing_time": 47.4020893573761,
      "citing_paper_id": "279392166",
      "cited_paper_id": 3544741
    },
    {
      "context_text": "‚Ä¶and effectively mine rules from KGs. Early rule learning methods treated the triples in KGs as facts described by binary predicates, enabling the use of inductive logic programming (ILP) [155] techniques to learn first-order logic rules from KGs, such as FOIL [156], MDIE [157], and Inspire [158].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses rule learning methods in KGs and mentions specific techniques but does not refer to any datasets. The cited papers are about ILP techniques, which are methods, not datasets.",
      "processing_time": 47.4020893573761,
      "citing_paper_id": "279392166",
      "cited_paper_id": 12643399
    },
    {
      "context_text": "To address the heterogeneity between entities and relations, RPE [104] maps entities into both relation and path spaces, reducing the heterogeneity between entities and relations and representing more complete paths.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RPE) for handling heterogeneity in knowledge graphs.",
      "processing_time": 45.010220766067505,
      "citing_paper_id": "279392166",
      "cited_paper_id": 3696627
    },
    {
      "context_text": "Besides, several studies such as RippleNet [400], AKUPM [401], RCoLM [402], KGCN [390], KGAT [403], IntentGC [404], and AKGE [405] integrate KG semantic information with multi-hop path reasoning to improve the representation of users and items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods that integrate KG semantic information with multi-hop path reasoning. No specific datasets are named or described.",
      "processing_time": 45.583494663238525,
      "citing_paper_id": "279392166",
      "cited_paper_id": 3766110
    },
    {
      "context_text": "Besides, several studies such as RippleNet [400], AKUPM [401], RCoLM [402], KGCN [390], KGAT [403], IntentGC [404], and AKGE [405] integrate KG semantic information with multi-hop path reasoning to improve the representation of users and items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods that integrate KG semantic information with multi-hop path reasoning. No specific datasets are named or described.",
      "processing_time": 45.583494663238525,
      "citing_paper_id": "279392166",
      "cited_paper_id": 196173551
    },
    {
      "context_text": "Besides, several studies such as RippleNet [400], AKUPM [401], RCoLM [402], KGCN [390], KGAT [403], IntentGC [404], and AKGE [405] integrate KG semantic information with multi-hop path reasoning to improve the representation of users and items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods that integrate KG semantic information with multi-hop path reasoning. No specific datasets are named or described.",
      "processing_time": 45.583494663238525,
      "citing_paper_id": "279392166",
      "cited_paper_id": 196204964
    },
    {
      "context_text": "Besides, several studies such as RippleNet [400], AKUPM [401], RCoLM [402], KGCN [390], KGAT [403], IntentGC [404], and AKGE [405] integrate KG semantic information with multi-hop path reasoning to improve the representation of users and items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods that integrate KG semantic information with multi-hop path reasoning. No specific datasets are named or described.",
      "processing_time": 45.583494663238525,
      "citing_paper_id": "279392166",
      "cited_paper_id": 201142785
    },
    {
      "context_text": "Besides, several studies such as RippleNet [400], AKUPM [401], RCoLM [402], KGCN [390], KGAT [403], IntentGC [404], and AKGE [405] integrate KG semantic information with multi-hop path reasoning to improve the representation of users and items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods that integrate KG semantic information with multi-hop path reasoning. No specific datasets are named or described.",
      "processing_time": 45.583494663238525,
      "citing_paper_id": "279392166",
      "cited_paper_id": 204788875
    },
    {
      "context_text": "Subsequently, they utilize syntactic-semantic inference rules learned from large-scale web text corpora and KGs to propose a path-constrained random walk model [183], employing these learned rules for inference.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of large-scale web text corpora and KGs, but does not specify any named datasets. The focus is on the method of learning syntactic-semantic inference rules.",
      "processing_time": 47.96643590927124,
      "citing_paper_id": "279392166",
      "cited_paper_id": 3814153
    },
    {
      "context_text": "Inspired by techniques from computer vision, ConvE [70] and ConvKB [71] reshape the embedded embeddings of entities and relations into two-dimensional matrices and utilize convolutional kernels to capture the interactions between entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the methodology of ConvE and ConvKB, which are not datasets.",
      "processing_time": 47.11231184005737,
      "citing_paper_id": "279392166",
      "cited_paper_id": 3882054
    },
    {
      "context_text": "Generally, negative triples are constructed by replacing one entity or relation in a positive triple with another, following the local closed-world assumption [115], to generate a triple not present in the KG and treating it as a negative triple.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for constructing negative triples in knowledge graphs.",
      "processing_time": 45.57238483428955,
      "citing_paper_id": "279392166",
      "cited_paper_id": 4557963
    },
    {
      "context_text": "Currently, one of the most challenging tasks is inductive KGR, which primarily evaluates a model‚Äôs ability to reason over unseen entities and relations [311].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task in knowledge graph reasoning.",
      "processing_time": 44.357065200805664,
      "citing_paper_id": "279392166",
      "cited_paper_id": 4755450
    },
    {
      "context_text": "HolE [67] combines the expressive power of tensor decomposition with the simplicity and efficiency of translation-based methods, modeling the relationship between entity pairs via the circular correlation of their vectors.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called HolE which is a holographic embedding technique for knowledge graphs.",
      "processing_time": 45.77755355834961,
      "citing_paper_id": "279392166",
      "cited_paper_id": 6071257
    },
    {
      "context_text": "To effectively learn embeddings for long-tailed relations from a limited number of triples, some studies have lever-aged the Model-Agnostic Meta-Learning (MAML) mechanism [285] to transfer existing knowledge into relation meta-representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MAML) for fast adaptation of deep networks. The context focuses on the use of MAML for transferring knowledge into relation meta-representations.",
      "processing_time": 48.220109939575195,
      "citing_paper_id": "279392166",
      "cited_paper_id": 6719686
    },
    {
      "context_text": "Both Das et al. [105] and Jiang et al. [106] represent each entity in a path by taking the average of all its type embeddings, using entity types to reduce the heterogeneity between entities and relations, and combine the path relations to learn path embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for representing entities and learning path embeddings in knowledge graphs.",
      "processing_time": 45.209288358688354,
      "citing_paper_id": "279392166",
      "cited_paper_id": 7431082
    },
    {
      "context_text": "To address this limitation, Wang et al. [407] introduce the dataset named FVQA, which represents each instance as an tuple ‚Äúimage‚Äìquestion‚Äìanswer‚Äìsupporting fact subgraph‚Äù, requiring the integration of background knowledge.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FVQA"
      ],
      "dataset_descriptions": {
        "FVQA": "Used to integrate background knowledge in visual question answering, representing instances as tuples of image, question, answer, and supporting fact subgraph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context explicitly mentions the creation and use of the FVQA dataset, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 50.980124711990356,
      "citing_paper_id": "279392166",
      "cited_paper_id": 7483388
    },
    {
      "context_text": "(a) Rule-Enhanced KGE : Models such as KALE [174] and RUGE [175] incorporate axiomatized rules to capture various relational patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the integration of logical rules into knowledge graph embedding models.",
      "processing_time": 46.50052833557129,
      "citing_paper_id": "279392166",
      "cited_paper_id": 7958862
    },
    {
      "context_text": "Know-Evolve [229] is the first model to introduce the deep evolutionary knowledge network, which captures causal dependencies among historical events to predict future occurrences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model called 'Know-Evolve'. The context focuses on the model's capabilities and contributions.",
      "processing_time": 46.67974400520325,
      "citing_paper_id": "279392166",
      "cited_paper_id": 8040343
    },
    {
      "context_text": "NTN [42] feeds the head and tail entity vectors into a relation-specific neural network, deriving the triple score through bilinear operations followed by a nonlinear activation function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (NTN) for knowledge base completion, not a dataset. No specific dataset is mentioned.",
      "processing_time": 45.552003145217896,
      "citing_paper_id": "279392166",
      "cited_paper_id": 8429835
    },
    {
      "context_text": "The framework diagrams of NTN [42], R-GCN [43] and KG-BERT [44] are directly derived from their original papers. difference between the head and tail entities.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are used for their framework diagrams, not for data.",
      "processing_time": 46.67958474159241,
      "citing_paper_id": "279392166",
      "cited_paper_id": 8429835
    },
    {
      "context_text": "The framework diagrams of NTN [42], R-GCN [43] and KG-BERT [44] are directly derived from their original papers. difference between the head and tail entities.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are used for their framework diagrams, not for data.",
      "processing_time": 46.67958474159241,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "For instance, IKRL [254] encodes images corresponding to entities into the entity embedding space, thereby modeling the association among an entity pair and the linked relation in each triple to learn entity and relation embeddings in a manner similar to TransE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'IKRL' which is a method for encoding images into entity embedding space. No specific dataset is mentioned, only a method.",
      "processing_time": 45.97951889038086,
      "citing_paper_id": "279392166",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "(a) Inductive Logic Programming : For large-scale KGs, efficient rule learning algorithms have been developed, including AMIE+ [160], ScaLeKB [161], RDF2rules [162], Swarm [163], RuDiK [164], and RuLES [165].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and tools for rule learning in large-scale knowledge graphs, but does not refer to any specific datasets.",
      "processing_time": 45.975470304489136,
      "citing_paper_id": "279392166",
      "cited_paper_id": 10717843
    },
    {
      "context_text": "(a) Inductive Logic Programming : For large-scale KGs, efficient rule learning algorithms have been developed, including AMIE+ [160], ScaLeKB [161], RDF2rules [162], Swarm [163], RuDiK [164], and RuLES [165].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and tools for rule learning in large-scale knowledge graphs, but does not refer to any specific datasets.",
      "processing_time": 45.975470304489136,
      "citing_paper_id": "279392166",
      "cited_paper_id": 19441281
    },
    {
      "context_text": "(a) Inductive Logic Programming : For large-scale KGs, efficient rule learning algorithms have been developed, including AMIE+ [160], ScaLeKB [161], RDF2rules [162], Swarm [163], RuDiK [164], and RuLES [165].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and tools for rule learning in large-scale knowledge graphs, but does not refer to any specific datasets.",
      "processing_time": 45.975470304489136,
      "citing_paper_id": "279392166",
      "cited_paper_id": 51989311
    },
    {
      "context_text": "(a) Inductive Logic Programming : For large-scale KGs, efficient rule learning algorithms have been developed, including AMIE+ [160], ScaLeKB [161], RDF2rules [162], Swarm [163], RuDiK [164], and RuLES [165].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods and tools for rule learning in large-scale knowledge graphs, but does not refer to any specific datasets.",
      "processing_time": 45.975470304489136,
      "citing_paper_id": "279392166",
      "cited_paper_id": 52042083
    },
    {
      "context_text": "A. Incremental KGE Model 1) Multiple Spaces-based Model: PuTransE [206] is among the earliest incremental KGE methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PuTransE. The context is about incremental KGE methods, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 47.79223442077637,
      "citing_paper_id": "279392166",
      "cited_paper_id": 13614891
    },
    {
      "context_text": "TFSC [305] improves upon FAAN [280] by replacing its original approach with TTransE [220] to encode entities enriched with temporal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 46.31930351257324,
      "citing_paper_id": "279392166",
      "cited_paper_id": 13846713
    },
    {
      "context_text": "2) Meta Learning-based Model: MetaRT [308] adapts the MetaR [286] framework to temporal KGR by replacing the traditional TransE scoring function with that of TTransE [220].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on adapting a meta-learning framework for temporal knowledge graph reasoning.",
      "processing_time": 46.8076868057251,
      "citing_paper_id": "279392166",
      "cited_paper_id": 13846713
    },
    {
      "context_text": "2) Meta Learning-based Model: MetaRT [308] adapts the MetaR [286] framework to temporal KGR by replacing the traditional TransE scoring function with that of TTransE [220].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on adapting a meta-learning framework for temporal knowledge graph reasoning.",
      "processing_time": 46.8076868057251,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202541491
    },
    {
      "context_text": "2) Meta Learning-based Model: MetaRT [308] adapts the MetaR [286] framework to temporal KGR by replacing the traditional TransE scoring function with that of TTransE [220].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on adapting a meta-learning framework for temporal knowledge graph reasoning.",
      "processing_time": 46.8076868057251,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258240832
    },
    {
      "context_text": "To address this issue, TTransE [220] extends TransE by modeling the interactions among event quadruples as translations that incorporate both predicates and time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TTransE) that extends another method (TransE).",
      "processing_time": 45.727758169174194,
      "citing_paper_id": "279392166",
      "cited_paper_id": 13846713
    },
    {
      "context_text": "(a) Random Uniform Negative Sampling : For KGE models, the simplest negative sampling mechanism involves randomly replacing one entity or relation in a positive triple to generate negative triples, with the replacement probability following a uniform distribution [47].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for generating negative samples in Knowledge Graph Embedding models.",
      "processing_time": 45.72473621368408,
      "citing_paper_id": "279392166",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "5 have been proposed, which can be broadly categorized into three approaches: (a) projection mapping of entities and relations [47], (b) employing specialized representation spaces [23], and (c) extending the translation operation to a rotation operation [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 46.469536542892456,
      "citing_paper_id": "279392166",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "5 have been proposed, which can be broadly categorized into three approaches: (a) projection mapping of entities and relations [47], (b) employing specialized representation spaces [23], and (c) extending the translation operation to a rotation operation [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 46.469536542892456,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253384318
    },
    {
      "context_text": "In contrast, HyTE [221] projects entities and predicates onto time-specific hyperplanes (inspired by TransH [47]) for distinguishing event representations across different timestemps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions HyTE, which is a method for knowledge graph embedding, and TransH, another method. No specific datasets are mentioned.",
      "processing_time": 46.128034830093384,
      "citing_paper_id": "279392166",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "In contrast, HyTE [221] projects entities and predicates onto time-specific hyperplanes (inspired by TransH [47]) for distinguishing event representations across different timestemps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions HyTE, which is a method for knowledge graph embedding, and TransH, another method. No specific datasets are mentioned.",
      "processing_time": 46.128034830093384,
      "citing_paper_id": "279392166",
      "cited_paper_id": 53082197
    },
    {
      "context_text": "Notably, this approach develops a MTransH module by integrating the MAML mechanism into the KGE model TransH [47], which computes triple scores and enhances few-shot KGR performance in scenarios with complex relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransH) and a technique (MAML). The focus is on the integration of these methods to enhance few-shot knowledge graph reasoning.",
      "processing_time": 48.53679704666138,
      "citing_paper_id": "279392166",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "‚Ä¶relations among them, representing the image as a directed graph with entities as nodes and relations as directed edges similar to a KG. GB-Net [417] uses Faster R-CNN to generate an initial scene graph, which is then enhanced by bridging it with external KGs such as ConceptNet [418] and WordNet.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet",
        "WordNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to enhance an initial scene graph generated by Faster R-CNN, integrating general knowledge to improve multi-modal reasoning in images.",
        "WordNet": "Used to enhance an initial scene graph generated by Faster R-CNN, providing lexical information to improve multi-modal reasoning in images."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the use of external KGs such as ConceptNet and WordNet to enhance an initial scene graph. ConceptNet is a specific, verifiable dataset, while WordNet is a lexical database.",
      "processing_time": 61.31831645965576,
      "citing_paper_id": "279392166",
      "cited_paper_id": 15206880
    },
    {
      "context_text": "Subsequent adversarial-based approaches, such as IGAN [123], GraphGAN [124], KS-GAN [125], and RUGA [126], are designed based on KBGAN with improvements like different generators or additional steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the presence of specific datasets.",
      "processing_time": 47.31352782249451,
      "citing_paper_id": "279392166",
      "cited_paper_id": 19131678
    },
    {
      "context_text": "Subsequent adversarial-based approaches, such as IGAN [123], GraphGAN [124], KS-GAN [125], and RUGA [126], are designed based on KBGAN with improvements like different generators or additional steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the presence of specific datasets.",
      "processing_time": 47.31352782249451,
      "citing_paper_id": "279392166",
      "cited_paper_id": 19140125
    },
    {
      "context_text": "Subsequent adversarial-based approaches, such as IGAN [123], GraphGAN [124], KS-GAN [125], and RUGA [126], are designed based on KBGAN with improvements like different generators or additional steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the presence of specific datasets.",
      "processing_time": 47.31352782249451,
      "citing_paper_id": "279392166",
      "cited_paper_id": 203658320
    },
    {
      "context_text": "Subsequent adversarial-based approaches, such as IGAN [123], GraphGAN [124], KS-GAN [125], and RUGA [126], are designed based on KBGAN with improvements like different generators or additional steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the presence of specific datasets.",
      "processing_time": 47.31352782249451,
      "citing_paper_id": "279392166",
      "cited_paper_id": 237785556
    },
    {
      "context_text": "Poincar¬¥e [57] was among the first to achieve this by mapping entities into the Poincar¬¥e ball, where entities closer to the center represent higher-level concepts, while those near the boundary represent lower-level ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for hierarchical representation learning.",
      "processing_time": 44.27890682220459,
      "citing_paper_id": "279392166",
      "cited_paper_id": 25418227
    },
    {
      "context_text": "Additionally, TorusE [56] embeds the KG onto a compact Lie group, learning embeddings under a TransE-like optimization objective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TorusE, which is a method for embedding knowledge graphs, not a dataset. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 47.02232766151428,
      "citing_paper_id": "279392166",
      "cited_paper_id": 25753806
    },
    {
      "context_text": "To integrate entities and relations in the path and learn complete path embeddings, PARL [103] composes the entities and relations in a path into a sequence and inputs them into an RNN for path encoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PARL) for path-based relation extraction in knowledge graphs.",
      "processing_time": 46.11249804496765,
      "citing_paper_id": "279392166",
      "cited_paper_id": 37591724
    },
    {
      "context_text": "To address this issue, TransD [50] employs different projection matrices for the head and tail entities, while TranSparse [51] measures the sparsity of relation connected entity pairs to design an adaptive sparse transfer matrix for projection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing the methodologies used in TransD and TranSparse.",
      "processing_time": 47.31088399887085,
      "citing_paper_id": "279392166",
      "cited_paper_id": 38485677
    },
    {
      "context_text": "Xiong et al. [278] firstly define the few-shot KGR task and propose the metric learning-based model GMatching, primarily addressing the extreme 1-shot setting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (GMatching) and a task (few-shot KGR).",
      "processing_time": 47.017836570739746,
      "citing_paper_id": "279392166",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "Recognizing that a single triple in the support set is insufficient to effectively represent a long-tailed relation, GMatching [278] leverages the neighborhood subgraph of an entity to encode each candidate head-tail entity pair.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GMatching) and a general approach to knowledge graph reasoning. No verifiable resources are identified.",
      "processing_time": 47.86317038536072,
      "citing_paper_id": "279392166",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "Besides, MUKGE [453] introduces an uncertainty resource ranking reasoning algorithm for more precise confidence inference, while UKRM [454] employs the rule learning algorithm for treating rule mining as a sequence-to-sequence task and utilizes a differentiable reasoning mechanism based on‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MUKGE and UKRM, which are methods or algorithms, not datasets. No specific datasets are mentioned or used according to the provided context.",
      "processing_time": 47.859665393829346,
      "citing_paper_id": "279392166",
      "cited_paper_id": 53775725
    },
    {
      "context_text": "Besides, MUKGE [453] introduces an uncertainty resource ranking reasoning algorithm for more precise confidence inference, while UKRM [454] employs the rule learning algorithm for treating rule mining as a sequence-to-sequence task and utilizes a differentiable reasoning mechanism based on‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MUKGE and UKRM, which are methods or algorithms, not datasets. No specific datasets are mentioned or used according to the provided context.",
      "processing_time": 47.859665393829346,
      "citing_paper_id": "279392166",
      "cited_paper_id": 272884970
    },
    {
      "context_text": "(f) Efficient Negative Sampling : NSCaching [138] uses a caching mechanism for negative triple candidates.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions NSCaching, which is a method for negative sampling in knowledge graph embedding, not a dataset.",
      "processing_time": 45.67628788948059,
      "citing_paper_id": "279392166",
      "cited_paper_id": 56472583
    },
    {
      "context_text": "(a) KGR for Mining User Preferences : KGCN [389] is the first to employ GCN to aggregate neighborhood information for encoding entities in a KG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KGCN) and its application in recommender systems using knowledge graphs.",
      "processing_time": 47.27920055389404,
      "citing_paper_id": "279392166",
      "cited_paper_id": 86631164
    },
    {
      "context_text": "To enhance sampling efficiency, the Batch NS method [117] performs negative sampling within a batch.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for negative sampling. The context does not provide information about a dataset being used.",
      "processing_time": 47.276175022125244,
      "citing_paper_id": "279392166",
      "cited_paper_id": 88523916
    },
    {
      "context_text": "While RotatE restricts operations to rotations on a hyperplane, QuatE [63] represents entities and relations in a quaternion space and leverages Hamiltonian multiplication to associate the embeddings of the head entity, relation, and tail entity, thereby enabling spatial rotations in a‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the methodological differences between RotatE and QuatE.",
      "processing_time": 48.329734086990356,
      "citing_paper_id": "279392166",
      "cited_paper_id": 128363050
    },
    {
      "context_text": "MuRP [58] further refines this idea by introducing Mobius matrix‚Äìvector multiplication and addition to learn relation-aware entity projections in hyperbolic space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MuRP) and its enhancements. The context is focused on the methodological improvements rather than the use of a particular dataset.",
      "processing_time": 49.05505156517029,
      "citing_paper_id": "279392166",
      "cited_paper_id": 168633605
    },
    {
      "context_text": "DE-SimplE [225] addresses this limitation by learning distinct entity embed-dings for different timestamps using temporal embeddings derived from word embeddings.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (DE-SimplE) that uses temporal embeddings derived from word embeddings.",
      "processing_time": 47.56104803085327,
      "citing_paper_id": "279392166",
      "cited_paper_id": 195833229
    },
    {
      "context_text": "JOIE [87] addresses this limitation by jointly learning embeddings for both the ontology and instance layers, thus harnessing the hierarchical nature of concepts and their interrelations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for joint embedding of instances and ontological concepts.",
      "processing_time": 47.260897397994995,
      "citing_paper_id": "279392166",
      "cited_paper_id": 196187271
    },
    {
      "context_text": "In contrast, dual-stream models such as LXMERT [269] and ViLBERT [270] process visual and textual information separately before fusing them via cross-channel or joint attention mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (LXMERT, ViLBERT) but does not refer to any specific datasets. The citation is about model architecture and processing methods, not data sources.",
      "processing_time": 48.7710235118866,
      "citing_paper_id": "279392166",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "For instance, single-stream PLMs like VLBERT [265], VisualBERT [266], Unicoder-VL [267], and UNITER [268] encode image and text embeddings in a unified framework to learn deep contextual embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (VLBERT, VisualBERT, Unicoder-VL, UNITER) but does not refer to any specific datasets. The focus is on describing the models and their capabilities.",
      "processing_time": 50.01485228538513,
      "citing_paper_id": "279392166",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "For instance, single-stream PLMs like VLBERT [265], VisualBERT [266], Unicoder-VL [267], and UNITER [268] encode image and text embeddings in a unified framework to learn deep contextual embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (VLBERT, VisualBERT, Unicoder-VL, UNITER) but does not refer to any specific datasets. The focus is on describing the models and their capabilities.",
      "processing_time": 50.01485228538513,
      "citing_paper_id": "279392166",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "For instance, single-stream PLMs like VLBERT [265], VisualBERT [266], Unicoder-VL [267], and UNITER [268] encode image and text embeddings in a unified framework to learn deep contextual embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (VLBERT, VisualBERT, Unicoder-VL, UNITER) but does not refer to any specific datasets. The focus is on describing the models and their capabilities.",
      "processing_time": 50.01485228538513,
      "citing_paper_id": "279392166",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "To enable multi-step KGR in few-shot scenarios, Meta-KGR [300] incorporates MAML mechanism into a reinforcement learning framework for multi-step reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Meta-KGR) and a technique (MAML).",
      "processing_time": 47.55468463897705,
      "citing_paper_id": "279392166",
      "cited_paper_id": 201698166
    },
    {
      "context_text": "In addition, KG-BERT [44] and StAR [82] utilize pre-trained language models to jointly encode entities, relations, and their associated textual descriptions while preserving the intrinsic structure of the KG. Relphormer [83] proposes a novel Transformer variant tailored to the topological‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of pre-trained language models and a novel Transformer variant for encoding knowledge graph elements.",
      "processing_time": 49.42361116409302,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "In contrast, KG-BERT [44] leverages a Transformer-based BERT model to jointly encode entities, relations, and their associated textual descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KG-BERT) and its components. The context focuses on the model and its capabilities rather than a dataset.",
      "processing_time": 49.271382093429565,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "MetaR [286] encodes entity pairs from the support set to obtain relation meta representations, which are then rapidly optimized using standard KGE models (e.g., TransE) and gradient descent strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the methodology of encoding entity pairs and optimizing relation meta-representations.",
      "processing_time": 48.454153299331665,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202541491
    },
    {
      "context_text": "1) Textual Description-Enhanced Model: TCVAE [292] enriches the KG by leveraging textual descriptions of entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TCVAE) that leverages textual descriptions of entities and relations in a knowledge graph.",
      "processing_time": 48.87484550476074,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202749877
    },
    {
      "context_text": "(b) Neural Networks-based Model : NeuralLP [168] and DRUM [154] utilize one-hot entity representations and matrix-encoded relations, learning rule patterns end-to-end and assessing rule confidence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodologies used in the cited papers.",
      "processing_time": 48.44699954986572,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "Generally, a rule typically consists of a rule head and a rule body, structured as: Rule Head ‚áê Rule Body , indicating that the knowledge in the rule head can be directly deduced from the rule body [154].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general structure of rules in knowledge graphs. No verifiable resources are identified.",
      "processing_time": 48.59272265434265,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202775885
    },
    {
      "context_text": "RARL [192] integrates high-quality logical rules as prior knowledge to inform action selection, while AttnPath [193] combines LSTM networks with graph attention mechanisms to enrich state embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on integrating logical rules and using graph attention mechanisms, which are not datasets.",
      "processing_time": 49.70778441429138,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202776155
    },
    {
      "context_text": "To enhance the temporal order characteristics of relations when representing paths, OPTransE [99] projects the head and tail entities of each relation into different spaces and introduces a sequence matrix to maintain the order of relations in the path.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (OPTransE) and its application to knowledge graph completion.",
      "processing_time": 48.273425340652466,
      "citing_paper_id": "279392166",
      "cited_paper_id": 202777324
    },
    {
      "context_text": "AN [129] and EANS [130] use K-means clustering to group similar entities, replacing originals with cluster members.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (K-means clustering). The context is about entity grouping in knowledge graphs, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 51.09380125999451,
      "citing_paper_id": "279392166",
      "cited_paper_id": 203158029
    },
    {
      "context_text": "Jagvaral et al. [109] use convolutional operations and BiLSTM to encode paths and employ attention mechanisms to capture the semantic correlations between candidate relations and each path, thereby performing a weighted fusion of the embeddings of multiple paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for encoding paths in a knowledge graph using CNN-BiLSTM and attention mechanisms, but does not mention any specific datasets.",
      "processing_time": 49.819117307662964,
      "citing_paper_id": "279392166",
      "cited_paper_id": 204076452
    },
    {
      "context_text": "This dynamic nature necessitates the construction of dynamic KGs that support incremental updates through triple additions, modifications, and deletions [205].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of dynamic knowledge graphs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 49.537869453430176,
      "citing_paper_id": "279392166",
      "cited_paper_id": 204576077
    },
    {
      "context_text": "For instance, KG2E [54] represents entities and relations as Gaussian distributions, where the covariance encodes the inherent uncertainty.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It describes a method (KG2E) for representing entities and relations in knowledge graphs.",
      "processing_time": 49.96632981300354,
      "citing_paper_id": "279392166",
      "cited_paper_id": 207228784
    },
    {
      "context_text": "KG2E [54] introduces uncertainty into the embedding space by modeling entites and relations as multi-dimensional Gaussian distributions, where the mean vector captures the definitive position of an entity or relation, while the covariance matrix reflects its uncertainty.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for representing knowledge graphs with Gaussian embeddings.",
      "processing_time": 48.40999984741211,
      "citing_paper_id": "279392166",
      "cited_paper_id": 207228784
    },
    {
      "context_text": "The evaluation methods for tail entity prediction and relation prediction follow a similar procedure [40].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general procedure for evaluation methods.",
      "processing_time": 49.10388898849487,
      "citing_paper_id": "279392166",
      "cited_paper_id": 207852450
    },
    {
      "context_text": "Both CoKE [77] and HittER [78] use Transformers to encode the contextual information within triples, transform the KGR task into a cloze test task for entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two methods, CoKE and HittER, which use Transformers to encode contextual information in knowledge graphs. No specific datasets are named.",
      "processing_time": 50.951165437698364,
      "citing_paper_id": "279392166",
      "cited_paper_id": 207880490
    },
    {
      "context_text": "Both CoKE [77] and HittER [78] use Transformers to encode the contextual information within triples, transform the KGR task into a cloze test task for entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two methods, CoKE and HittER, which use Transformers to encode contextual information in knowledge graphs. No specific datasets are named.",
      "processing_time": 50.951165437698364,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221376974
    },
    {
      "context_text": "Besides, both DBPedia50k [345] and Wikidata5M [364] offer inductive as well as transductive versions, though only the inductive versions are considered here.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBPedia50k",
        "Wikidata5M"
      ],
      "dataset_descriptions": {
        "DBPedia50k": "Used for inductive reasoning in knowledge graphs, focusing on the inductive version to enhance model performance.",
        "Wikidata5M": "Used for inductive reasoning in knowledge graphs, focusing on the inductive version to enhance model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, DBPedia50k and Wikidata5M, which are used for inductive reasoning in the context of knowledge graphs.",
      "processing_time": 64.0505518913269,
      "citing_paper_id": "279392166",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "Specifically, HAKE [59] employs the modulus to indicate hierarchical levels and the phase angle to differentiate entities within the same level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HAKE) and its usage in knowledge graph embeddings.",
      "processing_time": 49.9580295085907,
      "citing_paper_id": "279392166",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "On the basis on GMatching, FSRL [279] employs a relation-aware neighbor encoder to capture the varying influences of neighboring entities, and utilizes a recurrent auto-encoder to aggregate multiple reference entity pairs from the support set.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the methodology used in the FSRL paper.",
      "processing_time": 50.78563189506531,
      "citing_paper_id": "279392166",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "1) Metric Learning-based Model: FTMO [304] extends FSRL [279] from a temporal information modeling perspective by proposing a temporal relation-aware neighbor encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on extending a model from a temporal information modeling perspective.",
      "processing_time": 51.2132248878479,
      "citing_paper_id": "279392166",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "1) Metric Learning-based Model: FTMO [304] extends FSRL [279] from a temporal information modeling perspective by proposing a temporal relation-aware neighbor encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on extending a model from a temporal information modeling perspective.",
      "processing_time": 51.2132248878479,
      "citing_paper_id": "279392166",
      "cited_paper_id": 259630548
    },
    {
      "context_text": "However, ILP-based rule learning methods are designed for smaller datasets and are challenging to apply to large-scale KGs [159].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of ILP-based rule learning methods regarding scalability to large-scale KGs.",
      "processing_time": 51.486719369888306,
      "citing_paper_id": "279392166",
      "cited_paper_id": 209318312
    },
    {
      "context_text": "‚Ä¶relations among them, representing the image as a directed graph with entities as nodes and relations as directed edges similar to a KG. GB-Net [417] uses Faster R-CNN to generate an initial scene graph, which is then enhanced by bridging it with external KGs such as ConceptNet [418] and‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to enhance an initial scene graph generated by Faster R-CNN, specifically by adding external knowledge to improve the representation of entities and relations in images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of external KGs such as ConceptNet to enhance an initial scene graph generated by Faster R-CNN. ConceptNet is a specific, verifiable dataset.",
      "processing_time": 61.347856283187866,
      "citing_paper_id": "279392166",
      "cited_paper_id": 210064217
    },
    {
      "context_text": "RARL [171] efficiently traverses term facts (TBox), learning Horn rules by leveraging semantic relevance between relations in body and head atoms, and separates candidate rule generation from quality evaluation for scalability.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for rule learning in knowledge bases.",
      "processing_time": 50.34285759925842,
      "citing_paper_id": "279392166",
      "cited_paper_id": 212848342
    },
    {
      "context_text": "TAPR [112] designs a path-level attention mechanism to assign different weights to different paths and then fuses the weighted path embeddings to predict the semantic relationships between entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for knowledge graph completion.",
      "processing_time": 50.21855592727661,
      "citing_paper_id": "279392166",
      "cited_paper_id": 214063723
    },
    {
      "context_text": "‚Ä¶ARP [336] employs an attention network to extract the most relevant subgraph and contextual features for the current query triple, and TransNS [337] selects pertinent neighbors as attributes of the entity while leveraging the semantic affinity between entities to choose related negative‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are referenced for their methodologies, not for providing datasets.",
      "processing_time": 51.9581241607666,
      "citing_paper_id": "279392166",
      "cited_paper_id": 214076868
    },
    {
      "context_text": "For example, Belthd et al. [457] propose a set of soft rules based on common graphical patterns to specify valid inter-entity relations and flag anomalies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for specifying valid inter-entity relations in knowledge graphs.",
      "processing_time": 51.34342002868652,
      "citing_paper_id": "279392166",
      "cited_paper_id": 214612503
    },
    {
      "context_text": "For instance, R-MeN [76] employs the self-attention mechanism to de-sign a relation memory network that facilitates interaction between triple encodings and memory embeddings, thereby capturing latent dependencies among triples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (R-MeN) and its components. The context focuses on the model's architecture and functionality.",
      "processing_time": 53.027172565460205,
      "citing_paper_id": "279392166",
      "cited_paper_id": 214802539
    },
    {
      "context_text": "FCLEntity-Att [332] combines convolutional neural networks and graph attention networks to encode the contextual representations of unseen entities and relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining CNNs and GATs for encoding unseen entities and relations.",
      "processing_time": 52.2794930934906,
      "citing_paper_id": "279392166",
      "cited_paper_id": 218589276
    },
    {
      "context_text": "ATiSE [226] represents entities and predicates as time-related Gaussian distributions, thereby modeling uncertainty over time.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for representing entities and predicates in a temporal knowledge graph.",
      "processing_time": 52.10382056236267,
      "citing_paper_id": "279392166",
      "cited_paper_id": 218900866
    },
    {
      "context_text": "Similarly, models such as TRL-KEQA [367], TransE-QA [368], and CAP-KGQA [369] integrate KGE into KGQA systems to improve reasoning over previously unknown answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses models that integrate knowledge graph embeddings (KGE) into knowledge graph question answering (KGQA) systems. No specific datasets are mentioned.",
      "processing_time": 53.29045534133911,
      "citing_paper_id": "279392166",
      "cited_paper_id": 219295273
    },
    {
      "context_text": "Similarly, models such as TRL-KEQA [367], TransE-QA [368], and CAP-KGQA [369] integrate KGE into KGQA systems to improve reasoning over previously unknown answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses models that integrate knowledge graph embeddings (KGE) into knowledge graph question answering (KGQA) systems. No specific datasets are mentioned.",
      "processing_time": 53.29045534133911,
      "citing_paper_id": "279392166",
      "cited_paper_id": 244946393
    },
    {
      "context_text": "Similarly, models such as TRL-KEQA [367], TransE-QA [368], and CAP-KGQA [369] integrate KGE into KGQA systems to improve reasoning over previously unknown answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses models that integrate knowledge graph embeddings (KGE) into knowledge graph question answering (KGQA) systems. No specific datasets are mentioned.",
      "processing_time": 53.29045534133911,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252625996
    },
    {
      "context_text": "In contrast, CRONKGQA [382] extends the static KGQA model EmbedKGQA [370] to accommodate temporal queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on extending a static KGQA model to handle temporal queries.",
      "processing_time": 52.5475811958313,
      "citing_paper_id": "279392166",
      "cited_paper_id": 220047862
    },
    {
      "context_text": "(b) Question Answering Systems for Unknown Answers : For more intricate QA tasks that require multi-step reasoning, EmbedKGQA [370] extends KEQA by incorporating a BERT model to encode complex queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EmbedKGQA) that extends another method (KEQA) by incorporating BERT for encoding complex queries.",
      "processing_time": 53.268474817276,
      "citing_paper_id": "279392166",
      "cited_paper_id": 220047862
    },
    {
      "context_text": "Besides, KGECF [391] treats user‚Äìitem interactions as a KG with a single relation and leverages the KGE model RotatE [62] to transform collaborative filtering into a single-step KGR task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KGECF) and a model (RotatE).",
      "processing_time": 52.881880044937134,
      "citing_paper_id": "279392166",
      "cited_paper_id": 220886470
    },
    {
      "context_text": "Thus, knowledge graph reasoning (KGR) (also known as knowledge graph completion) techniques [16] aim to discover semantic associations from the existing knowledge in a KG and infer unknown facts to complete the KG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general technique for knowledge graph reasoning.",
      "processing_time": 51.80276083946228,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221090697
    },
    {
      "context_text": "OWE-MRC [342] leverages machine reading comprehension techniques to extract meaningful short text snippets from lengthy descriptions for improving accurate text embedding.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting short text snippets to improve text embedding.",
      "processing_time": 52.41027092933655,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221192811
    },
    {
      "context_text": "Chai et al. [423] apply the KGE model TransE [45] to derive embeddings for a medical KG and then utilize a BiLSTM network to predict associations between pathologies and diseases.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of a knowledge graph for deriving embeddings and predicting associations, but does not specify a named dataset.",
      "processing_time": 52.25549912452698,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221280346
    },
    {
      "context_text": "InductivE [351] combines PLM and word embeddings to learn text embeddings for entities and further leverages a GNN to encode neighbor subgraphs, enhancing the representations of unseen entities by integrating both text embeddings and subgraph features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the combination of PLM and word embeddings with GNN for entity representation.",
      "processing_time": 54.44365978240967,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221819493
    },
    {
      "context_text": "For instance, WOWE [340] and Caps-OWKG [341] employ attention networks and relation-aware similarity measures to compute weights for each word in an entity‚Äôs textual description.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of attention networks and relation-aware similarity measures in the cited works.",
      "processing_time": 54.43308901786804,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221839825
    },
    {
      "context_text": "To further improve the generalization of rule-based reasoning approaches, ARGCN [316] and ELPE [317] both employ GCN and Graph Transformers, respectively, to encode embeddings for un-seen entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the methodologies used to improve rule-based reasoning approaches.",
      "processing_time": 53.242538928985596,
      "citing_paper_id": "279392166",
      "cited_paper_id": 221954184
    },
    {
      "context_text": "To further improve the generalization of rule-based reasoning approaches, ARGCN [316] and ELPE [317] both employ GCN and Graph Transformers, respectively, to encode embeddings for un-seen entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the methodologies used to improve rule-based reasoning approaches.",
      "processing_time": 53.242538928985596,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251719495
    },
    {
      "context_text": "To alleviate this issue, both DacKGR [448] and RuMER-RL [449] dynamically add edges as auxiliary actions during reinforcement learning-based path searching processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two methods, DacKGR and RuMER-RL, which are used for dynamic edge addition during reinforcement learning-based path searching. No specific datasets are mentioned.",
      "processing_time": 54.96100401878357,
      "citing_paper_id": "279392166",
      "cited_paper_id": 222133165
    },
    {
      "context_text": "To alleviate this issue, both DacKGR [448] and RuMER-RL [449] dynamically add edges as auxiliary actions during reinforcement learning-based path searching processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two methods, DacKGR and RuMER-RL, which are used for dynamic edge addition during reinforcement learning-based path searching. No specific datasets are mentioned.",
      "processing_time": 54.96100401878357,
      "citing_paper_id": "279392166",
      "cited_paper_id": 270990971
    },
    {
      "context_text": "To overcome this limitation, RE-NET [230] aggregates historical events related to a target entity at each time step into subgraphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RE-NET) and its functionality. No verifiable datasets are referenced.",
      "processing_time": 53.43648147583008,
      "citing_paper_id": "279392166",
      "cited_paper_id": 222205878
    },
    {
      "context_text": "RNNLogic [170] employs an RNN-based rule generator coupled with a reasoning predictor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RNNLogic) and its components. The cited paper title confirms that RNNLogic is a method, not a dataset.",
      "processing_time": 55.882654905319214,
      "citing_paper_id": "279392166",
      "cited_paper_id": 222208985
    },
    {
      "context_text": "Xiao et al. [218] propose a temporal KG incremental construction model that captures the dynamic characteristics of entities and relations over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing temporal knowledge graphs.",
      "processing_time": 52.04739260673523,
      "citing_paper_id": "279392166",
      "cited_paper_id": 224769806
    },
    {
      "context_text": "CGR [420] further bridges the semantic gap between visual scenes and external KGs by selectively composing knowledge routing paths through matching and retrieval of diverse paths, which improves generalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving generalization in visual relationship detection.",
      "processing_time": 52.50362157821655,
      "citing_paper_id": "279392166",
      "cited_paper_id": 226203042
    },
    {
      "context_text": "PairE [52] learns pairwise embeddings for each relation, which are then used to project the head and tail entities separately before computing their distance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning embeddings. The context focuses on the methodology rather than the data used.",
      "processing_time": 53.220646142959595,
      "citing_paper_id": "279392166",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "OWE-RST [343] designs a transfer function to map text embeddings into the KGR space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transforming text embeddings into a knowledge graph reasoning space.",
      "processing_time": 52.70996856689453,
      "citing_paper_id": "279392166",
      "cited_paper_id": 227231162
    },
    {
      "context_text": "Gong et al. [426] integrate electronic medical records with a medical KG, using KGE techniques to recommend safe medications.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "electronic medical records"
      ],
      "dataset_descriptions": {
        "electronic medical records": "Used to integrate with a medical knowledge graph for safe medication recommendation, focusing on the application of knowledge graph embedding techniques."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions the integration of electronic medical records with a medical knowledge graph for safe medication recommendation, which aligns with multi-modal knowledge graph reasoning.",
      "processing_time": 61.40832781791687,
      "citing_paper_id": "279392166",
      "cited_paper_id": 227239576
    },
    {
      "context_text": "Besides, to specifically model recurring events over time, CyGNet [232] draws inspiration from replication strategies in natural language generation tasks by introducing a time-aware replication mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CyGNet) and a general reference to natural language generation tasks. No verifiable datasets are identified.",
      "processing_time": 54.92240762710571,
      "citing_paper_id": "279392166",
      "cited_paper_id": 229180723
    },
    {
      "context_text": "Moreover, models such as TACT [325], NRTG [326], and CoMPILE [327] further refine GraIL.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets in the provided text.",
      "processing_time": 53.41223406791687,
      "citing_paper_id": "279392166",
      "cited_paper_id": 229213013
    },
    {
      "context_text": "Moreover, models such as TACT [325], NRTG [326], and CoMPILE [327] further refine GraIL.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets in the provided text.",
      "processing_time": 53.41223406791687,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253386566
    },
    {
      "context_text": "Besides, models such as the Hypergraph Transformer [410], CMRL [411], and KRISP [412] build query-relevant KGs by integrating information from the question, image targets, and external KGs, and design tailored reasoning methods to derive effective answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and their methodologies but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 53.408769607543945,
      "citing_paper_id": "279392166",
      "cited_paper_id": 229339845
    },
    {
      "context_text": "Besides, models such as the Hypergraph Transformer [410], CMRL [411], and KRISP [412] build query-relevant KGs by integrating information from the question, image targets, and external KGs, and design tailored reasoning methods to derive effective answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and their methodologies but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 53.408769607543945,
      "citing_paper_id": "279392166",
      "cited_paper_id": 261431541
    },
    {
      "context_text": "The most frequently employed datasets for dynamic KGR tasks are derived from two open-source databases: the Global Database of Events, Language, and Tone (GDELT) [357] and the Integrated Crisis Early Warning System (ICEWS) [358].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Global Database of Events, Language, and Tone (GDELT)",
        "Integrated Crisis Early Warning System (ICEWS)"
      ],
      "dataset_descriptions": {
        "Global Database of Events, Language, and Tone (GDELT)": "Used for dynamic knowledge graph reasoning tasks, providing event data and linguistic tone analysis for global events.",
        "Integrated Crisis Early Warning System (ICEWS)": "Used for dynamic knowledge graph reasoning tasks, offering structured data on political events and crisis situations for predictive modeling."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific databases used for dynamic KGR tasks, which are relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 70.15596413612366,
      "citing_paper_id": "279392166",
      "cited_paper_id": 229377193
    },
    {
      "context_text": "SAGNN [333] assigns different weights to various neighbor nodes of an unseen entity based on topological features such as in-degree, out-degree, and co-occurrence frequency, thereby learning robust embeddings for unseen entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SAGNN) and its application in learning embeddings for unseen entities and relations.",
      "processing_time": 54.11754393577576,
      "citing_paper_id": "279392166",
      "cited_paper_id": 229703490
    },
    {
      "context_text": "2) Ontology-Enhanced Model: Given that KGs typically include an ontological layer comprising concepts and their relations, OntoZSL [295] leverages the inherent concept hierarchy to construct a prior concept graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (OntoZSL) that leverages concept hierarchies in knowledge graphs.",
      "processing_time": 54.11404371261597,
      "citing_paper_id": "279392166",
      "cited_paper_id": 231925372
    },
    {
      "context_text": "BERTRL [350] linearizes the neighbor subgraph of an entity into serialized paths, which are then input into BERT for fine-tuning to encode neighbor information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT) and a technique for using it (linearizing neighbor subgraphs).",
      "processing_time": 54.11051034927368,
      "citing_paper_id": "279392166",
      "cited_paper_id": 232222958
    },
    {
      "context_text": "To address this issue, HMNet [283] introduces a hybrid matching network that jointly computes matching scores at both the entity and relation levels via entity-aware and relation-aware matching networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HMNet) and its application in few-shot link prediction.",
      "processing_time": 53.5762619972229,
      "citing_paper_id": "279392166",
      "cited_paper_id": 233224480
    },
    {
      "context_text": "TIE [212] addresses this by using deleted triples as negative triples and fine-tuning the model with new ones, effectively adapting to semantic changes in the KG.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling semantic changes in knowledge graphs.",
      "processing_time": 52.67658472061157,
      "citing_paper_id": "279392166",
      "cited_paper_id": 233295959
    },
    {
      "context_text": "Based on reinforcement learning-based approaches from static multi-step KGR, TPath [240] formulates path reasoning in TKGs within an RL framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (TPath) and a general approach (reinforcement learning).",
      "processing_time": 54.090550661087036,
      "citing_paper_id": "279392166",
      "cited_paper_id": 233303189
    },
    {
      "context_text": "CCS [141] introduces clustering caching based on entity similarity, and NS-KGE [142] converts the loss function to squared loss to reduce negative triple dependence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing techniques and their improvements.",
      "processing_time": 53.5788938999176,
      "citing_paper_id": "279392166",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "Besides, Zhang et al. [435] design a pre-trained KGE model for e-commerce product graphs, which is subsequently applied to tasks such as product recommendation and classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a pre-trained KGE model for e-commerce product graphs but does not specify a dataset. The focus is on the model and its application.",
      "processing_time": 54.36173367500305,
      "citing_paper_id": "279392166",
      "cited_paper_id": 233481294
    },
    {
      "context_text": "Furthermore, DualE [64] models relations as exhibiting both translation and rotation characteristics in a dual quaternion space, offering enhanced representational capabilities compared to QuatE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DualE which is a knowledge graph embedding model.",
      "processing_time": 53.57170486450195,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235306387
    },
    {
      "context_text": "Similarly, KE-GCN [74] integrates conventional graph convolution operations with KGE techniques to iteratively refine entity and relation embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining graph convolution operations with knowledge graph embedding techniques.",
      "processing_time": 53.151325702667236,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235324797
    },
    {
      "context_text": "1) Subgraph and Path-based Model: NBFNet [319] is a representative approach that endows the model with inductive learning capabilities by combining path-based together with GNN-based KGE models and defining three operators to enhance the inductive capability of the GNN. Similarly, GraIL [320]‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the NBFNet model and its capabilities.",
      "processing_time": 54.665547370910645,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235422273
    },
    {
      "context_text": "‚Ä¶effectively combining symbolic reasoning with KGE, NeuSTIP improves both the accuracy and explainability of TKGR. 4) Multi-Step TKGR Model: Multi-step TKGR is a comprehensive task that integrates both temporal KGR and multi-step KGR. Han et al. [239] introduce the first multi-step TKGR model xERTE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (xERTE) and a task (multi-step TKGR). No verifiable resources are identified.",
      "processing_time": 56.121307373046875,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235614395
    },
    {
      "context_text": "Besides, the well-known KG Wikidata [239] can be leveraged for constructing dynamic KGR datasets when temporal information is incorporated.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikidata"
      ],
      "dataset_descriptions": {
        "Wikidata": "Used to construct dynamic KGR datasets by incorporating temporal information, enhancing the reasoning capabilities over time."
      },
      "confidence_score": 0.9,
      "reasoning": "Wikidata is mentioned as a knowledge graph that can be used to construct dynamic KGR datasets, particularly when temporal information is added.",
      "processing_time": 60.335954904556274,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235614395
    },
    {
      "context_text": "In contrast, MMRG [415] constructs both text-based and visual KGs, and enhances visual relational reasoning by employing pre-training tasks namely attribute masking and contextual prediction similar to those in BERT, improving the performance of graph matching-based cross-modal retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMRG, which is a method for constructing multi-modal knowledge graphs and enhancing visual relational reasoning. No specific dataset is mentioned.",
      "processing_time": 54.86805009841919,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235703270
    },
    {
      "context_text": "Furthermore, MetaP [284] employs a CNN-based pattern learner to extract relation-specific patterns from a limited number of reference triples, and a balanced pattern matcher computes the similarity between the pattern representations of both positive and negative reference triples together with‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MetaP) and its components. The context focuses on the methodology and approach rather than a particular dataset.",
      "processing_time": 56.32989835739136,
      "citing_paper_id": "279392166",
      "cited_paper_id": 235792443
    },
    {
      "context_text": "Similarly, TransET [94] utilizes a projection mechanism to map entity embeddings into corresponding type embeddings, thereby enhancing entity embeddings with type information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (TransET) and its functionality. No verifiable resources are identified.",
      "processing_time": 53.530938148498535,
      "citing_paper_id": "279392166",
      "cited_paper_id": 236279449
    },
    {
      "context_text": "Meta-iKG [289] reformulates the few-shot KGR task as a subgraph modeling task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the reformulation of a task, not the use of a dataset.",
      "processing_time": 56.11097812652588,
      "citing_paper_id": "279392166",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "The T-GAP model [241] further improves KGR by encoding contextual knowledge related to the query event using a GNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (T-GAP) and a method (GNN). The context focuses on the improvement of Knowledge Graph Reasoning (KGR) through the use of the T-GAP model.",
      "processing_time": 59.70167255401611,
      "citing_paper_id": "279392166",
      "cited_paper_id": 236979995
    },
    {
      "context_text": "By incorporating this representation into a re-inforcement learning framework, RTTI facilitates more effective multi-step reasoning in TKGs. TITer [243] further addresses the challenge of multi-step reasoning for unseen timestamps by incorporating relative time encoding into an RL-based path search‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for temporal knowledge graph reasoning.",
      "processing_time": 52.98935508728027,
      "citing_paper_id": "279392166",
      "cited_paper_id": 237454564
    },
    {
      "context_text": "For instance, FITCARL [310] uses a time-aware Transformer to encode embeddings for unseen entities in a temporal KG. TITer [243] designs a relative time encoding function and a time reward strategy within a reinforcement learning-based multi-step reasoning framework, enabling multi-step reasoning‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodologies used in the papers rather than the datasets.",
      "processing_time": 55.17056488990784,
      "citing_paper_id": "279392166",
      "cited_paper_id": 237454564
    },
    {
      "context_text": "For addressing complex temporal QA, EXAQT [385] initially extracts dense KG subgraphs relevant to the query and enhances event data via a fine-tuned BERT model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EXAQT) and a model (BERT).",
      "processing_time": 54.04347276687622,
      "citing_paper_id": "279392166",
      "cited_paper_id": 237571654
    },
    {
      "context_text": "PathCon [110], SNRI [321], REPORT [322], LogCo [323], and RPC-IR [324] all improve upon NBFNet by extracting the relation context subgraphs and relation paths of unseen entity pairs, then aggregating their representations to perform relation reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods (PathCon, SNRI, REPORT, LogCo, RPC-IR) that improve upon NBFNet, but does not mention any specific datasets. The cited papers also do not provide clear dataset names.",
      "processing_time": 59.52185869216919,
      "citing_paper_id": "279392166",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "PathCon [110], SNRI [321], REPORT [322], LogCo [323], and RPC-IR [324] all improve upon NBFNet by extracting the relation context subgraphs and relation paths of unseen entity pairs, then aggregating their representations to perform relation reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods (PathCon, SNRI, REPORT, LogCo, RPC-IR) that improve upon NBFNet, but does not mention any specific datasets. The cited papers also do not provide clear dataset names.",
      "processing_time": 59.52185869216919,
      "citing_paper_id": "279392166",
      "cited_paper_id": 257912512
    },
    {
      "context_text": "Since different relations in a real-world relation path may have varying degrees of semantic importance for expressing the relationship between entities, both TransE&RW [100] and HARPA [101] employ hierarchical attention networks to select important relations in the path and learn path embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of hierarchical attention networks in relation path modeling.",
      "processing_time": 55.15961265563965,
      "citing_paper_id": "279392166",
      "cited_paper_id": 240417169
    },
    {
      "context_text": "(b) Iterative Integration of Rule Learning and KGE : To achieve a more sufficient integration of rule learning and KGE, models such as IterE [178], UniKER [179], and EngineKG [180] develop iterative mechanisms that alternate between these two processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the integration of rule learning and KGE using various models.",
      "processing_time": 55.52059197425842,
      "citing_paper_id": "279392166",
      "cited_paper_id": 243696417
    },
    {
      "context_text": "(b) Iterative Integration of Rule Learning and KGE : To achieve a more sufficient integration of rule learning and KGE, models such as IterE [178], UniKER [179], and EngineKG [180] develop iterative mechanisms that alternate between these two processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the integration of rule learning and KGE using various models.",
      "processing_time": 55.52059197425842,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251718718
    },
    {
      "context_text": "Moreover, H2E [60] and HBE [61] combine hyperbolic geometry with polar coordinates to capture hierarchical structures more comprehensively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (H2E and HBE) that combine hyperbolic geometry with polar coordinates. No verifiable datasets are referenced.",
      "processing_time": 57.15554141998291,
      "citing_paper_id": "279392166",
      "cited_paper_id": 244119616
    },
    {
      "context_text": "Moreover, H2E [60] and HBE [61] combine hyperbolic geometry with polar coordinates to capture hierarchical structures more comprehensively.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (H2E and HBE) that combine hyperbolic geometry with polar coordinates. No verifiable datasets are referenced.",
      "processing_time": 57.15554141998291,
      "citing_paper_id": "279392166",
      "cited_paper_id": 244894903
    },
    {
      "context_text": "P-INT [298] exploits a metric learning approach that, unlike earlier methods, represents entity pairs by leveraging the multi-hop paths connecting the head and tail entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for representing entity pairs using multi-hop paths in a knowledge graph.",
      "processing_time": 54.29468393325806,
      "citing_paper_id": "279392166",
      "cited_paper_id": 244119769
    },
    {
      "context_text": "MMKRL [274] further augments SGMPT by integrating the original multi-modal data with structural knowledge using a TransE model and a pre-training phase to reconstruct embeddings in a unified space, accompanied by an alignment module that minimizes reconstruction error.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'multi-modal data' but does not specify a named dataset. The focus is on the methodological augmentation of SGMPT with structural knowledge.",
      "processing_time": 55.509169816970825,
      "citing_paper_id": "279392166",
      "cited_paper_id": 244222941
    },
    {
      "context_text": "Furthermore, existing temporal rule learning models such as StreamLearner [234], TLogic [235], LCGE [228], and TLIP [236] have only addressed a few simple temporal patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing existing temporal rule learning models and their limitations.",
      "processing_time": 55.50589203834534,
      "citing_paper_id": "279392166",
      "cited_paper_id": 245144534
    },
    {
      "context_text": "Furthermore, existing temporal rule learning models such as StreamLearner [234], TLogic [235], LCGE [228], and TLIP [236] have only addressed a few simple temporal patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing existing temporal rule learning models and their limitations.",
      "processing_time": 55.50589203834534,
      "citing_paper_id": "279392166",
      "cited_paper_id": 254096435
    },
    {
      "context_text": "TLogic [235] employs temporal random walks to mine rules with more diverse temporal characteristics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TLogic) and its application. The context focuses on the technique rather than a particular dataset.",
      "processing_time": 56.52709078788757,
      "citing_paper_id": "279392166",
      "cited_paper_id": 245144534
    },
    {
      "context_text": "ABIE [208] employs k - shell decomposition from complex networks to identify key knowledge anchors within the KG.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for identifying key knowledge anchors within a knowledge graph.",
      "processing_time": 53.83193588256836,
      "citing_paper_id": "279392166",
      "cited_paper_id": 245313643
    },
    {
      "context_text": "Similarly, Ryotaro et al. [406] present a modified KGAT model that compresses auxiliary information to reduce computational costs while maintaining accuracy and explainability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KGAT model).",
      "processing_time": 53.09587645530701,
      "citing_paper_id": "279392166",
      "cited_paper_id": 245395720
    },
    {
      "context_text": "Similarly, UOKE [217] performs localized updates on modified triples by encoding subgraph information with an RGCN to update the embeddings of affected entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (UOKE) for updating knowledge graph embeddings, which is not a dataset.",
      "processing_time": 56.82267904281616,
      "citing_paper_id": "279392166",
      "cited_paper_id": 245648744
    },
    {
      "context_text": "Furthermore, Hsu et al. [395] designs a recommendation algorithm that identifies fund entities likely to have purchasing correlations with users by searching for intermediate paths between user and fund entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It focuses on a method for recommending mutual funds using knowledge graph embeddings.",
      "processing_time": 54.7967472076416,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246026190
    },
    {
      "context_text": "Since most publicly available KGs fall under the static category, the majority of existing KGR research focuses on static KGs. (2) Dynamic KGs [35]: In real-world scenarios, data from domains such as finance, news, and academia often exhibit inherent dynamic characteristics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the nature of static and dynamic knowledge graphs without referencing any particular dataset.",
      "processing_time": 55.740090131759644,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246063616
    },
    {
      "context_text": "More-over, RED-GNN [314] constructs rule patterns by modeling relation-directed graphs composed of overlapping segments of relation paths, thereby enabling the learning of more complex logical rules.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RED-GNN) and its application in constructing rule patterns.",
      "processing_time": 54.5685396194458,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246210481
    },
    {
      "context_text": "GenKGC [79] transforms KGR into a sequence-to-sequence generation task by leveraging pre-trained language models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transforming knowledge graph reasoning into a sequence-to-sequence generation task.",
      "processing_time": 54.572362422943115,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246608097
    },
    {
      "context_text": "Niu et al. [113] develop a rule and data co-driven path embedding scheme to represent each path between entities, then integrate the embeddings of multiple paths and utilize a bidirectional reasoning mechanism to achieve multi-step reasoning in KGs. 4) Negative Sampling for KGE: KGR operates under‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a method for path representation and reasoning in knowledge graphs but does not specify a dataset. The focus is on the methodological approach rather than a specific dataset.",
      "processing_time": 57.52024865150452,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246671483
    },
    {
      "context_text": "[25] reviews approaches that integrate symbolic logic with KGE models and highlights the advantages of incorporating symbolic logic into KG embeddings in terms of explainability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of methods and approaches.",
      "processing_time": 53.056201696395874,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246863638
    },
    {
      "context_text": "In specific, our survey highlights the differences and main characteristics as shown in TABLE I. ‚Ä¢ The previous reviews [23]‚Äì[25] all introduce single-step KGR models specific to static KGs. Specifically, [23] discusses knowledge graph embedding (KGE) models from the perspective of embedding spaces‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a discussion of knowledge graph embedding models and their characteristics. No verifiable resources are identified.",
      "processing_time": 55.70217704772949,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246863638
    },
    {
      "context_text": "In specific, our survey highlights the differences and main characteristics as shown in TABLE I. ‚Ä¢ The previous reviews [23]‚Äì[25] all introduce single-step KGR models specific to static KGs. Specifically, [23] discusses knowledge graph embedding (KGE) models from the perspective of embedding spaces‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a discussion of knowledge graph embedding models and their characteristics. No verifiable resources are identified.",
      "processing_time": 55.70217704772949,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253384318
    },
    {
      "context_text": "The descriptions of these tasks are as follows: (1) Static Single-Step KGR [25]: This foundational task involves mining semantic associations between entities from known triples in a static KG and transferring these associations to predict missing elements in unknown triples.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task description for knowledge graph reasoning.",
      "processing_time": 53.44102334976196,
      "citing_paper_id": "279392166",
      "cited_paper_id": 246863638
    },
    {
      "context_text": "Similarly, SimKGC [81] combines a PLM with a contrastive learning strategy to encode the textual information of entities, thereby achieving superior KGR performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SimKGC) and its approach to knowledge graph reasoning.",
      "processing_time": 54.781343936920166,
      "citing_paper_id": "279392166",
      "cited_paper_id": 247244896
    },
    {
      "context_text": "SimKGC [81] demonstrates superior reasoning performance by combining pre-trained language models with contrastive learning techniques.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SimKGC) and its performance. The context focuses on the method's capabilities rather than the use of a particular dataset.",
      "processing_time": 58.349223613739014,
      "citing_paper_id": "279392166",
      "cited_paper_id": 247244896
    },
    {
      "context_text": "For instance, MKG-former [271] and VISITA [272] both utilize a Transformer-based framework employing Vision Transformer (ViT) for image encoding and BERT for text encoding to construct a multi-level fused encoder that integrates image and text features for entity representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the use of a method (Transformer-based framework) rather than a specific dataset. No dataset names are mentioned.",
      "processing_time": 54.77356696128845,
      "citing_paper_id": "279392166",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "For instance, MKG-former [271] and VISITA [272] both utilize a Transformer-based framework employing Vision Transformer (ViT) for image encoding and BERT for text encoding to construct a multi-level fused encoder that integrates image and text features for entity representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the use of a method (Transformer-based framework) rather than a specific dataset. No dataset names are mentioned.",
      "processing_time": 54.77356696128845,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266166905
    },
    {
      "context_text": "Moreover, BERT-ConvE [447] integrates the transfer learning capabilities of BERT with the KGE model ConvE [70] by using textual representations to compensate for structural sparsity within the graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The integration of BERT and ConvE is described, but no dataset is referenced for training or evaluation.",
      "processing_time": 57.61410307884216,
      "citing_paper_id": "279392166",
      "cited_paper_id": 248545885
    },
    {
      "context_text": "GRRN [375] further exploits deep path extraction techniques to infer potential relations between entities, thereby reinforcing commonsense QA.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for inferring relations between entities using deep path extraction techniques.",
      "processing_time": 53.422847747802734,
      "citing_paper_id": "279392166",
      "cited_paper_id": 248729692
    },
    {
      "context_text": "The few-shot KGR (FSKGR) task follows the conventional N-way K-shot paradigm commonly used in few-shot learning research in fields such as image and text classification [277].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general paradigm used in few-shot learning research.",
      "processing_time": 53.76775312423706,
      "citing_paper_id": "279392166",
      "cited_paper_id": 248798765
    },
    {
      "context_text": "‚Ä¶to jointly encode entities, relations, and their associated textual descriptions while preserving the intrinsic structure of the KG. Relphormer [83] proposes a novel Transformer variant tailored to the topological characteristics of KGs by dynamically sampling contextual subgraph sequences as‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Relphormer) and its application to knowledge graphs. No verifiable datasets are referenced.",
      "processing_time": 56.76564073562622,
      "citing_paper_id": "279392166",
      "cited_paper_id": 248986385
    },
    {
      "context_text": "Recognizing that relations convey substantial semantic information in multi-step paths, Li et al. [376] leverage relation embeddings derived from KGE models to refine both query and path representations, which results in more efficient multi-step QA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of relation embeddings derived from KGE models. No clear, verifiable dataset names are provided.",
      "processing_time": 55.66874146461487,
      "citing_paper_id": "279392166",
      "cited_paper_id": 249109773
    },
    {
      "context_text": "For instance, CG-AGG [331] generates entity representations with multiple semantic perspectives using a global aggregator based on a hyper-graph neural network (HGNN) in conjunction with a local aggregator based on a GNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the methodological aspects of generating entity representations.",
      "processing_time": 55.68377089500427,
      "citing_paper_id": "279392166",
      "cited_paper_id": 250289207
    },
    {
      "context_text": "Moreover, Gao et al. [428] leverage KGE to infer novel drug‚Äìdisease interactions, demonstrating its effectiveness and practical value in drug repurposing for Alzheimer‚Äôs disease.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of KGE (Knowledge Graph Embedding) for inferring drug-disease interactions. No clear, verifiable dataset names are provided.",
      "processing_time": 58.700923442840576,
      "citing_paper_id": "279392166",
      "cited_paper_id": 250511662
    },
    {
      "context_text": "Moreover, CF-KGQA [380] integrates causal relationships into the multi-hop QA process, thereby achieving more precise answers by effectively utilizing inherent causal structures within the KG. (d) Temporal Question Answering Systems with Explicit Time Representations : Temporal KG question‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CF-KGQA) and its application in multi-hop question answering over knowledge graphs.",
      "processing_time": 56.45677971839905,
      "citing_paper_id": "279392166",
      "cited_paper_id": 250562885
    },
    {
      "context_text": "In addition, HAPZSL [294] introduces a hybrid attention mechanism that incorporates both relation and entity attention during the encoding of textual descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HAPZSL) and its components. The context focuses on the hybrid attention mechanism used in the method.",
      "processing_time": 57.785332679748535,
      "citing_paper_id": "279392166",
      "cited_paper_id": 250635180
    },
    {
      "context_text": "(e) Temporal Question Answering Systems with Implicit Temporal Constraints : To conduct QA tasks involving implicit temporal constraints, CTRN [384] extends the model TempoQR by developing a complex temporal reasoning network capable of capturing implicit temporal features and relational‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CTRN) that extends another method (TempoQR).",
      "processing_time": 54.37115120887756,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251086467
    },
    {
      "context_text": "To address this, Evoda [166] employs a genetic logic programming algorithm, extending Horn rules to generalized Datalog rules [167] and incorporating a scoring mechanism based on relational representation learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (genetic logic programming algorithm) and its application to rule learning over knowledge graphs.",
      "processing_time": 56.183709144592285,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251294990
    },
    {
      "context_text": "KRACL [446] combines contrastive loss with cross-entropy loss to provide stronger supervisory signals for sparse entities by introducing a larger number of negative triples.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KRACL) and its application to sparse entities in knowledge graphs.",
      "processing_time": 55.05178475379944,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251594533
    },
    {
      "context_text": "Besides, BRGNN [374] utilizes bidirectional reasoning to generate multi-step query paths, capturing richer semantic correlations than the previous methods and consequently improving QA accuracy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BRGNN) and its improvements over previous methods in generating multi-step query paths for QA.",
      "processing_time": 57.04032278060913,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251708766
    },
    {
      "context_text": "1) Graph Search-based Model: Unlike the algorithm AMIE [160] presented in Section III-B1, which computes rule confidence scores over the entire KG, RuleNet [312] proposes a dual graph traversal approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RuleNet) and a comparison to another method (AMIE).",
      "processing_time": 55.961121559143066,
      "citing_paper_id": "279392166",
      "cited_paper_id": 251779418
    },
    {
      "context_text": "MD-NCaching [139] generates semantically invalid but structurally similar negative triples using a KGE model to evaluate triple likelihood, addressing false negatives.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MD-NCaching' but does not refer to it as a dataset. It is described as a method for generating negative triples, which is not a dataset.",
      "processing_time": 57.067307233810425,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252015770
    },
    {
      "context_text": "HiRe [290] employs a three-level framework to learn relation meta representations: first, a neighbor aggregator captures entity-level representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called HiRe. The context focuses on the methodology of learning relation meta representations.",
      "processing_time": 56.72103238105774,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252070677
    },
    {
      "context_text": "Besides, MMKGR [261] utilizes a gated attention network to generate multi-modal complementary features, facilitating extensive multi-modal interaction and noise reduction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMKGR) and its functionality. No verifiable resources are identified.",
      "processing_time": 56.16192865371704,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "Furthermore, DEKG-ILP [330] designs a GNN-based subgraph modeling module to leverage the global relation-based semantic features shared between source and target KGs and mine local subgraph information around each relation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DEKG-ILP) and its components. The context focuses on the design of a GNN-based subgraph modeling module for link prediction in knowledge graphs.",
      "processing_time": 59.871174335479736,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252090252
    },
    {
      "context_text": "IRGR [416] further refines this approach by introducing three distinct reasoning mechanisms namely intra-modal, inter-modal, and instance-based KGR that collectively model the relations among the various KGs and their neighboring instances to achieve a more effective similarity matrix.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reasoning mechanisms and approaches. The context is focused on methodological improvements.",
      "processing_time": 55.02477169036865,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252292167
    },
    {
      "context_text": "Within the transport sector, Zeng et al. [442] pioneer the development of a metro KG using historical origin‚Äìdestination matrices and complex network construction techniques, then assign semantic types to each station to build a subway KG.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions the use of historical origin-destination matrices, which could be considered a dataset, but it lacks a specific name or identifier. No other datasets are explicitly named.",
      "processing_time": 57.41726112365723,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252364875
    },
    {
      "context_text": "RAILD [349] fine-tunes a PLM to encode textual descriptions of entities and relations, learning embeddings for unseen entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions RAILD, which is a method for fine-tuning a pre-trained language model (PLM) to encode textual descriptions of entities and relations. No specific dataset is mentioned.",
      "processing_time": 58.84592151641846,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252564311
    },
    {
      "context_text": "Subsequently, ADK-KG [302] refines FIRE by designing a text-enhanced GNN to improve entity encoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ADK-KG) that refines another method (FIRE).",
      "processing_time": 56.142735719680786,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252716986
    },
    {
      "context_text": "The Transformer-based Rule-former [172] treats rule mining as a sequence-to-sequence task, using an enhanced attention mechanism to encode head entity subgraph context and decode relation probability distributions, simultaneously scoring reasoning results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Rule-former) and its application to rule mining over knowledge graphs.",
      "processing_time": 55.918673515319824,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252819373
    },
    {
      "context_text": "TwiRGCN [381] incorporates a time-weighted mechanism within GCN to encode entities and employs a gating mechanism to determine whether the answer corresponds to an entity or a timestamp, with the prediction result informing the answer scoring.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TwiRGCN) and its components. No verifiable resources are identified.",
      "processing_time": 56.709285259246826,
      "citing_paper_id": "279392166",
      "cited_paper_id": 252846570
    },
    {
      "context_text": "For example, Bi-Link [348] leverages the pre-trained language model BERT to learn text embeddings from the descriptions of entities and relations, enriching the relation embeddings with probabilistic rule-based prompt expressions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of BERT, which is a model, not a dataset. The focus is on the method and the approach used.",
      "processing_time": 33.04457426071167,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253117165
    },
    {
      "context_text": "RulE [177] addresses this by jointly embedding entities, relations and symbolic rules within a unified space, allowing for the calculation of confidence scores for each rule to ensure consistency with observed triples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for knowledge graph reasoning.",
      "processing_time": 53.32030248641968,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253157669
    },
    {
      "context_text": "In the recent three years, there are several representative review papers in the field of KGR [23]‚Äì[33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to review papers in the field of KGR. No verifiable resources are identified.",
      "processing_time": 57.02127766609192,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253384318
    },
    {
      "context_text": "In the recent three years, there are several representative review papers in the field of KGR [23]‚Äì[33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to review papers in the field of KGR. No verifiable resources are identified.",
      "processing_time": 57.02127766609192,
      "citing_paper_id": "279392166",
      "cited_paper_id": 257220329
    },
    {
      "context_text": "‚Ä¶characteristics as shown in TABLE I. ‚Ä¢ The previous reviews [23]‚Äì[25] all introduce single-step KGR models specific to static KGs. Specifically, [23] discusses knowledge graph embedding (KGE) models from the perspective of embedding spaces based on algebraic, geometric, and analytic structures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 55.330012798309326,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253384318
    },
    {
      "context_text": "Additionally, ARP [336] employs an attention network to extract the most relevant subgraph and contextual features for the current query triple, and TransNS [337] selects pertinent neighbors as attributes of the entity while leveraging the semantic affinity between entities to choose related‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of attention networks and semantic affinities in knowledge graph reasoning.",
      "processing_time": 57.51080870628357,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253448309
    },
    {
      "context_text": "To this end, LCILP [328] and ReCoLe [329] both employ local clustering methods to extract subgraphs that are semantically relevant to the current relation, which are then encoded via a GNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'LCILP' and 'ReCoLe', but these are methods or models, not datasets. No specific datasets are mentioned.",
      "processing_time": 56.971906661987305,
      "citing_paper_id": "279392166",
      "cited_paper_id": 253761315
    },
    {
      "context_text": "To this end, LCILP [328] and ReCoLe [329] both employ local clustering methods to extract subgraphs that are semantically relevant to the current relation, which are then encoded via a GNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'LCILP' and 'ReCoLe', but these are methods or models, not datasets. No specific datasets are mentioned.",
      "processing_time": 56.971906661987305,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256610018
    },
    {
      "context_text": "LCGE [228] integrates time-aware entity and predicate embeddings with temporal rule learning, meanwhile, leverages temporal rules to regularize predicate embeddings for modeling both timeliness and temporal order properties of events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on integrating time-aware entity and predicate embeddings with temporal rule learning.",
      "processing_time": 56.968464612960815,
      "citing_paper_id": "279392166",
      "cited_paper_id": 254096435
    },
    {
      "context_text": "EmReCo [344] employs a relation-aware attention aggregator and gating mechanism to learn text embeddings for unseen entities under specific relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning text embeddings. No verifiable resources are identified.",
      "processing_time": 54.974194049835205,
      "citing_paper_id": "279392166",
      "cited_paper_id": 254205278
    },
    {
      "context_text": "To address the inefficiencies in path search caused by random walks, ANet [186] integrates the heuristic A* search algorithm into the path search process, selecting the most pertinent entities and relations as actions to enhance training and reasoning efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses a method (ANet) that integrates the A* search algorithm to improve path search in knowledge graphs. No specific dataset is mentioned.",
      "processing_time": 56.961344957351685,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256597814
    },
    {
      "context_text": "IterE [186] and HoGRN [444] leverage logical rules to capture inter-relational correlations for enriching semantics and enhancing the embeddings of entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (IterE and HoGRN) that leverage logical rules for knowledge graph reasoning.",
      "processing_time": 56.63886857032776,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256597814
    },
    {
      "context_text": "A critical sub-class of dynamic KGs incorporates explicit temporal annotations, namely temporal KGs (TKGs) [29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a class of knowledge graphs. No verifiable resources are identified.",
      "processing_time": 55.30382823944092,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256598084
    },
    {
      "context_text": "(6) Inductive KGR [29]: Inductive KGR distinguishes itself by handling unseen entities (e.g., A , B , C and D in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for handling unseen entities in knowledge graphs.",
      "processing_time": 54.958884716033936,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256598084
    },
    {
      "context_text": "Existing review papers on KGR have primarily focused on specific scenarios of KGR, such as multi-modal KGR [34], temporal KGR [26], [27], and KGR with unseen entities and relations [29], [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of knowledge graph reasoning. No verifiable resources are identified.",
      "processing_time": 55.53515386581421,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256598084
    },
    {
      "context_text": "Existing review papers on KGR have primarily focused on specific scenarios of KGR, such as multi-modal KGR [34], temporal KGR [26], [27], and KGR with unseen entities and relations [29], [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of knowledge graph reasoning. No verifiable resources are identified.",
      "processing_time": 55.53515386581421,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268860898
    },
    {
      "context_text": "‚Ä¢ The existing reviews [29]‚Äì[31] summarize models for the few-shot KGR task, and [29] and [30] further discuss inductive KGR approaches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only reviews and models. No verifiable resources are identified.",
      "processing_time": 28.35139274597168,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256598084
    },
    {
      "context_text": "‚Ä¢ The existing reviews [29]‚Äì[31] summarize models for the few-shot KGR task, and [29] and [30] further discuss inductive KGR approaches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only reviews and models. No verifiable resources are identified.",
      "processing_time": 28.35139274597168,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266149723
    },
    {
      "context_text": "Specifically, [29] systematically reviews existing few-shot and inductive KGR models from two specific perspectives: unseen entities and unseen relations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.918089866638184,
      "citing_paper_id": "279392166",
      "cited_paper_id": 256598084
    },
    {
      "context_text": "‚Ä¶(1) solely on triples, (2) incorporating external information, (3) temporal KGs, (4) common-sense KGs, and (5) hyper-relational KGs. Liang et al. [33] categorize KGR models based on static, dynamic, and multi-modal KGs, providing a comprehensive summary of the approaches from the perspective of‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of knowledge graphs and reasoning models. The context is focused on summarizing approaches rather than using a particular dataset.",
      "processing_time": 56.96905589103699,
      "citing_paper_id": "279392166",
      "cited_paper_id": 257220329
    },
    {
      "context_text": "In contrast, ICEWS focuses on military and political events, providing event data with a daily temporal granularity [33].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ICEWS"
      ],
      "dataset_descriptions": {
        "ICEWS": "Used to provide event data with daily temporal granularity, focusing on military and political events for multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions ICEWS as a dataset providing event data with daily temporal granularity, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 61.846736431121826,
      "citing_paper_id": "279392166",
      "cited_paper_id": 257220329
    },
    {
      "context_text": "Liang et al. [33] provided a review of KGR approaches for static, dynamic, and multi-modal KGs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of KGR approaches. No verifiable resources are identified.",
      "processing_time": 55.525349855422974,
      "citing_paper_id": "279392166",
      "cited_paper_id": 257220329
    },
    {
      "context_text": "Similarly, iHT [80] pre-trains a Transformer-based language model on large-scale KGs and fine-tunes it on domain-specific graphs, thus achieving robust transferability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (iHT) and a general reference to 'large-scale KGs' and 'domain-specific graphs'. These are not specific enough to be considered verifiable resources.",
      "processing_time": 60.058101177215576,
      "citing_paper_id": "279392166",
      "cited_paper_id": 257771370
    },
    {
      "context_text": "To overcome this limitation, models such as PKEEQA [371], PA-KGQA [372], and HamQA [373] explicitly integrate path information extracted from KGs into the KGE process, thereby enhancing multi-step reasoning and enhancing the prediction of unknown answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 56.317121744155884,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258219651
    },
    {
      "context_text": "To overcome this limitation, models such as PKEEQA [371], PA-KGQA [372], and HamQA [373] explicitly integrate path information extracted from KGs into the KGE process, thereby enhancing multi-step reasoning and enhancing the prediction of unknown answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 56.317121744155884,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258333655
    },
    {
      "context_text": "An extension of this approach, MMRotatH [214], employs multi-modal embedding techniques to handle previously unseen modalities in new entities, incrementally embedding them into the pre-trained space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMRotatH) for handling unseen modalities in knowledge graphs.",
      "processing_time": 56.607524156570435,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258264587
    },
    {
      "context_text": "Truncated NS [131] leverages structural and attribute embeddings, DNS [132] uses same-type entities, ESNS [133] incorporates entity context, while RCWC [134] and conditional constraint sampling [135] utilize relation domain/range restrictions to measure the similarity among entities and generate‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.948177337646484,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258281574
    },
    {
      "context_text": "CPConvKE [107] designs a gate-based path embedding method to filter noisy paths, ensuring that path embeddings are highly relevant to the semantic relationships between entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for filtering noisy paths in knowledge graphs.",
      "processing_time": 54.3647985458374,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258363396
    },
    {
      "context_text": "3) Cybersecurity Domain: Sikos [436] constructs a domain-specific KG for information security and employs rule-learning-based reasoning models to automatically infer relations among vulnerabilities, weaknesses, platforms, and attack patterns with targeted defense strategies.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the construction of a domain-specific knowledge graph for cybersecurity, which is not a dataset but a method or resource for reasoning about cybersecurity data.",
      "processing_time": 57.63372302055359,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258423071
    },
    {
      "context_text": "‚Ä¶interpret relations, and extract subgraph structures to enhance the performance of KGR. (b) Utilizing Complex Logical Reasoning of LLMs : LARK [465] combines graph extraction algorithms with LLMs, reformulating complex KGR as a synthesis of context search within the KG and logical query‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on combining graph extraction algorithms with LLMs for complex KGR.",
      "processing_time": 57.6486291885376,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258436828
    },
    {
      "context_text": "AIR [211] is an adaptive incremental update framework, which measures the importance of triples to select those most affected by updates, and employs an embedding propagation mechanism to avoid full retraining.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (AIR) for updating embeddings in dynamic knowledge graphs, but does not mention any specific datasets.",
      "processing_time": 56.02352213859558,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258440205
    },
    {
      "context_text": "TR-Match [306] designs a multi-scale temporal relation attention encoder that adaptively captures both local and global information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the design of a multi-scale temporal relation attention encoder.",
      "processing_time": 57.64205884933472,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258440388
    },
    {
      "context_text": "Jia et al. [445] augment sparse KGs by incorporating external knowledge sources and fusing them using GNN technique, achieving performance improvements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions augmenting sparse KGs but does not specify any particular dataset. The focus is on the method and its application.",
      "processing_time": 56.581681966781616,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258457252
    },
    {
      "context_text": "(a) Models relying solely on LLMs : PPT [244] redefines TKGR as a masked word prediction task tailored for language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the approach of using language models for temporal knowledge graph reasoning.",
      "processing_time": 57.96339559555054,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258686648
    },
    {
      "context_text": "For instance, StructGPT [196] introduces an iterative call-linearize-generation framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called StructGPT. The context focuses on the introduction of a framework, not on the use of a dataset.",
      "processing_time": 58.7187876701355,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258714753
    },
    {
      "context_text": "GPT-NeoX [248] fine-tunes a LLM through in-context learning to endow it with TKGR capabilities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LLM fine-tuning) and a capability (TKGR).",
      "processing_time": 56.87846636772156,
      "citing_paper_id": "279392166",
      "cited_paper_id": 258762793
    },
    {
      "context_text": "These strategies position LLMs as agents to navigate KGs, thereby enhancing reasoning performance and addressing issues such as cold starts and missing answers [195].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only strategies for using LLMs with KGs. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.94425559043884,
      "citing_paper_id": "279392166",
      "cited_paper_id": 259165563
    },
    {
      "context_text": "Con-cept2Box [90] employs box embeddings to capture overlapping characteristics among ontology concepts and represents hierarchical relationships via the geometric properties (e.g., the area) of rectangular boxes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Concept2Box) and its application to ontology concepts and hierarchical relationships.",
      "processing_time": 56.86975145339966,
      "citing_paper_id": "279392166",
      "cited_paper_id": 259341618
    },
    {
      "context_text": "SToKE [246] can be regarded as an extension of K-BERT [247] specific to static KGs, jointly encoding graph topology and temporal context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing the relationship between SToKE and K-BERT, which are methods for encoding knowledge graphs.",
      "processing_time": 59.22218298912048,
      "citing_paper_id": "279392166",
      "cited_paper_id": 259858848
    },
    {
      "context_text": "ToG [199] employs beam search algorithms on KGs, leveraging LLMs to uncover valuable paths and deliver the most probable reasoning outcomes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of beam search algorithms on knowledge graphs and leveraging large language models.",
      "processing_time": 56.26156735420227,
      "citing_paper_id": "279392166",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "[28] provides a detailed introduction to the research status and development of temporal KGR, cate-gorizing the approaches into two main principles namely KG completion and event prediction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a categorization of approaches in temporal KGR.",
      "processing_time": 55.221808433532715,
      "citing_paper_id": "279392166",
      "cited_paper_id": 260611397
    },
    {
      "context_text": "(3) Dynamic KGR [28]: In dynamic KGR task, it is essential to capture both the semantic information of changes such as additions, deletions, and modifications of knowledge as well as the temporal information related to the evolution of events.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of dynamic KGR and the importance of capturing semantic and temporal information.",
      "processing_time": 56.85612869262695,
      "citing_paper_id": "279392166",
      "cited_paper_id": 260611397
    },
    {
      "context_text": "‚Ä¢ Three surveys [26]‚Äì[28] all provide review of existing KGR approaches for dynamic KGs.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'surveys' but does not specify any particular dataset. The context is about reviewing existing approaches, not using a specific dataset.",
      "processing_time": 57.60223865509033,
      "citing_paper_id": "279392166",
      "cited_paper_id": 260611397
    },
    {
      "context_text": "‚Ä¢ Three surveys [26]‚Äì[28] all provide review of existing KGR approaches for dynamic KGs.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'surveys' but does not specify any particular dataset. The context is about reviewing existing approaches, not using a specific dataset.",
      "processing_time": 57.60223865509033,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268860898
    },
    {
      "context_text": "‚Ä¶the generalised way of describing events, a temporal quadruple that model the time-dependent event is formed as ( subject, predicate, object, timestamp/time interval ) in TKGs. Temporal KGR (TKGR) task focuses on predicting events through historical pattern analysis under temporal constraints [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of the Temporal Knowledge Graph Representation (TKGR) task.",
      "processing_time": 56.88494062423706,
      "citing_paper_id": "279392166",
      "cited_paper_id": 260611397
    },
    {
      "context_text": "KD-CoT [198] formalizes the LLM-based multi-step reasoning process into a structured multi-turn question-answering (QA) format.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called KD-CoT. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.102782011032104,
      "citing_paper_id": "279392166",
      "cited_paper_id": 261214582
    },
    {
      "context_text": "SACN [72] further extends this paradigm by employing a weighted GCN to extract structural features and learn matrix embeddings for entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SACN) and its extension using a weighted GCN. No verifiable datasets are referenced.",
      "processing_time": 58.48104500770569,
      "citing_paper_id": "279392166",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "Finally, Markchom et al. [397] propose a meta-path grammar coupled with a machine translation model to convert complex meta-paths into natural language-like explanations, enhancing the explainability of the recommendation results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing explainability in recommender systems.",
      "processing_time": 54.508734941482544,
      "citing_paper_id": "279392166",
      "cited_paper_id": 263147330
    },
    {
      "context_text": "To address this gap, RTTI [242] introduces a novel time interval representation that leverages the median of two timestamps and variations in timestamp embeddings.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for representing time intervals in temporal knowledge graphs.",
      "processing_time": 55.43388271331787,
      "citing_paper_id": "279392166",
      "cited_paper_id": 263807455
    },
    {
      "context_text": "KoPA [84] introduces a new paradigm for KGR that leverages LLMs by designing knowledge prefix adapters, which convert structural embeddings from traditional KGE into text embeddings, thereby bridging the gap between structured and unstructured modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called KoPA. The context focuses on the introduction of a new paradigm for Knowledge Graph Reasoning using LLMs and knowledge prefix adapters.",
      "processing_time": 60.29957437515259,
      "citing_paper_id": "279392166",
      "cited_paper_id": 263830580
    },
    {
      "context_text": "(a) Utilizing Single-Step Reasoning of LLMs : KG-GPT [463] employs LLMs to retrieve relevant KG subgraphs and generate reasoning results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for reasoning on knowledge graphs.",
      "processing_time": 56.214030504226685,
      "citing_paper_id": "279392166",
      "cited_paper_id": 264172465
    },
    {
      "context_text": "Unlike previous methods focused on rule mining, Ott et al. [173] exploit the dependencies among rules, enhancing aggregation performance of multiples rules.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context focuses on the enhancement of rule aggregation performance, which is a methodological improvement.",
      "processing_time": 58.83526372909546,
      "citing_paper_id": "279392166",
      "cited_paper_id": 264350156
    },
    {
      "context_text": "[31] provides an overview of the latest research on inductive, zero-shot, and few-shot KGR models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an overview of research on KGR models.",
      "processing_time": 55.4375524520874,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266149723
    },
    {
      "context_text": "To address this limitation, NeuSTIP [238] is developed within a neural-symbolic framework, utilizing an intuitive rule-based language to enhance temporal consistency between adjacent predicates in the rule body, and introducing a confidence evaluation mechanism for temporal rules by integrating‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NeuSTIP) and its features. No verifiable resources are identified.",
      "processing_time": 58.07313942909241,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266201568
    },
    {
      "context_text": "In a similar idea, Jiao et al. [378] incorporate relation embeddings to capture nuanced interactions among entities, supporting multi-hop reasoning and enabling seamless traversal across relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for incorporating relation embeddings. The context focuses on the methodological approach rather than a specific dataset.",
      "processing_time": 57.561702489852905,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266231035
    },
    {
      "context_text": "To fill this gap, Zhang et al [462] introduce complex logical hypothesis generation to implement abductive KGR, proposing RLF-KG with reinforcement learning to enhance hypothesis generation by minimizing discrepancies between observations and conclusions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RLF-KG) and a general approach to abductive reasoning in knowledge graphs.",
      "processing_time": 58.07860279083252,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266550719
    },
    {
      "context_text": "Based on TILP, TEILP [237] introduces a differentiable random walk method for efficient rule learning and path construction within TKGs. TEILP associates each rule with a conditional probability density function that represents the probability distribution of an event‚Äôs occurrence given certain‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for reasoning over temporal knowledge graphs.",
      "processing_time": 55.94242262840271,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266551134
    },
    {
      "context_text": "Mitropoulou et al. [432] utilize various KGE techniques to uncover latent links among dispersed customer needs and to identify implicit relations between customer requirements and product features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to KGE techniques and their application, which are methods, not datasets.",
      "processing_time": 58.44043278694153,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266681126
    },
    {
      "context_text": "At present, the explainability of KGR mainly relies on rule learning-based models and multi-hop reasoning techniques, which provide explainability through symbolic rules [459] and explicit path information [460].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques for explainability in knowledge graph reasoning.",
      "processing_time": 56.19593906402588,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266755771
    },
    {
      "context_text": "Besides, both Xia et al. [469] and Luo et al. [470] exploit the CoT reasoning capabilities of LLMs by iteratively reasoning based on higher-order historical information, effectively treating temporal KGR as a dual process of fine-tuning LLMs along historical chains and generating reasoning results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the reasoning capabilities of LLMs and their application to temporal KGR.",
      "processing_time": 58.27092432975769,
      "citing_paper_id": "279392166",
      "cited_paper_id": 266933295
    },
    {
      "context_text": "GATQR [386] further employs a graph attention network to capture implicit temporal information in complex queries and uses a BERT model to learn temporal relation representations, which are integrated with pretrained temporal KG embeddings to identify the highest-scoring entity or timestamp as the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the integration of a graph attention network and BERT for temporal relation representation.",
      "processing_time": 58.98226499557495,
      "citing_paper_id": "279392166",
      "cited_paper_id": 267180466
    },
    {
      "context_text": "Besides, GLaM [204] introduces a fine-tuning framework that aligns LLMs with domain-specific KGs, transforming them into alternative text representations with labeled question-answer pairs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (fine-tuning framework) rather than a specific dataset. No dataset names are provided in the context.",
      "processing_time": 57.50671935081482,
      "citing_paper_id": "279392166",
      "cited_paper_id": 267627414
    },
    {
      "context_text": "Nguyen et al. [201] propose a discriminative and generative chain-of-thought (CoT) evaluation framework, assessing both the reasoning results and the intermediate steps generated by CoT prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological framework for evaluating chain-of-thought processes.",
      "processing_time": 37.42327308654785,
      "citing_paper_id": "279392166",
      "cited_paper_id": 267751000
    },
    {
      "context_text": "(d) Utilizing Multi-Hop Reasoning of LLMs : Nguyen et al. [201] explore the chain-of-thought (CoT) reasoning ability of LLMs in multi-hop reasoning tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the exploration of CoT reasoning in LLMs for multi-hop reasoning tasks.",
      "processing_time": 57.507055044174194,
      "citing_paper_id": "279392166",
      "cited_paper_id": 267751000
    },
    {
      "context_text": "KG-Agent [202] employs an iterative mechanism that autonomously selects tools for KGR and fine-tunes the base LLM using an instruction dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions an 'instruction dataset' used for fine-tuning the base LLM, but does not provide a specific name for the dataset.",
      "processing_time": 58.618462800979614,
      "citing_paper_id": "279392166",
      "cited_paper_id": 267751414
    },
    {
      "context_text": "Similarly, GenTKGQA [388] develops a two-stage approach, which proposes a subgraph retrieval enabling LLMs to extract subgraphs under temporal constraints and fuses subgraph encodings with textual representations generated by LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (GenTKGQA) rather than a dataset. No specific dataset is mentioned or used in the described research context.",
      "processing_time": 57.847262382507324,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268032370
    },
    {
      "context_text": "In another approach, Wang et al. [89] incorporate ontology information such as concept hierarchies and attribute constraints directly as constraints during the learning of entity and relation embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions incorporating ontology information but does not specify a named dataset. The focus is on the method of integrating ontology into knowledge graph embeddings.",
      "processing_time": 57.51476335525513,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268157585
    },
    {
      "context_text": "In pursuit of improved inference efficiency, SMetaR [288] streamlines the model GANA by applying a linear mapping to head-tail entity pairs in the support set to learn relation meta-representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SMetaR) and a model (GANA). The context focuses on the methodology and improvements in inference efficiency.",
      "processing_time": 59.73616862297058,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268717837
    },
    {
      "context_text": "Prog-TQA [387] introduces fundamental temporal operators and a novel self-evolving programming approach that leverages the contextual learning capabilities of LLMs to interpret combinatorial temporal constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on introducing temporal operators and a programming approach.",
      "processing_time": 58.231168270111084,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268856632
    },
    {
      "context_text": "It is worth noting that these surveys focus on the temporal KGR task, which is a sub-task of the dynamic KGR. Specifically, [26] discusses temporal KGR models from the perspectives of causal reasoning and commonsense reasoning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion of temporal KGR models. No verifiable resources are identified.",
      "processing_time": 57.83260226249695,
      "citing_paper_id": "279392166",
      "cited_paper_id": 268860898
    },
    {
      "context_text": "(c) Question Answering Systems for Complex Queries : DSSAGN [377] integrates both syntactic and semantic information by employing an enhanced graph neural network to model dependencies between various segments of the KG and the query, thereby facilitating accurate multi-hop reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DSSAGN) and its application to multi-hop reasoning in knowledge graphs.",
      "processing_time": 58.76362204551697,
      "citing_paper_id": "279392166",
      "cited_paper_id": 269100293
    },
    {
      "context_text": "Furthermore, recent research [413] has demonstrated that leveraging multi-modal LLMs could outperform fine-tuned models on VQA tasks that require ex-ternal knowledge.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to VQA tasks. No clear, verifiable datasets are identified.",
      "processing_time": 58.37794637680054,
      "citing_paper_id": "279392166",
      "cited_paper_id": 269157470
    },
    {
      "context_text": "MTRN [291] introduces a self-attention-based entity pair encoder to capture interactions between head and tail entities associated with long-tailed relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MTRN) and its components. The context focuses on the introduction of a self-attention-based entity pair encoder, which is part of the method, not a dataset.",
      "processing_time": 61.66908812522888,
      "citing_paper_id": "279392166",
      "cited_paper_id": 269580299
    },
    {
      "context_text": "Considering the challenges posed by unbalanced multi-modal information distribution, NativE [260] introduces a relation-guided bi-adaptive fusion module that adaptively fuses any modality, and employs a collaborative modal adversarial training framework to augment under-represented modal‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NativE) and its components. The context focuses on the technical aspects of the method rather than the use of a particular dataset.",
      "processing_time": 60.56403732299805,
      "citing_paper_id": "279392166",
      "cited_paper_id": 270711106
    },
    {
      "context_text": "(b) KGR for Enhancing Explainability of Recommendations : To address the challenge of explainability in recommendation systems, the existing techniques typically employ explicit reasoning over the KG by using path information to assess the similarity between items or users [392].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general techniques and approaches. No clear identifiers for datasets are present.",
      "processing_time": 57.467785596847534,
      "citing_paper_id": "279392166",
      "cited_paper_id": 271003412
    },
    {
      "context_text": "CISS [88] further considers concept inheritance and structural similarity between these graphs, utilizing class sets composed of concepts at the same level to learn fine-grained concept representations and constructs a virtual entity layer view from clustered entities, subsequently comparing the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for constructing and comparing knowledge graphs.",
      "processing_time": 56.770326137542725,
      "citing_paper_id": "279392166",
      "cited_paper_id": 271961547
    },
    {
      "context_text": "However, real-world KGs often exhibit a long-tail distribution, and dynamic KGs may introduce new entities or relations [276].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general characteristics of real-world knowledge graphs.",
      "processing_time": 12.9137544631958,
      "citing_paper_id": "279392166",
      "cited_paper_id": 273037266
    },
    {
      "context_text": "For symbolic unstructured knowledge, Karpukhin et al. [15] approached the question‚Äì answer task as an open-domain question‚Äìanswer challenge and proposed a dense retrieval model (DPR) [6] based on vector representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DPR) which is not included as per instructions.",
      "processing_time": 56.760599851608276,
      "citing_paper_id": "257576049",
      "cited_paper_id": 7483388
    },
    {
      "context_text": "Wang et al. [6] were pioneers in this Ô¨Åeld, suggesting the use of a knowledge graph (KG) to incorporate factual knowledge into the model as triples, enabling answer prediction using representational learning [7,8] or graph neural networks [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of knowledge graphs and methods for incorporating factual knowledge into models.",
      "processing_time": 57.97679162025452,
      "citing_paper_id": "257576049",
      "cited_paper_id": 7483388
    },
    {
      "context_text": "Wang et al. [6] were pioneers in this Ô¨Åeld, suggesting the use of a knowledge graph (KG) to incorporate factual knowledge into the model as triples, enabling answer prediction using representational learning [7,8] or graph neural networks [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of knowledge graphs and methods for incorporating factual knowledge into models.",
      "processing_time": 57.97679162025452,
      "citing_paper_id": "257576049",
      "cited_paper_id": 52158102
    },
    {
      "context_text": "Wang et al. [6] were pioneers in this Ô¨Åeld, suggesting the use of a knowledge graph (KG) to incorporate factual knowledge into the model as triples, enabling answer prediction using representational learning [7,8] or graph neural networks [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of knowledge graphs and methods for incorporating factual knowledge into models.",
      "processing_time": 57.97679162025452,
      "citing_paper_id": "257576049",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "Wang et al. [6] were pioneers in this Ô¨Åeld, suggesting the use of a knowledge graph (KG) to incorporate factual knowledge into the model as triples, enabling answer prediction using representational learning [7,8] or graph neural networks [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of knowledge graphs and methods for incorporating factual knowledge into models.",
      "processing_time": 57.97679162025452,
      "citing_paper_id": "257576049",
      "cited_paper_id": 218486837
    },
    {
      "context_text": "Wang et al. [6] constructed a factual knowledge repository by utilizing three KGs and retrieved subgraphs by querying visual elements.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions the use of three KGs to construct a factual knowledge repository, but does not specify the names of these KGs. The context focuses on the construction and querying of subgraphs using visual elements.",
      "processing_time": 60.5395770072937,
      "citing_paper_id": "257576049",
      "cited_paper_id": 7483388
    },
    {
      "context_text": "In comparison to visual pre-trained models, such as Fast R-CNN [21], the utilization of multi-modal pre-trained models that encode both images and question text can result in a more comprehensive understanding of non-symbolic knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models. The context focuses on comparing visual pre-trained models to multi-modal pre-trained models.",
      "processing_time": 59.05951476097107,
      "citing_paper_id": "257576049",
      "cited_paper_id": 10328909
    },
    {
      "context_text": "‚Ä¶nodes V s , representing English words, is transformed into a feature vector with dimension d using pre-trained entity embeddings from Feng et al. [8]. s The resulting feature vector of the symbolic node is denoted as ùëø ùíî : s where ùëø ùíî ‚àà ùì° (ùíå ùíó ‚àí2)√óùì≠ and the ùëò ùë£ is the total number of‚Ä¶",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for transforming nodes into feature vectors using pre-trained entity embeddings.",
      "processing_time": 57.957468032836914,
      "citing_paper_id": "257576049",
      "cited_paper_id": 218486837
    },
    {
      "context_text": "On one hand, we initialize symbolic nodes V , representing English words, is transformed into a feature vector with dimension d using pre-trained entity embeddings from Feng et al. [8].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions pre-trained entity embeddings from Feng et al., but does not specify a dataset name. The title suggests the use of knowledge graphs, but no specific dataset is named.",
      "processing_time": 60.395283460617065,
      "citing_paper_id": "257576049",
      "cited_paper_id": 218486837
    },
    {
      "context_text": "Some researchers [27,28] argued that scene graphs are insufÔ¨Åcient for reasoning tasks and proposed a mutual attention mechanism that incorporates dense image captions in addition to the scene graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for improving reasoning tasks using scene graphs and dense image captions.",
      "processing_time": 58.548152446746826,
      "citing_paper_id": "257576049",
      "cited_paper_id": 221397171
    },
    {
      "context_text": "The QA-GNN model [22] advocated for the integration of structural knowledge from knowledge graphs into semantic graphs for multiple-choice QA tasks by extracting entity concepts and intermediate nodes and edges and encoding the text of the question and answers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (QA-GNN) and its application. The context focuses on the integration of knowledge graphs and language models for question answering.",
      "processing_time": 60.52180480957031,
      "citing_paper_id": "257576049",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "The QA-GNN model [22] goes a step further by using a language model to encode each node with the question, which then serves as a noise-Ô¨Åltering rate indicator for the model to determine the most important information for aggregation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (QA-GNN) and its methodology. The context focuses on the model's functionality rather than a dataset.",
      "processing_time": 59.99965143203735,
      "citing_paper_id": "257576049",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "Drawing in-spiration from QA-GNN [22], we introduce new nodes that are connected to all nodes of the aforementioned three types of semantic graphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (QA-GNN) and a general reference to semantic graphs. No verifiable datasets are identified.",
      "processing_time": 59.82793474197388,
      "citing_paper_id": "257576049",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "Semantic graphs, also known as semantic networks, are complex, directed graphs that associate concept nodes through edges that represent semantic relationships [12,22‚Äì25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about semantic graphs. No verifiable resources are identified.",
      "processing_time": 58.32720947265625,
      "citing_paper_id": "257576049",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "However, Zheng et al. [13] argued that the method of retrieving answers by querying the knowledge base is susceptible to the precision of the query codes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the limitations of a method for retrieving answers from a knowledge base.",
      "processing_time": 59.81354880332947,
      "citing_paper_id": "257576049",
      "cited_paper_id": 237418303
    },
    {
      "context_text": "Additionally, we utilize the external knowledge collection constructed by Vis-DPR [19] for the OK-VQA dataset as the textual knowledge base, designated as P .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OK-VQA"
      ],
      "dataset_descriptions": {
        "OK-VQA": "Used as a textual knowledge base for knowledge-based question answering, specifically to provide external knowledge for the Vis-DPR model."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'OK-VQA dataset' as a textual knowledge base used in the research. It is a specific, verifiable dataset used for knowledge-based question answering.",
      "processing_time": 65.83331727981567,
      "citing_paper_id": "257576049",
      "cited_paper_id": 237453242
    },
    {
      "context_text": "Luo et al. [19] developed Vis-DPR, a framework for retrieving knowledge text, building on the work of Qu et al. [18] in question answering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only frameworks and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.86964464187622,
      "citing_paper_id": "257576049",
      "cited_paper_id": 237453242
    },
    {
      "context_text": "The VQA-GNN model [23] extended this approach to the VQA domain by incorporating image concepts and multi-modal data, although it only considers knowledge graphs that incorporate image information and does not take into account knowledge texts with richer semantic information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the VQA-GNN model but does not reference any specific datasets. It discusses the model's approach to incorporating image concepts and multi-modal data, which is relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 20.405311584472656,
      "citing_paper_id": "257576049",
      "cited_paper_id": 263895473
    },
    {
      "context_text": "In this regard, Wang et al. [20] introduced the concept of joint embedding, which incorporates two types of embedding vectors into the model for the purpose of incorporating external knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.320250511169434,
      "citing_paper_id": "257576049",
      "cited_paper_id": null
    },
    {
      "context_text": "First, the image data is processed using the VinVl object detection algorithm [31], which is an improved version of the traditional target detection method and is capable of detecting a greater number of visual concepts in the image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the VinVl object detection algorithm but does not refer to any specific dataset. The context focuses on the algorithm's capabilities rather than a dataset.",
      "processing_time": 59.19088840484619,
      "citing_paper_id": "257576049",
      "cited_paper_id": null
    },
    {
      "context_text": "According to the different scopes, there are several subsets of it, e.g., Location [208], sports [208], NELL23k [210], NELL-995 [211].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Location",
        "sports",
        "NELL23k",
        "NELL-995"
      ],
      "dataset_descriptions": {
        "Location": "Used to evaluate knowledge graph embedding models, focusing on location-related entities and relations.",
        "sports": "Used to evaluate knowledge graph embedding models, focusing on sports-related entities and relations.",
        "NELL23k": "Used to train and evaluate knowledge graph embedding models, containing a large subset of NELL knowledge base.",
        "NELL-995": "Used to evaluate knowledge graph embedding models, focusing on a smaller, curated subset of NELL knowledge base."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'Location', 'sports', 'NELL23k', and 'NELL-995' as subsets of a larger dataset. These are specific, identifiable datasets used in the context of knowledge graph embedding.",
      "processing_time": 82.05355501174927,
      "citing_paper_id": "254564635",
      "cited_paper_id": 205692
    },
    {
      "context_text": "ProPPR [108] further introduces space similarity heuristics by incorporating textual content to alleviate the feature sparsity issue in PRA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ProPPR) and its enhancements. The context focuses on the methodological improvements rather than the use of a particular dataset.",
      "processing_time": 61.018895387649536,
      "citing_paper_id": "254564635",
      "cited_paper_id": 577805
    },
    {
      "context_text": "2019 MuRP [26] Transductive Translational 2014 ProPPR [109] Transductive Relation Path",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on describing different models and their inference mechanisms.",
      "processing_time": 59.35156440734863,
      "citing_paper_id": "254564635",
      "cited_paper_id": 577805
    },
    {
      "context_text": "ProPPR [109] further introduces space similarity heuristics by incorporating textual content to alleviate the feature sparsity issue in PRA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ProPPR) and its enhancements. The context focuses on the methodological improvements rather than the use of a particular dataset.",
      "processing_time": 61.01779222488403,
      "citing_paper_id": "254564635",
      "cited_paper_id": 577805
    },
    {
      "context_text": "To simplify it, DistMult [35] uses bi-linear diagonal matrices to reduce parameters to O ( d ) per relation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (DistMult).",
      "processing_time": 56.64868521690369,
      "citing_paper_id": "254564635",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "For example, [246] and [247] both perform reasoning on the KG constructed from the electronic medical database.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "electronic medical database"
      ],
      "dataset_descriptions": {
        "electronic medical database": "Used to construct a health knowledge graph, focusing on reasoning tasks using patient data from electronic medical records."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions an electronic medical database used for constructing a knowledge graph, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 63.89407300949097,
      "citing_paper_id": "254564635",
      "cited_paper_id": 3226443
    },
    {
      "context_text": "2019 QuatE [43] Transductive Tensor Decompositional 2013 AMIE [110] Inductive Logic Rule",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on describing different approaches to knowledge graph reasoning.",
      "processing_time": 59.61955451965332,
      "citing_paper_id": "254564635",
      "cited_paper_id": 4090850
    },
    {
      "context_text": "Logical rules can be extracted from KG for reasoning by rule mining tools, e.g., AMIE [109], RuleN [98], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AMIE as a tool for rule mining in knowledge graphs, but does not refer to it as a dataset. It is used to illustrate a method or tool for extracting logical rules.",
      "processing_time": 61.5631058216095,
      "citing_paper_id": "254564635",
      "cited_paper_id": 4090850
    },
    {
      "context_text": "ConvE [51] Ô¨Årst leverages 2D convolutional layers for knowledge graph reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ConvE) for knowledge graph reasoning.",
      "processing_time": 58.98115420341492,
      "citing_paper_id": "254564635",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "‚Ä¢ Hetionet [207] is a knowledge graph derived from biomedical studies based on public resources.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hetionet"
      ],
      "dataset_descriptions": {
        "Hetionet": "Used as a knowledge graph derived from public biomedical resources to prioritize drugs for repurposing, integrating diverse types of biomedical data."
      },
      "confidence_score": 0.9,
      "reasoning": "Hetionet is mentioned as a knowledge graph derived from public resources, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 65.00472402572632,
      "citing_paper_id": "254564635",
      "cited_paper_id": 4437414
    },
    {
      "context_text": "Then, a uniÔ¨Åed variational inference framework is proposed by DIVA [97], which separates multi-hop reasoning into two steps, i.e., path-Ô¨Ånding and path-reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DIVA) and a general approach to multi-hop reasoning in knowledge graphs.",
      "processing_time": 60.096213817596436,
      "citing_paper_id": "254564635",
      "cited_paper_id": 4669223
    },
    {
      "context_text": "RGCN [57] uses the relation-speciÔ¨Åc transformation to aggregate neighborhood information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (RGCN) and its functionality. No verifiable resource is identified.",
      "processing_time": 60.083627700805664,
      "citing_paper_id": "254564635",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "It builds the reward function based on the embeddings generated by the RGCN [57], which makes those path-based models more explainable.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RGCN) used for generating embeddings. The context focuses on the explainability of path-based models using RGCN embeddings.",
      "processing_time": 61.545812368392944,
      "citing_paper_id": "254564635",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "‚Ä¢ WordNet [231] is a lexical database of semantic relations, such as synonyms, hyponyms, and meronyms, between words.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNet"
      ],
      "dataset_descriptions": {
        "WordNet": "Used as a lexical database to explore semantic relations between words, specifically synonyms, hyponyms, and meronyms, enhancing the understanding of lexical semantics."
      },
      "confidence_score": 0.6,
      "reasoning": "WordNet is a well-known lexical database, but it is not a dataset in the context of multi-modal knowledge graph reasoning. It is a lexical resource.",
      "processing_time": 66.7302086353302,
      "citing_paper_id": "254564635",
      "cited_paper_id": 5958691
    },
    {
      "context_text": "Mean-while, HolE [37] models the holographic reduced representations and circular correlation, and Analogy [38] designs the bi-linear scoring function with analogical structural constraints for analogical reasoning, which both try to capture rich interactions between entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 60.08237028121948,
      "citing_paper_id": "254564635",
      "cited_paper_id": 6071257
    },
    {
      "context_text": "‚Ä¢ Nation [200] contains relations among nations [209].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Nation"
      ],
      "dataset_descriptions": {
        "Nation": "Used to represent relations among nations, specifically for reasoning about multi-modal knowledge graphs and their interconnected entities."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Nation' as a resource containing relations among nations, which fits the criteria for a dataset. However, the name is not highly specific.",
      "processing_time": 65.20330381393433,
      "citing_paper_id": "254564635",
      "cited_paper_id": 6911541
    },
    {
      "context_text": "‚Ä¢ FAMILY [200] consists of relations among family members.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FAMILY"
      ],
      "dataset_descriptions": {
        "FAMILY": "Used to represent and reason about relations among family members, focusing on multi-modal knowledge graph construction and inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'FAMILY' as a dataset consisting of relations among family members, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 64.98147916793823,
      "citing_paper_id": "254564635",
      "cited_paper_id": 6911541
    },
    {
      "context_text": "‚Ä¢ DBpedia [226] consists of structured content from the information created in various Wikimedia projects.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia"
      ],
      "dataset_descriptions": {
        "DBpedia": "Used as a structured content source from Wikimedia projects, providing a nucleus for a web of open data, enhancing multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "DBpedia is mentioned as a structured content source from Wikimedia projects, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 65.1962194442749,
      "citing_paper_id": "254564635",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "1) Traditional NN Model : SME [47] Ô¨Årst encodes the entities and relations into the latent space using neural networks.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SME) for encoding entities and relations.",
      "processing_time": 59.29749584197998,
      "citing_paper_id": "254564635",
      "cited_paper_id": 9095914
    },
    {
      "context_text": "For instance, the Path-Ranking Algorithm (PRA) [110] derives the path-based logic rules under path constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Path-Ranking Algorithm (PRA) but does not refer to any specific dataset. The context is about a method or algorithm, not a dataset.",
      "processing_time": 61.35910129547119,
      "citing_paper_id": "254564635",
      "cited_paper_id": 10811631
    },
    {
      "context_text": "According to the entity set size, several subsets generated from it, including FB13 [201], FB122 [202], FB15k [203], FB20k [198], FB24k [204], FB5M [18], FB15k-237 [205], FB60k-NYT10 [206].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB13",
        "FB122",
        "FB15k",
        "FB20k",
        "FB24k",
        "FB5M",
        "FB15k-237",
        "FB60k-NYT10"
      ],
      "dataset_descriptions": {
        "FB13": "Used to train and evaluate knowledge graph completion models, focusing on a smaller subset of Freebase with 13 relations.",
        "FB122": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 122 relations.",
        "FB15k": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 15k entities and 1,345 relations.",
        "FB20k": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 20k entities.",
        "FB24k": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 24k entities.",
        "FB5M": "Used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase with 5 million entities.",
        "FB15k-237": "Used to train and evaluate knowledge graph completion models, focusing on a filtered subset of FB15k with 237 relations to reduce redundancy.",
        "FB60k-NYT10": "Used to train and evaluate knowledge graph completion models, combining a subset of Freebase with New York Times articles."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several subsets of Freebase, which are commonly used datasets in knowledge graph research. These datasets are used for training and evaluating knowledge graph completion models.",
      "processing_time": 116.84658408164978,
      "citing_paper_id": "254564635",
      "cited_paper_id": 11223539
    },
    {
      "context_text": "According to the entity set size, several subsets generated from it, including FB13 [201], FB122 [202], FB15k [203], FB20k [198], FB24k [204], FB5M [18], FB15k-237 [205], FB60k-NYT10 [206].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB13",
        "FB122",
        "FB15k",
        "FB20k",
        "FB24k",
        "FB5M",
        "FB15k-237",
        "FB60k-NYT10"
      ],
      "dataset_descriptions": {
        "FB13": "Used to train and evaluate knowledge graph completion models, focusing on a smaller subset of Freebase with 13 relations.",
        "FB122": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 122 relations.",
        "FB15k": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 15k entities and 1,345 relations.",
        "FB20k": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 20k entities.",
        "FB24k": "Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 24k entities.",
        "FB5M": "Used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase with 5 million entities.",
        "FB15k-237": "Used to train and evaluate knowledge graph completion models, focusing on a filtered subset of FB15k with 237 relations to reduce redundancy.",
        "FB60k-NYT10": "Used to train and evaluate knowledge graph completion models, combining a subset of Freebase with New York Times articles."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several subsets of Freebase, which are commonly used datasets in knowledge graph research. These datasets are used for training and evaluating knowledge graph completion models.",
      "processing_time": 116.84658408164978,
      "citing_paper_id": "254564635",
      "cited_paper_id": 19139252
    },
    {
      "context_text": "‚Ä¢ Countries [194] consists of relations among countries based on public geographical data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Countries"
      ],
      "dataset_descriptions": {
        "Countries": "Used to represent and reason about geographical relations among countries, focusing on low-rank vector spaces for approximate reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Countries' as a dataset consisting of relations among countries based on public geographical data, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 65.56980514526367,
      "citing_paper_id": "254564635",
      "cited_paper_id": 13955854
    },
    {
      "context_text": "According to the entity set size, we can derive several subsets from it, i.e., DBpedia50 [198], DBpedia500 [198] and DB100K [199].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia50",
        "DBpedia500",
        "DB100K"
      ],
      "dataset_descriptions": {
        "DBpedia50": "Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets for efficient testing and validation.",
        "DBpedia500": "Used to assess the scalability and performance of knowledge graph embedding models on medium-sized entity sets.",
        "DB100K": "Used to test the robustness and generalization capabilities of knowledge graph reasoning algorithms on larger datasets."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific subsets derived from a larger entity set, which are likely datasets used in knowledge graph research.",
      "processing_time": 76.66111159324646,
      "citing_paper_id": "254564635",
      "cited_paper_id": 19139252
    },
    {
      "context_text": "According to the entity set size, we can derive several subsets from it, i.e., DBpedia50 [198], DBpedia500 [198] and DB100K [199].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia50",
        "DBpedia500",
        "DB100K"
      ],
      "dataset_descriptions": {
        "DBpedia50": "Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets for efficient testing and validation.",
        "DBpedia500": "Used to assess the scalability and performance of knowledge graph embedding models on medium-sized entity sets.",
        "DB100K": "Used to test the robustness and generalization capabilities of knowledge graph reasoning algorithms on larger datasets."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific subsets derived from a larger entity set, which are likely datasets used in knowledge graph research.",
      "processing_time": 76.66111159324646,
      "citing_paper_id": "254564635",
      "cited_paper_id": 19187663
    },
    {
      "context_text": "ChronoTranslate [159] learns a universal representation of entities and time-speciÔ¨Åc representations of the Temporal KGs, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called ChronoTranslate. The context focuses on the capabilities of ChronoTranslate rather than the use of a specific dataset.",
      "processing_time": 62.43460655212402,
      "citing_paper_id": "254564635",
      "cited_paper_id": 21883095
    },
    {
      "context_text": "After that, TorusE [24] projects embeddings in a compact Lie group torus, and MuRP [25] designs a M¬®obius matrix-vector multiplication and M¬®obius addition for entity embedding projection, which all show better accuracy and scalability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodologies used for knowledge graph embedding.",
      "processing_time": 60.94635844230652,
      "citing_paper_id": "254564635",
      "cited_paper_id": 25753806
    },
    {
      "context_text": "Besides, inspired by the translation-based static KGR models, MTRL [173] is a translation-based model with three energy functions corresponding to visual, linguistic, and structural information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MTRL as a model, not a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 60.57148098945618,
      "citing_paper_id": "254564635",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "‚Ä¢ FB-IMG-TXT [173] is the KG combined with textual descriptions and images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-IMG-TXT"
      ],
      "dataset_descriptions": {
        "FB-IMG-TXT": "Used to combine textual descriptions and images in a knowledge graph, specifically for multimodal translation-based representation learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'FB-IMG-TXT' as a knowledge graph combined with textual descriptions and images, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 67.54673838615417,
      "citing_paper_id": "254564635",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "[54] leverages the fully connected layer to avoid wrapping and generate relation-specific convolutional filters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating relation-specific convolutional filters in a hypernetwork.",
      "processing_time": 60.03746509552002,
      "citing_paper_id": "254564635",
      "cited_paper_id": 52056218
    },
    {
      "context_text": "Later on, HypER [53] makes use of the fully connected layer and relation-speciÔ¨Åc convolutional Ô¨Ålters for better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HypER) and its components. The context is about the methodology and performance improvements.",
      "processing_time": 61.82084059715271,
      "citing_paper_id": "254564635",
      "cited_paper_id": 52056218
    },
    {
      "context_text": "2019 HypER [54] Transductive CNN 2011 RESCAL [35] Transductive Tensor Decompositional",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 61.07988452911377,
      "citing_paper_id": "254564635",
      "cited_paper_id": 52056218
    },
    {
      "context_text": "Moreover, MKBE [174] and MRCGN [175] integrate different neural encoders and decoders with relational models for embedding learning and multi-modal data for reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'multi-modal data for reasoning' but does not specify any particular dataset. The context is about integrating neural encoders and decoders with relational models, which is methodological rather than dataset-specific.",
      "processing_time": 63.51001310348511,
      "citing_paper_id": "254564635",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "According to the scopes of relations, YAGO3-10 [217], YAGO37 [218] and YAGO39k [219] are the subsets of it.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YAGO3-10",
        "YAGO37",
        "YAGO39k"
      ],
      "dataset_descriptions": {
        "YAGO3-10": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
        "YAGO37": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
        "YAGO39k": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions YAGO3-10, YAGO37, and YAGO39k as subsets of a larger dataset, which are likely knowledge graphs used in multi-modal reasoning.",
      "processing_time": 83.4065682888031,
      "citing_paper_id": "254564635",
      "cited_paper_id": 53080423
    },
    {
      "context_text": "According to the scopes of relations, YAGO3-10 [217], YAGO37 [218] and YAGO39k [219] are the subsets of it.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YAGO3-10",
        "YAGO37",
        "YAGO39k"
      ],
      "dataset_descriptions": {
        "YAGO3-10": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
        "YAGO37": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
        "YAGO39k": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions YAGO3-10, YAGO37, and YAGO39k as subsets of a larger dataset, which are likely knowledge graphs used in multi-modal reasoning.",
      "processing_time": 83.4065682888031,
      "citing_paper_id": "254564635",
      "cited_paper_id": null
    },
    {
      "context_text": "HyTE [157] represents each timestamp as a learnable hyper-plane in the embedding space, then projects entity and relation embeddings into the hyper-plane and utilizes the TransE scoring function on the projections.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (HyTE) for temporal knowledge graph embedding, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 61.95774793624878,
      "citing_paper_id": "254564635",
      "cited_paper_id": 53082197
    },
    {
      "context_text": "YOGA11k/YOGA [157], YOGA15k [222], YOGA-3SP [220] and YOGA1830 [116] are generated from it according to different periods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YOGA11k",
        "YOGA15k",
        "YOGA-3SP",
        "YOGA1830"
      ],
      "dataset_descriptions": {
        "YOGA11k": "Used to generate temporal knowledge graph embeddings, focusing on different periods for multi-modal reasoning.",
        "YOGA15k": "Used to generate temporal knowledge graph embeddings, focusing on different periods for multi-modal reasoning.",
        "YOGA-3SP": "Used to generate temporal knowledge graph embeddings, focusing on different periods for multi-modal reasoning.",
        "YOGA1830": "Used to generate temporal knowledge graph embeddings, focusing on different periods for multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets (YOGA11k, YOGA15k, YOGA-3SP, YOGA1830) that are generated from a common source, likely for multi-modal knowledge graph reasoning.",
      "processing_time": 88.90192675590515,
      "citing_paper_id": "254564635",
      "cited_paper_id": 53082197
    },
    {
      "context_text": "To alleviate the problem, many researchers regard the knowledge graphs (KGs), which store the human knowledge facts in intuitive graph-structure [1], [2], [3], as potential solutions since they can be easily referred to as books to provide the domain-specific knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to knowledge graphs in general terms without naming any particular dataset.",
      "processing_time": 61.666202545166016,
      "citing_paper_id": "254564635",
      "cited_paper_id": 56657926
    },
    {
      "context_text": "To alleviate the problem, many researchers regard the knowledge graphs (KGs), which store the human knowledge facts in intuitive graph structures [1], [2], as potential solutions since they can be easily referred to as books to provide domain-speciÔ¨Åc knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to knowledge graphs in general terms without naming any particular dataset.",
      "processing_time": 61.796221017837524,
      "citing_paper_id": "254564635",
      "cited_paper_id": 56915438
    },
    {
      "context_text": "SimplE [39] enhances the Canonical Polyadic (CP) decompositial for two independent entity embeddings, and Tucker Decomposition is Ô¨Årst used by Tucker [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on describing enhancements to tensor decomposition methods for knowledge graph completion.",
      "processing_time": 62.382105112075806,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "SimplE [40] enhances the Canonical Polyadic (CP) decompositial for two independent entity embeddings, and Tucker Decomposition is first used by Tucker [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing enhancements to tensor factorization methods for knowledge graph completion.",
      "processing_time": 62.562278509140015,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "2019 TuckER [41] Transductive Tensor Decompositional 2013 SME [48] Transductive Traditional NN",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is focused on describing different approaches to knowledge graph completion.",
      "processing_time": 62.08939266204834,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "A factorized bi-linear pooling model is proposed based on Tucker decomposition, termed LowFER [45], which is more efÔ¨Åcient and lightweight.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'LowFER' as a model, not a dataset. No datasets are explicitly mentioned or used in the given citation context.",
      "processing_time": 62.37885618209839,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "A factorized bi-linear pooling model is proposed based on Tucker decomposition, termed LowFER [45], which is more efÔ¨Åcient and lightweight.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'LowFER' as a model, not a dataset. No datasets are explicitly mentioned or used in the given citation context.",
      "processing_time": 62.37885618209839,
      "citing_paper_id": "254564635",
      "cited_paper_id": 221082536
    },
    {
      "context_text": "TuckERTNT [138] proposes a novel tensor decomposition model for Temporal KGs inspired by the Tucker decomposition of a 4-order tensor with the extra time embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (TuckERTNT) and does not reference any specific dataset. The context is about a methodological contribution rather than a dataset.",
      "processing_time": 63.045087814331055,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "In summary, there are 33 interpolation reasoning models, including TTransE, ChronoTranslate, HyTE, TA-DISTMULT, DacKGR, DyERNIE, Diachronic embeddings, ATiSE, TeMP, xERTE, ChronoR, TeLM, T-GAP, TIE, TPmod, DBKGE, Tpath, RTFE, DKGE, TuckERTNT, BoxTE, TempoQR, CEN, TLT-KGE, TRHyTE, ExKGR, RotateQVS, CENET, HiS-Match, FILT, TKGC-AGP, TNTComplEX, TeRo.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span lists various models and methods for interpolation reasoning in knowledge graphs, but does not mention any specific datasets.",
      "processing_time": 61.76934504508972,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "For instance, [249] proposes a case-based reasoning method to assist people in verifying the information to discriminate against fraud in advance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fraud detection using case-based reasoning.",
      "processing_time": 40.546749114990234,
      "citing_paper_id": "254564635",
      "cited_paper_id": 59821525
    },
    {
      "context_text": "Moreover, RotatE [27] proposes a rotation-based translational method with complex-valued embeddings to better infer the symmetry, anti-symmetry, inversion, and composition facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called RotatE. The context focuses on the method's capabilities and does not indicate the use of a particular dataset.",
      "processing_time": 63.7604763507843,
      "citing_paper_id": "254564635",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "‚Ä¢ MMKG [224] provides three subsets, including MMKG-FB15k-IMG [224], MMKG-DB15k [224] and Yago15k-IMG-TXT [224], which integrates the speciÔ¨Åc KGs with numeric literals and images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMKG-FB15k-IMG",
        "MMKG-DB15k",
        "Yago15k-IMG-TXT"
      ],
      "dataset_descriptions": {
        "MMKG-FB15k-IMG": "Used to integrate Freebase knowledge graph with images, enhancing multi-modal reasoning capabilities.",
        "MMKG-DB15k": "Used to integrate DBpedia knowledge graph with numeric literals, supporting multi-modal reasoning with structured data.",
        "Yago15k-IMG-TXT": "Used to integrate YAGO knowledge graph with images and text, facilitating comprehensive multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used in the research, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 81.72422790527344,
      "citing_paper_id": "254564635",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "As a model for both graph learning and KGR, DyRep [153] captures the interleaved dynamics within history, which is further parameterized by a temporal-attentive representation network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DyRep) for dynamic graph representation learning.",
      "processing_time": 62.067299365997314,
      "citing_paper_id": "254564635",
      "cited_paper_id": 108296188
    },
    {
      "context_text": "Then, a more scalable rule mining approach via the techniques of rule searching and pruning is designed by RLvLR [91].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the technique of rule mining using reinforcement learning.",
      "processing_time": 62.89778661727905,
      "citing_paper_id": "254564635",
      "cited_paper_id": 125343003
    },
    {
      "context_text": "‚Ä¢ YAGO [233], [234], as a lightweight and extensible ontology, is built from Wikidata and uniÔ¨Åed with WordNet .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YAGO"
      ],
      "dataset_descriptions": {
        "YAGO": "Used as a lightweight and extensible ontology built from Wikidata and unified with WordNet, focusing on semantic knowledge integration."
      },
      "confidence_score": 1.0,
      "reasoning": "YAGO is mentioned as a lightweight and extensible ontology built from Wikidata and unified with WordNet. It is a specific, verifiable resource.",
      "processing_time": 68.97046399116516,
      "citing_paper_id": "254564635",
      "cited_paper_id": 207163173
    },
    {
      "context_text": "‚Ä¢ YAGO [233], [234] for temporal KGR contains extra time information.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YAGO"
      ],
      "dataset_descriptions": {
        "YAGO": "Used to enhance temporal Knowledge Graph Reasoning by incorporating extra time information, specifically focusing on the integration of temporal data into the graph structure."
      },
      "confidence_score": 1.0,
      "reasoning": "YAGO is mentioned as a dataset containing extra time information for temporal Knowledge Graph Reasoning. It is a specific, verifiable resource with clear provenance.",
      "processing_time": 69.97753858566284,
      "citing_paper_id": "254564635",
      "cited_paper_id": 207163173
    },
    {
      "context_text": "Similarly, COMPGCN [66] also jointly learns the representations with various entity-relation composition operations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to a method (COMPGCN) and its operations.",
      "processing_time": 62.34213376045227,
      "citing_paper_id": "254564635",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "Besides, HAKE [28] models the semantic hierarchy rather than relation patterns based on the polar coordinate space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HAKE) for modeling semantic hierarchies in knowledge graphs.",
      "processing_time": 61.89805054664612,
      "citing_paper_id": "254564635",
      "cited_paper_id": 208201975
    },
    {
      "context_text": "2020 ExpressGNN [107] Inductive Logic Rule 2015 KG2E [22] Transductive Translational",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 62.75931763648987,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211003696
    },
    {
      "context_text": "Based on it, ExpressGNN [106] further generalizes it by Ô¨Ånetuning GNN models for more efÔ¨Åcient reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ExpressGNN) and its application to efficient reasoning with GNN models.",
      "processing_time": 63.160144329071045,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211003696
    },
    {
      "context_text": "Based on it, ExpressGNN [107] further generalizes it by finetuning GNN models for more efficient reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ExpressGNN) and its application for efficient reasoning using GNN models.",
      "processing_time": 63.27430200576782,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211003696
    },
    {
      "context_text": "To make better use of such symbolic features, Rule-based models are proposed [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to rule-based models in the context of knowledge graphs.",
      "processing_time": 62.999858140945435,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "After that, how to inject logical rules into embeddings for better reasoning performance has drawn increasing research attention [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in research.",
      "processing_time": 62.18710541725159,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "These KGR models are proven to signiÔ¨Åcantly beneÔ¨Åt the usage of KGs in many applications, e.g., question answering [3], recommendation system [4], information extraction [5], image classiÔ¨Åcation [6], etc.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 63.42332696914673,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "These KGR models are proven to signiÔ¨Åcantly beneÔ¨Åt the usage of KGs in many applications, e.g., question answering [3], recommendation system [4], information extraction [5], image classiÔ¨Åcation [6], etc.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 63.42332696914673,
      "citing_paper_id": "254564635",
      "cited_paper_id": 231925372
    },
    {
      "context_text": "Meanwhile, MINERVA [100] takes path Ô¨Ånding between entities as a sequential optimization problem by maximizing the expected reward [3], which excludes the target answer entity for more capable reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MINERVA) and a general approach to knowledge graph reasoning. No verifiable datasets are referenced.",
      "processing_time": 19.787135124206543,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "‚Ä¢ Richpedia [225] is the KGs, composed of the triplets, textual descriptions, and images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Richpedia"
      ],
      "dataset_descriptions": {
        "Richpedia": "Used as a comprehensive multi-modal knowledge graph, integrating triplets, textual descriptions, and images to support reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "Richpedia is identified as a multi-modal knowledge graph, which aligns with the research topic of multi-modal knowledge graph reasoning. It is used as a reusable resource.",
      "processing_time": 70.37491488456726,
      "citing_paper_id": "254564635",
      "cited_paper_id": 211123242
    },
    {
      "context_text": "‚Ä¢ MKG [181], [241] consists of two subsets, i.e., MKG-Wikipedia and MKG-YAGO .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-Wikipedia",
        "MKG-YAGO"
      ],
      "dataset_descriptions": {
        "MKG-Wikipedia": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using Wikipedia data.",
        "MKG-YAGO": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using YAGO data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MKG-Wikipedia' and 'MKG-YAGO' as subsets of MKG, which are specific datasets used in multimodal knowledge graph reasoning.",
      "processing_time": 77.22455739974976,
      "citing_paper_id": "254564635",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "‚Ä¢ MKG [181], [241] consists of two subsets, i.e., MKG-Wikipedia and MKG-YAGO .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-Wikipedia",
        "MKG-YAGO"
      ],
      "dataset_descriptions": {
        "MKG-Wikipedia": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using Wikipedia data.",
        "MKG-YAGO": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using YAGO data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MKG-Wikipedia' and 'MKG-YAGO' as subsets of MKG, which are specific datasets used in multimodal knowledge graph reasoning.",
      "processing_time": 77.22455739974976,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "Later on, DPMPN [63] and RGHAT [64] designs a two-GNN framework to simultaneously encode information in different levels separately, i.e., global & local information for DPMPN and relation & entity information for RGHAT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the design of two-GNN frameworks.",
      "processing_time": 64.12559628486633,
      "citing_paper_id": "254564635",
      "cited_paper_id": 214198198
    },
    {
      "context_text": "GEN [67] and HRFN [68] learn entity embeddings based on meta-learning for both seen-to-unseen and unseen-to-unseen facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'entity embeddings' and 'meta-learning', which are methods rather than datasets. No specific datasets are named or described in the citation context.",
      "processing_time": 64.65294861793518,
      "citing_paper_id": "254564635",
      "cited_paper_id": 219573823
    },
    {
      "context_text": "GEN [67] and HRFN [68] learn entity embeddings based on meta-learning for both seen-to-unseen and unseen-to-unseen facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'entity embeddings' and 'meta-learning', which are methods rather than datasets. No specific datasets are named or described in the citation context.",
      "processing_time": 64.65294861793518,
      "citing_paper_id": "254564635",
      "cited_paper_id": 240230810
    },
    {
      "context_text": "A factorized bi-linear pooling model is proposed based on Tucker decomposition, termed LowFER [46], which is more efficient and lightweight.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called LowFER. The context focuses on the model and its efficiency, not on any particular dataset.",
      "processing_time": 64.81875133514404,
      "citing_paper_id": "254564635",
      "cited_paper_id": 221082536
    },
    {
      "context_text": "2020 LowFER [46] Transductive Tensor Decompositional 2016 ComplEx [37] Transductive Tensor Decompositional",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on describing the methods used for link prediction in knowledge graphs.",
      "processing_time": 64.64659452438354,
      "citing_paper_id": "254564635",
      "cited_paper_id": 221082536
    },
    {
      "context_text": "Meanwhile, ConEx [56] integrates the afÔ¨Åne transformation and a Hermitian inner product on complex-valued embeddings with the convolutional operation, which shows good expressiveness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called ConEx. The context focuses on the technical aspects of the method rather than the use of a dataset.",
      "processing_time": 65.4105954170227,
      "citing_paper_id": "254564635",
      "cited_paper_id": 221083274
    },
    {
      "context_text": "Inspired by RotatE, to evaluate the given fact‚Äôs semantic scores, TeRo [145] introduces a novel temporal guided rotation operation between head and tail entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TeRo, which is a method, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 64.23350930213928,
      "citing_paper_id": "254564635",
      "cited_paper_id": 222124934
    },
    {
      "context_text": "More recently, TeMP [149] is proposed, which leverages message-passing graph neural networks (MPNNs) to learn structure-based entity representations at each timestamp, and then combines representations from all timestamps using an encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TeMP, which is a method for temporal knowledge graph completion, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 64.35474991798401,
      "citing_paper_id": "254564635",
      "cited_paper_id": 222177069
    },
    {
      "context_text": "Besides, to handle the complex relational facts with a uniÔ¨Åed model, PairRE [30] models each relation representation with paired vectors to adaptive adjustment for complex relations, and HousE [31] involves a novel parameterization based on the designed Householder transformations for rotation and‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 64.79111289978027,
      "citing_paper_id": "254564635",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "Among them, TACT [70] and CoMPILE [71] both raise the importance of relation embeddings in the task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models (TACT and CoMPILE). The context focuses on the importance of relation embeddings in the task, which is not a dataset.",
      "processing_time": 66.90623307228088,
      "citing_paper_id": "254564635",
      "cited_paper_id": 229213013
    },
    {
      "context_text": "Among them, TACT [70] and CoMPILE [71] both raise the importance of relation embeddings in the task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models (TACT and CoMPILE). The context focuses on the importance of relation embeddings in the task, which is not a dataset.",
      "processing_time": 66.90623307228088,
      "citing_paper_id": "254564635",
      "cited_paper_id": 232135385
    },
    {
      "context_text": "Some recent attempts provide potential solutions for inferring unseen entities, which are known as inductive reasoning models, such as [7], [70], [71], [73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to inductive reasoning models. No verifiable resources are identified.",
      "processing_time": 64.78473591804504,
      "citing_paper_id": "254564635",
      "cited_paper_id": 229213013
    },
    {
      "context_text": "Some recent attempts provide potential solutions for inferring unseen entities, which are known as inductive reasoning models, such as [7], [70], [71], [73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to inductive reasoning models. No verifiable resources are identified.",
      "processing_time": 64.78473591804504,
      "citing_paper_id": "254564635",
      "cited_paper_id": 232135385
    },
    {
      "context_text": "Concretely, TACT [70] uses topology-aware correlations between relations to generate representations for triplet scoring, which also inspires RMPI [75] and TEMP [76].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TACT, RMPI, and TEMP, which are methods or models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 66.07969999313354,
      "citing_paper_id": "254564635",
      "cited_paper_id": 232135385
    },
    {
      "context_text": "Concretely, TACT [70] uses topology-aware correlations between relations to generate representations for triplet scoring, which also inspires RMPI [75] and TEMP [76].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TACT, RMPI, and TEMP, which are methods or models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 66.07969999313354,
      "citing_paper_id": "254564635",
      "cited_paper_id": 235262529
    },
    {
      "context_text": "ChronoR [124] builds on the basis of RotatE, which connects relation and time embeddings to obtain the overall rotation embedding applied to the Ô¨Ånal entity embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ChronoR) that builds on another method (RotatE).",
      "processing_time": 29.47800636291504,
      "citing_paper_id": "254564635",
      "cited_paper_id": 232269660
    },
    {
      "context_text": "RE-GCN [8] focuses on the evolutional dynamics in temporal KGs and generates entity embeddings by modeling the KG sequence of a Ô¨Åxed length at the latest a few timestamps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RE-GCN) and its application to temporal knowledge graphs.",
      "processing_time": 65.00952887535095,
      "citing_paper_id": "254564635",
      "cited_paper_id": 233324265
    },
    {
      "context_text": "TPmod [152] aggregates the attributes of entities and relations and learns dynamic weights to different events.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TPmod) for aggregating attributes and learning dynamic weights in temporal knowledge graphs.",
      "processing_time": 65.37845778465271,
      "citing_paper_id": "254564635",
      "cited_paper_id": 233486383
    },
    {
      "context_text": "Inspired by it, DualE [43] projects the embeddings in dual quaternion space to achieve a uniÔ¨Åed framework for both translation and rotation operations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DualE) and its application in knowledge graph embeddings.",
      "processing_time": 64.7770426273346,
      "citing_paper_id": "254564635",
      "cited_paper_id": 235306387
    },
    {
      "context_text": "For instance, [11] Ô¨Årst categorizes the KGR as symbolic reasoning and statistical reasoning, while [12] further summarizes the models into three types, i.e., symbolic, neural, and hybrid.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of reasoning and model types.",
      "processing_time": 63.91640496253967,
      "citing_paper_id": "254564635",
      "cited_paper_id": 235845598
    },
    {
      "context_text": "Besides, Meta-iKG [72] veriÔ¨Åes the effectiveness of meta-learning in the KGR task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (Meta-iKG) and its application to the KGR task.",
      "processing_time": 65.73203873634338,
      "citing_paper_id": "254564635",
      "cited_paper_id": 236772282
    },
    {
      "context_text": "After that, motivated by the great success of contrastive mechanisms [77], [78], [79], [80], contrastive learning models have been increasingly proposed, e.g., RPC-IR [74], SNRI [73] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on contrastive learning models and their application in relation reasoning.",
      "processing_time": 65.90043354034424,
      "citing_paper_id": "254564635",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "2021 RPC-IR [74] Inductive GNN 2016 TranSparse [24] Transductive Translational",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is too limited to infer the use of any datasets.",
      "processing_time": 65.8973593711853,
      "citing_paper_id": "254564635",
      "cited_paper_id": 239016536
    },
    {
      "context_text": "INDIGO [69] is then proposed based on a GNN using pair-wise encoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (INDIGO) based on a GNN using pair-wise encoding.",
      "processing_time": 65.72245359420776,
      "citing_paper_id": "254564635",
      "cited_paper_id": 245119728
    },
    {
      "context_text": "For example, CURL [93] Ô¨Årst separates the KGs into different clusters according to the entity semantics and then Ô¨Åne-grains the path-Ô¨Ånding procedure into two-level, i.e., the intra-cluster level and the inter-cluster level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CURL) for knowledge graph reasoning.",
      "processing_time": 65.13113784790039,
      "citing_paper_id": "254564635",
      "cited_paper_id": 245502155
    },
    {
      "context_text": "Moreover, two agents in different levels, i.e., DWARF AGENT at the entity level and GIANT AGENT at the cluster level, are proposed in CURL [93], which collaborate to achieve optimal reasoning performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CURL) and its components (DWARF AGENT, GIANT AGENT).",
      "processing_time": 66.33180713653564,
      "citing_paper_id": "254564635",
      "cited_paper_id": 245502155
    },
    {
      "context_text": "Nowadays, there are also some interesting attempts for translational models for more sufÔ¨Åcient interactions, such as TripleRE [32] and InterHT [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on translational models for interactions, which are not datasets.",
      "processing_time": 66.610830783844,
      "citing_paper_id": "254564635",
      "cited_paper_id": 245591477
    },
    {
      "context_text": "EvoKG [150] introduces the RNN model to mine the dynamic evolving structural information and models entity interactions by combining the neighborhood information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions EvoKG, which is a method for modeling temporal knowledge graphs, but does not refer to a specific dataset.",
      "processing_time": 66.16708755493164,
      "citing_paper_id": "254564635",
      "cited_paper_id": 246828738
    },
    {
      "context_text": "‚Ä¶handle the complex relational facts with a uniÔ¨Åed model, PairRE [30] models each relation representation with paired vectors to adaptive adjustment for complex relations, and HousE [31] involves a novel parameterization based on the designed Householder transformations for rotation and projection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 66.6036729812622,
      "citing_paper_id": "254564635",
      "cited_paper_id": 246867470
    },
    {
      "context_text": "A length-aware CNN is leveraged in CEN [132] to handle historical facts via an easy-to-difÔ¨Åcult curriculum learning strategy over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (length-aware CNN) and a strategy (curriculum learning).",
      "processing_time": 66.31275248527527,
      "citing_paper_id": "254564635",
      "cited_paper_id": 247451243
    },
    {
      "context_text": "ExKGR [146] introduces LSTM for entity reasoning in temporal KGs and provides the reasoning paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ExKGR' but does not refer to it as a dataset. It is described as a method or system that introduces LSTM for entity reasoning in temporal KGs.",
      "processing_time": 68.19382166862488,
      "citing_paper_id": "254564635",
      "cited_paper_id": 248435922
    },
    {
      "context_text": "For example, MM-RNS [181] and CKGC [182] leverage contrastive learning strategies, and a hybrid transformer with multi-level fusion is designed in MKGformer [183] designs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and models (MM-RNS, CKGC, MKGformer) but does not explicitly reference any datasets. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 36.85037136077881,
      "citing_paper_id": "254564635",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "For example, MM-RNS [181] and CKGC [182] leverage contrastive learning strategies, and a hybrid transformer with multi-level fusion is designed in MKGformer [183] designs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and models (MM-RNS, CKGC, MKGformer) but does not explicitly reference any datasets. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 36.85037136077881,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "Compared to it, FB15K-237-IMG [183] changes the scope of triplets to FB15k-237 [205].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-237-IMG",
        "FB15k-237"
      ],
      "dataset_descriptions": {
        "FB15K-237-IMG": "Used to extend the scope of triplets in multimodal knowledge graph completion, integrating image data with textual information.",
        "FB15k-237": "Serves as a baseline dataset for evaluating knowledge graph completion methods, focusing on relational triples without multimodal data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, FB15K-237-IMG and FB15k-237, which are specific and relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 34.13381862640381,
      "citing_paper_id": "254564635",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Compared to it, WN18-IMG [183] changes the scope of triplets to the whole WN18 [215].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN18-IMG",
        "WN18"
      ],
      "dataset_descriptions": {
        "WN18-IMG": "Used to extend the scope of triplets in multimodal knowledge graph completion, focusing on integrating image data with textual information.",
        "WN18": "Served as the baseline dataset for triplets in knowledge graph completion, providing a textual-only dataset for comparison."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions WN18-IMG and WN18, which are specific datasets used in the research. The citation indicates a change in the scope of triplets from WN18 to WN18-IMG.",
      "processing_time": 32.63668990135193,
      "citing_paper_id": "254564635",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Moreover, A ‚àó Star [242] Net further optimizes the aggregation procedure with the greedy algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (A* Star Net) and an optimization technique (greedy algorithm).",
      "processing_time": 15.119783163070679,
      "citing_paper_id": "254564635",
      "cited_paper_id": 249210042
    },
    {
      "context_text": "After that, rGalT [123] Ô¨Årst designs the attention mechanism in both intra-graph and inter-graph levels to leverage the historical semantics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (rGalT) and its design aspects. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 20.442706823349,
      "citing_paper_id": "254564635",
      "cited_paper_id": 250630286
    },
    {
      "context_text": "TiRGN [142] uses two encoders to mine the information at both local and global levels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TiRGN) and its components. The context focuses on the method's functionality rather than a dataset.",
      "processing_time": 28.604127407073975,
      "citing_paper_id": "254564635",
      "cited_paper_id": 250631397
    },
    {
      "context_text": "MarT [181] proposes a model-agnostic reasoning framework with a transformer for analogical reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and a reasoning framework. The context is focused on the method rather than a reusable dataset.",
      "processing_time": 19.283429861068726,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252683295
    },
    {
      "context_text": "Similarly to it, DA-Net [117] also tries to learn attention weights on repetitive facts at different historical timestamps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DA-Net). The context focuses on the method's approach to learning attention weights on repetitive facts over time.",
      "processing_time": 18.218984127044678,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252904804
    },
    {
      "context_text": "TLT-KGE [130] captures semantic and time information as different axes of complex space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TLT-KGE) for capturing semantic and time information in temporal knowledge graphs.",
      "processing_time": 16.589234590530396,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252905085
    },
    {
      "context_text": "‚Ä¢ GDELT [130] is a dense KG derived from the Global Database of Events, Language, and Tone.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GDELT"
      ],
      "dataset_descriptions": {
        "GDELT": "Used as a dense knowledge graph derived from global events, language, and tone data, focusing on temporal knowledge graph completion and embedding."
      },
      "confidence_score": 1.0,
      "reasoning": "GDELT is identified as a dataset derived from the Global Database of Events, Language, and Tone, which is a dense KG. It is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 25.005388259887695,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252905085
    },
    {
      "context_text": "Moreover, Knowledge-CLIP [190] injects semantic information into the widely used CLIP [191] model for a better pre-training model considering the semantic connections between concepts from different modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Knowledge-CLIP, which integrates semantic information into CLIP using knowledge graphs, but does not specify a dataset. The context focuses on the method rather than a specific dataset.",
      "processing_time": 16.97851538658142,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252917745
    },
    {
      "context_text": "Moreover, Knowledge-CLIP [189] leverages the CLIP [190] model for a better pre-trained model considering the semantic connections between multi-modal concepts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP) and a concept (Knowledge-CLIP).",
      "processing_time": 26.72822618484497,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252917745
    },
    {
      "context_text": "Besides, the idea of graph clustering [243], [244], [245] is also used for it.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of graph clustering. No verifiable resources are identified.",
      "processing_time": 16.947561502456665,
      "citing_paper_id": "254564635",
      "cited_paper_id": 252992488
    },
    {
      "context_text": "Besides, the idea of graph clustering [243], [244], [245] is also used for it.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of graph clustering. No verifiable resources are identified.",
      "processing_time": 16.947561502456665,
      "citing_paper_id": "254564635",
      "cited_paper_id": 263886314
    },
    {
      "context_text": "2022 RED-GNN [83] Inductive GNN 2019 SACN [62] Transductive GNN",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 17.631092071533203,
      "citing_paper_id": "254564635",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "Meanwhile, SACN [62] provides an effective model composed of the encoder, i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (SACN). There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 19.413012742996216,
      "citing_paper_id": "254564635",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "Meanwhile, SACN [61] leverages the weighted graph convolutional network (WGCN) as the encoder and a convolutional network called Conv-TransE as the decoder, which is effective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture and effectiveness of the model.",
      "processing_time": 13.982488870620728,
      "citing_paper_id": "254564635",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "The typical solutions for VQA are based on the CNN-RNN architecture [Malinowski et al. , 2015] and leverage global visual features to represent image, which may introduce noisy information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CNN-RNN architecture) and a potential issue with global visual features. No verifiable resources are identified.",
      "processing_time": 33.334959983825684,
      "citing_paper_id": "219708313",
      "cited_paper_id": 738850
    },
    {
      "context_text": "Various attention mechanisms [Yang et al. , 2016; Lu et al. , 2016; Anderson et al. , 2018 ] have been exploited to highlight visual objects that are relevant to the question.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to attention mechanisms in various works. No verifiable resources are identified.",
      "processing_time": 15.545406103134155,
      "citing_paper_id": "219708313",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "Various attention mechanisms [Yang et al. , 2016; Lu et al. , 2016; Anderson et al. , 2018 ] have been exploited to highlight visual objects that are relevant to the question.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to attention mechanisms in various works. No verifiable resources are identified.",
      "processing_time": 15.545406103134155,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[ Schlichtkrull et al. , 2018 ] generalizes graph convolutional network (GCN) to handle different relationships be-tween entities in a knowledge base, where each edge with distinct relationships is encoded independently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling relational data in knowledge graphs.",
      "processing_time": 13.970285654067993,
      "citing_paper_id": "219708313",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "[ Schlichtkrull et al. , 2018 ] generalizes graph convolutional network (GCN) to handle different relationships be-tween entities in a knowledge base, where each edge with distinct relationships is encoded independently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling relational data in knowledge graphs.",
      "processing_time": 13.970285654067993,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "2, the node representations of each layer of graphs are updated following the message-passing framework [Gilmer et al. , 2017].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for updating node representations in graphs.",
      "processing_time": 12.981258630752563,
      "citing_paper_id": "219708313",
      "cited_paper_id": 9665943
    },
    {
      "context_text": "In our work, we leverage dense captions [Johnson et al., 2016] to extract a set of local-level semantics in an image, ranging from the properties of a single object (color, shape, emotion, etc.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'dense captions' which is a method for generating local-level semantic descriptions in images. However, no specific dataset is mentioned.",
      "processing_time": 17.897221088409424,
      "citing_paper_id": "219708313",
      "cited_paper_id": 14521054
    },
    {
      "context_text": "Existing works [Wang et al. , 2017; Wang et al. , 2018] parse the question as keywords and retrieve the supporting-entity only by keyword matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for parsing questions and retrieving entities using keyword matching.",
      "processing_time": 27.96479368209839,
      "citing_paper_id": "219708313",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "Existing works [Wang et al. , 2017; Wang et al. , 2018] parse the question as keywords and retrieve the supporting-entity only by keyword matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for parsing questions and retrieving entities using keyword matching.",
      "processing_time": 27.96479368209839,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[ Wang et al. , 2017 ] reduces the question to one of the available query templates and this limits the types of questions that can be asked.",
      "catation_intent": "limitation",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of a method.",
      "processing_time": 15.08277440071106,
      "citing_paper_id": "219708313",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "[ Wang et al. , 2017 ] reduces the question to one of the available query templates and this limits the types of questions that can be asked.",
      "catation_intent": "limitation",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of a method.",
      "processing_time": 15.08277440071106,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Overall Accuracy top-1 top-3 Q-Only [Marino et al. , 2019] 14.93 - MLP [Marino et al. , 2019] 20.67 - BAN [Kim et al. , 2018] 25.17 - MUTAN [Ben-Younes et al. , 2017] 26.41 - ArticleNet (AN) [Marino et al. , 2019] 5.28 - BAN + AN [Marino et al. , 2019] 25.61 - MUTAN + AN [Marino et al. , 2019]‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 13.430439710617065,
      "citing_paper_id": "219708313",
      "cited_paper_id": 29150617
    },
    {
      "context_text": "[Battaglia et al. , 2018] demonstrates that human‚Äôs ability of combinatorial generalization highly depends on the mechanisms for reasoning over relationships.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to human combinatorial generalization and reasoning mechanisms.",
      "processing_time": 17.88556480407715,
      "citing_paper_id": "219708313",
      "cited_paper_id": 46935302
    },
    {
      "context_text": "[ Narasimhan et al. , 2018 ] applies GCN on the fact graph where each node is represented by the Ô¨Åxed form of image-question-entity embedding.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions the use of a fact graph for visual question answering, which suggests the presence of a dataset. However, no specific dataset name is provided.",
      "processing_time": 16.145225286483765,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "To resolve these problems, [Narasimhan et al. , 2018] introduces visual information into the fact graph and infers the answer by implicit graph reasoning under the guidance of the question.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating visual information into a fact graph for reasoning.",
      "processing_time": 15.069767475128174,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "We also test the performance of Out of the Box (OB) [Narasimhan et al., 2018] on Visual7W-KB and report the results in Table 5.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual7W-KB"
      ],
      "dataset_descriptions": {
        "Visual7W-KB": "Used to test the performance of the Out of the Box method, focusing on factual visual question answering using graph convolution networks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Visual7W-KB' as a dataset used to test the performance of a method. The dataset is specific and relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 24.697017908096313,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "[Narasimhan et al., 2018] applies GCN on the fact graph where each node is represented by the fixed form of imagequestion-entity embedding.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'fact graph' and 'image-question-entity embedding', which suggests the use of a specific dataset or knowledge graph, but does not provide a clear, named dataset.",
      "processing_time": 16.87825894355774,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "To resolve these problems, [Narasimhan et al., 2018] introduces visual information into the fact graph and infers the answer by implicit graph reasoning under the guidance of the question.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating visual information into a fact graph for reasoning.",
      "processing_time": 27.070573806762695,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "To Ô¨Ånd the optimal supporting-fact, we Ô¨Årst retrieve relevant candidate facts from knowledge base of facts following a scored based approach proposed in [Narasimhan et al. , 2018].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to a knowledge base of facts. No clear, verifiable dataset is identified.",
      "processing_time": 15.50506329536438,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "Straight to the Facts (STTF) [Narasimhan and Schwing, 2018] and Out of the Box (OB) [Narasimhan et al., 2018], and",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Out of the Box' which is a method or model, not a dataset. No specific datasets are mentioned in the citation context.",
      "processing_time": 30.65056562423706,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "We also test the performance of Out of the Box (OB) [ Narasimhan et al. , 2018 ] on Visual7W-KB and report the results in Table 5.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual7W-KB"
      ],
      "dataset_descriptions": {
        "Visual7W-KB": "Used to test the performance of the Out of the Box method, focusing on factual visual question answering using graph convolutional networks."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Visual7W-KB' which appears to be a specific dataset used for testing the performance of a method. The dataset is likely relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 24.42975902557373,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "Out of the Box (OB) [Narasimhan et al. , 2018] 57 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called 'Out of the Box'. The context does not provide information about datasets used.",
      "processing_time": 16.526400566101074,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "To find the optimal supportingfact, we first retrieve relevant candidate facts from knowledge base of facts following a scored based approach proposed in [Narasimhan et al., 2018].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for retrieving facts from a knowledge base.",
      "processing_time": 14.75966739654541,
      "citing_paper_id": "219708313",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "Graph neural networks are gaining fast momentum in the last few years [ Wu et al. , 2019 ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in graph neural networks.",
      "processing_time": 14.756891965866089,
      "citing_paper_id": "219708313",
      "cited_paper_id": 57375753
    },
    {
      "context_text": "Recent works [ Cadene et al. , 2019; We represent an image by multi-layer graphs and cross-modal knowledge reasoning is conducted on the graphs to infer the optimal answer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for multimodal relational reasoning. The cited paper title 'MUREL: Multimodal Relational Reasoning for Visual Question Answering' confirms the focus on a method rather than a dataset.",
      "processing_time": 21.916174173355103,
      "citing_paper_id": "219708313",
      "cited_paper_id": 67856593
    },
    {
      "context_text": "As an extension, [Jiang et al. , 2020] exploits natural language to enrich the graph-based visual representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enriching graph-based visual representations using natural language.",
      "processing_time": 15.497704267501831,
      "citing_paper_id": "219708313",
      "cited_paper_id": 208138178
    },
    {
      "context_text": "[Wang et al., 2019b; Hu et al., 2019a] propose heterogeneous graph attention networks with dual-level attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on proposing a network architecture.",
      "processing_time": 13.910334587097168,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[Wang et al., 2019b; Hu et al., 2019a] propose heterogeneous graph attention networks with dual-level attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on proposing a network architecture.",
      "processing_time": 13.910334587097168,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[ Wang et al. , 2019b; Hu et al. , 2019a] propose heterogeneous graph attention networks with dual-level attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.15491247177124,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[ Wang et al. , 2019b; Hu et al. , 2019a] propose heterogeneous graph attention networks with dual-level attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.15491247177124,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[ Wang et al. , 2019b; Hu et al. , 2019a] propose heterogeneous graph attention networks with dual-level attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.15491247177124,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Consistent with such proposal, there is an emerging trend to represent the image by graph structure to depict objects and relationships in VQA and other vision-language tasks [Hu et al. , 2019b; Wang et al. , 2019a; Li et al. , 2019b].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in representing images by graph structures in VQA and other vision-language tasks.",
      "processing_time": 29.014251708984375,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Consistent with such proposal, there is an emerging trend to represent the image by graph structure to depict objects and relationships in VQA and other vision-language tasks [Hu et al. , 2019b; Wang et al. , 2019a; Li et al. , 2019b].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in representing images by graph structures in VQA and other vision-language tasks.",
      "processing_time": 29.014251708984375,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Consistent with such proposal, there is an emerging trend to represent the image by graph structure to depict objects and relationships in VQA and other vision-language tasks [Hu et al. , 2019b; Wang et al. , 2019a; Li et al. , 2019b].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in representing images by graph structures in VQA and other vision-language tasks.",
      "processing_time": 29.014251708984375,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Various attention mechanisms [Yang et al., 2016; Lu et al., 2016; Anderson et al., 2018] have been exploited to highlight visual objects that are relevant to the question.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only attention mechanisms from various papers. There are no verifiable resources or datasets mentioned.",
      "processing_time": 16.115227222442627,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[Schlichtkrull et al., 2018] generalizes graph convolutional network (GCN) to handle different relationships between entities in a knowledge base, where each edge with distinct relationships is encoded independently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling different relationships in a knowledge base.",
      "processing_time": 13.642594575881958,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "[Wang et al., 2017] reduces the question to one of the available query templates and this limits the types of questions that can be asked.",
      "catation_intent": "limitation",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of a method.",
      "processing_time": 13.152633428573608,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing works [Wang et al., 2017; Wang et al., 2018] parse the question as keywords and retrieve the supporting-entity only by keyword matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. No verifiable resources are identified.",
      "processing_time": 12.401422262191772,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "Consistent with such proposal, there is an emerging trend to represent the image by graph structure to depict objects and relationships in VQA and other vision-language tasks [Hu et al., 2019b; Wang et al., 2019a; Li et al., 2019b].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a trend in representing images by graph structures in VQA and other vision-language tasks.",
      "processing_time": 16.84323787689209,
      "citing_paper_id": "219708313",
      "cited_paper_id": null
    },
    {
      "context_text": "A number of large-scale knowledge graphs are available commercially or in open source, which are generally constructed based on common sense [50], Wikipedia[3], English word [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNet"
      ],
      "dataset_descriptions": {
        "WordNet": "Used as a lexical database for English, providing a structured vocabulary for semantic relationships in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Wikipedia' and 'English word', which could refer to datasets. However, 'Wikipedia' is not a specific dataset but a general source, and 'English word' is too generic. 'WordNet' is a specific lexical database mentioned in the cited paper title, which is relevant.",
      "processing_time": 27.604485273361206,
      "citing_paper_id": "252782967",
      "cited_paper_id": 1671874
    },
    {
      "context_text": "A number of large-scale knowledge graphs are available commercially or in open source, which are generally constructed based on common sense [50], Wikipedia[3], English word [40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNet"
      ],
      "dataset_descriptions": {
        "WordNet": "Used as a lexical database for English, providing a structured vocabulary for semantic relationships in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Wikipedia' and 'English word', which could refer to datasets. However, 'Wikipedia' is not a specific dataset but a general source, and 'English word' is too generic. 'WordNet' is a specific lexical database mentioned in the cited paper title, which is relevant.",
      "processing_time": 27.604485273361206,
      "citing_paper_id": "252782967",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "Recently, researchers attempt to leverage it as an external knowledge to boost computer vision tasks [9, 15, 37, 55, 61].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to leveraging external knowledge. No specific resource names are provided.",
      "processing_time": 16.482244968414307,
      "citing_paper_id": "252782967",
      "cited_paper_id": 2237901
    },
    {
      "context_text": "Recently, researchers attempt to leverage it as an external knowledge to boost computer vision tasks [9, 15, 37, 55, 61].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to leveraging external knowledge. No specific resource names are provided.",
      "processing_time": 16.482244968414307,
      "citing_paper_id": "252782967",
      "cited_paper_id": 8701238
    },
    {
      "context_text": "Recently, researchers attempt to leverage it as an external knowledge to boost computer vision tasks [9, 15, 37, 55, 61].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to leveraging external knowledge. No specific resource names are provided.",
      "processing_time": 16.482244968414307,
      "citing_paper_id": "252782967",
      "cited_paper_id": 26517743
    },
    {
      "context_text": "Different from the existing datasets [1, 20, 49], it provides human part annotations, i.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide specific names of datasets, only a reference to 'existing datasets' without naming them. The cited paper title 'UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild' suggests a specific dataset, but it is not mentioned in the context.",
      "processing_time": 35.03260350227356,
      "citing_paper_id": "252782967",
      "cited_paper_id": 7197134
    },
    {
      "context_text": "Specifically, we equip each action node with a textual description from Wikipedia, e.g., an action of hammer throw is a field event in which the hammer is thrown for distance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to textual descriptions from Wikipedia, which is not considered a dataset in this context.",
      "processing_time": 18.4923038482666,
      "citing_paper_id": "252782967",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "Then, we summarize visual knowledge graph from all the video sub-graphs, and equip each action node with its semantic knowledge of Wikipedia descriptions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Wikipedia descriptions' but does not specify a dataset. The cited paper title 'DBpedia: A Nucleus for a Web of Open Data' suggests a potential dataset, but it is not explicitly mentioned in the context.",
      "processing_time": 20.966349601745605,
      "citing_paper_id": "252782967",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "Moreover, we equip each action node with a description from Wikipedia, which provides semantic knowledge to understand human actions from discriminative texts.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions using Wikipedia descriptions for action nodes, but does not specify a dataset. DBpedia is a potential candidate but is not explicitly mentioned in the context.",
      "processing_time": 16.49770951271057,
      "citing_paper_id": "252782967",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "In particular, as for video classification using knowledge graph [18, 19, 63], there are several limitations in these works.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions limitations in works related to video classification using knowledge graphs but does not specify any datasets.",
      "processing_time": 12.88686990737915,
      "citing_paper_id": "252782967",
      "cited_paper_id": 40114756
    },
    {
      "context_text": "Moreover, [63] works on the general video classification, instead of human action understanding.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a general area of research. The context is too vague to identify a verifiable dataset.",
      "processing_time": 18.072954893112183,
      "citing_paper_id": "252782967",
      "cited_paper_id": 40114756
    },
    {
      "context_text": "such discriminative information to understand human poses and activities [8, 14, 28, 33, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other works. The cited papers' titles suggest they deal with human-object interactions and activity recognition, but do not specify datasets.",
      "processing_time": 18.476161003112793,
      "citing_paper_id": "252782967",
      "cited_paper_id": 51880810
    },
    {
      "context_text": "such discriminative information to understand human poses and activities [8, 14, 28, 33, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other works. The cited papers' titles suggest they deal with human-object interactions and activity recognition, but do not specify datasets.",
      "processing_time": 18.476161003112793,
      "citing_paper_id": "252782967",
      "cited_paper_id": 204960442
    },
    {
      "context_text": "such discriminative information to understand human poses and activities [8, 14, 28, 33, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other works. The cited papers' titles suggest they deal with human-object interactions and activity recognition, but do not specify datasets.",
      "processing_time": 18.476161003112793,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "since they are not the key designs in our AKU framework and easily obtained by remarkable pose estimators [8, 64] and object detectors [4, 5, 48].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools or methods for pose estimation and object detection.",
      "processing_time": 12.367239952087402,
      "citing_paper_id": "252782967",
      "cited_paper_id": 57573854
    },
    {
      "context_text": "since they are not the key designs in our AKU framework and easily obtained by remarkable pose estimators [8, 64] and object detectors [4, 5, 48].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools or methods for pose estimation and object detection.",
      "processing_time": 12.367239952087402,
      "citing_paper_id": "252782967",
      "cited_paper_id": 206596979
    },
    {
      "context_text": "However, most existing approaches treat action recognition as a high-level video classification problem, and focus on designing backbones [16, 30, 53] for representation learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models for action recognition and video classification.",
      "processing_time": 15.683250188827515,
      "citing_paper_id": "252782967",
      "cited_paper_id": 102350405
    },
    {
      "context_text": "However, most existing approaches treat action recognition as a high-level video classification problem, and focus on designing backbones [16, 30, 53] for representation learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models for action recognition and video classification.",
      "processing_time": 15.683250188827515,
      "citing_paper_id": "252782967",
      "cited_paper_id": 246240170
    },
    {
      "context_text": "Different from the existing data sets in action recognition [26, 31, 34, 36, 54], Kinetics-TPS provides over 15M part-level annotations for detailed human action understanding, including 7.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_descriptions": {
        "Kinetics-TPS": "Used to provide over 15M part-level annotations for detailed human action understanding, enhancing multi-modal knowledge graph reasoning with fine-grained action data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Kinetics-TPS' as a dataset providing part-level annotations for detailed human action understanding, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 24.230527877807617,
      "citing_paper_id": "252782967",
      "cited_paper_id": 208202400
    },
    {
      "context_text": "The great advancement of this research has been made with fast development of deep learning [26, 31, 34, 36, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the advancement of research due to deep learning.",
      "processing_time": 28.09152603149414,
      "citing_paper_id": "252782967",
      "cited_paper_id": 208202400
    },
    {
      "context_text": ", action genome [23]), without deep studies on body part movements in the videos.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'action genome' but does not indicate it is a dataset. It appears to be a method or model, and there is no clear indication of a specific dataset being used.",
      "processing_time": 19.276866674423218,
      "citing_paper_id": "252782967",
      "cited_paper_id": 209376177
    },
    {
      "context_text": "To alleviate such problem, several works have been proposed for more detailed action understanding [23, 38], by mimicking human beings with compositional knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to action understanding. No verifiable resources are identified.",
      "processing_time": 15.701657772064209,
      "citing_paper_id": "252782967",
      "cited_paper_id": 209376177
    },
    {
      "context_text": "something-else [38] mainly works on human hands, while action genome [23] constructs scene graph about the whole human instances.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Action Genome"
      ],
      "dataset_descriptions": {
        "Action Genome": "Used to construct scene graphs about whole human instances, focusing on spatio-temporal relationships and compositions of actions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'action genome' as a resource that constructs scene graphs about whole human instances, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 22.89508819580078,
      "citing_paper_id": "252782967",
      "cited_paper_id": 209376177
    },
    {
      "context_text": "works have been recently introduced for action understanding, by exploiting human-object relations in the video [23, 38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to works that exploit human-object relations in videos.",
      "processing_time": 15.208504915237427,
      "citing_paper_id": "252782967",
      "cited_paper_id": 209376177
    },
    {
      "context_text": "In particular, a PaStaNet [33] has been proposed with a human activity knowledge engine, which encodes semantic representation of body part states (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (PaStaNet) and its application. No verifiable datasets are referenced.",
      "processing_time": 16.460613250732422,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "Second, we compare our AKU paradigm with PaStaNet [33], a recent knowledge-based human action recognition",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PaStaNet). The context focuses on comparing the AKU paradigm with PaStaNet, which is a knowledge-based human action recognition system.",
      "processing_time": 20.922586917877197,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "9% accuracy on Kinetics-TPS while PaStaNet [33] achieves 63.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_descriptions": {
        "Kinetics-TPS": "Used to evaluate the performance of human activity recognition models, specifically comparing accuracy between different approaches."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Kinetics-TPS' which appears to be a dataset, but does not provide enough information about its usage or the specific research context. The cited paper title does not help in disambiguating further.",
      "processing_time": 25.564141750335693,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "Recently, a PastaNet [33] attempts to explore human activity knowledge by part-level state (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called PastaNet. The citation is used to reference the method, not a dataset.",
      "processing_time": 16.071011543273926,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "Finally, we also perform PaStaNet [33] with our visual knowledge graph.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (PaStaNet). The context does not provide information about a dataset being used.",
      "processing_time": 14.915425777435303,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "Hence, we transform all the part features through this tree structure, as suggested in [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is too vague to infer any dataset usage.",
      "processing_time": 15.675395727157593,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "activity knowledge approach PaStaNet [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide enough context to identify a specific dataset. 'PaStaNet' appears to be a method or tool rather than a dataset.",
      "processing_time": 15.640837669372559,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "Action Prediction Ground Truth Categories PaStaNet [33] Our AKU PaStaNet [33] Our AKU",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide any specific dataset names or details about the usage of datasets. It only mentions PaStaNet, which is a method or model, not a dataset.",
      "processing_time": 17.450466632843018,
      "citing_paper_id": "252782967",
      "cited_paper_id": 214775220
    },
    {
      "context_text": "Note that, we do not further discuss how to obtain part and object annotations in this paper, since they are not the key designs in our AKU framework and easily obtained by remarkable pose estimators [8, 64] and object detectors [4, 5, 48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods for obtaining annotations. The context focuses on pose estimators and object detectors, which are methods, not datasets.",
      "processing_time": 17.464941024780273,
      "citing_paper_id": "252782967",
      "cited_paper_id": 216080778
    },
    {
      "context_text": "Based on the recent trends of vision transformers [11‚Äì13, 32, 35, 51, 59, 62], researchers pro-pose to leverage self-attention for spatial-temporal relation learning [2, 30, 41, 42, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to vision transformers and self-attention mechanisms. No verifiable resources are identified.",
      "processing_time": 17.46128797531128,
      "citing_paper_id": "252782967",
      "cited_paper_id": 231741093
    },
    {
      "context_text": "Based on the recent trends of vision transformers [11‚Äì13, 32, 35, 51, 59, 62], researchers pro-pose to leverage self-attention for spatial-temporal relation learning [2, 30, 41, 42, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to vision transformers and self-attention mechanisms. No verifiable resources are identified.",
      "processing_time": 17.46128797531128,
      "citing_paper_id": "252782967",
      "cited_paper_id": 233387838
    },
    {
      "context_text": "Based on the recent trends of vision transformers [11‚Äì13, 32, 35, 51, 59, 62], researchers pro-pose to leverage self-attention for spatial-temporal relation learning [2, 30, 41, 42, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to vision transformers and self-attention mechanisms. No verifiable resources are identified.",
      "processing_time": 17.46128797531128,
      "citing_paper_id": "252782967",
      "cited_paper_id": 235694438
    },
    {
      "context_text": "First, we evaluate our AKU paradigm on various video backbones including 2D CNNs [31, 34, 58], 3D CNNs [16], and video transformer [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various video backbone models. The context focuses on evaluating the AKU paradigm using different types of neural network architectures.",
      "processing_time": 16.402008533477783,
      "citing_paper_id": "252782967",
      "cited_paper_id": 246240170
    },
    {
      "context_text": "Specifically, we investigate our paradigm on the recent Kinetics-TPS benchmark [25], since this dataset aims at parsing human actions by compositional learning of body part movements.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_descriptions": {
        "Kinetics-TPS": "Used to parse human actions by compositional learning of body part movements, focusing on the specific research question of action recognition through multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Kinetics-TPS benchmark, which is a specific dataset used for parsing human actions through the compositional learning of body part movements.",
      "processing_time": 23.467040061950684,
      "citing_paper_id": "252782967",
      "cited_paper_id": null
    },
    {
      "context_text": "To tackle these difficulties, we propose to construct a novel visual knowledge graph for human action understanding in videos, based on the recent action parsing benchmark of Kinetics-TPS [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_descriptions": {
        "Kinetics-TPS": "Used as a benchmark for constructing a visual knowledge graph for human action understanding in videos, focusing on action parsing."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Kinetics-TPS' as a benchmark, which is likely a dataset used for action parsing in videos. However, since benchmarks are generally excluded unless they refer to a specific, downloadable dataset, and no specific usage details are provided, the confidence is lowered.",
      "processing_time": 25.788241863250732,
      "citing_paper_id": "252782967",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, we systematically evaluate our AKU on the action parsing benchmark, i.e., Kinetics-TPS [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.5,
      "reasoning": "The citation mentions 'Kinetics-TPS' as a benchmark used for evaluating the action parsing performance of their model. However, benchmarks are excluded unless they refer to a specific, downloadable dataset.",
      "processing_time": 18.405952215194702,
      "citing_paper_id": "252782967",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, we use the recent action parsing benchmark (Kinetics-TPS [25]) as video data base to achieve this goal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_descriptions": {
        "Kinetics-TPS": "Used as a video database for action parsing, focusing on temporal action localization and segmentation in videos."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Kinetics-TPS' as a video database used for action parsing, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 22.597522020339966,
      "citing_paper_id": "252782967",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast, our visual knowledge graph is built upon an action parsing benchmark of Kinetics-TPS [25].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_descriptions": {
        "Kinetics-TPS": "Used to build a visual knowledge graph, focusing on action parsing to enhance multi-modal reasoning capabilities."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Kinetics-TPS' as a benchmark, which is likely a dataset used for action parsing. It is used to build a visual knowledge graph, indicating its relevance to multi-modal knowledge graph reasoning.",
      "processing_time": 24.96929931640625,
      "citing_paper_id": "252782967",
      "cited_paper_id": null
    },
    {
      "context_text": "We initialize words in Q with GloVe embeddings [29] with dimension d = 300.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GloVe embeddings but does not refer to a specific dataset. GloVe is a method for generating word embeddings, not a dataset.",
      "processing_time": 16.030215740203857,
      "citing_paper_id": "235829220",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "In order to optimize the training objective given by Equation (4), we use REIN-FORCE [37] to obtain the gradient approximation where Œ≥ is the discount factor for the reward.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reinforcement learning algorithm (REINFORCE).",
      "processing_time": 13.255227088928223,
      "citing_paper_id": "235829220",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "Rewards and Optimization After sampling T transitions, a terminal reward is assigned according to We employ REINFORCE [37] to maximize the expected rewards.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reinforcement learning algorithm (REINFORCE).",
      "processing_time": 13.25257682800293,
      "citing_paper_id": "235829220",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "Plenty of VQA approaches are agnostic towards the explicit relational structure of the objects in the presented scene and rely on monolithic neural network architectures that process regional features of the image separately [2,39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and architectures. No verifiable resources are identified.",
      "processing_time": 13.017055034637451,
      "citing_paper_id": "235829220",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "are frequently employed to couple the vision and the language models allowing for interactions between objects from both modalities [20,2,5,40,41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the coupling of vision and language models. No verifiable resources are identified.",
      "processing_time": 28.859745502471924,
      "citing_paper_id": "235829220",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "are frequently employed to couple the vision and the language models allowing for interactions between objects from both modalities [20,2,5,40,41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the coupling of vision and language models. No verifiable resources are identified.",
      "processing_time": 28.859745502471924,
      "citing_paper_id": "235829220",
      "cited_paper_id": 11117517
    },
    {
      "context_text": "Most of the popular explicit reasoning approaches follow the idea of neural module networks (NMNs) [3] which perform a sequence of reasoning steps realized by forward passes through specialized neural networks that each correspond to predefined reasoning subtasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach.",
      "processing_time": 12.607783555984497,
      "citing_paper_id": "235829220",
      "cited_paper_id": 5276660
    },
    {
      "context_text": "Training the Graphhopper: In terms of the training procedure, the GAT, the Transformer, and the policy networks are initialized with Glorot [11] initialization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only initialization methods for neural networks.",
      "processing_time": 13.764681339263916,
      "citing_paper_id": "235829220",
      "cited_paper_id": 5575601
    },
    {
      "context_text": "We employ the Adam optimizer [21] with a learning rate of 10 ‚àí 4 for all trainable weights.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method for stochastic optimization, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 15.97529411315918,
      "citing_paper_id": "235829220",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "The weight updates can be performed via gradient ascent, Œ∏ ‚Üê Œ∏ + Œ∑ ÀÜ g or more advanced optimization methods such as Adam [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only optimization methods. The cited paper 'Adam: A Method for Stochastic Optimization' is a method, not a dataset.",
      "processing_time": 19.642947673797607,
      "citing_paper_id": "235829220",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "These approaches are closely related to the scene graph representations [19] of an image, where detected objects are labeled as nodes and relationships between the objects are labeled as edges.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to scene graph representations, which are a method or model, not a dataset.",
      "processing_time": 17.96350884437561,
      "citing_paper_id": "235829220",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "However, a conceptual diÔ¨Äerence is that the agents in MINERVA and DeepPath perform walks on large-scale knowledge graphs exploiting repeating statistical patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the conceptual difference between agents in MINERVA and DeepPath.",
      "processing_time": 19.401458263397217,
      "citing_paper_id": "235829220",
      "cited_paper_id": 20667722
    },
    {
      "context_text": "Recently, multi-hop reasoning methods such as MINERVA [8] and DeepPath [38] were proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (MINERVA and DeepPath) but does not refer to any specific datasets. The context is about multi-hop reasoning methods, not datasets.",
      "processing_time": 15.964726448059082,
      "citing_paper_id": "235829220",
      "cited_paper_id": 20667722
    },
    {
      "context_text": "Inspired by multi-hop reasoning methods on KGs such as [8,38,12], we propose Graphhopper, a novel method that models the VQA task as a path-finding problem on scene graphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodology and the proposed method, Graphhopper.",
      "processing_time": 13.746584415435791,
      "citing_paper_id": "235829220",
      "cited_paper_id": 20667722
    },
    {
      "context_text": "Next to other standard Python libraries, we mainly employed PyTorch [28].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PyTorch, which is a software toolkit, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 13.744722604751587,
      "citing_paper_id": "235829220",
      "cited_paper_id": 40027675
    },
    {
      "context_text": "Other works [34,31,15] perform reasoning on explicitly detected objects and interactive semantic and spatial relationships among them.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 13.216667652130127,
      "citing_paper_id": "235829220",
      "cited_paper_id": 54458106
    },
    {
      "context_text": "Other works [34,31,15] perform reasoning on explicitly detected objects and interactive semantic and spatial relationships among them.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 13.216667652130127,
      "citing_paper_id": "235829220",
      "cited_paper_id": 195847902
    },
    {
      "context_text": ", [14,26]) exhibit good performance on synthetic datasets such as CLEVR [18], they require functional module layouts as additional supervision signals to obtain good results.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CLEVR"
      ],
      "dataset_descriptions": {
        "CLEVR": "Used to evaluate the performance of neuro-symbolic concept learners on visual reasoning tasks, requiring functional module layouts as additional supervision signals."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'synthetic datasets' and specifically cites 'CLEVR'. CLEVR is a well-known synthetic dataset used for visual reasoning tasks.",
      "processing_time": 22.296040773391724,
      "citing_paper_id": "235829220",
      "cited_paper_id": 108296442
    },
    {
      "context_text": "challenging, real-world datasets such as GQA (see [17]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GQA"
      ],
      "dataset_descriptions": {
        "GQA": "Used to evaluate real-world visual reasoning and compositional question answering, focusing on complex interactions between images and questions."
      },
      "confidence_score": 1.0,
      "reasoning": "GQA is identified as a dataset in the citation context, and the title confirms it is a dataset for visual reasoning and compositional question answering.",
      "processing_time": 21.95181107521057,
      "citing_paper_id": "235829220",
      "cited_paper_id": 152282269
    },
    {
      "context_text": ", red is a reasonable color of an apple reasonable, blue is not), as proposed in [17].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, model, or method. It only refers to a concept of visual reasoning and compositional question answering.",
      "processing_time": 15.561100244522095,
      "citing_paper_id": "235829220",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "To address these shortcomings, the GQA dataset [17] has been developed.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GQA"
      ],
      "dataset_descriptions": {
        "GQA": "Used to develop and evaluate models for real-world visual reasoning and compositional question answering, addressing shortcomings in existing datasets."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GQA dataset, which is a specific, verifiable dataset used for visual reasoning and compositional question answering.",
      "processing_time": 21.9504656791687,
      "citing_paper_id": "235829220",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "The GQA dataset [17] has been introduced with the goal of addressing key shortcomings of previous VQA datasets, such as CLEVR [18] or the VQA dataset [4].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GQA",
        "CLEVR",
        "VQA"
      ],
      "dataset_descriptions": {
        "GQA": "Used to address shortcomings in previous VQA datasets, focusing on real-world visual reasoning and compositional question answering.",
        "CLEVR": "Compared against GQA to highlight improvements in visual reasoning and compositional question answering.",
        "VQA": "Compared against GQA to highlight improvements in visual reasoning and compositional question answering."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the GQA dataset and compares it to other VQA datasets, indicating its use for visual reasoning and compositional question answering.",
      "processing_time": 30.44601273536682,
      "citing_paper_id": "235829220",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "Table 1 shows the performance of Graphhopper and compares it with the human performance reported in [17] and with the performance of NSM on the same underlying",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics. No clear dataset names are provided in the context.",
      "processing_time": 15.534311056137085,
      "citing_paper_id": "235829220",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "Visual Question Answering: Various models have been proposed that perform VQA on both real-world [4,17] and artificial datasets [18].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'real-world' and 'artificial datasets', but does not specify any particular dataset names. The cited paper title 'GQA' suggests a specific dataset, but it is not explicitly mentioned in the citation context.",
      "processing_time": 20.8151957988739,
      "citing_paper_id": "235829220",
      "cited_paper_id": 152282269
    },
    {
      "context_text": "We Ô¨Ånd that Graph-hopper consistently outperforms NSM [15] based on the generated graph.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (Graph-hopper and NSM).",
      "processing_time": 15.08103084564209,
      "citing_paper_id": "235829220",
      "cited_paper_id": 195847902
    },
    {
      "context_text": "Method Binary Open Consistency Validity Plausibility Accuracy NSM [15] 51.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is too sparse to infer any dataset usage.",
      "processing_time": 15.928809404373169,
      "citing_paper_id": "235829220",
      "cited_paper_id": 195847902
    },
    {
      "context_text": "We have used the the available parameters from [15] and the implementation from [9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only parameters and an implementation. There are no clear identifiers for datasets in the context.",
      "processing_time": 15.57077670097351,
      "citing_paper_id": "235829220",
      "cited_paper_id": 195847902
    },
    {
      "context_text": "Reproducing NSM: [15] proposed the state of the art method named NSM for VQA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called NSM. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 17.64170479774475,
      "citing_paper_id": "235829220",
      "cited_paper_id": 195847902
    },
    {
      "context_text": "Inspired by multi-hop reasoning methods on KGs such as [8,38,12], we propose Graphhopper, a novel method that models the VQA task as a path-Ô¨Ånding problem on scene graphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on the methodology and the proposed method, Graphhopper.",
      "processing_time": 17.91471290588379,
      "citing_paper_id": "235829220",
      "cited_paper_id": 209532101
    },
    {
      "context_text": "We have used state of the art Relation Trans-former Network (RTN) [23] for the scene graph generation and DetectoRS [30] for object detection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (Relation Transformer Network, DetectoRS) but does not reference any specific datasets. The citation is focused on the tools and models used rather than datasets.",
      "processing_time": 19.966873168945312,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "Contextualized object embedding from RTN [23] is used for attribute prediction as where W , Obj context , P obj , P attribute are the weight matrices of a linear layer, the contextual embedding of an object, the probability distribution over all objects and the probability distribution over the attributes. œÉ denotes the sigmoid function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RTN) and its components. There are no clear identifiers for datasets.",
      "processing_time": 17.359179496765137,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "88 (c) Experiments on Generated Scene Graph using DetectoRS [30] object detector and RTN [23] as Scene Graph generator.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of DetectoRS and RTN, but these are tools/models, not datasets. No specific dataset is mentioned.",
      "processing_time": 18.31283473968506,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "Generation of Scene Graph: To address these problems, first, we choose two state-of-the-art network, RTN [23] for scene graph generation, and DetectoRS [30] for object detection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (RTN, DetectoRS) but does not refer to any specific datasets. The context is about using these models for scene graph generation and object detection.",
      "processing_time": 19.96300721168518,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "To address these problems, Ô¨Årst, we choose two state-of-the-art network, RTN [23] for scene graph generation, and DetectoRS [30] for object detection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (RTN and DetectoRS) but does not refer to any specific datasets. The context is about choosing models for specific tasks, not using datasets.",
      "processing_time": 19.75609040260315,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "Graphhopper (pr) indicates that we employed predicted relations from RTN [23].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (RTN) used to predict relations. No verifiable dataset is referenced.",
      "processing_time": 17.621461629867554,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "The transformer [35] based architecture of RTN and its contextual scene graph embedding is most closely related to our architecture and for our future expansion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture. There are no verifiable resources or datasets mentioned.",
      "processing_time": 16.641303062438965,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "Contextualized object embedding from RTN [23] is used for attribute prediction as",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RTN) for attribute prediction.",
      "processing_time": 15.064491748809814,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "(c) Experiments on Generated Scene Graph using DetectoRS [30] object detector and RTN [23] as Scene Graph generator.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of a generated scene graph but does not specify a named dataset. The components mentioned are tools or methods, not datasets.",
      "processing_time": 17.613855361938477,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "To make Graphhopper generic to any scene graph generator, we haven‚Äôt use contextualized embedding from RTN, instead we rely on GAT for contextualization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The citation is about the choice of method (GAT) over another (RTN) for contextualization in a graph model.",
      "processing_time": 20.76490092277527,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "Second, where we only use the predicted relationships from RTN [23] (with ground truth objects and attributes).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (RTN) and its use in predicting relationships. No verifiable dataset is referenced.",
      "processing_time": 18.946619987487793,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "(b) Experiments on Ground Truth objects and predicted relation from RTN [23] as Relation predictor.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RTN) and its use in experiments. No verifiable dataset names are provided.",
      "processing_time": 19.322521924972534,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "We have used state of the art Relation Transformer Network (RTN) [23] for the scene graph generation and DetectoRS [30] for object detection.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using Relation Transformer Network (RTN) and DetectoRS, but neither are datasets. They are models or methods, which are excluded according to the instructions.",
      "processing_time": 31.396640062332153,
      "citing_paper_id": "235829220",
      "cited_paper_id": 215754564
    },
    {
      "context_text": "To address this, we use STITCH (Search Tool For Interactions Of Chemicals) [56], a database integrating information on small molecules (drugs) and their associations with proteins.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "STITCH"
      ],
      "dataset_descriptions": {
        "STITCH": "Used to integrate information on small molecules and their protein interactions, supporting the research on chemical-protein relationships."
      },
      "confidence_score": 1.0,
      "reasoning": "STITCH is mentioned as a database used to integrate information on small molecules and their associations with proteins, which is relevant to the research context.",
      "processing_time": 22.717963933944702,
      "citing_paper_id": "279243397",
      "cited_paper_id": 2622646
    },
    {
      "context_text": "That said, we attempt to train three GNN-based retrievers, i.e., Graph Convolutional Network (GCN) [82], Graph SAGE [30], and Graph Attention Network [118] for our task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of GNN-based retrievers for a task, but does not specify the dataset used.",
      "processing_time": 20.623932600021362,
      "citing_paper_id": "279243397",
      "cited_paper_id": 4755450
    },
    {
      "context_text": "Treating our knowledge graph as a heterogeneous graph (multiple node/edge types) and using RGCN (Relational-GCN) layers [97] did not seem to work either.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RGCN) and a general reference to a knowledge graph. No verifiable datasets are identified.",
      "processing_time": 19.724023818969727,
      "citing_paper_id": "279243397",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Shannon Entropy : We measure Shannon Entropy [117] to gauge semantic coverage.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric (Shannon Entropy). The context does not provide information about a dataset being used.",
      "processing_time": 25.963110208511353,
      "citing_paper_id": "279243397",
      "cited_paper_id": 26419490
    },
    {
      "context_text": "‚Ä¶are evaluated using (i) Lexical EM (exact match): 0 (miss)/1 (hit) measure of equality between two strings, (ii) Lexical F1: Balanced proportion of token overlap between the predicted and reference strings and (iii) BERTScore F1 [137]: An embedding-based similarity measure between two strings.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. BERTScore is a method, not a dataset.",
      "processing_time": 25.51752758026123,
      "citing_paper_id": "279243397",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "L, training GNNs on our graph directly is difficult due to its small size (KG‚Äôs typically have millions of nodes and edges [42]) and complexity.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the general characteristics of knowledge graphs, which are not specific datasets.",
      "processing_time": 26.206849575042725,
      "citing_paper_id": "279243397",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "Retrievers Used For text-only retrieval, we test two types of embeddings, i.e., sparse - BM25 [98] and dense - [ MedCPT [48], DPR [52], MolLM [112], OpenAI‚Äôs text-embedding-3-large (TE3L) [85]].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not specify any datasets. The citation intent is to describe the models used for text-only retrieval, which are not datasets.",
      "processing_time": 27.840250730514526,
      "citing_paper_id": "279243397",
      "cited_paper_id": 215737187
    },
    {
      "context_text": "Retrievers Used For text-only retrieval, we test two types of embeddings, i.e., sparse - BM25 [98] and dense - [ MedCPT [48], DPR [52], MolLM [112], OpenAI‚Äôs text-embedding-3-large (TE3L) [85]].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not specify any datasets. The citation intent is to describe the models used for text-only retrieval, which are not datasets.",
      "processing_time": 27.840250730514526,
      "citing_paper_id": "279243397",
      "cited_paper_id": 265455405
    },
    {
      "context_text": "Retrievers Used For text-only retrieval, we test two types of embeddings, i.e., sparse - BM25 [98] and dense - [ MedCPT [48], DPR [52], MolLM [112], OpenAI‚Äôs text-embedding-3-large (TE3L) [85]].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not specify any datasets. The citation intent is to describe the models used for text-only retrieval, which are not datasets.",
      "processing_time": 27.840250730514526,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "MedCPT , DPR and MolLM are BERT-style [16] encoders, which are trained for different purposes such as molecular data understanding [112] and QA [52] tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (MedCPT, DPR, MolLM) and their training purposes, but does not refer to any specific datasets. The cited papers do not introduce new datasets either.",
      "processing_time": 28.98367953300476,
      "citing_paper_id": "279243397",
      "cited_paper_id": 215737187
    },
    {
      "context_text": "MedCPT , DPR and MolLM are BERT-style [16] encoders, which are trained for different purposes such as molecular data understanding [112] and QA [52] tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (MedCPT, DPR, MolLM) and their training purposes, but does not refer to any specific datasets. The cited papers do not introduce new datasets either.",
      "processing_time": 28.98367953300476,
      "citing_paper_id": "279243397",
      "cited_paper_id": 265455405
    },
    {
      "context_text": "‚Ä¢ MedCPT is specifically tailored to the medical domain by extensively tuning a PubMedBERT [24] checkpoint for query-article retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PubMedBERT, which is a model, not a dataset. No specific datasets are mentioned or used in the described research context.",
      "processing_time": 26.724791765213013,
      "citing_paper_id": "279243397",
      "cited_paper_id": 220919723
    },
    {
      "context_text": "We tried different settings (such as using a simpler regex-based entity recogniser, different embedding models such as PubMedBERT [24], etc.) and landed on this version, which provided decent results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is about methodological choices and not about reusing a dataset.",
      "processing_time": 26.91718888282776,
      "citing_paper_id": "279243397",
      "cited_paper_id": 220919723
    },
    {
      "context_text": "Modalities For Medical-QA In medical QA, we typically see datasets dealing with one of three modalities exclusively [47], i.e., text [81, 95], knowledge graphs [90, 131] or images [139, 38].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions datasets dealing with different modalities in medical QA, but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 28.66247320175171,
      "citing_paper_id": "279243397",
      "cited_paper_id": 224722163
    },
    {
      "context_text": "Modalities For Medical-QA In medical QA, we typically see datasets dealing with one of three modalities exclusively [47], i.e., text [81, 95], knowledge graphs [90, 131] or images [139, 38].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions datasets dealing with different modalities in medical QA, but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 28.66247320175171,
      "citing_paper_id": "279243397",
      "cited_paper_id": 249284566
    },
    {
      "context_text": "‚Ä¶grow the dataset, such as including unanswerable questions, exploring additional modalities like protein structures (long-chain amino acid sequence [66]) for testing the length constraints of LLMs [123], and scaling up sample size and considering multi-agent workflows [80] to enhance question‚Ä¶",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses potential expansions to a dataset, including unanswerable questions and protein structures, but does not name any existing datasets.",
      "processing_time": 28.971946239471436,
      "citing_paper_id": "279243397",
      "cited_paper_id": 225883893
    },
    {
      "context_text": "Switching gears from text, we do find frameworks that have begun incorporating different data modalities for solving tasks such as image captioning (generating text descriptions of images [107]) and code completion [69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions image captioning and code completion as examples of multi-modal tasks but does not specify any datasets used for these tasks.",
      "processing_time": 26.009724140167236,
      "citing_paper_id": "279243397",
      "cited_paper_id": 244772950
    },
    {
      "context_text": "Switching gears from text, we do find frameworks that have begun incorporating different data modalities for solving tasks such as image captioning (generating text descriptions of images [107]) and code completion [69].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions image captioning and code completion as examples of multi-modal tasks but does not specify any datasets used for these tasks.",
      "processing_time": 26.009724140167236,
      "citing_paper_id": "279243397",
      "cited_paper_id": 247450969
    },
    {
      "context_text": "‚Ä¶7 ) is much higher than existing biomedical QA datasets such as PubMedQA [46] and even datasets designed specifically for long-context QA such as QuALITY [88] (66 v/s 14 and 12.5 respectively) indicating a need for deep reasoning and tracking of facts across multiple segments for our questions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubMedQA",
        "QuALITY"
      ],
      "dataset_descriptions": {
        "PubMedQA": "Used to compare the complexity of questions requiring deep reasoning and fact tracking across multiple segments, highlighting the need for advanced multi-modal reasoning.",
        "QuALITY": "Used to compare the complexity of questions requiring deep reasoning and fact tracking across multiple segments, highlighting the need for advanced multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for comparison in the research, indicating their use in evaluating the complexity and requirements of the current dataset.",
      "processing_time": 39.283257246017456,
      "citing_paper_id": "279243397",
      "cited_paper_id": 245218982
    },
    {
      "context_text": "However, these frameworks typically target retrieval and reasoning over a single modality [77, 45, 3] such as text [115, 9, 103, 41], knowledge graph [17, 34, 76, 37], or image [101, 73, 122, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only modalities and frameworks. No verifiable resources are identified.",
      "processing_time": 26.135982513427734,
      "citing_paper_id": "279243397",
      "cited_paper_id": 251371732
    },
    {
      "context_text": "However, these frameworks typically target retrieval and reasoning over a single modality [77, 45, 3] such as text [115, 9, 103, 41], knowledge graph [17, 34, 76, 37], or image [101, 73, 122, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only modalities and frameworks. No verifiable resources are identified.",
      "processing_time": 26.135982513427734,
      "citing_paper_id": "279243397",
      "cited_paper_id": 254877499
    },
    {
      "context_text": "However, these frameworks typically target retrieval and reasoning over a single modality [77, 45, 3] such as text [115, 9, 103, 41], knowledge graph [17, 34, 76, 37], or image [101, 73, 122, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only modalities and frameworks. No verifiable resources are identified.",
      "processing_time": 26.135982513427734,
      "citing_paper_id": "279243397",
      "cited_paper_id": 276317958
    },
    {
      "context_text": "However, these frameworks typically target retrieval and reasoning over a single modality [77, 45, 3] such as text [115, 9, 103, 41], knowledge graph [17, 34, 76, 37], or image [101, 73, 122, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only modalities and frameworks. No verifiable resources are identified.",
      "processing_time": 26.135982513427734,
      "citing_paper_id": "279243397",
      "cited_paper_id": 276742400
    },
    {
      "context_text": "Wang et al. [124] performs molecule synthesis by retrieving similar samples from a labelled database.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a 'labelled database' but does not provide a specific name. The context suggests the database is used for molecule synthesis by retrieving similar samples.",
      "processing_time": 28.649561882019043,
      "citing_paper_id": "279243397",
      "cited_paper_id": 251765079
    },
    {
      "context_text": "Wang et al. [124] perform molecule synthesis by retrieving similar samples from a labeled database.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a 'labeled database' but does not provide a specific name or identifier. The context suggests the database is used for retrieving similar molecular samples for synthesis.",
      "processing_time": 29.22006344795227,
      "citing_paper_id": "279243397",
      "cited_paper_id": 251765079
    },
    {
      "context_text": "Specifically, we obtain entity names by querying various databases such as PubChem and STRING [110], depending on which one can provide a generic name.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem"
      ],
      "dataset_descriptions": {
        "PubChem": "Used to query and obtain generic names for chemical entities, supporting the identification and standardization of compound names in the research."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions querying databases such as PubChem and STRING, but does not specify their usage in the research context. STRING is a method/database for protein-protein associations, not a dataset.",
      "processing_time": 35.387425661087036,
      "citing_paper_id": "279243397",
      "cited_paper_id": 253479599
    },
    {
      "context_text": "For example, IRCoT [115] performs a sequence of retrieval + reasoning steps based on intermediate LLM responses; Self-RAG [9] forces the LLM to critique its response based on the retrieved documents, etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing different techniques for integrating retrieval and reasoning in LLMs.",
      "processing_time": 28.154200077056885,
      "citing_paper_id": "279243397",
      "cited_paper_id": 254877499
    },
    {
      "context_text": "However, they mainly focus on retrieving information from a single modality [45], e.g., text [115], knowledge graph [17], or image [101] only.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to modalities such as text, knowledge graph, and image, which are not specific datasets.",
      "processing_time": 28.94979977607727,
      "citing_paper_id": "279243397",
      "cited_paper_id": 254877499
    },
    {
      "context_text": "However, they mainly focus on retrieving information from a single modality [45], e.g., text [115], knowledge graph [17], or image [101] only.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to modalities such as text, knowledge graph, and image, which are not specific datasets.",
      "processing_time": 28.94979977607727,
      "citing_paper_id": "279243397",
      "cited_paper_id": 276317958
    },
    {
      "context_text": "This is because it depends on the target task, where using advanced retrieval methods such as FLARE [44] can be outperformed [45] by naive RAG [58] as it forces the LLM or retriever to overanalyse a problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the text.",
      "processing_time": 26.89764952659607,
      "citing_paper_id": "279243397",
      "cited_paper_id": 258615731
    },
    {
      "context_text": "Despite their strong abilities, LLMs also face issues such as hallucination [39], knowledge cutoff [15] and lacking domain-specific knowledge [132, 23].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues faced by LLMs. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 28.14552068710327,
      "citing_paper_id": "279243397",
      "cited_paper_id": 258822888
    },
    {
      "context_text": "Despite their strong abilities, LLMs also face issues such as hallucination [39], knowledge cutoff [15] and lacking domain-specific knowledge [132, 23].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues faced by LLMs. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 28.14552068710327,
      "citing_paper_id": "279243397",
      "cited_paper_id": 265067168
    },
    {
      "context_text": "Despite their strong abilities, LLMs also face issues such as hallucination [39], knowledge cutoff [15] and lacking domain-specific knowledge [132, 23].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues faced by LLMs. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 28.14552068710327,
      "citing_paper_id": "279243397",
      "cited_paper_id": 265610070
    },
    {
      "context_text": "Despite their strong abilities, LLMs also face issues such as hallucination [39], knowledge cutoff [15] and lacking domain-specific knowledge [132, 23].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues faced by LLMs. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 28.14552068710327,
      "citing_paper_id": "279243397",
      "cited_paper_id": 268531479
    },
    {
      "context_text": "We try to hot start our optimisation, by pretraining a GNN for a link prediction [61] (detecting if an edge exists between nodes).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a general task of link prediction using a GNN, which is not a dataset.",
      "processing_time": 28.92446756362915,
      "citing_paper_id": "279243397",
      "cited_paper_id": 259204112
    },
    {
      "context_text": "Additionally, many summaries are quite verbose, which could potentially inun-date an LLM‚Äôs context window, leading to a lost-in-the-middle effect [64], i.e., ignoring information in the middle of the prompt while focusing on the start/end.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a phenomenon related to language models and their handling of long contexts.",
      "processing_time": 28.37217354774475,
      "citing_paper_id": "279243397",
      "cited_paper_id": 259360665
    },
    {
      "context_text": "As shown by prior studies [28, 33], LLMs have strong capabilities in reasoning through chemistry tasks, including molecular property prediction from SMILES analysis [28].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLMs in chemistry tasks. No verifiable resources are identified.",
      "processing_time": 27.536154985427856,
      "citing_paper_id": "279243397",
      "cited_paper_id": 262138540
    },
    {
      "context_text": "Despite the proficiency of the LLM-as-a-judge setup, they are prone to issues such as preferring verbose outputs over shorter ones and sensitivity to the instructions in the prompt [54].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues with LLMs as judges. No verifiable resources are identified.",
      "processing_time": 27.06961727142334,
      "citing_paper_id": "279243397",
      "cited_paper_id": 263310448
    },
    {
      "context_text": "Although Wang et al. [125] performs QA over a multi-modal knowledge graph of biological entities, including drugs and proteins, the questions themselves do not integrate each source, essentially collapsing into unimodal retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called BioBridge. The context focuses on the limitations of the approach rather than the use of a specific dataset.",
      "processing_time": 28.962657690048218,
      "citing_paper_id": "279243397",
      "cited_paper_id": 263671998
    },
    {
      "context_text": "LLMs have demonstrated great performance in rewriting or generating desired questions that follow a given prompt and context [2, 129, 35, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss LLM performance in generating questions.",
      "processing_time": 26.662039518356323,
      "citing_paper_id": "279243397",
      "cited_paper_id": 267335111
    },
    {
      "context_text": "‚Ä¶the dataset, such as including unanswerable questions, exploring additional modalities like protein structures (long-chain amino acid sequence [66]) for testing the length constraints of LLMs [123], and scaling up sample size and considering multi-agent workflows [80] to enhance question diversity.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses general ideas about enhancing datasets, such as adding unanswerable questions, exploring protein structures, and increasing sample size, but does not name any particular datasets.",
      "processing_time": 32.35922837257385,
      "citing_paper_id": "279243397",
      "cited_paper_id": 267412232
    },
    {
      "context_text": "[141] investigates patient readmission/mortality by utilising both time-series and text data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of data (time-series and text) used in the research.",
      "processing_time": 26.396440505981445,
      "citing_paper_id": "279243397",
      "cited_paper_id": 267626929
    },
    {
      "context_text": "Human Automatic (LLM) Inference (can the answer be derived by studying the provided data [50]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'provided data'. No clear, verifiable dataset names are present.",
      "processing_time": 27.77729105949402,
      "citing_paper_id": "279243397",
      "cited_paper_id": 269005493
    },
    {
      "context_text": "However, using the same LLM for generation and evaluation is inadvisable as they have a tendency to be biased towards their own generations[89].",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological concern about LLMs being biased towards their own generations.",
      "processing_time": 27.507734775543213,
      "citing_paper_id": "279243397",
      "cited_paper_id": 269293311
    },
    {
      "context_text": "The most popular knowledge source used in current RAG methods is unstructured text documents [18, 20].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to unstructured text documents as a knowledge source, which is too generic.",
      "processing_time": 29.178727626800537,
      "citing_paper_id": "279243397",
      "cited_paper_id": 269740933
    },
    {
      "context_text": "Various RAG methods [18, 20] have emerged over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 27.528795957565308,
      "citing_paper_id": "279243397",
      "cited_paper_id": 269740933
    },
    {
      "context_text": "That said, we do find two new datasets that directly support RAG, viz., CRAG [133] and STaRK [130].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CRAG",
        "STaRK"
      ],
      "dataset_descriptions": {
        "CRAG": "Used to benchmark RAG models, focusing on comprehensive evaluation of retrieval-augmented generation systems.",
        "STaRK": "Used to support RAG models, focusing on specific aspects of retrieval-augmented reasoning and knowledge graph integration."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, CRAG and STaRK, which are used to support RAG. The title 'CRAG - Comprehensive RAG Benchmark' confirms CRAG is a dataset.",
      "processing_time": 40.70811986923218,
      "citing_paper_id": "279243397",
      "cited_paper_id": 270357492
    },
    {
      "context_text": "Multi-Modal QA Apart from STaRK, we find related datasets for multi-modal QA/RAG including ProMQA [31], MultiModalQA [111] and SPIQA [92].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ProMQA",
        "MultiModalQA",
        "SPIQA"
      ],
      "dataset_descriptions": {
        "ProMQA": "Used for multimodal procedural activity understanding, focusing on question answering that integrates visual and textual information.",
        "MultiModalQA": "Applied to multimodal question answering, integrating various modalities to enhance reasoning and answer generation.",
        "SPIQA": "Utilized for multimodal question answering on scientific papers, combining text and images to improve understanding and accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets specifically designed for multi-modal QA/RAG, which are directly relevant to the research topic of multi-modal knowledge graph reasoning.",
      "processing_time": 58.37281155586243,
      "citing_paper_id": "279243397",
      "cited_paper_id": 271161780
    },
    {
      "context_text": "Multi-Modal QA Apart from STaRK, we find related datasets for multi-modal QA/RAG including ProMQA [31], MultiModalQA [111] and SPIQA [92].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ProMQA",
        "MultiModalQA",
        "SPIQA"
      ],
      "dataset_descriptions": {
        "ProMQA": "Used for multimodal procedural activity understanding, focusing on question answering that integrates visual and textual information.",
        "MultiModalQA": "Applied to multimodal question answering, integrating various modalities to enhance reasoning and answer generation.",
        "SPIQA": "Utilized for multimodal question answering on scientific papers, combining text and images to improve understanding and accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets specifically designed for multi-modal QA/RAG, which are directly relevant to the research topic of multi-modal knowledge graph reasoning.",
      "processing_time": 58.37281155586243,
      "citing_paper_id": "279243397",
      "cited_paper_id": 273662191
    },
    {
      "context_text": "RAG Datasets Current benchmarks for RAG, such as RAGBench [19] and ChatRAG Bench [65], are a combination of existing QA datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RAGBench and ChatRAG Bench as benchmarks for RAG systems, which are combinations of existing QA datasets. However, these are benchmark suites, not specific, downloadable datasets.",
      "processing_time": 31.893142700195312,
      "citing_paper_id": "279243397",
      "cited_paper_id": 271218596
    },
    {
      "context_text": "The first two datasets either create questions combining multiple modalities [111] or involve them for reasoning tasks [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions datasets involving multiple modalities for reasoning tasks, but does not specify the names of the datasets. The context is too vague to confidently extract specific dataset names.",
      "processing_time": 30.775179624557495,
      "citing_paper_id": "279243397",
      "cited_paper_id": 273662191
    },
    {
      "context_text": "By describing a set of grading criteria, or rubric, in the prompt, frontier LLMs such as GPT [4, 40] and Claude [8] can provide assessments aligning with real annotators [22, 59, 60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about using LLMs for assessment, which does not involve a dataset.",
      "processing_time": 29.95259952545166,
      "citing_paper_id": "279243397",
      "cited_paper_id": 274234014
    },
    {
      "context_text": "Using LLMs to assess data quality ( LLM-as-a-judge ) [22, 59, 60] has emerged as a promising proxy for human evaluation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using LLMs to assess data quality. There are no verifiable resources or datasets mentioned.",
      "processing_time": 29.92232036590576,
      "citing_paper_id": "279243397",
      "cited_paper_id": 274234014
    },
    {
      "context_text": "Considering the complex and detailed nature of our queries, especially multi-hop ones, such question lengths are justified as being almost akin to medical case reports [109].",
      "catation_intent": "findings",
      "resource_type": "case report",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a case report, which is not a reusable dataset.",
      "processing_time": 29.15564227104187,
      "citing_paper_id": "279243397",
      "cited_paper_id": 278166534
    },
    {
      "context_text": "Finally, we do attempt to use Google‚Äôs Gemini 2.0 Flash [21].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method (Gemini 2.0). There are no verifiable resources or datasets mentioned.",
      "processing_time": 29.622753620147705,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "SMILES [93] is a popular notation used to represent chemical compounds in electronic databases.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a notation used in chemical databases. No verifiable resource is identified.",
      "processing_time": 27.74372363090515,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "After discovery, a potential drug compound is first registered in a medical database, such as PubChem 2 , and then goes through several layers of clinical trials to test for efficacy and ultimately marketability where it gets a generic name like, Fluoxetine [74, 104].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem"
      ],
      "dataset_descriptions": {
        "PubChem": "Mentioned as a medical database where drug compounds are registered, but not used as a dataset for the research described."
      },
      "confidence_score": 0.3,
      "reasoning": "The citation mentions PubChem as a medical database where drug compounds are registered, but does not indicate that it is used as a dataset for the research described.",
      "processing_time": 35.322782039642334,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "We provide a link to all system and user prompts [96] for our models here: https://github.com/ saptarshi059/biomolqa/tree/main/code/common_scripts .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a GitHub repository for system and user prompts. No verifiable datasets are referenced.",
      "processing_time": 27.21637272834778,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "Next, it creates embeddings (using all-MiniLM-L6-v2 [100]) for each relation in the database to locate relationships semantically similar to the one described by the question.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (all-MiniLM-L6-v2) which is excluded according to the rules.",
      "processing_time": 28.315147638320923,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "The query and triples are encoded using all-MiniLM-L6-v2 sentence embeddings [100] (other embedding models performed worse).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only an embedding model. No verifiable resources are identified.",
      "processing_time": 27.00694751739502,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "We use all-MiniLM-L6-v2 [100] as our query encoder.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (all-MiniLM-L6-v2) but does not refer to any specific dataset. The citation is used to reference a method or tool, not a dataset.",
      "processing_time": 31.68464469909668,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "Next, it looks for relations closest to the query using the same sentence encoder ( all-MiniLM-L6-v2 [100]) used to embed the relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model or method (all-MiniLM-L6-v2).",
      "processing_time": 27.008947134017944,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "Lozano et al. [68] builds an application for QA over PubMed [128] articles.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PubMed but does not specify it as a dataset. It is referenced as a source of articles for an application, not as a dataset for multi-modal knowledge graph reasoning.",
      "processing_time": 30.75713348388672,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "We also consider sourcing texts (abstracts) from the PubMed database [128].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubMed"
      ],
      "dataset_descriptions": {
        "PubMed": "Used to source abstracts for multi-modal reasoning, focusing on integrating textual information with other data types."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the PubMed database as a source of abstracts, which is a specific, verifiable dataset.",
      "processing_time": 33.58704614639282,
      "citing_paper_id": "279243397",
      "cited_paper_id": null
    },
    {
      "context_text": "[4] This involves knowledge reasoning oriented to knowledge graphs, and conducts the in-depth analysis and reasoning of equipment operation and maintenance multi-modal information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of multi-modal information in the context of equipment operation and maintenance.",
      "processing_time": 28.86846661567688,
      "citing_paper_id": "229028881",
      "cited_paper_id": 220599074
    },
    {
      "context_text": ", equipment management and maintenance work experience summary exchanges, various troubleshooting methods, [4] equipment maintenance cases, equipment operating procedures, and equipment operating data collected by various sensors).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'equipment operating data collected by various sensors', which could be considered a dataset. However, it lacks a specific, identifiable name and is too generic.",
      "processing_time": 30.540320873260498,
      "citing_paper_id": "229028881",
      "cited_paper_id": 220599074
    },
    {
      "context_text": "[1] Accordingly, equipment operation and maintenance is facing severe challenges, and massive, multi-source, heterogeneous equipment operation and maintenance big data should be integrated to achieve intelligent equipment operation and maintenance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about big data in equipment operation and maintenance.",
      "processing_time": 27.19457721710205,
      "citing_paper_id": "229028881",
      "cited_paper_id": 224888856
    },
    {
      "context_text": "Besides straightforward solutions, several works apply bilinear pooling [28, 8, 14] or more complex fusion methods [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided text.",
      "processing_time": 14.279812574386597,
      "citing_paper_id": "56895321",
      "cited_paper_id": 2209131
    },
    {
      "context_text": "Besides straightforward solutions, several works apply bilinear pooling [28, 8, 14] or more complex fusion methods [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided text.",
      "processing_time": 14.279812574386597,
      "citing_paper_id": "56895321",
      "cited_paper_id": null
    },
    {
      "context_text": "GCN model: In this model, a layer of traditional GCN [15] is added to replace the image modeling component in Figure 5.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GCN). The context is about using a GCN layer to replace an image modeling component, which does not involve a dataset.",
      "processing_time": 31.82861018180847,
      "citing_paper_id": "56895321",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "Given the graphs, Graph Convolutional Networks (GCN)[15] or Rela-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Graph Convolutional Networks).",
      "processing_time": 27.199617385864258,
      "citing_paper_id": "56895321",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "An upgraded Graph Convolutional Networks (GCN) [15] are then proposed to explore the directional and embedded visual relations for reasoning about more complex relations and enriching the object representations in the semantic graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Graph Convolutional Networks). The context focuses on the methodology and its application in reasoning about complex relations in semantic graphs.",
      "processing_time": 31.839417695999146,
      "citing_paper_id": "56895321",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "We follow the standard splitting of the dataset and use the tool provided by [2] to evaluate the accuracy.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context does not specify a dataset name, only mentions 'the dataset' without a clear identifier. The cited paper title 'VQA: Visual Question Answering' suggests a potential dataset, but it is not explicitly named in the citation span.",
      "processing_time": 33.14539813995361,
      "citing_paper_id": "56895321",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "[18] leverages attention mechanism to focus on essential image regions and words for correlation learning.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image-text matching using attention mechanisms.",
      "processing_time": 27.19327974319458,
      "citing_paper_id": "56895321",
      "cited_paper_id": 3994012
    },
    {
      "context_text": "Another research trend is based on deep learning[33, 30, 19, 18], leveraging existing techniques to provide rich semantics by nonlinear transformations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in deep learning. No verifiable resources are identified.",
      "processing_time": 28.844101190567017,
      "citing_paper_id": "56895321",
      "cited_paper_id": 3994012
    },
    {
      "context_text": "Later on, in [7], interactions (e.g. wear , carry ) between paired objects are exploited, visual relation reasoning is then formulated as a classiÔ¨Åcation task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for exploiting interactions between paired objects for visual relation reasoning.",
      "processing_time": 28.54442596435547,
      "citing_paper_id": "56895321",
      "cited_paper_id": 7748515
    },
    {
      "context_text": "Yang et al. [31] perform visual attention multiple times via stacked attention networks, and Anderson et al. [1] use a top-down attention on pre-detected salient regions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of attention mechanisms in neural networks.",
      "processing_time": 29.569392204284668,
      "citing_paper_id": "56895321",
      "cited_paper_id": 8849206
    },
    {
      "context_text": "‚Ä¶in Computer Vision (CV) and Natu-‚àó ral Language Processing (NLP), researchers make a further step towards breaking the boundary of vision and nat-ural language, such as visual question answering (VQA) [34, 31, 1] , cross-modal information retrieval (CMIR)[33, 30, 19], image captioning[32], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No verifiable resources are identified.",
      "processing_time": 28.540855646133423,
      "citing_paper_id": "56895321",
      "cited_paper_id": 8849206
    },
    {
      "context_text": "‚Ä¶in Computer Vision (CV) and Natu-‚àó ral Language Processing (NLP), researchers make a further step towards breaking the boundary of vision and nat-ural language, such as visual question answering (VQA) [34, 31, 1] , cross-modal information retrieval (CMIR)[33, 30, 19], image captioning[32], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. No verifiable resources are identified.",
      "processing_time": 28.540855646133423,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "Several statistical methods are based on Canonical Correlation Analysis (CCA) [25, 24] to maximize the pairwise correlations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only statistical methods based on CCA. There are no verifiable resources or datasets mentioned.",
      "processing_time": 29.32134771347046,
      "citing_paper_id": "56895321",
      "cited_paper_id": 10347107
    },
    {
      "context_text": "Inspired by recent works in Visual Relationship Detection [6, 13] and Scene Graph Generation [20], we train a relation classiÔ¨Åer to generate relationship embeddings on Visual Genome [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used to train a relation classifier for generating relationship embeddings, focusing on visual relationship detection and scene graph generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Visual Genome' as a dataset used for training a relation classifier to generate relationship embeddings, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 35.41330528259277,
      "citing_paper_id": "56895321",
      "cited_paper_id": 21277943
    },
    {
      "context_text": "SpeciÔ¨Åcally, there are only connections be-tween two object nodes when the relation detection trained on Visual Genome [32] predicts existing relations, regardless of the relation types.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used to train a relation detection model for predicting existing relations between object nodes in a knowledge graph, enhancing multi-modal reasoning capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Visual Genome' as a dataset used for training a relation detection model. The dataset is used to predict existing relations between object nodes in a knowledge graph.",
      "processing_time": 36.13963055610657,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "[32] treats visual relations as labeled directional edges between two object nodes in the spatial and semantic graph and apply Graph Convolutional Networks(GCN) to reason about their implicit relations for image captioning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It focuses on the method of using visual relationships and GCNs for image captioning.",
      "processing_time": 30.491920232772827,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "Compared with previous work of relation detection [32], our proposed relation embedding can be more effective and accurate to model the Ô¨Åne-grained semantics inherent in the object , subject and their interaction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with previous work on relation detection. No verifiable resources are identified.",
      "processing_time": 29.579522132873535,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "State-of-the-art works have proved that reasoning visual relations is crucial to improve the performance of VQA [27] and image captioning [32].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that visual relationship reasoning improves VQA and image captioning.",
      "processing_time": 28.52638840675354,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "Although [32] makes relation reasoning sensitive to the relation types, it only applies different biases for ‚Äúrigid-categorized‚Äù relation types and ignores the inÔ¨Çu-ence of the connected objects in the reasoning process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the limitations of the cited work regarding relation reasoning.",
      "processing_time": 30.49698257446289,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "Previous work [32] primarily studies appearance-based models to detect visual relations categorically ‚àí they learn the relations as a classiÔ¨Åcation task and output the rigid-divided category for each predict as the corresponding relation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to visual relationship detection. No verifiable resources are identified.",
      "processing_time": 29.549100637435913,
      "citing_paper_id": "56895321",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "On the other hand, we evaluated a range of classical GNN architectures: Graph Convolutional Networks (GCN) [46], Graph Attention Networks (GAT) [47], Graph Isomorphism Networks (GIN) [48], and Relational Graph Convolutional Networks (R-GCN) [49],to explore their performance when combined with‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various graph neural network architectures. The context is focused on evaluating these architectures.",
      "processing_time": 29.842409133911133,
      "citing_paper_id": "279999373",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "On the other hand, we evaluated a range of classical GNN architectures: Graph Convolutional Networks (GCN) [46], Graph Attention Networks (GAT) [47], Graph Isomorphism Networks (GIN) [48], and Relational Graph Convolutional Networks (R-GCN) [49],to explore their performance when combined with‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various graph neural network architectures. The context is focused on evaluating these architectures.",
      "processing_time": 29.842409133911133,
      "citing_paper_id": "279999373",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "However, interpreting spectral data in isolation frequently yields ambiguous results due to overlapping spectral features, limited dataset sizes, and insufficient contextual information [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses general issues with interpreting spectral data, which is not sufficient to identify a specific dataset.",
      "processing_time": 18.04132652282715,
      "citing_paper_id": "279999373",
      "cited_paper_id": 3593775
    },
    {
      "context_text": "It involves techniques such as near-infrared (NIR), mid-infrared (MIR), Raman, and ultraviolet-visible (UV-Vis) spectroscopy, each providing valuable insights into molecular vibrations, electronic transitions, and absorption characteristics at microscopic scales [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only spectroscopic techniques. No clear identifiers for datasets are present.",
      "processing_time": 27.425868034362793,
      "citing_paper_id": "279999373",
      "cited_paper_id": 3593775
    },
    {
      "context_text": "‚Ä¶(GCN) [46], Graph Attention Networks (GAT) [47], Graph Isomorphism Networks (GIN) [48], and Relational Graph Convolutional Networks (R-GCN) [49],to explore their performance when combined with different LLMs and identify the most effective pairing for optimal classification accuracy within‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various graph neural network models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 30.8797128200531,
      "citing_paper_id": "279999373",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Spectral analysis serves as a fundamental tool in numerous scientific and industrial domains, including material science, chemical analysis, biomedical diagnostics, food safety, and environmental monitoring [1, 2, 3, 4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to spectral analysis as a tool in various domains without naming any particular datasets.",
      "processing_time": 30.877238988876343,
      "citing_paper_id": "279999373",
      "cited_paper_id": 9697423
    },
    {
      "context_text": "Spectral analysis serves as a fundamental tool in numerous scientific and industrial domains, including material science, chemical analysis, biomedical diagnostics, food safety, and environmental monitoring [1, 2, 3, 4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to spectral analysis as a tool in various domains without naming any particular datasets.",
      "processing_time": 30.877238988876343,
      "citing_paper_id": "279999373",
      "cited_paper_id": 95861012
    },
    {
      "context_text": "Spectral analysis serves as a fundamental tool in numerous scientific and industrial domains, including material science, chemical analysis, biomedical diagnostics, food safety, and environmental monitoring [1, 2, 3, 4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to spectral analysis as a tool in various domains without naming any particular datasets.",
      "processing_time": 30.877238988876343,
      "citing_paper_id": "279999373",
      "cited_paper_id": 221364718
    },
    {
      "context_text": "Spectral analysis serves as a fundamental tool in numerous scientific and industrial domains, including material science, chemical analysis, biomedical diagnostics, food safety, and environmental monitoring [1, 2, 3, 4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to spectral analysis as a tool in various domains without naming any particular datasets.",
      "processing_time": 30.877238988876343,
      "citing_paper_id": "279999373",
      "cited_paper_id": 266412890
    },
    {
      "context_text": "As a baseline for comparison, we also included the traditional Bag-of-Words (BoW) encoding method [45].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Bag-of-Words).",
      "processing_time": 27.687552452087402,
      "citing_paper_id": "279999373",
      "cited_paper_id": 12255087
    },
    {
      "context_text": "Multi-modal analysis, which integrates data from diverse modalities such as images, text, graphs, and structured domain knowledge, has emerged as a powerful methodology to overcome the inherent limitations of single-modality data analysis [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the concept of multi-modal analysis and its benefits.",
      "processing_time": 29.827189683914185,
      "citing_paper_id": "279999373",
      "cited_paper_id": 26071662
    },
    {
      "context_text": "Multi-modal analysis, which integrates data from diverse modalities such as images, text, graphs, and structured domain knowledge, has emerged as a powerful methodology to overcome the inherent limitations of single-modality data analysis [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the concept of multi-modal analysis and its benefits.",
      "processing_time": 29.827189683914185,
      "citing_paper_id": "279999373",
      "cited_paper_id": 212748233
    },
    {
      "context_text": "‚Ä¶of classical GNN architectures: Graph Convolutional Networks (GCN) [46], Graph Attention Networks (GAT) [47], Graph Isomorphism Networks (GIN) [48], and Relational Graph Convolutional Networks (R-GCN) [49],to explore their performance when combined with different LLMs and identify the most‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various graph neural network architectures. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.12672185897827,
      "citing_paper_id": "279999373",
      "cited_paper_id": 52895589
    },
    {
      "context_text": "In 2019, Zhou et al.[20] proposed a unique method for learning latent representations, which can handle multi-modal data, such as MRI, PET, and genetic information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions multi-modal data types (MRI, PET, genetic information) but does not specify a dataset name. The context focuses on the method rather than a specific dataset.",
      "processing_time": 31.12427043914795,
      "citing_paper_id": "279999373",
      "cited_paper_id": 133608068
    },
    {
      "context_text": "For instance, Guo et al.[19] provided a comprehensive survey on deep multimodal representation learning, highlighting its capacity to model complex interactions across modalities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey on deep multimodal representation learning.",
      "processing_time": 28.227643966674805,
      "citing_paper_id": "279999373",
      "cited_paper_id": 169032532
    },
    {
      "context_text": "The selected models include: (1) BERT [40], with a vector dimension of 1 √ó 768 and 110M parameters; (2) Sentence Transformer (ST) [41], with a vector dimension of 1 √ó 768 and 70M parameters; (3) e5 [42], with a vector dimension of 1√ó1024 and 330M parameters; (4) LLaMa2-7b [43], with a vector‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on describing the models and their parameters rather than datasets.",
      "processing_time": 31.130680322647095,
      "citing_paper_id": "279999373",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "The selected models include: (1) BERT [40], with a vector dimension of 1 √ó 768 and 110M parameters; (2) Sentence Transformer (ST) [41], with a vector dimension of 1 √ó 768 and 70M parameters; (3) e5 [42], with a vector dimension of 1√ó1024 and 330M parameters; (4) LLaMa2-7b [43], with a vector‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on describing the models and their parameters rather than datasets.",
      "processing_time": 31.130680322647095,
      "citing_paper_id": "279999373",
      "cited_paper_id": 231963511
    },
    {
      "context_text": "To evaluate this, we compared two prompt styles(Figuer 7): ‚Ä¢ Detailed Prompts : Multi-sentence natural language descriptions (~6‚Äì7 sentences), offering rich task context.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only prompt styles used in an evaluation. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 30.87023639678955,
      "citing_paper_id": "279999373",
      "cited_paper_id": 212748233
    },
    {
      "context_text": "In chemical analysis, they are widely used to link compounds with elements, diseases, and drugs [34, 35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to linking compounds with elements, diseases, and drugs. No verifiable resource names are provided.",
      "processing_time": 30.672817707061768,
      "citing_paper_id": "279999373",
      "cited_paper_id": 231981018
    },
    {
      "context_text": "In chemical analysis, they are widely used to link compounds with elements, diseases, and drugs [34, 35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to linking compounds with elements, diseases, and drugs. No verifiable resource names are provided.",
      "processing_time": 30.672817707061768,
      "citing_paper_id": "279999373",
      "cited_paper_id": null
    },
    {
      "context_text": "Deep learning approaches have been instrumental in this regard[18], offering robust frameworks for capturing intricate relationships between different data types.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general advancements in deep learning. No verifiable resources are identified.",
      "processing_time": 29.501925230026245,
      "citing_paper_id": "279999373",
      "cited_paper_id": 235166820
    },
    {
      "context_text": "On the other hand, Mao et al.[21] introduced a novel approach using an attention-based multi-modal framework to generate a joint speech and text representation,.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the introduction of a novel multi-modal framework.",
      "processing_time": 30.863375425338745,
      "citing_paper_id": "279999373",
      "cited_paper_id": 247115730
    },
    {
      "context_text": "More recently, Zhou et al.[23] explored the benefits of pre-training BERT on a cancer-specific dataset to extract breast cancer phenotypes from pathology reports and clinical records.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "cancer-specific dataset"
      ],
      "dataset_descriptions": {
        "cancer-specific dataset": "Used to pre-train BERT for extracting breast cancer phenotypes from pathology reports and clinical records, focusing on improving the accuracy of phenotype extraction in a cancer domain."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions a cancer-specific dataset used for pre-training BERT to extract breast cancer phenotypes from pathology reports and clinical records.",
      "processing_time": 37.10047388076782,
      "citing_paper_id": "279999373",
      "cited_paper_id": 247677346
    },
    {
      "context_text": "‚Ä¶vector dimension of 1 √ó 768 and 110M parameters; (2) Sentence Transformer (ST) [41], with a vector dimension of 1 √ó 768 and 70M parameters; (3) e5 [42], with a vector dimension of 1√ó1024 and 330M parameters; (4) LLaMa2-7b [43], with a vector dimension of 1 √ó 4096 and 7 billion parameters; and (5)‚Ä¶",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their parameters. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.36068630218506,
      "citing_paper_id": "279999373",
      "cited_paper_id": 254366618
    },
    {
      "context_text": "For instance, Agbavor et al.[22] leveraged the deep representation features of Data2Vec and Wav2Vec2-classical semantic embeddings introduced in 2019-and refined an end-to-end model with fully connected layers to enhance Alzheimer‚Äôs disease detection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the use of deep representation features and semantic embeddings for Alzheimer's disease detection.",
      "processing_time": 32.39113998413086,
      "citing_paper_id": "279999373",
      "cited_paper_id": 255083414
    },
    {
      "context_text": "An example is the work by Wang et al.[25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide specific details about datasets used in the research. It only references a work by Wang et al. without mentioning any datasets.",
      "processing_time": 31.764050722122192,
      "citing_paper_id": "279999373",
      "cited_paper_id": 256549502
    },
    {
      "context_text": "Recently, Large Language Models (LLMs), including GPT-4, PaLM, and BERT, have demonstrated exceptional capabilities in multi-modal reasoning, generalization, and adaptability, significantly advancing fields beyond traditional natural language processing (NLP) [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.737006664276123,
      "citing_paper_id": "279999373",
      "cited_paper_id": 257039063
    },
    {
      "context_text": "Recently, Large Language Models (LLMs), including GPT-4, PaLM, and BERT, have demonstrated exceptional capabilities in multi-modal reasoning, generalization, and adaptability, significantly advancing fields beyond traditional natural language processing (NLP) [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.737006664276123,
      "citing_paper_id": "279999373",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Recently, Large Language Models (LLMs), including GPT-4, PaLM, and BERT, have demonstrated exceptional capabilities in multi-modal reasoning, generalization, and adaptability, significantly advancing fields beyond traditional natural language processing (NLP) [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.737006664276123,
      "citing_paper_id": "279999373",
      "cited_paper_id": 271334161
    },
    {
      "context_text": "Recently, Large Language Models (LLMs), including GPT-4, PaLM, and BERT, have demonstrated exceptional capabilities in multi-modal reasoning, generalization, and adaptability, significantly advancing fields beyond traditional natural language processing (NLP) [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.737006664276123,
      "citing_paper_id": "279999373",
      "cited_paper_id": null
    },
    {
      "context_text": "By leveraging complementary features across modalities, multi-modal techniques offer enhanced analytical capabilities, improved interpretability, and superior generalization performance [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the benefits of multi-modal techniques.",
      "processing_time": 29.01787495613098,
      "citing_paper_id": "279999373",
      "cited_paper_id": 257122091
    },
    {
      "context_text": "‚Ä¶dimension of 1 √ó 768 and 110M parameters; (2) Sentence Transformer (ST) [41], with a vector dimension of 1 √ó 768 and 70M parameters; (3) e5 [42], with a vector dimension of 1√ó1024 and 330M parameters; (4) LLaMa2-7b [43], with a vector dimension of 1 √ó 4096 and 7 billion parameters; and (5) Qwen2-0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their parameters. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 15.982421159744263,
      "citing_paper_id": "279999373",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "ChatGPT, as explored in Zhou et al.[29], demonstrates remarkable flexibility across a wide range of tasks without extensive retraining, owing to its ability to generalize knowledge from pre-training.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of ChatGPT. There are no verifiable resources or datasets mentioned.",
      "processing_time": 31.576374053955078,
      "citing_paper_id": "279999373",
      "cited_paper_id": 257326550
    },
    {
      "context_text": "Trained on vast corpora encompassing a wide spectrum of textual and contextual information, LLMs are particularly adept at encoding semantic relationships, reasoning over structured data, and generating coherent, interpretable insights [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 31.735413074493408,
      "citing_paper_id": "279999373",
      "cited_paper_id": 257378479
    },
    {
      "context_text": "Building on the comprehensive graph by Fang et al. [36], which includes elements, functional groups, and chemical properties, we further incorporate vibrational modes and absorption data to construct a spectral-relevant knowledge base.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions incorporating additional data into a knowledge graph, but does not specify a named dataset. The reference to Fang et al. [36] suggests a knowledge base, but it is not a verifiable, named dataset.",
      "processing_time": 34.287004709243774,
      "citing_paper_id": "279999373",
      "cited_paper_id": 258519932
    },
    {
      "context_text": "In spectral research, a novel multi-stage active transfer learning framework[28] has been proposed for near-infrared, integrating a domain shift method and using in the pharmaceutical, chemical, and food industries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method and its application areas. The cited paper title confirms the focus on a method rather than a dataset.",
      "processing_time": 32.72052574157715,
      "citing_paper_id": "279999373",
      "cited_paper_id": 258573462
    },
    {
      "context_text": "Prior studies have primarily applied multi-modal spectral analysis to the classification and analysis of complex materials, utilizing combinations of Fourier Transform Infrared (FTIR), Raman, and Laser-induced Breakdown Spectroscopy (LIBS) data [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to methods and techniques rather than datasets.",
      "processing_time": 29.808600187301636,
      "citing_paper_id": "279999373",
      "cited_paper_id": 259692010
    },
    {
      "context_text": "For example, in the field of biomedicine, where tasks often require models to adapt to dynamic and diverse data sources, Tu et al.[27] have developed generalist biomedical AI systems capable of interpreting multiple data modalities, such as clinical language, imaging, and genomics, within a unified‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the development of a generalist biomedical AI system. No clear, verifiable datasets are referenced.",
      "processing_time": 32.56302332878113,
      "citing_paper_id": "279999373",
      "cited_paper_id": 260164663
    },
    {
      "context_text": "In the context of near-infrared (NIR) spectroscopy, a novel interpretable ensemble learning method has been developed to enhance the rapid characterization of petroleum products[26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a novel method for NIR spectroscopy. No verifiable resources are identified.",
      "processing_time": 31.090595722198486,
      "citing_paper_id": "279999373",
      "cited_paper_id": 260722930
    },
    {
      "context_text": "A key example is the work by Li et al.[24], which utilized a clinical knowledge graph to enhance diagnostic accuracy in cancer patients by integrating patient records, treatment regimens, and disease pathways.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a clinical knowledge graph used to enhance diagnostic accuracy in cancer patients, but does not specify a named dataset. The focus is on the method and findings.",
      "processing_time": 33.05516195297241,
      "citing_paper_id": "279999373",
      "cited_paper_id": 263777406
    },
    {
      "context_text": "These models exhibit remarkable proficiency in understanding and generating human-like text, processing diverse data modalities, and extracting complex patterns from extensive datasets [15, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of models. No verifiable resources are identified.",
      "processing_time": 32.17386484146118,
      "citing_paper_id": "279999373",
      "cited_paper_id": 264172683
    },
    {
      "context_text": "These models exhibit remarkable proficiency in understanding and generating human-like text, processing diverse data modalities, and extracting complex patterns from extensive datasets [15, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of models. No verifiable resources are identified.",
      "processing_time": 32.17386484146118,
      "citing_paper_id": "279999373",
      "cited_paper_id": 267938234
    },
    {
      "context_text": "Given a natural language input, a fine-tuned LLM extracts the molecular formula, which is encoded and propagated through the task graph to produce a response node embedding ùê° ùë£ ùëü .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting molecular formulas using a fine-tuned LLM.",
      "processing_time": 32.558701276779175,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265189794
    },
    {
      "context_text": "We evaluated four classification tasks using two LLM-GNN configurations: ST+R-GCN and BERT+R-GCN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model configurations and tasks. There are no verifiable resources or datasets mentioned.",
      "processing_time": 33.04788255691528,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "After embedding, a GNN is employed to process the entire graph and produce analytical results.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general method involving GNNs. No verifiable resources are identified.",
      "processing_time": 33.94409966468811,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "While training separate GNNs for each task is intuitive, it is often impractical in real-world scenarios due to the difficulty of collecting large-scale data for a single task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the challenge of collecting large-scale data for a single task.",
      "processing_time": 33.45055437088013,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "The GNN was fixed to R-GCN for consistency, while ST and BERT were selected as moderate baselines.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 34.620569705963135,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "However, instability across tasks and weak node-level performance highlight current limitations, primarily due to the modest capacity of selected LLMs and GNNs‚Äô inability to fully leverage semantic embeddings.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of models and methods.",
      "processing_time": 34.110917806625366,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "The GNN aggregates and propagates relevant information within Óà≥ ùë°ùëéùë†ùëò , and after multiple network layers, the \"Response Node\" output the final classification results of the framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GNN) and its application. There are no clear identifiers for datasets in the text.",
      "processing_time": 35.616939544677734,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "Since each \"Prompt Node\" links to only one \"Absorption Node,\" the GNN‚Äôs receptive field remains narrow.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological aspect of graph neural networks.",
      "processing_time": 35.733736753463745,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "This decoding process is formulated as: Here, Óà¥ decoder denotes the output feature matrix containing hidden representations of all nodes in the task graph after multiple GNN layers.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of a decoding process in a graph neural network model.",
      "processing_time": 36.674078702926636,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "For GNN comparison, we fixed LLaMa2-7b as the encoder and evaluated different architectures: ‚Ä¢ LLaMa2+R-GCN: 364.36 R-GCN outperformed other GNNs, likely due to its ability to model relational edges and apply attention mechanisms, yielding richer graph representations and more consistent results across tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures and their performance. The context focuses on comparing GNN architectures using a fixed encoder (LLaMa2-7b).",
      "processing_time": 38.612674713134766,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "However, performance in node and graph-level tasks remained below threshold, suggesting that the LLMs‚Äô limited parameter size and the GNN‚Äôs coarse aggregation hinder deeper semantic reasoning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance of models in certain tasks.",
      "processing_time": 14.156835079193115,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "Finally, a GNN processes the entire graph structure to generate the desired analytical outputs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general process involving a Graph Neural Network (GNN). There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 17.521689891815186,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "On the other hand, we evaluated a range of classical GNN architectures: Graph Convolutional Networks (GCN) [46], Graph Attention Networks (GAT) [47], Graph Isomorphism Networks (GIN) [48], and Relational Graph Convolutional Networks (R-GCN) [49],to explore their performance when combined with different LLMs and identify the most effective pairing for optimal classification accuracy within our framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various graph neural network architectures. The context focuses on evaluating these architectures with LLMs.",
      "processing_time": 17.53847885131836,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "After passing the task graph Óà≥ Task through a multi-layer GNN [31], the final representation ùê° ùë£ ùëü of the response node is obtained via an attention-based layer aggregation mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (multi-layer GNN) and a process (attention-based layer aggregation).",
      "processing_time": 28.1498806476593,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "To assess overall performance across tasks, we fixed the GNN as R-GCN and aggregated scores (Accuracy + AUC*100) for each LLM: ‚Ä¢ LLaMa2-7b+R-GCN: 364.36 ‚Ä¢ ST+R-GCN: 359.78 ‚Ä¢ BERT+R-GCN: 358.30 ‚Ä¢ e5+R-GCN: 359.50 ‚Ä¢ Qwen+R-GCN: 343.96 Despite differing metrics (Acc vs. AUC), all are scaled to a max of 100, making summation a reasonable approximation.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and metrics. The focus is on comparing the performance of different LLMs combined with R-GCN.",
      "processing_time": 15.18668007850647,
      "citing_paper_id": "279999373",
      "cited_paper_id": 265871676
    },
    {
      "context_text": "We use the SDAAP dataset[33], a structured literature corpus on spectral analysis.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SDAAP"
      ],
      "dataset_descriptions": {
        "SDAAP": "Used as a structured literature corpus on spectral analysis to enhance spectral knowledge interrogation, focusing on reliable retrieval-augmented generative frameworks in large language models."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the SDAAP dataset, which is a structured literature corpus on spectral analysis. This fits the criteria for a dataset and is relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 24.503891944885254,
      "citing_paper_id": "279999373",
      "cited_paper_id": 271916060
    },
    {
      "context_text": "5b [44], with a vector dimension of 1 √ó 4096 and 550M parameters.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model parameters. There are no verifiable resources or datasets mentioned.",
      "processing_time": 13.66361117362976,
      "citing_paper_id": "279999373",
      "cited_paper_id": 274859421
    },
    {
      "context_text": "‚Ä¢ Spectral Data : Sourced from SpectraPlot [37], which simulates high-precision IR spectra using databases like NIST ASD, HITRAN2012, and HITEMP2010.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NIST ASD",
        "HITRAN2012",
        "HITEMP2010"
      ],
      "dataset_descriptions": {
        "NIST ASD": "Used to simulate high-precision IR spectra, providing atomic and molecular spectral data for the research.",
        "HITRAN2012": "Used to simulate high-precision IR spectra, offering comprehensive molecular spectroscopic data for the research.",
        "HITEMP2010": "Used to simulate high-precision IR spectra, providing high-temperature molecular spectroscopic data for the research."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'databases like NIST ASD, HITRAN2012, and HITEMP2010' which are specific, verifiable resources. These databases are used to simulate high-precision IR spectra.",
      "processing_time": 36.414695501327515,
      "citing_paper_id": "279999373",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¢ Functional Group Infomation : Extracted from Principles of Instrumental Analysis [39], this includes vibrational modes (e.g., stretching, bending), characteristic absorption wavenumbers, and absorbance intensities.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to information extracted from a book, which is not a dataset.",
      "processing_time": 13.31852674484253,
      "citing_paper_id": "279999373",
      "cited_paper_id": null
    },
    {
      "context_text": "Current approaches primarily focus on integrating multiple spectral modalities or limited structured data, neglecting broader chemical textual knowledge and comprehensive knowledge graphs [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and limitations. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 15.532935619354248,
      "citing_paper_id": "279999373",
      "cited_paper_id": null
    },
    {
      "context_text": "Concur-rently, graph-based approaches have gained attention due to their ability to explicitly model relationships between spectral features and relevant chemical or biomedical properties, further enhancing predictive accuracy and interpretability [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 12.777652263641357,
      "citing_paper_id": "279999373",
      "cited_paper_id": null
    },
    {
      "context_text": "Generally, our idea is to inject dual learning [13] into adversarial representation learning [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and concepts. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 17.245749473571777,
      "citing_paper_id": "239011700",
      "cited_paper_id": 1082740
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 1560943
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 8770925
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 9492646
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 10347107
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 122166830
    },
    {
      "context_text": "Traditional approaches for CMR mainly based on linear model to generate a lower-dimensional common space with handcrafted or learnable features [4, 6, 10, 12, 14, 28, 30, 31, 33] for multimedia data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to multimedia data and linear models. No verifiable resources are identified.",
      "processing_time": 17.24507212638855,
      "citing_paper_id": "239011700",
      "cited_paper_id": 255105361
    },
    {
      "context_text": "Traditional methods tend to tackle this issue by employing the statistical correlation analysis for learning the common sub-space, e.g., canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.574782371520996,
      "citing_paper_id": "239011700",
      "cited_paper_id": 1560943
    },
    {
      "context_text": "Traditional methods tend to tackle this issue by employing the statistical correlation analysis for learning the common sub-space, e.g., canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.574782371520996,
      "citing_paper_id": "239011700",
      "cited_paper_id": 10347107
    },
    {
      "context_text": "To better learn parameters of GDL, including fŒ¶2 , fŒ®2 , DŒ¥ and DœÅ , we draw inspiration from the expectation‚Äìmaximization (EM) algorithm [7], and fix some parameters and learn the others in an interactive manner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EM algorithm) for parameter learning. No verifiable resources are referenced.",
      "processing_time": 13.661298036575317,
      "citing_paper_id": "239011700",
      "cited_paper_id": 4193919
    },
    {
      "context_text": "Pascal Sentence [27] contains 1, 000 images, each of which is annotated with five English sentences describing its content.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal Sentence"
      ],
      "dataset_descriptions": {
        "Pascal Sentence": "Used to collect image annotations with five English sentences per image, focusing on content description using Amazon‚Äôs Mechanical Turk."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Pascal Sentence' as a specific dataset with a clear structure and purpose, making it a verifiable resource.",
      "processing_time": 21.736899852752686,
      "citing_paper_id": "239011700",
      "cited_paper_id": 5583509
    },
    {
      "context_text": "Figure 3: The visualisation for the test data on Pascal Sentence [27] by using the t-SNE method [34].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal Sentence"
      ],
      "dataset_descriptions": {
        "Pascal Sentence": "Used to visualize test data, specifically applying the t-SNE method to explore data structure and patterns."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'Pascal Sentence' which appears to be a dataset used for visualization. However, the title does not help in disambiguating it further.",
      "processing_time": 22.232450485229492,
      "citing_paper_id": "239011700",
      "cited_paper_id": 5583509
    },
    {
      "context_text": "To further study the effectiveness of our novel designs, we use t-SNE tool to visualize the distribution of the generated features on Pascal Sentence [27].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal Sentence"
      ],
      "dataset_descriptions": {
        "Pascal Sentence": "Used to visualize the distribution of generated features using t-SNE, focusing on the effectiveness of novel designs in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Pascal Sentence' which appears to be a specific dataset. However, the cited paper title does not help in confirming this as a dataset. The context is minimal and does not provide details on usage.",
      "processing_time": 24.3086838722229,
      "citing_paper_id": "239011700",
      "cited_paper_id": 5583509
    },
    {
      "context_text": ", canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.396884202957153,
      "citing_paper_id": "239011700",
      "cited_paper_id": 8770925
    },
    {
      "context_text": ", canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.396884202957153,
      "citing_paper_id": "239011700",
      "cited_paper_id": 9492646
    },
    {
      "context_text": ", canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.396884202957153,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": ", canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.396884202957153,
      "citing_paper_id": "239011700",
      "cited_paper_id": 122166830
    },
    {
      "context_text": ", canonical correlation analysis [6, 10, 14, 28], partial least squares [4, 30] or bilinear model [31, 33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 14.396884202957153,
      "citing_paper_id": "239011700",
      "cited_paper_id": 255105361
    },
    {
      "context_text": "Similarly, the given text data T is converted to a real-valued vector v as the domain-specific features: v = fŒ®1 (T), where the mapping function fŒ®1 (T) can be either a Text-CNN [19, 48] or Bag of Words (BoW).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Text-CNN and BoW).",
      "processing_time": 11.974456071853638,
      "citing_paper_id": "239011700",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Similarly, the given text data T is converted to a real-valued vector ùë£ as the domain-specific features: ùë£ = ùëì Œ® 1 ( T ) , where the mapping function ùëì Œ® 1 ( T ) can be either a Text-CNN [19, 48] or Bag of Words (BoW).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Text-CNN and BoW).",
      "processing_time": 25.605311155319214,
      "citing_paper_id": "239011700",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Recent deep learning based methods[9, 15, 22, 23, 25, 40, 48] transform data of multiple sources to the common space with end-to-end learnable projections, yielding a more powerful, highly non-linear solution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches. There are no clear identifiers for datasets in the text.",
      "processing_time": 13.647536516189575,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "To better leverage the intra-and inter-modality relevances, Peng et al. [23] deep networks (CMDN) to learn the common representation by hierarchical learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CMDN) for learning common representations across modalities.",
      "processing_time": 14.861668586730957,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Wecompareourmethodwith10state-of-the-art(SOTA)approaches, including2traditionalcross-modalretrievalmethods,namelyCCA [12]andJRL[46],and8deeplearningmodels,namelyCorr-AE[9], DCCA[2],CMDN[23],Deep-SM[37],DSCMR[48],ACMR[35], CM-GANs[24]andMHTN[17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not reference any specific datasets. The context is focused on comparing various cross-modal retrieval methods and deep learning models.",
      "processing_time": 18.93759536743164,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Wecompareourmethodwith10state-of-the-art(SOTA)approaches, including2traditionalcross-modalretrievalmethods,namelyCCA [12]andJRL[46],and8deeplearningmodels,namelyCorr-AE[9], DCCA[2],CMDN[23],Deep-SM[37],DSCMR[48],ACMR[35], CM-GANs[24]andMHTN[17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not reference any specific datasets. The context is focused on comparing various cross-modal retrieval methods and deep learning models.",
      "processing_time": 18.93759536743164,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "Unlike prior arts [15, 22, 23, 25, 48] that explore crossmodality semantic relations within individual image-text pair to learn the common representation, our idea is to leverage the structural semantics within the data to learn semantic discriminative yet modality-invariant representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'data'. No clear, verifiable resource names are provided.",
      "processing_time": 13.627180576324463,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Image2Text Text2Image Average CCA [12] 0.159 0.189 0.174 CCA ‚Ä† [12] 0.258 0.250 0.254 Corr-AE [9] 0.306 0.340 0.323 Corr-AE ‚Ä† [9] 0.402 0.395 0.399 Deep-SM [37] 0.389 0.496 0.443 CMDN [23] 0.410 0.450 0.430 CMDN ‚Ä† [23] 0.492 0.515 0.504 DSCMR [48] 0.512 0.527 0.520 DSCMR ‚Ä† [48] 0.552 0.542 0.547 ACMR [35] 0.445 0.473 0.459 ACMR ‚Ä† [35] 0.544 0.538 0.541 MHTN [17] 0.520 0.534 0.527 MHTN ‚Ä† [17] 0 Performance on XMedia: We also compare our GCR with SOTAs on XMedia.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models used for cross-modal retrieval. The cited papers do not provide additional information about datasets.",
      "processing_time": 17.879923105239868,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Image2Text Text2Image Average CCA [12] 0.159 0.189 0.174 CCA ‚Ä† [12] 0.258 0.250 0.254 Corr-AE [9] 0.306 0.340 0.323 Corr-AE ‚Ä† [9] 0.402 0.395 0.399 Deep-SM [37] 0.389 0.496 0.443 CMDN [23] 0.410 0.450 0.430 CMDN ‚Ä† [23] 0.492 0.515 0.504 DSCMR [48] 0.512 0.527 0.520 DSCMR ‚Ä† [48] 0.552 0.542 0.547 ACMR [35] 0.445 0.473 0.459 ACMR ‚Ä† [35] 0.544 0.538 0.541 MHTN [17] 0.520 0.534 0.527 MHTN ‚Ä† [17] 0 Performance on XMedia: We also compare our GCR with SOTAs on XMedia.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models used for cross-modal retrieval. The cited papers do not provide additional information about datasets.",
      "processing_time": 17.879923105239868,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "[23] proposed cross-modal multiple Poster Session 5 MM ‚Äô21, October 20‚Äì24, 2021, Virtual Event, China",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.220629453659058,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Image2Text Text2Image Average CCA [12] 0.257 0.341 0.299 JRL [46] 0.770 0.800 0.785 Corr-AE [9] 0.450 0.437 0.443 DCCA [2] 0.472 0.466 0.469 Deep-SM [37] 0.822 0.807 0.814 CMDN [23] 0.794 0.805 0.799 DSCMR [48] 0.865 0.853 0.859 ACMR [35] 0.704 0.710 0.707 MHTN [17] 0 [46] 0.527 0.534 0.531 Corr-AE [9] 0.532 0.521 0.527 DCCA [2] 0.678 0.677 0.678 Deep-SM [37] 0.560 0.539 0.550 CMDN [23] 0.544 0.526 0.535 DSCMR [48] 0.710 0.722 0.716 ACMR [35] 0.671 0.676 0.673 CM-GANs [24] 0.603 0.604 0.604 GCR (Ours) 0.726 0.730 0.728 Thus, once trained, they can more reliably project the raw image or text to the common feature space for retrieval across modalities.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and their performance metrics. No clear identifiers for datasets are present.",
      "processing_time": 15.140506029129028,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Image2Text Text2Image Average CCA [12] 0.257 0.341 0.299 JRL [46] 0.770 0.800 0.785 Corr-AE [9] 0.450 0.437 0.443 DCCA [2] 0.472 0.466 0.469 Deep-SM [37] 0.822 0.807 0.814 CMDN [23] 0.794 0.805 0.799 DSCMR [48] 0.865 0.853 0.859 ACMR [35] 0.704 0.710 0.707 MHTN [17] 0 [46] 0.527 0.534 0.531 Corr-AE [9] 0.532 0.521 0.527 DCCA [2] 0.678 0.677 0.678 Deep-SM [37] 0.560 0.539 0.550 CMDN [23] 0.544 0.526 0.535 DSCMR [48] 0.710 0.722 0.716 ACMR [35] 0.671 0.676 0.673 CM-GANs [24] 0.603 0.604 0.604 GCR (Ours) 0.726 0.730 0.728 Thus, once trained, they can more reliably project the raw image or text to the common feature space for retrieval across modalities.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and their performance metrics. No clear identifiers for datasets are present.",
      "processing_time": 15.140506029129028,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "Recently, deep neural network (DNN) based approaches learn the common representations with end-to-end learnable projections [9, 15, 22, 23, 25, 40, 48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to deep neural network approaches. No verifiable resources are identified.",
      "processing_time": 13.2788405418396,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "We compare ourmethodwith 10 state-of-the-art (SOTA) approaches, including 2 traditional cross-modal retrieval methods, namely CCA [12] and JRL [46], and 8 deep learning models, namely Corr-AE [9], DCCA [2], CMDN [23], Deep-SM [37] , DSCMR [48], ACMR [35], CM-GANs [24] and MHTN [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. Therefore, no datasets are extracted.",
      "processing_time": 15.879089832305908,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "We compare ourmethodwith 10 state-of-the-art (SOTA) approaches, including 2 traditional cross-modal retrieval methods, namely CCA [12] and JRL [46], and 8 deep learning models, namely Corr-AE [9], DCCA [2], CMDN [23], Deep-SM [37] , DSCMR [48], ACMR [35], CM-GANs [24] and MHTN [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. Therefore, no datasets are extracted.",
      "processing_time": 15.879089832305908,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "Image2Text Text2Image Average CCA [12] 0.176 0.178 0.177 CCA ‚Ä† [12] 0.258 0.250 0.254 JRL [46] 0.408 0.353 0.381 JRL ‚Ä° [46] 0.449 0.418 0.434 Corr-AE [9] 0.373 0.357 0.365 Corr-AE ‚Ä† [9] 0.402 0.395 0.399 DCCA [2] 0.409 0.355 0.382 DCCA ‚Ä° [2] 0.444 0.396 0.420 Deep-SM [37] 0.458 0.345 0.402 CMDN [23] 0.409 0.364 0.387 CMDN ‚Ä° [23] 0.487 0.427 0.457 DSCMR [48] 0.516 0.462 0.489 DSCMR ‚Ä° [48] 0.521 0.479 0.500 ACMR [35] 0.439 0.361 0.400 ACMR ‚Ä† [35] 0.518 0.412 0.465 ACMR ‚Ä° [35] 0.511 0.467 0.489 MHTN [17] 0",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and their performance metrics. No clear identifiers for datasets are present.",
      "processing_time": 15.490793228149414,
      "citing_paper_id": "239011700",
      "cited_paper_id": 14061443
    },
    {
      "context_text": "Image2Text Text2Image Average CCA [12] 0.176 0.178 0.177 CCA ‚Ä† [12] 0.258 0.250 0.254 JRL [46] 0.408 0.353 0.381 JRL ‚Ä° [46] 0.449 0.418 0.434 Corr-AE [9] 0.373 0.357 0.365 Corr-AE ‚Ä† [9] 0.402 0.395 0.399 DCCA [2] 0.409 0.355 0.382 DCCA ‚Ä° [2] 0.444 0.396 0.420 Deep-SM [37] 0.458 0.345 0.402 CMDN [23] 0.409 0.364 0.387 CMDN ‚Ä° [23] 0.487 0.427 0.457 DSCMR [48] 0.516 0.462 0.489 DSCMR ‚Ä° [48] 0.521 0.479 0.500 ACMR [35] 0.439 0.361 0.400 ACMR ‚Ä† [35] 0.518 0.412 0.465 ACMR ‚Ä° [35] 0.511 0.467 0.489 MHTN [17] 0",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and their performance metrics. No clear identifiers for datasets are present.",
      "processing_time": 15.490793228149414,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "Following [17], the 3,000-dimensional, 1,000-dimensonal, and 3,000-dimensional BoW features are separately used for Wikipedia, NUS-WIDE-10K, and XMedia datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia",
        "NUS-WIDE-10K",
        "XMedia"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used for cross-modal retrieval experiments with 3,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval.",
        "NUS-WIDE-10K": "Used for cross-modal retrieval experiments with 1,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval.",
        "XMedia": "Used for cross-modal retrieval experiments with 3,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: Wikipedia, NUS-WIDE-10K, and XMedia. These are used for cross-modal multimedia retrieval experiments with different dimensional BoW features.",
      "processing_time": 49.72948336601257,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "Following [17], the 3,000-dimensional, 1,000-dimensonal, and 3,000-dimensional BoW features are separately used for Wikipedia, NUS-WIDE-10K, and XMedia datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia",
        "NUS-WIDE-10K",
        "XMedia"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used for cross-modal retrieval experiments with 3,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval.",
        "NUS-WIDE-10K": "Used for cross-modal retrieval experiments with 1,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval.",
        "XMedia": "Used for cross-modal retrieval experiments with 3,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: Wikipedia, NUS-WIDE-10K, and XMedia. These are used for cross-modal multimedia retrieval experiments with different dimensional BoW features.",
      "processing_time": 49.72948336601257,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "In this section, we study the efficacy of our core ideas and essential model designs, over Wikipedia [6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to Wikipedia, which is not considered a dataset in this context.",
      "processing_time": 16.227138996124268,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "Performance on Wikipedia: Table 1 reports the comparison re-sults with 10 SOTAs on Wikipedia.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with state-of-the-art methods on Wikipedia. Wikipedia itself is not considered a dataset in this context.",
      "processing_time": 16.223801612854004,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "With Table 1: Performance comparison in terms of mAP on the Wikipedia.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Wikipedia' but does not specify a dataset. It appears to be a performance comparison, likely using Wikipedia as a source, but there is no clear indication of a specific, named dataset.",
      "processing_time": 20.68351411819458,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "Wikipedia [6] consists of 2, 866 imagetext pairs, which is generated from Wikipedia‚Äôs ‚Äúfeatured article‚Äù.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used to generate 2,866 imagetext pairs from Wikipedia‚Äôs ‚Äòfeatured articles‚Äô for cross-modal multimedia retrieval, focusing on correlation and abstraction in the data."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Wikipedia' as a source of imagetext pairs, which fits the criteria for a dataset. However, it is not a specific dataset name but rather a general reference to Wikipedia content.",
      "processing_time": 64.88775539398193,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "We demonstrate that our GCR can achieve state-of-the-art (SOTA) performance on Wikipedia, NUS-WIDE-10K, PKU XMedia and Pascal Sentence datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia",
        "NUS-WIDE-10K",
        "PKU XMedia",
        "Pascal Sentence"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used to evaluate the GCR model's performance in cross-modal retrieval, focusing on the integration of textual and visual information.",
        "NUS-WIDE-10K": "Used to assess the GCR model's effectiveness in cross-modal multimedia retrieval, emphasizing the correlation and abstraction in image-text pairs.",
        "PKU XMedia": "Used to test the GCR model's capabilities in cross-modal retrieval, specifically in handling diverse multimedia content.",
        "Pascal Sentence": "Used to evaluate the GCR model's ability to align images with descriptive sentences, focusing on fine-grained visual and textual matching."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used to evaluate the performance of the GCR model. These datasets are relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 80.0229606628418,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "Wikipedia [6] consists of 2 , 866 image-text pairs, which is generated from Wikipedia‚Äôs ‚Äúfeatured article‚Äù.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Wikipedia' with a specific number of image-text pairs, which suggests a dataset. However, the name is too generic without a specific qualifier or version tag.",
      "processing_time": 55.79505205154419,
      "citing_paper_id": "239011700",
      "cited_paper_id": 15512280
    },
    {
      "context_text": "When compared with the state-of-the-art DSCMRvgg16 [48] and MHTNvgg16 [17], our GCRvgg16 achieves clear performance improvements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model comparisons. No verifiable resources are identified.",
      "processing_time": 52.9676878452301,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "When compared with the state-of-the-art DSCMR 16 [48] and MHTN 16 [17], our GCR 16 achieves clear performance improvements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing performance of different models.",
      "processing_time": 53.60145425796509,
      "citing_paper_id": "239011700",
      "cited_paper_id": 27837890
    },
    {
      "context_text": "It plays a crucial role in a wide variety of high-level AI applications, such as visual captioning [1, 29], visual-question answering [16, 18] and visual grounding [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only high-level AI applications. No dataset names are present in the context.",
      "processing_time": 53.60484552383423,
      "citing_paper_id": "239011700",
      "cited_paper_id": 212628625
    },
    {
      "context_text": "It plays a crucial role in a wide variety of high-level AI applications, such as visual captioning [1, 29], visual-question answering [16, 18] and visual grounding [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only high-level AI applications. No dataset names are present in the context.",
      "processing_time": 53.60484552383423,
      "citing_paper_id": "239011700",
      "cited_paper_id": 233024902
    },
    {
      "context_text": "Graph convolutional network (GCN) [3] has been widely used for handling the underlying relationships among structured data [20, 21, 36, 38, 41, 44, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of graph convolutional networks (GCNs) for handling structured data. No verifiable datasets are named.",
      "processing_time": 55.24189758300781,
      "citing_paper_id": "239011700",
      "cited_paper_id": 213176935
    },
    {
      "context_text": "Graph convolutional network (GCN) [3] has been widely used for handling the underlying relationships among structured data [20, 21, 36, 38, 41, 44, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of graph convolutional networks (GCNs) for handling structured data. No verifiable datasets are named.",
      "processing_time": 55.24189758300781,
      "citing_paper_id": "239011700",
      "cited_paper_id": 219615944
    },
    {
      "context_text": "Graph convolutional network (GCN) [3] has been widely used for handling the underlying relationships among structured data [20, 21, 36, 38, 41, 44, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of graph convolutional networks (GCNs) for handling structured data. No verifiable datasets are named.",
      "processing_time": 55.24189758300781,
      "citing_paper_id": "239011700",
      "cited_paper_id": 222278217
    },
    {
      "context_text": "SpeciÔ¨Åcally, we use the Glove embedding (Pennington, Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on‚Ä¶",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (Glove, Bi-LSTM, SSD, VGG16, RetinaNet, Resnet-50) but does not reference any specific datasets. The cited papers are also about models and methods, not datasets.",
      "processing_time": 58.524165630340576,
      "citing_paper_id": "250301328",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "SpeciÔ¨Åcally, we use the Glove embedding (Pennington, Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on‚Ä¶",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (Glove, Bi-LSTM, SSD, VGG16, RetinaNet, Resnet-50) but does not reference any specific datasets. The cited papers are also about models and methods, not datasets.",
      "processing_time": 58.524165630340576,
      "citing_paper_id": "250301328",
      "cited_paper_id": 2141740
    },
    {
      "context_text": "SpeciÔ¨Åcally, we use the Glove embedding (Pennington, Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on ImageNet (Deng et al. 2009) for backbone image encoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to pre-train the VGG16 and ResNet-50 networks for the backbone image encoder, enhancing feature extraction capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a pre-training dataset for the backbone image encoder. No other datasets are mentioned as specific, reusable resources.",
      "processing_time": 60.92132616043091,
      "citing_paper_id": "250301328",
      "cited_paper_id": 2141740
    },
    {
      "context_text": "SpeciÔ¨Åcally, we use the Glove embedding (Pennington, Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on ImageNet (Deng et al. 2009) for backbone image encoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to pre-train the VGG16 and ResNet-50 networks for the backbone image encoder, enhancing feature extraction capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a pre-training dataset for the backbone image encoder. No other datasets are mentioned as specific, reusable resources.",
      "processing_time": 60.92132616043091,
      "citing_paper_id": "250301328",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "The dense candidate proposal features are usually from single-stage object detection such as SSD (Liu et al. 2016), Yolov3 (Redmon and Farhadi 2018), FPN (Lin et al. 2017a), RetinaNet (Lin et al. 2017b) and multiple other vision tasks (Yeh et al. 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The citation intent is to reference reusable resources (models/methods) rather than datasets.",
      "processing_time": 54.901485443115234,
      "citing_paper_id": "250301328",
      "cited_paper_id": 2141740
    },
    {
      "context_text": "The dense candidate proposal features are usually from single-stage object detection such as SSD (Liu et al. 2016), Yolov3 (Redmon and Farhadi 2018), FPN (Lin et al. 2017a), RetinaNet (Lin et al. 2017b) and multiple other vision tasks (Yeh et al. 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The citation intent is to reference reusable resources (models/methods) rather than datasets.",
      "processing_time": 54.901485443115234,
      "citing_paper_id": "250301328",
      "cited_paper_id": 4714433
    },
    {
      "context_text": "Localizing objects in an image referenced by noun phrases in a query (Chen, Kovvuri, and Nevatia 2017; Plummer et al. 2015; Yu et al. 2018), a fundamental problem known as phrase grounding or referring expressions , has drawn extensive attention in both the natural language processing and computer‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'phrase grounding' and 'referring expressions', which are directly related to the cited papers. However, no specific dataset names are mentioned in the citation span.",
      "processing_time": 55.347981214523315,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3365209
    },
    {
      "context_text": "Localizing objects in an image referenced by noun phrases in a query (Chen, Kovvuri, and Nevatia 2017; Plummer et al. 2015; Yu et al. 2018), a fundamental problem known as phrase grounding or referring expressions , has drawn extensive attention in both the natural language processing and computer‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'phrase grounding' and 'referring expressions', which are directly related to the cited papers. However, no specific dataset names are mentioned in the citation span.",
      "processing_time": 55.347981214523315,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3441497
    },
    {
      "context_text": "Localizing objects in an image referenced by noun phrases in a query (Chen, Kovvuri, and Nevatia 2017; Plummer et al. 2015; Yu et al. 2018), a fundamental problem known as phrase grounding or referring expressions , has drawn extensive attention in both the natural language processing and computer‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'phrase grounding' and 'referring expressions', which are directly related to the cited papers. However, no specific dataset names are mentioned in the citation span.",
      "processing_time": 55.347981214523315,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "Localizing objects in an image referenced by noun phrases in a query (Chen, Kovvuri, and Nevatia 2017; Plummer et al. 2015; Yu et al. 2018), a fundamental problem known as phrase grounding or referring expressions , has drawn extensive attention in both the natural language processing and computer‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'phrase grounding' and 'referring expressions', which are directly related to the cited papers. However, no specific dataset names are mentioned in the citation span.",
      "processing_time": 55.347981214523315,
      "citing_paper_id": "250301328",
      "cited_paper_id": 12856358
    },
    {
      "context_text": "Model Comparison We compare our models with the following baseline models: (1) QRG (Chen, Kovvuri, and Nevatia 2017) employs query-guided regression network by reinforcement learning for proposal generation and policy learning; we had its Ô¨Årst-stage object detector pre-trained on Pascal-VOC‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal-VOC"
      ],
      "dataset_descriptions": {
        "Pascal-VOC": "Used to pre-train the first-stage object detector in the QRG model, focusing on object detection and proposal generation for phrase grounding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Pascal-VOC' as a dataset used for pre-training the first-stage object detector in the QRG model. No other datasets are explicitly mentioned.",
      "processing_time": 62.401976346969604,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3365209
    },
    {
      "context_text": "Model Comparison We compare our models with the following baseline models: (1) QRG (Chen, Kovvuri, and Nevatia 2017) employs query-guided regression network by reinforcement learning for proposal generation and policy learning; we had its Ô¨Årst-stage object detector pre-trained on Pascal-VOC‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal-VOC"
      ],
      "dataset_descriptions": {
        "Pascal-VOC": "Used to pre-train the first-stage object detector in the QRG model, focusing on object detection and proposal generation for phrase grounding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Pascal-VOC' as a dataset used for pre-training the first-stage object detector in the QRG model. No other datasets are explicitly mentioned.",
      "processing_time": 62.401976346969604,
      "citing_paper_id": "250301328",
      "cited_paper_id": 12856358
    },
    {
      "context_text": "Evaluation Metric We use the Intersection over Union (IoU) as in (Chen, Kovvuri, and Nevatia 2017).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions IoU as an evaluation metric but does not refer to any specific dataset. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 54.661004066467285,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3365209
    },
    {
      "context_text": "Evaluation Metric We use the Intersection over Union (IoU) as in (Chen, Kovvuri, and Nevatia 2017).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions IoU as an evaluation metric but does not refer to any specific dataset. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 54.661004066467285,
      "citing_paper_id": "250301328",
      "cited_paper_id": 12856358
    },
    {
      "context_text": "‚Ä¶have been two major lines of models for phrase grounding: (1) The two-stage phrase grounding models (Wang et al. 2018; Plummer et al. 2015, 2018; Chen et al. 2017a) Ô¨Årst obtain candidate proposals from an explicit object detector and then perform matching according to their similarities to the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for phrase grounding. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 53.891249895095825,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3365209
    },
    {
      "context_text": "‚Ä¶have been two major lines of models for phrase grounding: (1) The two-stage phrase grounding models (Wang et al. 2018; Plummer et al. 2015, 2018; Chen et al. 2017a) Ô¨Årst obtain candidate proposals from an explicit object detector and then perform matching according to their similarities to the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for phrase grounding. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 53.891249895095825,
      "citing_paper_id": "250301328",
      "cited_paper_id": 12856358
    },
    {
      "context_text": "‚Ä¶have been two major lines of models for phrase grounding: (1) The two-stage phrase grounding models (Wang et al. 2018; Plummer et al. 2015, 2018; Chen et al. 2017a) Ô¨Årst obtain candidate proposals from an explicit object detector and then perform matching according to their similarities to the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for phrase grounding. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 53.891249895095825,
      "citing_paper_id": "250301328",
      "cited_paper_id": 49420146
    },
    {
      "context_text": "The two-stage models (Plummer et al. 2015; Wang et al. 2018; Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited paper titles do not introduce any new datasets either.",
      "processing_time": 53.71308755874634,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3365209
    },
    {
      "context_text": "The two-stage models (Plummer et al. 2015; Wang et al. 2018; Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited paper titles do not introduce any new datasets either.",
      "processing_time": 53.71308755874634,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3441497
    },
    {
      "context_text": "The two-stage models (Plummer et al. 2015; Wang et al. 2018; Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited paper titles do not introduce any new datasets either.",
      "processing_time": 53.71308755874634,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "The two-stage models (Plummer et al. 2015; Wang et al. 2018; Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited paper titles do not introduce any new datasets either.",
      "processing_time": 53.71308755874634,
      "citing_paper_id": "250301328",
      "cited_paper_id": 12856358
    },
    {
      "context_text": "The two-stage models (Plummer et al. 2015; Wang et al. 2018; Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited paper titles do not introduce any new datasets either.",
      "processing_time": 53.71308755874634,
      "citing_paper_id": "250301328",
      "cited_paper_id": 49420146
    },
    {
      "context_text": "Model Comparison We compare our models with the following baseline models: (1) QRG (Chen, Kovvuri, and Nevatia 2017) employs query-guided regression network by reinforcement learning for proposal generation and policy learning; we had its Ô¨Årst-stage object detector pre-trained on Pascal-VOC (Everingham et al. 2010) and then Ô¨Åne-tuned on the training set of the speciÔ¨Åc dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal-VOC"
      ],
      "dataset_descriptions": {
        "Pascal-VOC": "Used to pre-train a first-stage object detector, which was then fine-tuned on the specific dataset's training set for proposal generation and policy learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Pascal-VOC' as a dataset used for pre-training a first-stage object detector. No other datasets are explicitly named.",
      "processing_time": 62.16219091415405,
      "citing_paper_id": "250301328",
      "cited_paper_id": 3365209
    },
    {
      "context_text": "Model Comparison We compare our models with the following baseline models: (1) QRG (Chen, Kovvuri, and Nevatia 2017) employs query-guided regression network by reinforcement learning for proposal generation and policy learning; we had its Ô¨Årst-stage object detector pre-trained on Pascal-VOC (Everingham et al. 2010) and then Ô¨Åne-tuned on the training set of the speciÔ¨Åc dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pascal-VOC"
      ],
      "dataset_descriptions": {
        "Pascal-VOC": "Used to pre-train a first-stage object detector, which was then fine-tuned on the specific dataset's training set for proposal generation and policy learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Pascal-VOC' as a dataset used for pre-training a first-stage object detector. No other datasets are explicitly named.",
      "processing_time": 62.16219091415405,
      "citing_paper_id": "250301328",
      "cited_paper_id": 12856358
    },
    {
      "context_text": "Extensive experiments were performed on zero-shot phrase grounding splits introduced by (Sadhu, Chen, and Nevatia 2019), which were developed on Visual Genome (Krishna et al. 2017) and Flickr30K Entities (Plummer et al. 2015; Young et al. 2014).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome",
        "Flickr30K Entities"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used for zero-shot phrase grounding experiments, focusing on region-to-phrase correspondences in images.",
        "Flickr30K Entities": "Used for zero-shot phrase grounding experiments, providing region-to-phrase correspondences to enhance image-to-sentence models."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for zero-shot phrase grounding experiments, including Visual Genome and Flickr30K Entities.",
      "processing_time": 64.45029497146606,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "Our experiments on different zero-shot grounding datasets sub-sampled from Flickr30K Entities and Visual Genome show that the proposed model sig-niÔ¨Åcantly outperforms baselines.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30K Entities",
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Flickr30K Entities": "Used for zero-shot grounding experiments, focusing on region-to-phrase correspondences to enhance image-to-sentence models.",
        "Visual Genome": "Used for zero-shot grounding experiments, providing a rich set of annotations for images to test model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Flickr30K Entities and Visual Genome, which are used for zero-shot grounding experiments.",
      "processing_time": 64.89877319335938,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "There have been two major lines of models for phrase grounding: (1) The two-stage phrase grounding models (Wang et al. 2018; Plummer et al. 2015, 2018; Chen et al. 2017a) Ô¨Årst obtain candidate proposals from an explicit object detector and then perform matching according to their similarities to‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about phrase grounding models and their methodologies, not the datasets used.",
      "processing_time": 54.0035719871521,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "Data Following (Sadhu, Chen, and Nevatia 2019), we use sub-sampled dataset split Flickr-0 and Flickr-1 from Flickr30K Entities (Plummer et al. 2015) with region-phrase correspondence annotated on the original Flickr30K (Young et al. 2014).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30K Entities",
        "Flickr-0",
        "Flickr-1"
      ],
      "dataset_descriptions": {
        "Flickr30K Entities": "Used to train and evaluate models on region-phrase correspondences, enhancing image-to-sentence models with detailed annotations.",
        "Flickr-0": "Sub-sampled split of Flickr30K Entities used for training and evaluation, focusing on region-phrase correspondences.",
        "Flickr-1": "Sub-sampled split of Flickr30K Entities used for training and evaluation, focusing on region-phrase correspondences."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific splits of the Flickr30K Entities dataset, which is a multi-modal dataset used for image-to-sentence modeling.",
      "processing_time": 73.75103569030762,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "Note that all noun phrases in Flickr30K Entities can be classiÔ¨Åed as 8 general categories, i.e., people , clothing , bodyparts , animals , vehicles , instructs , scene , and other .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30K Entities"
      ],
      "dataset_descriptions": {
        "Flickr30K Entities": "Used to collect region-to-phrase correspondences for richer image-to-sentence models, focusing on 8 general categories including people, clothing, and scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "Flickr30K Entities is a specific dataset used for image-to-sentence modeling, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 62.125019788742065,
      "citing_paper_id": "250301328",
      "cited_paper_id": 6941275
    },
    {
      "context_text": "2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.66285991668701,
      "citing_paper_id": "250301328",
      "cited_paper_id": 10328909
    },
    {
      "context_text": "‚Ä¶Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding boxes and ROI-pooled features and then rank/classify the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods used for object detection and feature extraction.",
      "processing_time": 52.794337034225464,
      "citing_paper_id": "250301328",
      "cited_paper_id": 10328909
    },
    {
      "context_text": "The two-stage models (Plummer et al. 2015; Wang et al. 2018; Plummer et al. 2018; Chen et al. 2017a; Rohrbach et al. 2016; Yu et al. 2018, 2016; Mao et al. 2016) leverage an object detector such as FasterRCNN (Ren et al. 2015) and MaskR-CNN (He et al. 2017) in the Ô¨Årst stage to obtain the bounding boxes and ROI-pooled features and then rank/classify the proposals in the second step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of object detection models in multi-modal reasoning systems.",
      "processing_time": 54.1653265953064,
      "citing_paper_id": "250301328",
      "cited_paper_id": 10328909
    },
    {
      "context_text": "Constructing MMKG We employ detectron2 (Wu et al. 2019) trained on the MSCOCO object detection classes (Lin et al. 2014) to recognize instances of objects and return their bounding boxes and object categories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to train object detection models, specifically for recognizing instances of objects and returning their bounding boxes and categories in multi-modal knowledge graph construction."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of MSCOCO object detection classes for recognizing objects and returning bounding boxes and categories. This indicates the use of a specific dataset.",
      "processing_time": 60.91245436668396,
      "citing_paper_id": "250301328",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "Constructing MMKG We employ detectron2 (Wu et al. 2019) trained on the MSCOCO object detection classes (Lin et al. 2014) to recognize instances of objects and return their bounding boxes and object categories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to train object detection models, specifically for recognizing instances of objects and returning their bounding boxes and categories in multi-modal knowledge graph construction."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of MSCOCO object detection classes for recognizing objects and returning bounding boxes and categories. This indicates the use of a specific dataset.",
      "processing_time": 60.91245436668396,
      "citing_paper_id": "250301328",
      "cited_paper_id": null
    },
    {
      "context_text": "A good phrase grounding sys-tem can beneÔ¨Åt many other downstream tasks such as visual question answering (Antol et al. 2015; Goyal et al. 2017), image retrieval (Johnson et al. 2015; Hu et al. 2016), and image captioning (Lu et al. 2018; Dai, Fidler, and Lin 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several tasks but does not specify any datasets. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 53.912689447402954,
      "citing_paper_id": "250301328",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "A good phrase grounding sys-tem can beneÔ¨Åt many other downstream tasks such as visual question answering (Antol et al. 2015; Goyal et al. 2017), image retrieval (Johnson et al. 2015; Hu et al. 2016), and image captioning (Lu et al. 2018; Dai, Fidler, and Lin 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several tasks but does not specify any datasets. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 53.912689447402954,
      "citing_paper_id": "250301328",
      "cited_paper_id": 53036732
    },
    {
      "context_text": ", language inference (Chen et al. 2017b), visual question answering (Singh et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and tasks. No clear identifiers for datasets are present.",
      "processing_time": 53.214696645736694,
      "citing_paper_id": "250301328",
      "cited_paper_id": 49420146
    },
    {
      "context_text": "(2) Single-stage phrase grounding models (Sadhu, Chen, and Nevatia 2019; Yang et al. 2019) directly generate dense candidate proposals on sliced image regions with various resolutions and perform matching with the query. is more strict than setting (a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 53.54618978500366,
      "citing_paper_id": "250301328",
      "cited_paper_id": 201070217
    },
    {
      "context_text": "Note that since the method for YOLOG (Yang et al. 2019) is similar to ZSG except for the backbone encoder, we view it as the same baseline as ZSG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (YOLOG) which is compared to another method (ZSG).",
      "processing_time": 53.62921357154846,
      "citing_paper_id": "250301328",
      "cited_paper_id": 201070217
    },
    {
      "context_text": "However, single-stage models (Sadhu, Chen, and Nevatia 2019; Yang et al. 2019; Zhao et al. 2018; Yeh et al. 2017; Yang, Li, and Yu 2020; Yang et al. 2020) instead use dense candidate proposals and directly fuse the text features from queries and visual representation from proposals to make the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 53.9155638217926,
      "citing_paper_id": "250301328",
      "cited_paper_id": 201070217
    },
    {
      "context_text": "To produce K feature maps f v k g Kk =1 at different resolutions, we use backbone image encoder ResNet-50 (He et al. 2016) with FPN (Lin et al. 2017a) as the default image encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 52.54786539077759,
      "citing_paper_id": "250301328",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "‚Ä¶Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on ImageNet (Deng et al. 2009) for backbone image encoder.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to pre-train the backbone image encoder (VGG16 or ResNet-50) for feature extraction in a multi-modal learning setup, enhancing the model's ability to recognize and reason about visual content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a pre-training dataset for the backbone image encoder, which is relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 63.374101877212524,
      "citing_paper_id": "250301328",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "‚Ä¶Knowledge We Ô¨Årst consider building an undirected graph G cms for a given image I and query q based on the commonsense knowledge from WordNet (Fell-baum 2012) and ConceptNet (Liu and Singh 2004), where the node set includes three types of nodes: entity nodes , relation nodes , and object nodes .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNet",
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "WordNet": "Used to build an undirected graph for commonsense knowledge, specifically incorporating entity and relation nodes for a given image and query.",
        "ConceptNet": "Used to build an undirected graph for commonsense knowledge, specifically incorporating entity and relation nodes for a given image and query."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions using WordNet and ConceptNet for building a graph based on commonsense knowledge. These are specific resources used for their knowledge bases.",
      "processing_time": 66.21675658226013,
      "citing_paper_id": "250301328",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "Commonsense Knowledge We Ô¨Årst consider building an undirected graph G cms for a given image I and query q based on the commonsense knowledge from WordNet (Fell-baum 2012) and ConceptNet (Liu and Singh 2004), where the node set includes three types of nodes: entity nodes , relation nodes , and object nodes .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNet",
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "WordNet": "Used to build an undirected graph for commonsense knowledge, providing entity and relation nodes for a given image and query.",
        "ConceptNet": "Used to build an undirected graph for commonsense knowledge, providing entity and relation nodes for a given image and query."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions using WordNet and ConceptNet for building a graph based on commonsense knowledge. These are specific resources used for their knowledge bases.",
      "processing_time": 65.64777374267578,
      "citing_paper_id": "250301328",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "Tointroduce external knowledge into our model, we pro-pose to jointly learn from commonsense knowledge (Fell-baum 2012; Liu and Singh 2004) and scene graph knowledge (Krishna et al. 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'commonsense knowledge' and 'scene graph knowledge' but does not specify any datasets. The cited papers are likely sources of knowledge rather than datasets.",
      "processing_time": 55.51955270767212,
      "citing_paper_id": "250301328",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "Uni-modal KGC methods: TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), RotatE (Sun et al., 2019), PairRE (Chao et al., 2021), and TuckER (Balazevic et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several uni-modal KGC methods but does not refer to any specific datasets. The cited papers are about methods, not datasets.",
      "processing_time": 54.1371066570282,
      "citing_paper_id": "270064299",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "Uni-modal KGC methods: TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), RotatE (Sun et al., 2019), PairRE (Chao et al., 2021), and TuckER (Balazevic et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several uni-modal KGC methods but does not refer to any specific datasets. The cited papers are about methods, not datasets.",
      "processing_time": 54.1371066570282,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "The first category is uni-modal KGC methods including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), RotatE (Sun et al., 2019), PairRE (Chao et al., 2021), and TuckER (Balazevic et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models for knowledge graph completion, which are not datasets. No specific datasets are mentioned or used in the described research context.",
      "processing_time": 54.134732723236084,
      "citing_paper_id": "270064299",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "The first category is uni-modal KGC methods including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), RotatE (Sun et al., 2019), PairRE (Chao et al., 2021), and TuckER (Balazevic et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models for knowledge graph completion, which are not datasets. No specific datasets are mentioned or used in the described research context.",
      "processing_time": 54.134732723236084,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 53.89579176902771,
      "citing_paper_id": "270064299",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 53.89579176902771,
      "citing_paper_id": "270064299",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 53.89579176902771,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 53.89579176902771,
      "citing_paper_id": "270064299",
      "cited_paper_id": 244222941
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 244222941
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 264492337
    },
    {
      "context_text": "Multi-modal KGC methods: IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods for multi-modal knowledge graph completion (KGC) but does not reference any specific datasets. The context is focused on describing various models and their contributions to the field.",
      "processing_time": 56.64738345146179,
      "citing_paper_id": "270064299",
      "cited_paper_id": 266166905
    },
    {
      "context_text": "‚Ä¶MMKGs by collaboratively modeling the triple structure and multi-modal information (e.g., images and textual descriptions) in the MMKGs. Mainstream MMKGC methods (Xie et al., 2017) explore multi-modal fusion in the same representation space to measure the triple plausibility from multi-views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods related to multi-modal knowledge graph construction.",
      "processing_time": 52.61822962760925,
      "citing_paper_id": "270064299",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 244222941
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 264492337
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not reference any specific datasets. The context focuses on describing various approaches rather than using datasets.",
      "processing_time": 54.882508993148804,
      "citing_paper_id": "270064299",
      "cited_paper_id": 266166905
    },
    {
      "context_text": "The second category is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022),‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models for multi-modal knowledge graph completion but does not refer to any specific datasets. The context is focused on describing various approaches rather than using a particular dataset.",
      "processing_time": 56.1011528968811,
      "citing_paper_id": "270064299",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Early approaches like IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b) employ multiple translation-based score functions for embeddings from diverse modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (IKRL, TBKGC, TransAE) but does not reference any specific datasets. The context focuses on methodologies rather than data sources.",
      "processing_time": 55.177278995513916,
      "citing_paper_id": "270064299",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Early approaches like IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b) employ multiple translation-based score functions for embeddings from diverse modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (IKRL, TBKGC, TransAE) but does not reference any specific datasets. The context focuses on methodologies rather than data sources.",
      "processing_time": 55.177278995513916,
      "citing_paper_id": "270064299",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "For image and text modality, a pre-trained model like VGG (Simonyan & Zisserman, 2015) and BERT (Devlin et al., 2019) would be employed to extract the raw modality feature.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG and BERT but does not refer to them as datasets. They are models used for feature extraction in multi-modal reasoning.",
      "processing_time": 54.00858449935913,
      "citing_paper_id": "270064299",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "For image and text modality, a pre-trained model like VGG (Simonyan & Zisserman, 2015) and BERT (Devlin et al., 2019) would be employed to extract the raw modality feature.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG and BERT but does not refer to them as datasets. They are models used for feature extraction in multi-modal reasoning.",
      "processing_time": 54.00858449935913,
      "citing_paper_id": "270064299",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The variation in Hit@10 results reveals that baseline methods like TBKGC (Sergieh et al., 2018) and AdaMF (Zhang et al., 2024b) undergo a noticeable performance degradation with increasing noise and decreasing training data, while our method‚Äôs performance remains relatively steady.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their performance. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 53.79206562042236,
      "citing_paper_id": "270064299",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "The variation in Hit@10 results reveals that baseline methods like TBKGC (Sergieh et al., 2018) and AdaMF (Zhang et al., 2024b) undergo a noticeable performance degradation with increasing noise and decreasing training data, while our method‚Äôs performance remains relatively steady.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their performance. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 53.79206562042236,
      "citing_paper_id": "270064299",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "‚Ä¶is multi-modal KGC methods considering multi-modal information of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several multi-modal KGC methods but does not refer to any specific datasets. The context focuses on models and methods rather than datasets.",
      "processing_time": 54.25788617134094,
      "citing_paper_id": "270064299",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "These experimental results are consistent with those shown in the paper for MKG-W and DB15K.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-W",
        "DB15K"
      ],
      "dataset_descriptions": {
        "MKG-W": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs.",
        "DB15K": "Used to assess the performance of multi-modal knowledge graph models, specifically in linking entities across different modalities."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions 'MKG-W' and 'DB15K', which appear to be specific datasets or resources used in the context of multi-modal knowledge graphs. However, without additional context, it is unclear if these are datasets or other types of resources.",
      "processing_time": 70.59751296043396,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "The trend reflected in the new results is the same as DB15K and KVC16K.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DB15K",
        "KVC16K"
      ],
      "dataset_descriptions": {
        "DB15K": "Used to compare trends in multi-modal knowledge graph reasoning, focusing on the consistency of results across different datasets.",
        "KVC16K": "Used to compare trends in multi-modal knowledge graph reasoning, focusing on the consistency of results across different datasets."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions 'DB15K' and 'KVC16K', which appear to be specific datasets. However, without additional context, it is unclear how these datasets are used in the research.",
      "processing_time": 68.69010281562805,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Model MKG-W MKG-Y DB15K KVC16K MRR Hit@1 MRR Hit@1 MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 Uni-modal KGC Methods",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and evaluation metrics. There are no clear identifiers for datasets.",
      "processing_time": 52.82099437713623,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Setting MKG-W DB15K MRR Hit@1 MRR Hit@1 Hit@3 Hit@10",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-W DB15K"
      ],
      "dataset_descriptions": {
        "MKG-W DB15K": "Used to evaluate multi-modal knowledge graph reasoning models, focusing on link prediction performance metrics such as MRR and Hit@1."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation span mentions 'MKG-W DB15K' which appears to be a specific dataset used in multi-modal knowledge graph reasoning. However, the context does not provide details on how it is used.",
      "processing_time": 63.82711958885193,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "MoMoK AdaMF QBE TBKGC Table 2: The ablation study results on MKG-W and DB15K datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-W",
        "DB15K"
      ],
      "dataset_descriptions": {
        "MKG-W": "Used for ablation studies in multi-modal knowledge graph reasoning, focusing on the impact of different components on performance.",
        "DB15K": "Used for ablation studies in multi-modal knowledge graph reasoning, evaluating the effectiveness of various model configurations."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'MKG-W' and 'DB15K' as datasets used for ablation studies. These names follow the pattern of multi-word proper nouns and are likely specific datasets.",
      "processing_time": 67.5022828578949,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "We conduct our experiments on four public MMKG benchmarks: MKG-W (Xu et al., 2022), MKG-Y (Xu et al., 2022), DB15K (Liu et al., 2019), and KVC16K (Zhang et al., 2024a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-W",
        "MKG-Y",
        "DB15K",
        "KVC16K"
      ],
      "dataset_descriptions": {
        "MKG-W": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs.",
        "MKG-Y": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs.",
        "DB15K": "Used to evaluate cross-lingual entity alignment in multi-modal knowledge graphs, focusing on linking entities across different languages.",
        "KVC16K": "Used to evaluate knowledge graph completion tasks in multi-modal settings, focusing on predicting missing links using both textual and visual data."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions four specific benchmarks used for experiments in multi-modal knowledge graphs. These benchmarks are clearly identified and are relevant to the research topic.",
      "processing_time": 78.44668078422546,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "We conduct our experiments on four public MMKG benchmarks: MKG-W (Xu et al., 2022), MKG-Y (Xu et al., 2022), DB15K (Liu et al., 2019), and KVC16K (Zhang et al., 2024a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MKG-W",
        "MKG-Y",
        "DB15K",
        "KVC16K"
      ],
      "dataset_descriptions": {
        "MKG-W": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs.",
        "MKG-Y": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs.",
        "DB15K": "Used to evaluate cross-lingual entity alignment in multi-modal knowledge graphs, focusing on linking entities across different languages.",
        "KVC16K": "Used to evaluate knowledge graph completion tasks in multi-modal settings, focusing on predicting missing links using both textual and visual data."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions four specific benchmarks used for experiments in multi-modal knowledge graphs. These benchmarks are clearly identified and are relevant to the research topic.",
      "processing_time": 78.44668078422546,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "For instance, M O M O K obtained 33.8% / 23.0% relative improvement of Hit@1 on DB15K and KVC16K respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DB15K",
        "KVC16K"
      ],
      "dataset_descriptions": {
        "DB15K": "Used to evaluate the performance of M O M O K, focusing on Hit@1 metric to assess the accuracy of entity linking in multi-modal knowledge graphs.",
        "KVC16K": "Used to evaluate the performance of M O M O K, focusing on Hit@1 metric to assess the accuracy of entity linking in multi-modal knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets (DB15K and KVC16K) used to evaluate the performance of M O M O K. These datasets are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 72.08171534538269,
      "citing_paper_id": "270064299",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Existing MMKGC methods (Wang et al., 2021; Xu et al., 2023) typically employ a multi-modal fusion module to integrate the information from different modalities to obtain joint entity em-‚àó beddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing existing methods for multi-modal knowledge graph completion.",
      "processing_time": 53.788798570632935,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "‚Ä¶of entities to enhance the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al.,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various KGC models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.382606983184814,
      "citing_paper_id": "270064299",
      "cited_paper_id": 226281660
    },
    {
      "context_text": "‚Ä¶the KGC models, including IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al.,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not explicitly reference any datasets. The context is focused on describing various KGC models rather than using specific datasets.",
      "processing_time": 54.69265007972717,
      "citing_paper_id": "270064299",
      "cited_paper_id": 244222941
    },
    {
      "context_text": "‚Ä¶IKRL (Xie et al., 2017), TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023),‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The context is focused on listing various approaches for multimodal knowledge graph reasoning.",
      "processing_time": 54.12223982810974,
      "citing_paper_id": "270064299",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "In this paper, our research focuses on designing a mixture-of-experts framework with ensemble learning for MMKGC, which would be the combination of the modality fusion and modality ensemble methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is focused on the design of a framework and does not reference any particular dataset.",
      "processing_time": 54.88390755653381,
      "citing_paper_id": "270064299",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Advanced multi-modal fusion techniques such as optimal transport (Cao et al., 2022), modality ensemble (Li et al., 2023), contrastive learning (Liang et al., 2024c), and adversarial training (Zhang et al., 2024a) have been continuously introduced into MMKGC. Due to the page limit, a detailed‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 53.69478392601013,
      "citing_paper_id": "270064299",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Advanced multi-modal fusion techniques such as optimal transport (Cao et al., 2022), modality ensemble (Li et al., 2023), contrastive learning (Liang et al., 2024c), and adversarial training (Zhang et al., 2024a) have been continuously introduced into MMKGC. Due to the page limit, a detailed‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 53.69478392601013,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "MoSE (Zhao et al., 2022) design three different ensemble methods to adaptively combine the single-modality models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only ensemble methods and single-modality models. The context is about methodological approaches rather than data sources.",
      "processing_time": 54.126598834991455,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "Based on our observation of the source code, MoSE achieves this result by setting the dimension of embedding to 2000, while our method MoMoK uses 250.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their configurations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.037442207336426,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "Only MoSE-BI is currently slightly better.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only compares performance between MoSE-BI and other unspecified systems.",
      "processing_time": 53.93015193939209,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "Compared with MoSE, MoMoK is an end-to-end training framework, MoSE needs to learn the parameters of the ensemble once more after the model is trained, which is also a feature of our design.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological differences between MoSE and MoMoK.",
      "processing_time": 52.537031173706055,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "‚Ä¶ensemble and combination method that is widely used in AI-related fields like computer vision (Chen et al., 2023b; Riquelme et al., 2021), natural language processing (Jiang et al., 2023; Zhao et al., 2024; 2025), recommendation (Ma et al., 2018; Bian et al., 2023; Hou et al., 2022), and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches used across various AI fields. No verifiable resources are identified.",
      "processing_time": 53.670342683792114,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "‚Ä¶ensemble and combination method that is widely used in AI-related fields like computer vision (Chen et al., 2023b; Riquelme et al., 2021), natural language processing (Jiang et al., 2023; Zhao et al., 2024; 2025), recommendation (Ma et al., 2018; Bian et al., 2023; Hou et al., 2022), and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches used across various AI fields. No verifiable resources are identified.",
      "processing_time": 53.670342683792114,
      "citing_paper_id": "270064299",
      "cited_paper_id": 270710718
    },
    {
      "context_text": "When contrasted with existing ensemble-based approaches such as MoSE (Zhao et al., 2022) and IMF (Li et al., 2023), M O M O K excels by fully exploiting the potential of the relational context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing MOMOK with other ensemble-based approaches.",
      "processing_time": 53.663175106048584,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "‚Ä¶2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023),‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models, but does not refer to any specific datasets. The context is focused on listing various approaches and techniques rather than datasets.",
      "processing_time": 54.859553813934326,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "Among these methods, MoSE and IMF are two methods using ensemble learning technologies, which have similarities to our design and are worth making comparisons.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (MoSE and IMF) for comparison. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.16964077949524,
      "citing_paper_id": "270064299",
      "cited_paper_id": 249625869
    },
    {
      "context_text": "Negative sampling methods: MANS (Zhang et al., 2023a) and MMRNS (Xu et al., 2022).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions negative sampling methods but does not refer to any specific datasets. The context is focused on methods rather than datasets.",
      "processing_time": 53.01243019104004,
      "citing_paper_id": "270064299",
      "cited_paper_id": 253018586
    },
    {
      "context_text": "‚Ä¶2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span mentions several acronyms and abbreviations, but none of them are clearly identified as datasets. They appear to be methods, models, or tools. The cited paper titles do not provide additional clarity on whether these are datasets.",
      "processing_time": 57.451801776885986,
      "citing_paper_id": "270064299",
      "cited_paper_id": 253018586
    },
    {
      "context_text": "‚Ä¶2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span mentions several acronyms and abbreviations, but none of them are clearly identified as datasets. They appear to be methods, models, or tools. The cited paper titles do not provide additional clarity on whether these are datasets.",
      "processing_time": 57.451801776885986,
      "citing_paper_id": "270064299",
      "cited_paper_id": 264492337
    },
    {
      "context_text": "‚Ä¶2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span mentions several acronyms and abbreviations, but none of them are clearly identified as datasets. They appear to be methods, models, or tools. The cited paper titles do not provide additional clarity on whether these are datasets.",
      "processing_time": 57.451801776885986,
      "citing_paper_id": "270064299",
      "cited_paper_id": 266166905
    },
    {
      "context_text": "‚Ä¶2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span mentions several acronyms and abbreviations, but none of them are clearly identified as datasets. They appear to be methods, models, or tools. The cited paper titles do not provide additional clarity on whether these are datasets.",
      "processing_time": 57.451801776885986,
      "citing_paper_id": "270064299",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "‚Ä¶MMKGs have evolved the emerging infrastructure of Artificial Intelligence (AI), contributing to numerous AI-related fields like large language models (Zhang et al., 2023b), recommendation systems (Wang et al., 2019a), and other practical applications (Chen et al., 2023a; Liang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general contributions of MMKGs to AI fields. No verifiable resources are identified.",
      "processing_time": 53.85479426383972,
      "citing_paper_id": "270064299",
      "cited_paper_id": 253018586
    },
    {
      "context_text": "‚Ä¶MMKGs have evolved the emerging infrastructure of Artificial Intelligence (AI), contributing to numerous AI-related fields like large language models (Zhang et al., 2023b), recommendation systems (Wang et al., 2019a), and other practical applications (Chen et al., 2023a; Liang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general contributions of MMKGs to AI fields. No verifiable resources are identified.",
      "processing_time": 53.85479426383972,
      "citing_paper_id": "270064299",
      "cited_paper_id": 265150374
    },
    {
      "context_text": "‚Ä¶MMKGs have evolved the emerging infrastructure of Artificial Intelligence (AI), contributing to numerous AI-related fields like large language models (Zhang et al., 2023b), recommendation systems (Wang et al., 2019a), and other practical applications (Chen et al., 2023a; Liang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general contributions of MMKGs to AI fields. No verifiable resources are identified.",
      "processing_time": 53.85479426383972,
      "citing_paper_id": "270064299",
      "cited_paper_id": 275054439
    },
    {
      "context_text": "For example, MANS (Zhang et al., 2023a) designs visual negative sampling strategies to align the visual modality with a triple structure.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'MANS' but does not provide enough context to determine if it is a dataset, method, or other resource. The title of the cited paper does not help in disambiguating the type of resource.",
      "processing_time": 57.07342505455017,
      "citing_paper_id": "270064299",
      "cited_paper_id": 253018586
    },
    {
      "context_text": "The third category is negative sampling methods including MANS (Zhang et al., 2023a) and MMRNS (Xu et al., 2022).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (MANS and MMRNS). The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.879424810409546,
      "citing_paper_id": "270064299",
      "cited_paper_id": 253018586
    },
    {
      "context_text": "The third category is negative sampling methods including MANS (Zhang et al., 2023a) and MMRNS (Xu et al., 2022).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (MANS and MMRNS). The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.879424810409546,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "Multi-modal knowledge graphs (MMKGs) (Chen et al., 2024) are an extension of traditional knowledge graphs (KGs) (Liang et al., 2024b), encompassing rich modality information such as images and textual descriptions of large-scale entities, which bridges structured knowledge triple and unstructured‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of multi-modal knowledge graphs and their relation to traditional knowledge graphs.",
      "processing_time": 53.05629515647888,
      "citing_paper_id": "270064299",
      "cited_paper_id": 257220329
    },
    {
      "context_text": "Multi-modal knowledge graphs (MMKGs) (Chen et al., 2024) are an extension of traditional knowledge graphs (KGs) (Liang et al., 2024b), encompassing rich modality information such as images and textual descriptions of large-scale entities, which bridges structured knowledge triple and unstructured‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of multi-modal knowledge graphs and their relation to traditional knowledge graphs.",
      "processing_time": 53.05629515647888,
      "citing_paper_id": "270064299",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "MMRNS (Xu et al., 2022) introduces a relation-enhanced negative sampling method by considering relation-guided negative sample generation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for relation-enhanced negative sampling. The context does not provide information about datasets used.",
      "processing_time": 53.94310688972473,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "Some of the baseline results refer to MMRNS (Xu et al., 2022).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'MMRNS' but does not provide enough context to determine if it is a dataset, method, or other resource. The title of the cited paper does not help in disambiguating the type of resource.",
      "processing_time": 57.250099420547485,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "‚Ä¶2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023), VISTA (Lee et al., 2023), and AdaMF (Zhang et al.,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models, but does not explicitly refer to any datasets. The context is focused on listing various approaches and techniques rather than specific datasets.",
      "processing_time": 54.87753343582153,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "Later approaches such as OTKGE (Cao et al., 2022), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b) introduced more sophisticated feature fusion techniques such as optimal transfer, transformer, and adversarial training into multi-modal fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (OTKGE, VISTA, AdaMF) rather than datasets. No specific datasets are named or described in the context.",
      "processing_time": 54.40435338020325,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "Later approaches such as OTKGE (Cao et al., 2022), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b) introduced more sophisticated feature fusion techniques such as optimal transfer, transformer, and adversarial training into multi-modal fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (OTKGE, VISTA, AdaMF) rather than datasets. No specific datasets are named or described in the context.",
      "processing_time": 54.40435338020325,
      "citing_paper_id": "270064299",
      "cited_paper_id": 266166905
    },
    {
      "context_text": "Later approaches such as OTKGE (Cao et al., 2022), VISTA (Lee et al., 2023), and AdaMF (Zhang et al., 2024b) introduced more sophisticated feature fusion techniques such as optimal transfer, transformer, and adversarial training into multi-modal fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (OTKGE, VISTA, AdaMF) rather than datasets. No specific datasets are named or described in the context.",
      "processing_time": 54.40435338020325,
      "citing_paper_id": "270064299",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "‚Ä¶TBKGC (Sergieh et al., 2018), TransAE (Wang et al., 2019b), RSME (Wang et al., 2021), MMKRL (Lu et al., 2022), VBKGC (Zhang & Zhang, 2022), OTKGE (Cao et al., 2022), MoSE (Zhao et al., 2022), MMRNS (Xu et al., 2022), MANS (Zhang et al., 2023a), IMF (Li et al., 2023), QEB (Wang et al., 2023),‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models, but does not explicitly refer to any datasets. The context is focused on listing various approaches and techniques rather than datasets.",
      "processing_time": 54.87196707725525,
      "citing_paper_id": "270064299",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "It aims to encode the complex information in the given KG to perform KG reasoning tasks like knowledge graph completion (KGC) (Liang et al., 2023), which is an interesting and important task seeking to automatically discover new knowledge from the existing KGs. KGC allows for the missing entity‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of knowledge graph completion (KGC).",
      "processing_time": 52.331542015075684,
      "citing_paper_id": "270064299",
      "cited_paper_id": 259145427
    },
    {
      "context_text": "Multi-modal Knowledge Graph Completion (MMKGC) MMKGC (Chen et al., 2024) aims to automatically discover new knowledge triples from the existing MMKGs by collaboratively modeling the triple structure and multi-modal information (e.g., images and textual descriptions) in the MMKGs. Mainstream MMKGC‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of Multi-modal Knowledge Graphs (MMKGs). No verifiable resource names are provided.",
      "processing_time": 54.63459801673889,
      "citing_paper_id": "270064299",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "The search process terminates either when a solution is found, or upon reaching a maximum depth, preventing overly deep and less relevant explorations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a general algorithmic process.",
      "processing_time": 52.54451036453247,
      "citing_paper_id": "273643557",
      "cited_paper_id": 9316331
    },
    {
      "context_text": "Traditional models like LightGBM [8] andGCN[34]wereusedforcomparisontoevaluateIndustryScopeGPT‚Äôs performanceinfunctionalplanning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods used for comparison. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.698893308639526,
      "citing_paper_id": "273643557",
      "cited_paper_id": 52284222
    },
    {
      "context_text": "However, many face a significant imbalance between industrial growth and urban service provision, leading to unsustainable development patterns [2].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses a general trend in industrial growth and urban service provision.",
      "processing_time": 53.69578266143799,
      "citing_paper_id": "273643557",
      "cited_paper_id": 237363704
    },
    {
      "context_text": "The advancement of artificial intelligence further enhances intelligent urban services and tasks such as traffic management [33],urban planning [22], urban function prediction [37], public safety [15], and site recommendation [10].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of AI in urban services. No verifiable resources are identified.",
      "processing_time": 53.19394135475159,
      "citing_paper_id": "273643557",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "The advancement of artificial intelligence further enhances intelligent urban services and tasks such as traffic management [33],urban planning [22], urban function prediction [37], public safety [15], and site recommendation [10].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of AI in urban services. No verifiable resources are identified.",
      "processing_time": 53.19394135475159,
      "citing_paper_id": "273643557",
      "cited_paper_id": null
    },
    {
      "context_text": "Researchers utilize deep learning models to extract representations from urban data like satellite images, Points of Interest (POI), grids, and road networks [23, 30, 31].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to types of urban data (satellite images, POIs, grids, road networks) without naming any particular dataset.",
      "processing_time": 56.28393602371216,
      "citing_paper_id": "273643557",
      "cited_paper_id": 254220898
    },
    {
      "context_text": "Researchers utilize deep learning models to extract representations from urban data like satellite images, Points of Interest (POI), grids, and road networks [23, 30, 31].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to types of urban data (satellite images, POIs, grids, road networks) without naming any particular dataset.",
      "processing_time": 56.28393602371216,
      "citing_paper_id": "273643557",
      "cited_paper_id": 257219550
    },
    {
      "context_text": "However, the application of intelligent operation and planning in industrial parks still holds significant untapped potential.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about the potential of intelligent operation and planning in industrial parks.",
      "processing_time": 54.60914421081543,
      "citing_paper_id": "273643557",
      "cited_paper_id": 257378479
    },
    {
      "context_text": "Urban knowledge graphs, organizing urban entities into a complex graph, have become crucial in modern smart cities [17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UUKG"
      ],
      "dataset_descriptions": {
        "UUKG": "Used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities, focusing on integrating diverse urban data sources."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Urban knowledge graphs' but does not specify a dataset. However, the cited paper title suggests a specific dataset.",
      "processing_time": 59.87807631492615,
      "citing_paper_id": "273643557",
      "cited_paper_id": 259203574
    },
    {
      "context_text": "Urban knowledge graphs, organizing urban entities into a complex graph, have become crucial in modern smart cities [17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UUKG"
      ],
      "dataset_descriptions": {
        "UUKG": "Used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities, focusing on integrating diverse urban data sources."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Urban knowledge graphs' but does not specify a dataset. However, the cited paper title suggests a specific dataset.",
      "processing_time": 59.87807631492615,
      "citing_paper_id": "273643557",
      "cited_paper_id": null
    },
    {
      "context_text": "LLMs are increasingly used as central controllers for autonomous agents with human-like decision-making skills [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs in autonomous agents. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.37821292877197,
      "citing_paper_id": "273643557",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "Recent advancements enable large models to access internal and external knowledge for improved decision-making [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general advancement in large models accessing knowledge.",
      "processing_time": 51.78209209442139,
      "citing_paper_id": "273643557",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "Industrial park knowledge graphs differ from general knowledge graphs by being heterogeneous, incorporating image, textual, numerical, and geospatial data with intricate entity relationships.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the characteristics of industrial park knowledge graphs.",
      "processing_time": 51.609758138656616,
      "citing_paper_id": "273643557",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "They have spurred innovation in urban research, such as developing mobility strategies [24], simulating disease spread [28],urban planning [39] and complex spatio-temporal question answering [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of research. No verifiable resources are identified.",
      "processing_time": 52.752273082733154,
      "citing_paper_id": "273643557",
      "cited_paper_id": 267782436
    },
    {
      "context_text": "They have spurred innovation in urban research, such as developing mobility strategies [24], simulating disease spread [28],urban planning [39] and complex spatio-temporal question answering [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of research. No verifiable resources are identified.",
      "processing_time": 52.752273082733154,
      "citing_paper_id": "273643557",
      "cited_paper_id": 268032947
    },
    {
      "context_text": "They have spurred innovation in urban research, such as developing mobility strategies [24], simulating disease spread [28],urban planning [39] and complex spatio-temporal question answering [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of research. No verifiable resources are identified.",
      "processing_time": 52.752273082733154,
      "citing_paper_id": "273643557",
      "cited_paper_id": null
    },
    {
      "context_text": "To extract localized object-level features, we utilize a bottom-up attention mechanism based on Faster R-CNN [22], pre-trained on the Visual Genome dataset [23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used to pre-train a Faster R-CNN model for extracting localized object-level features, enhancing multi-modal reasoning capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Visual Genome dataset, which is a specific, verifiable resource used for pre-training a model to extract localized object-level features.",
      "processing_time": 59.224191665649414,
      "citing_paper_id": "278782132",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "To effectively integrate these multimodal features, we employ a cross-modal attention mechanism [21] that allows different modalities to interact and refine their representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (cross-modal attention mechanism).",
      "processing_time": 50.937907457351685,
      "citing_paper_id": "278782132",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "To assess the effectiveness of our approach, we conduct experiments on two real-world datasets: Fakeddit [28] and Gossipcop [29].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Fakeddit",
        "Gossipcop"
      ],
      "dataset_descriptions": {
        "Fakeddit": "Used to evaluate fine-grained fake news detection, focusing on multimodal content and social context in a controlled environment.",
        "Gossipcop": "Used to assess the effectiveness of the approach in detecting fake news, leveraging real-world social media data and user interactions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Fakeddit and Gossipcop, which are used to assess the effectiveness of the approach in fake news detection.",
      "processing_time": 65.31776332855225,
      "citing_paper_id": "278782132",
      "cited_paper_id": 85528899
    },
    {
      "context_text": "To assess the effectiveness of our approach, we conduct experiments on two real-world datasets: Fakeddit [28] and Gossipcop [29].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Fakeddit",
        "Gossipcop"
      ],
      "dataset_descriptions": {
        "Fakeddit": "Used to evaluate fine-grained fake news detection, focusing on multimodal content and social context in a controlled environment.",
        "Gossipcop": "Used to assess the effectiveness of the approach in detecting fake news, leveraging real-world social media data and user interactions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, Fakeddit and Gossipcop, which are used to assess the effectiveness of the approach in fake news detection.",
      "processing_time": 65.31776332855225,
      "citing_paper_id": "278782132",
      "cited_paper_id": 238158425
    },
    {
      "context_text": "Multi-modal models include CLIP [6], a vision-language model for cross-modal features, ERNIE [33], which integrates external knowledge, and models like KAN [15], EANN [34], and SpotFake+ [35], which enhance detection through knowledge graph embeddings, adversarial learning, and feature fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various multi-modal models and their capabilities.",
      "processing_time": 54.36837553977966,
      "citing_paper_id": "278782132",
      "cited_paper_id": 158046772
    },
    {
      "context_text": "Multi-modal models include CLIP [6], a vision-language model for cross-modal features, ERNIE [33], which integrates external knowledge, and models like KAN [15], EANN [34], and SpotFake+ [35], which enhance detection through knowledge graph embeddings, adversarial learning, and feature fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various multi-modal models and their capabilities.",
      "processing_time": 54.36837553977966,
      "citing_paper_id": "278782132",
      "cited_paper_id": 219182395
    },
    {
      "context_text": "Multi-modal models include CLIP [6], a vision-language model for cross-modal features, ERNIE [33], which integrates external knowledge, and models like KAN [15], EANN [34], and SpotFake+ [35], which enhance detection through knowledge graph embeddings, adversarial learning, and feature fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various multi-modal models and their capabilities.",
      "processing_time": 54.36837553977966,
      "citing_paper_id": "278782132",
      "cited_paper_id": 235306349
    },
    {
      "context_text": "Uni-modal models include ResNet34 [31] and InceptionV3 [32], used for image feature extraction, and RoBERTa [20], which captures contextual information in text.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and their usage for feature extraction but does not reference any specific datasets.",
      "processing_time": 51.09104585647583,
      "citing_paper_id": "278782132",
      "cited_paper_id": 206593880
    },
    {
      "context_text": "Early approaches primarily relied on unimodal analysis, where linguistic features such as sentiment, syntax, and credibility cues were used to classify textual misinformation [8,9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general linguistic features used in unimodal analysis.",
      "processing_time": 51.706769943237305,
      "citing_paper_id": "278782132",
      "cited_paper_id": 207718082
    },
    {
      "context_text": "Many early approaches rely on static entity embeddings generated through knowledge graph embedding models to capture semantic relationships between entities and factual claims [15‚Äì17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and models. No verifiable resources are identified.",
      "processing_time": 52.14586663246155,
      "citing_paper_id": "278782132",
      "cited_paper_id": 235306349
    },
    {
      "context_text": "Other multi-modal methods such as SAFE [36], MCAN [37], CAFE [38], and HMCAN [39] utilize attention mechanisms for better alignment between text and images, while LogicDM [40] and FND-CLIP [12] introduce logical reasoning and fine-tuning for fake news detection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not refer to any specific datasets. The context is focused on describing various multi-modal methods and their functionalities.",
      "processing_time": 54.73214030265808,
      "citing_paper_id": "278782132",
      "cited_paper_id": 248367577
    },
    {
      "context_text": "First, irrelevant or excessive entity retrieval often introduces knowledge noise, leading to incorrect fact verification [18,19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues related to entity retrieval and knowledge noise.",
      "processing_time": 52.13164186477661,
      "citing_paper_id": "278782132",
      "cited_paper_id": 268718050
    },
    {
      "context_text": "Finally, MMDFND [41] focuses on domain-specific multimodal data for improved accuracy across various domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'domain-specific multimodal data' but does not specify a named dataset. The reference is to a method or system (MMDFND) rather than a dataset.",
      "processing_time": 55.34340715408325,
      "citing_paper_id": "278782132",
      "cited_paper_id": 272331125
    },
    {
      "context_text": "Early methods primarily relied on handcrafted features, extracting linguistic and visual cues separately before using machine learning classifiers to predict veracity [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches. There are no verifiable resources or specific datasets named.",
      "processing_time": 53.303245544433594,
      "citing_paper_id": "278782132",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent research has highlighted how multimodal misinformation spreads more effectively than text-based fake news, as users tend to trust visual evidence more than textual claims [1,2].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about multimodal misinformation. No verifiable resources are identified.",
      "processing_time": 52.542497396469116,
      "citing_paper_id": "278782132",
      "cited_paper_id": null
    },
    {
      "context_text": "However, the increasing use of manipulated images, misattributed videos, and misleading visual elements in fake news has demonstrated the need for multimodal approaches [1,10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general need for multimodal approaches in dealing with fake news.",
      "processing_time": 51.399869441986084,
      "citing_paper_id": "278782132",
      "cited_paper_id": null
    },
    {
      "context_text": "Most existing models for the above tasks [7, 15, 16, 25, 27, 39, 44, 46, 47] are limited in their capability to navigate complex problems requiring external knowledge, because themodels‚Äô feature construction framework is derived solely from the document image.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing models. No dataset names are provided in the context.",
      "processing_time": 53.7442786693573,
      "citing_paper_id": "252782478",
      "cited_paper_id": 2272015
    },
    {
      "context_text": "Most existing models for the above tasks [7, 15, 16, 25, 27, 39, 44, 46, 47] are limited in their capability to navigate complex problems requiring external knowledge, because themodels‚Äô feature construction framework is derived solely from the document image.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing models. No dataset names are provided in the context.",
      "processing_time": 53.7442786693573,
      "citing_paper_id": "252782478",
      "cited_paper_id": 52815006
    },
    {
      "context_text": "Most existing models for the above tasks [7, 15, 16, 25, 27, 39, 44, 46, 47] are limited in their capability to navigate complex problems requiring external knowledge, because themodels‚Äô feature construction framework is derived solely from the document image.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing models. No dataset names are provided in the context.",
      "processing_time": 53.7442786693573,
      "citing_paper_id": "252782478",
      "cited_paper_id": 85528598
    },
    {
      "context_text": "Most existing models for the above tasks [7, 15, 16, 25, 27, 39, 44, 46, 47] are limited in their capability to navigate complex problems requiring external knowledge, because themodels‚Äô feature construction framework is derived solely from the document image.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing models. No dataset names are provided in the context.",
      "processing_time": 53.7442786693573,
      "citing_paper_id": "252782478",
      "cited_paper_id": 202558968
    },
    {
      "context_text": "Most existing models for the above tasks [7, 15, 16, 25, 27, 39, 44, 46, 47] are limited in their capability to navigate complex problems requiring external knowledge, because themodels‚Äô feature construction framework is derived solely from the document image.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing models. No dataset names are provided in the context.",
      "processing_time": 53.7442786693573,
      "citing_paper_id": "252782478",
      "cited_paper_id": 218900797
    },
    {
      "context_text": "Most existing models for the above tasks [7, 15, 16, 25, 27, 39, 44, 46, 47] are limited in their capability to navigate complex problems requiring external knowledge, because themodels‚Äô feature construction framework is derived solely from the document image.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of existing models. No dataset names are provided in the context.",
      "processing_time": 53.7442786693573,
      "citing_paper_id": "252782478",
      "cited_paper_id": 235489690
    },
    {
      "context_text": "After that, the document image understanding task began to evolve from perception to induction with models encoding textural, visual, and layout information [7, 15, 16, 25, 27, 39, 44, 46, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the evolution of document image understanding tasks. No verifiable resources are named.",
      "processing_time": 52.54224419593811,
      "citing_paper_id": "252782478",
      "cited_paper_id": 2272015
    },
    {
      "context_text": "After that, the document image understanding task began to evolve from perception to induction with models encoding textural, visual, and layout information [7, 15, 16, 25, 27, 39, 44, 46, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the evolution of document image understanding tasks. No verifiable resources are named.",
      "processing_time": 52.54224419593811,
      "citing_paper_id": "252782478",
      "cited_paper_id": 52815006
    },
    {
      "context_text": "After that, the document image understanding task began to evolve from perception to induction with models encoding textural, visual, and layout information [7, 15, 16, 25, 27, 39, 44, 46, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the evolution of document image understanding tasks. No verifiable resources are named.",
      "processing_time": 52.54224419593811,
      "citing_paper_id": "252782478",
      "cited_paper_id": 85528598
    },
    {
      "context_text": "After that, the document image understanding task began to evolve from perception to induction with models encoding textural, visual, and layout information [7, 15, 16, 25, 27, 39, 44, 46, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the evolution of document image understanding tasks. No verifiable resources are named.",
      "processing_time": 52.54224419593811,
      "citing_paper_id": "252782478",
      "cited_paper_id": 202558968
    },
    {
      "context_text": "After that, the document image understanding task began to evolve from perception to induction with models encoding textural, visual, and layout information [7, 15, 16, 25, 27, 39, 44, 46, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the evolution of document image understanding tasks. No verifiable resources are named.",
      "processing_time": 52.54224419593811,
      "citing_paper_id": "252782478",
      "cited_paper_id": 218900797
    },
    {
      "context_text": "After that, the document image understanding task began to evolve from perception to induction with models encoding textural, visual, and layout information [7, 15, 16, 25, 27, 39, 44, 46, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the evolution of document image understanding tasks. No verifiable resources are named.",
      "processing_time": 52.54224419593811,
      "citing_paper_id": "252782478",
      "cited_paper_id": 235489690
    },
    {
      "context_text": "At this point, we can use public knowledge bases such as ConceptNet [33], Freebase [4], DBPedia [18], and Wiki-Data [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet",
        "Freebase",
        "DBPedia",
        "Wiki-Data"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to integrate common-sense knowledge into multi-modal reasoning systems, enhancing the understanding of complex relationships.",
        "Freebase": "Utilized to incorporate structured data and entity relationships, supporting the construction of comprehensive knowledge graphs.",
        "DBPedia": "Employed to leverage structured information extracted from Wikipedia, enriching the knowledge base for reasoning tasks.",
        "Wiki-Data": "Applied to access a collaborative, multilingual knowledge base, providing a rich source of structured data for reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several public knowledge bases, which are relevant to multi-modal knowledge graph reasoning. These resources are explicitly named and are likely to be used for integrating structured knowledge into reasoning systems.",
      "processing_time": 75.56101274490356,
      "citing_paper_id": "252782478",
      "cited_paper_id": 14494942
    },
    {
      "context_text": "In 2015, with the development of deep learning, models in document perception tasks achieved remarkable results [32, 35].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that models achieved remarkable results in document perception tasks with the development of deep learning.",
      "processing_time": 53.72146391868591,
      "citing_paper_id": "252782478",
      "cited_paper_id": 14728290
    },
    {
      "context_text": "tn}, we apply segmentation [20, 24] to the sequence to merge tokens belonging to the same entity together, using a combination of a NER model 2 and a regular expression extractor.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method (segmentation) and a tool (NER model), but no dataset names are provided.",
      "processing_time": 55.12158703804016,
      "citing_paper_id": "252782478",
      "cited_paper_id": 52841398
    },
    {
      "context_text": "Similar to previous work [30], a knowledge graph represented as a subset RQ,I of all ConceptNet RDF triplesR can be constructed by combining entities in document image and keywords in QA pairs from the dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to construct a knowledge graph by combining entities from document images and keywords from QA pairs, enhancing factual visual question answering."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions constructing a knowledge graph using ConceptNet RDF triples, which suggests the use of a specific dataset. However, the exact name 'ConceptNet' is not mentioned in the context, only inferred from the cited paper title.",
      "processing_time": 62.62569499015808,
      "citing_paper_id": "252782478",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "Even so, some VQAworkmay still be worth noticing for improving the common-sense reasoning ability of the document [10, 23, 28, 30, 38, 40, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to VQA work. No clear identifiers for datasets are present.",
      "processing_time": 53.27750873565674,
      "citing_paper_id": "252782478",
      "cited_paper_id": 53199920
    },
    {
      "context_text": "Even so, some VQAworkmay still be worth noticing for improving the common-sense reasoning ability of the document [10, 23, 28, 30, 38, 40, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to VQA work. No clear identifiers for datasets are present.",
      "processing_time": 53.27750873565674,
      "citing_paper_id": "252782478",
      "cited_paper_id": 173991173
    },
    {
      "context_text": "The textual embedding method of LayoutLMv2 is the same as UniLMv2 [3], while the visual embedding is based on ResNeXt-FPN with Mask-RCNN [13] as its backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the components of LayoutLMv2, including its textual and visual embedding methods.",
      "processing_time": 54.986064434051514,
      "citing_paper_id": "252782478",
      "cited_paper_id": 54465873
    },
    {
      "context_text": "The textual embedding method of LayoutLMv2 is the same as UniLMv2 [3], while the visual embedding is based on ResNeXt-FPN with Mask-RCNN [13] as its backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the components of LayoutLMv2, including its textual and visual embedding methods.",
      "processing_time": 54.986064434051514,
      "citing_paper_id": "252782478",
      "cited_paper_id": 211572655
    },
    {
      "context_text": "The textual embedding method of LayoutLMv2 is the same as UniLMv2 [3], while the visual embedding is based on ResNeXt-FPN with Mask-RCNN [13] as its backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the components of LayoutLMv2, including its textual and visual embedding methods.",
      "processing_time": 54.986064434051514,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "To answer the questions need external knowledge, after mining the information contained in the text, the current NLP model introduces external knowledge such as involving common sense in models [8], adopting logical mathematical operations to enhance models [22], and introducing other domain knowledge [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches to augmenting NLP models with external knowledge. No verifiable resources are named.",
      "processing_time": 54.199116230010986,
      "citing_paper_id": "252782478",
      "cited_paper_id": 189898023
    },
    {
      "context_text": "To answer the questions need external knowledge, after mining the information contained in the text, the current NLP model introduces external knowledge such as involving common sense in models [8], adopting logical mathematical operations to enhance models [22], and introducing other domain knowledge [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches to augmenting NLP models with external knowledge. No verifiable resources are named.",
      "processing_time": 54.199116230010986,
      "citing_paper_id": "252782478",
      "cited_paper_id": 235349192
    },
    {
      "context_text": "tn} document image tokens, which is typically organized in the natural reading order [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LayoutLM) for pre-training text and layout for document image understanding.",
      "processing_time": 52.93114256858826,
      "citing_paper_id": "252782478",
      "cited_paper_id": 209515395
    },
    {
      "context_text": "The textual embedding method of LayoutLMv2 is the same as UniLMv2 [3], while the visual embedding is based on ResNeXt-FPN with MaskRCNN [13] as its backbone.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the components of LayoutLMv2, which are not datasets.",
      "processing_time": 54.756791830062866,
      "citing_paper_id": "252782478",
      "cited_paper_id": 211572655
    },
    {
      "context_text": "Based on ConceptNet, CommonsenseQA [34] had been proposed for common-sense question answering, arousing the research upsurge of common-sense knowledge augmented models in the NLP field [8, 26, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ConceptNet and CommonsenseQA, which are related to common-sense knowledge and question answering. However, neither is a dataset but rather a knowledge graph and a benchmark, respectively.",
      "processing_time": 56.05325889587402,
      "citing_paper_id": "252782478",
      "cited_paper_id": 218487288
    },
    {
      "context_text": "Based on ConceptNet, CommonsenseQA [34] had been proposed for common-sense question answering, arousing the research upsurge of common-sense knowledge augmented models in the NLP field [8, 26, 37, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ConceptNet and CommonsenseQA, which are related to common-sense knowledge and question answering. However, neither is a dataset but rather a knowledge graph and a benchmark, respectively.",
      "processing_time": 56.05325889587402,
      "citing_paper_id": "252782478",
      "cited_paper_id": 232335877
    },
    {
      "context_text": "DocVQA The DocVQA [29] dataset represents the document",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DocVQA"
      ],
      "dataset_descriptions": {
        "DocVQA": "Used to evaluate visual question answering models on document images, focusing on the ability to understand and answer questions about textual and visual content in documents."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DocVQA' as a dataset, which is a specific, verifiable resource. The title confirms it is a dataset for VQA on document images.",
      "processing_time": 61.09446310997009,
      "citing_paper_id": "252782478",
      "cited_paper_id": 220280200
    },
    {
      "context_text": "Specifically, we have selected three different document image understanding tasks to comprehensively evaluate the performance of CALM, including key information extraction with the FUNSD dataset [14], document classification with the RVL-CDIP dataset [12], and document VQA with the DocVQA dataset [29].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FUNSD",
        "RVL-CDIP",
        "DocVQA"
      ],
      "dataset_descriptions": {
        "FUNSD": "Used for key information extraction, evaluating the performance of CALM on extracting structured information from document images.",
        "RVL-CDIP": "Used for document classification, assessing CALM's ability to categorize different types of document images.",
        "DocVQA": "Used for document visual question answering, testing CALM's capability to answer questions about the content of document images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for evaluating the performance of CALM on different document image understanding tasks.",
      "processing_time": 69.90028500556946,
      "citing_paper_id": "252782478",
      "cited_paper_id": 220280200
    },
    {
      "context_text": "DocVQA The DocVQA [29] dataset represents the document understanding task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DocVQA"
      ],
      "dataset_descriptions": {
        "DocVQA": "Used to evaluate visual question answering on document images, focusing on the integration of textual and visual information for document understanding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DocVQA' as a dataset representing the document understanding task, which is relevant to multi-modal knowledge graph reasoning involving text and image data.",
      "processing_time": 60.316147804260254,
      "citing_paper_id": "252782478",
      "cited_paper_id": 220280200
    },
    {
      "context_text": "The DocVQA comprises 50,000 questions on 12,767 images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DocVQA"
      ],
      "dataset_descriptions": {
        "DocVQA": "Used to train and evaluate models on visual question answering tasks using document images, focusing on the ability to understand and answer questions about textual and visual content in documents."
      },
      "confidence_score": 1.0,
      "reasoning": "DocVQA is a specific dataset with a clear name and purpose, relevant to multi-modal learning involving document images and question answering.",
      "processing_time": 60.319143295288086,
      "citing_paper_id": "252782478",
      "cited_paper_id": 220280200
    },
    {
      "context_text": "It is worth mentioning that the CALM model without any module has the same architecture as LayoutLMv2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model architecture. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.184186935424805,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "We further leverage a pretrained LayoutLMv2 model [43] for obtaining the corresponding textual, positional and visual embedding of ùë° ùëñ , denoted ùë° ùë°ùëíùë•ùë°ùëñ , ùë° , and ùë° ùë£ùëñùë†ùëñùëúùëõùëñ , respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using a pretrained LayoutLMv2 model but does not refer to any specific dataset. The focus is on the model's ability to extract multi-modal embeddings.",
      "processing_time": 55.30024242401123,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "We choose the base version of LayoutLMv2 as the multi-modal encoder, since the length of ùë• ùëô is 1068, which is similar to the hidden size of the large version of LayoutLMv2.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (LayoutLMv2). No verifiable resources are identified.",
      "processing_time": 53.60060787200928,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "The base version of LayoutLMv2 uses a 12-layer 12-head transformer encoder and the hidden size ùëë = 768, while in the large version of LayoutLMv2, the pre-trained model uses a 24-layer 16-head transformer encoder and the hidden size ùëë = 1024.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures and configurations. Therefore, no datasets can be extracted.",
      "processing_time": 53.18207359313965,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "Even for the best model, i.e., LayoutLMv2 LARGE , its ANLS score has dropped by 10%, which means its error has increased by 50%.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics of a model. No verifiable resources are identified.",
      "processing_time": 53.024364948272705,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "For LayoutLMv2 LARGE , the add-on module removes about 20% error Because of the insufficient numbers of the types of QA pair relationships in the DocVQA dataset, we perform a similar experiment on the CS-DVQA dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DocVQA",
        "CS-DVQA"
      ],
      "dataset_descriptions": {
        "DocVQA": "Used to evaluate the performance of LayoutLMv2, focusing on the types of QA pair relationships in visually-rich documents.",
        "CS-DVQA": "Used to perform a similar experiment to DocVQA, evaluating the effectiveness of the add-on module in reducing errors in visually-rich document understanding."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, DocVQA and CS-DVQA, which are used to evaluate the performance of LayoutLMv2. The datasets are relevant to multi-modal knowledge graph reasoning as they involve visually-rich document understanding.",
      "processing_time": 70.55856418609619,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "For pre-trained Bert, through the combination of three mechanisms, the model‚Äôs performance on the CS-DVQA dataset has surpassed the multi-modal pre-trained model, LayoutLMv2 BASE .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CS-DVQA"
      ],
      "dataset_descriptions": {
        "CS-DVQA": "Used to evaluate the performance of pre-trained Bert on visually-rich document understanding tasks, comparing it with LayoutLMv2 BASE."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the CS-DVQA dataset, which is used to evaluate the performance of the pre-trained Bert model against LayoutLMv2 BASE. The dataset is specific and relevant to the research context.",
      "processing_time": 62.704322814941406,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "LayoutLMv2 provides two models with different parameter sizes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context is about model parameter sizes, not datasets.",
      "processing_time": 53.28269648551941,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "This error occurs because LayoutLMv2 does not understand common-sense knowledge and cannot get the meaning of entities such as ‚Äúbuffet‚Äù and ‚Äúhost‚Äù that do not appear in the dataset.",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a limitation of LayoutLMv2 regarding common-sense knowledge and entity understanding, but does not specify a dataset name. The reference to 'the dataset' is too generic.",
      "processing_time": 55.16742014884949,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "In these two problems, LayoutLMv2, as the benchmark model, gives ‚Äú1997 RJR United Way Solicitors‚Äô Lun-cheon‚Äù and ‚ÄúSolicitors‚Äù‚Äô as answers, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (LayoutLMv2) and its performance on unnamed tasks. No verifiable resources are identified.",
      "processing_time": 54.177454233169556,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "The back-bone multi-modal representation for word tokens of CALM is the pre-trained LayoutLMv2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LayoutLMv2 as a pre-trained model, not a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 53.99885678291321,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "LayoutLMv2 produces the wrong answer from the red region.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (LayoutLMv2). The citation is about the model's performance issue rather than a dataset.",
      "processing_time": 54.172550439834595,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "Firstly, the Document Purifier needs to encode the multi-modal information of the document image, so we use LayoutLMv2 [43] as the backbone model.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using LayoutLMv2 as a backbone model, which is a method, not a dataset. No specific datasets are mentioned.",
      "processing_time": 54.40285801887512,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "We further leverage a pretrained LayoutLMv2 model [43] for obtaining the corresponding textual, positional and visual embedding of ti , denoted t i , t i , and t i , respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using a pretrained LayoutLMv2 model but does not refer to any specific dataset. The focus is on the model's capability to extract multi-modal embeddings.",
      "processing_time": 55.4883828163147,
      "citing_paper_id": "252782478",
      "cited_paper_id": 229923949
    },
    {
      "context_text": "There are such models as StructuralLM by Alibaba [19], LayoutLM and LayoutXLM by Microsoft [41‚Äì43], DocFormer by Amazon [2], and",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models but does not refer to any specific datasets. The context is focused on listing models rather than describing the use of datasets.",
      "processing_time": 54.396408796310425,
      "citing_paper_id": "252782478",
      "cited_paper_id": 235592814
    },
    {
      "context_text": "There are such models as StructuralLM by Alibaba [19], LayoutLM and LayoutXLM by Microsoft [41‚Äì43], DocFormer by Amazon [2], and SelfDoc by Adobe [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on listing models rather than describing the use of datasets.",
      "processing_time": 54.11469268798828,
      "citing_paper_id": "252782478",
      "cited_paper_id": 235592814
    },
    {
      "context_text": "Most of the present methods solve VQA by jointly learning interactions and performing inference over the question and image contents based on the recent success of deep learning [19, 2, 22, 9, 8], which can be further improved by introducing the attention mechanisms [34, 32, 31, 17, 33, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to deep learning and attention mechanisms in VQA. No verifiable resources are identified.",
      "processing_time": 54.10431504249573,
      "citing_paper_id": "20059796",
      "cited_paper_id": 758237
    },
    {
      "context_text": "Most of the present methods solve VQA by jointly learning interactions and performing inference over the question and image contents based on the recent success of deep learning [19, 2, 22, 9, 8], which can be further improved by introducing the attention mechanisms [34, 32, 31, 17, 33, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to deep learning and attention mechanisms in VQA. No verifiable resources are identified.",
      "processing_time": 54.10431504249573,
      "citing_paper_id": "20059796",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "Most of the present methods solve VQA by jointly learning interactions and performing inference over the question and image contents based on the recent success of deep learning [19, 2, 22, 9, 8], which can be further improved by introducing the attention mechanisms [34, 32, 31, 17, 33, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to deep learning and attention mechanisms in VQA. No verifiable resources are identified.",
      "processing_time": 54.10431504249573,
      "citing_paper_id": "20059796",
      "cited_paper_id": 10363459
    },
    {
      "context_text": "Most of the present methods solve VQA by jointly learning interactions and performing inference over the question and image contents based on the recent success of deep learning [19, 2, 22, 9, 8], which can be further improved by introducing the attention mechanisms [34, 32, 31, 17, 33, 1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to deep learning and attention mechanisms in VQA. No verifiable resources are identified.",
      "processing_time": 54.10431504249573,
      "citing_paper_id": "20059796",
      "cited_paper_id": 195347831
    },
    {
      "context_text": "We apply the memory update mechanism [24, 30] as where W 3 and b 3 are parameters to be optimized.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (memory update mechanism).",
      "processing_time": 51.857786417007446,
      "citing_paper_id": "20059796",
      "cited_paper_id": 1399322
    },
    {
      "context_text": "The memory networks [27, 24, 16] offer an opportunity to address these challenges by reading from and writing to the external memory module, which is modeled by the actions of neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the capabilities of memory networks.",
      "processing_time": 53.304463148117065,
      "citing_paper_id": "20059796",
      "cited_paper_id": 1399322
    },
    {
      "context_text": "In this paper, we adopt external knowledge mined from Concept-Net [23], an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to mine common-sense relationships for aiding reasoning in open-domain visual question answering, enhancing the model's ability to understand and answer questions."
      },
      "confidence_score": 1.0,
      "reasoning": "ConceptNet is identified as a knowledge graph used to aid reasoning in open-domain VQA, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 60.04361081123352,
      "citing_paper_id": "20059796",
      "cited_paper_id": 2924682
    },
    {
      "context_text": "Afterwards, we retrieve the Ô¨Årst-order subgraph using these selected nodes from ConceptNet [23], which includes all edges connecting with at least one candidate node.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConceptNet"
      ],
      "dataset_descriptions": {
        "ConceptNet": "Used to retrieve first-order subgraphs for multi-modal knowledge graph reasoning, focusing on edges connecting candidate nodes."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions ConceptNet, which is a knowledge graph, but does not specify it as a dataset used for training or evaluation. It is referenced as a source of relational knowledge.",
      "processing_time": 60.392011404037476,
      "citing_paper_id": "20059796",
      "cited_paper_id": 2924682
    },
    {
      "context_text": "Both of them are then associated with the corresponding semantic entities in ConceptNet [23] by matching all possible n-grams of words.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ConceptNet but does not indicate it is used as a dataset. It is referenced as a knowledge base or graph, which is not a traditional dataset.",
      "processing_time": 55.15640878677368,
      "citing_paper_id": "20059796",
      "cited_paper_id": 2924682
    },
    {
      "context_text": "However, most of questions in the current VQA dataset are quite simple, which are answerable by analyzing the question and image alone [2, 28].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'VQA dataset' but does not provide a specific name or version. It is a generic reference to a dataset used for visual question answering.",
      "processing_time": 54.98480153083801,
      "citing_paper_id": "20059796",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "In this case, it has garnered increasing attentions due to its numerous potential applications [2], such as providing a more natural way to improve human-computer interaction, enabling the visually impaired individuals to get information about images, etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only potential applications of visual question answering.",
      "processing_time": 52.31061887741089,
      "citing_paper_id": "20059796",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The proposed approach provides a general pipeline that simultaneously preserves the advantages of DNN-based approaches [2, 9, 34] and knowledge-based techniques [26, 25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to DNN-based approaches and knowledge-based techniques. No verifiable resources are identified.",
      "processing_time": 54.43217730522156,
      "citing_paper_id": "20059796",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The proposed approach provides a general pipeline that simultaneously preserves the advantages of DNN-based approaches [2, 9, 34] and knowledge-based techniques [26, 25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to DNN-based approaches and knowledge-based techniques. No verifiable resources are identified.",
      "processing_time": 54.43217730522156,
      "citing_paper_id": "20059796",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "The DNN-based approaches [2, 9, 34] are therefore not sufÔ¨Åcient, since they can only capture information present in the training images.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of DNN-based approaches.",
      "processing_time": 52.51773691177368,
      "citing_paper_id": "20059796",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The recent availability of large training datasets [28] makes it feasible to train a complex model in an end-to-end fashion by leveraging the recent advances in deep neural networks (DNN) [2, 9, 34, 17, 1].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large training datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 53.83775210380554,
      "citing_paper_id": "20059796",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The recent availability of large training datasets [28] makes it feasible to train a complex model in an end-to-end fashion by leveraging the recent advances in deep neural networks (DNN) [2, 9, 34, 17, 1].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large training datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 53.83775210380554,
      "citing_paper_id": "20059796",
      "cited_paper_id": 195347831
    },
    {
      "context_text": "Recent advances witness several attempts to link the knowledge to VQA meth-ods [26, 25], which make use of structured knowledge graphs and reason about an image on the supporting facts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches. There are no clear identifiers for datasets in the given context.",
      "processing_time": 54.18416380882263,
      "citing_paper_id": "20059796",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "Some efforts have been made in this direction, but most of them can only handle a limited number of predeÔ¨Åned types of questions [26, 25].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation in handling predefined types of questions.",
      "processing_time": 53.02155113220215,
      "citing_paper_id": "20059796",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "Recently, it has demonstrated the state-of-the-art performance in numerous NLP applications, including the reading comprehension [20] and textual question answering [6, 16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general NLP applications. No dataset names are provided in the context.",
      "processing_time": 53.82773399353027,
      "citing_paper_id": "20059796",
      "cited_paper_id": 22177955
    },
    {
      "context_text": "Different from the text-based QA problem, it is unfavourable to conduct the open-domain VQA based on the knowledge-based reasoning, since it is inevitably incomplete to describe an image with structured forms [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the limitations of using knowledge-based reasoning for open-domain VQA.",
      "processing_time": 54.37326622009277,
      "citing_paper_id": "20059796",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "A few attempts are made in this direction [29], but it may involve much irrelevant information and fail to implement multi-hop reasoning over several facts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to attempts in a certain direction. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.870644330978394,
      "citing_paper_id": "20059796",
      "cited_paper_id": 206594383
    },
    {
      "context_text": "We use a pre-trained ResNet-101 [12] model to extract image feature, and select 20 candidate knowledge triples for each QA pair through the experiments.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using a pre-trained ResNet-101 model but does not specify any datasets. The citation is focused on the method (ResNet-101) rather than a dataset.",
      "processing_time": 56.38672065734863,
      "citing_paper_id": "20059796",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "In general, VQA is a kind of Visual Turing Test, which rigorously assesses whether a system is able to achieve human-level semantic analysis of images [10, 13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to VQA datasets without naming any. The context focuses on the concept of VQA as a Visual Turing Test.",
      "processing_time": 55.79758262634277,
      "citing_paper_id": "20059796",
      "cited_paper_id": 207061473
    },
    {
      "context_text": "The BM25 algorithm establishes itself as a probabilistic retrieval framework that substantially improves upon traditional TF-IDF methods 45 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the BM25 algorithm and its improvement over TF-IDF methods.",
      "processing_time": 53.978859663009644,
      "citing_paper_id": "278821446",
      "cited_paper_id": 207178704
    },
    {
      "context_text": "The Cascade Binary Tagging Framework (Casrel) 41,42 partially addresses this through relationship-as-function modeling but suffers from oversimplified head entity representation, using mere the start and end vectors averages that discard crucial semantic information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Casrel) and its limitations. No verifiable resources are identified.",
      "processing_time": 54.69956088066101,
      "citing_paper_id": "278821446",
      "cited_paper_id": 208248243
    },
    {
      "context_text": "The current state-of-the-art in graph retrieval predominantly adopts a subgraph extraction approach, which entity linking first identifies query-relevant nodes followed by local subgraph expansion 29,30 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to graph retrieval. No verifiable resources are identified.",
      "processing_time": 53.9154839515686,
      "citing_paper_id": "278821446",
      "cited_paper_id": 231572861
    },
    {
      "context_text": "RAG techniques have gained widespread adoption across multiple domains where output reliability is critical, including clinical decision support 23 , financial risk assessment 24 , and personalized education 25 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application domains of RAG techniques. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.82265663146973,
      "citing_paper_id": "278821446",
      "cited_paper_id": 261076072
    },
    {
      "context_text": "This limitation has spurred reasoning-enhanced graph methods like ROG 34 , TOG 35 and COE 36 , which combine LLMs with KGs to perform multi-hop inference and path planning‚Äîcapabilities particularly valuable for tracing indirect fault relationships.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and capabilities for reasoning on graphs.",
      "processing_time": 53.36646771430969,
      "citing_paper_id": "278821446",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "‚Ä¶and domain-specific KG G , the retrieval task aims to find the optimal path w z that maximizes the probability of answering q : IR = arg max w z P ( w z q, G ) (10) Inspired by the graph-based LLM reasoning work 34‚Äì36 , we propose a Chain of Exploration with Planner-Agent for fault knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to graph-based LLM reasoning work. No verifiable datasets are identified.",
      "processing_time": 55.15122389793396,
      "citing_paper_id": "278821446",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "Although effective for capturing localized knowledge, this paradigm frequently fails to preserve long-range dependencies 31‚Äì33 .",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of a certain paradigm. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 55.396618604660034,
      "citing_paper_id": "278821446",
      "cited_paper_id": 263610099
    },
    {
      "context_text": "In recent years, there have been several convolutional neural networks for learning over graphs (Kipf & Welling, 2017; Niepert et al., 2016; Defferrard et al., 2016; Bruna et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers are about methods for applying CNNs to graph data, which is relevant to multi-modal knowledge graph reasoning but do not specify datasets.",
      "processing_time": 57.06543207168579,
      "citing_paper_id": "275357711",
      "cited_paper_id": 1430801
    },
    {
      "context_text": "In recent years, there have been several convolutional neural networks for learning over graphs (Kipf & Welling, 2017; Niepert et al., 2016; Defferrard et al., 2016; Bruna et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers are about methods for applying CNNs to graph data, which is relevant to multi-modal knowledge graph reasoning but do not specify datasets.",
      "processing_time": 57.06543207168579,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3016223
    },
    {
      "context_text": "In recent years, there have been several convolutional neural networks for learning over graphs (Kipf & Welling, 2017; Niepert et al., 2016; Defferrard et al., 2016; Bruna et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers are about methods for applying CNNs to graph data, which is relevant to multi-modal knowledge graph reasoning but do not specify datasets.",
      "processing_time": 57.06543207168579,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "In recent years, there have been several convolutional neural networks for learning over graphs (Kipf & Welling, 2017; Niepert et al., 2016; Defferrard et al., 2016; Bruna et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers are about methods for applying CNNs to graph data, which is relevant to multi-modal knowledge graph reasoning but do not specify datasets.",
      "processing_time": 57.06543207168579,
      "citing_paper_id": "275357711",
      "cited_paper_id": 17682909
    },
    {
      "context_text": "It is is also finetuned on VQA and Image captioning on MSCOCO captions (Chen et al., 2015), VQA 2.0 (Goyal et al., 2017b) and Flickr30k (Yang et al., 2018a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO captions",
        "VQA 2.0",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "MSCOCO captions": "Used for fine-tuning on image captioning, providing a large set of images paired with descriptive captions.",
        "VQA 2.0": "Used for fine-tuning on visual question answering, enhancing the model's ability to answer questions about images.",
        "Flickr30k": "Used for fine-tuning on image captioning, supplementing the training with diverse image-caption pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for fine-tuning models on visual question answering and image captioning tasks.",
      "processing_time": 71.84597492218018,
      "citing_paper_id": "275357711",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "It is is also finetuned on VQA and Image captioning on MSCOCO captions (Chen et al., 2015), VQA 2.0 (Goyal et al., 2017b) and Flickr30k (Yang et al., 2018a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO captions",
        "VQA 2.0",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "MSCOCO captions": "Used for fine-tuning on image captioning, providing a large set of images paired with descriptive captions.",
        "VQA 2.0": "Used for fine-tuning on visual question answering, enhancing the model's ability to answer questions about images.",
        "Flickr30k": "Used for fine-tuning on image captioning, supplementing the training with diverse image-caption pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for fine-tuning models on visual question answering and image captioning tasks.",
      "processing_time": 71.84597492218018,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "It is is also finetuned on VQA and Image captioning on MSCOCO captions (Chen et al., 2015), VQA 2.0 (Goyal et al., 2017b) and Flickr30k (Yang et al., 2018a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO captions",
        "VQA 2.0",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "MSCOCO captions": "Used for fine-tuning on image captioning, providing a large set of images paired with descriptive captions.",
        "VQA 2.0": "Used for fine-tuning on visual question answering, enhancing the model's ability to answer questions about images.",
        "Flickr30k": "Used for fine-tuning on image captioning, supplementing the training with diverse image-caption pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for fine-tuning models on visual question answering and image captioning tasks.",
      "processing_time": 71.84597492218018,
      "citing_paper_id": "275357711",
      "cited_paper_id": 54460890
    },
    {
      "context_text": "The seminal work of (Antol et al., 2015) released a large-scale dataset for Visual Question Answering.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VQA"
      ],
      "dataset_descriptions": {
        "VQA": "Used to develop and evaluate models that answer questions about images, focusing on the integration of visual and textual information in a multi-modal setting."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the release of a large-scale dataset for Visual Question Answering, which is directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 59.826329946517944,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The success of neural models on Visual Question Answering (VQA) (Antol et al., 2015; Goyal et al., 2017a) is a testament to this.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions VQA but does not refer to a specific dataset. It is used to highlight the success of neural models in the VQA task.",
      "processing_time": 55.14402222633362,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The work of (Goyal et al., 2017a) was aimed to balance the VQA dataset by having almost twice the original image-question pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VQA"
      ],
      "dataset_descriptions": {
        "VQA": "Used to balance the dataset by increasing the number of image-question pairs, enhancing the diversity and complexity of the visual question answering task."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the VQA dataset, which is a well-known multi-modal dataset used for visual question answering tasks. The context indicates that the dataset was balanced by increasing the number of image-question pairs.",
      "processing_time": 62.1697723865509,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "The seminal work of (Antol et al., 2015) released a largescale dataset for Visual Question Answering.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VQA"
      ],
      "dataset_descriptions": {
        "VQA": "Used to develop and evaluate models that answer questions about images, focusing on the integration of visual and textual information in multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the release of a large-scale dataset for Visual Question Answering, which is directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 59.66240835189819,
      "citing_paper_id": "275357711",
      "cited_paper_id": 3180429
    },
    {
      "context_text": "To learn the graph embedding we leverage from the learning of GraphSAGE (Hamilton et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GraphSAGE) for learning graph embeddings.",
      "processing_time": 54.20663404464722,
      "citing_paper_id": "275357711",
      "cited_paper_id": 4755450
    },
    {
      "context_text": "More recently, GraphSAGE (Hamilton et al., 2017) introduced a novel way of inductively learning node representations for large graphs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GraphSAGE) for inductive representation learning on large graphs.",
      "processing_time": 53.87600064277649,
      "citing_paper_id": "275357711",
      "cited_paper_id": 4755450
    },
    {
      "context_text": "In 2018, (Yang et al., 2018b) introduced a text-based QA dataset, HotPotQA that required reasoning over multiple supporting documents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotPotQA"
      ],
      "dataset_descriptions": {
        "HotPotQA": "Used to develop and evaluate text-based QA systems requiring reasoning over multiple documents, enhancing multi-modal knowledge graph reasoning capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the introduction of a specific dataset, HotPotQA, which is relevant to multi-modal knowledge graph reasoning as it involves reasoning over multiple documents.",
      "processing_time": 60.40631365776062,
      "citing_paper_id": "275357711",
      "cited_paper_id": 54460890
    },
    {
      "context_text": "The work from (Murahari et al., 2019) aims to improve the generative ability of models for real-life scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general improvement in generative models. No verifiable resources are identified.",
      "processing_time": 54.21700739860535,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202719040
    },
    {
      "context_text": "There are two major directions of pre-training: (a) Parallel streams of encoders one for each modality followed by fusion (Tan & Bansal, 2019)(Lu et al., 2019); (b) Unified encoder-decoder representations that can take both the language or image modalities (Zhou et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for multi-modal pre-training.",
      "processing_time": 53.83691143989563,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202734445
    },
    {
      "context_text": "VLP (Zhou et al., 2019) is a pre-trained multimodal trans-former well suited for both understanding and generative tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions VLP as a pre-trained model, not a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 54.75355553627014,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202734445
    },
    {
      "context_text": "More-over, VinVL (Zhang et al., 2021) pre-training focuses on improving the object-centric visual representations that can be used by the downstream VLP-based multimodal fusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VinVL, which is a method for pre-training visual representations, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 55.713043451309204,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202734445
    },
    {
      "context_text": "More-over, VinVL (Zhang et al., 2021) pre-training focuses on improving the object-centric visual representations that can be used by the downstream VLP-based multimodal fusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VinVL, which is a method for pre-training visual representations, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 55.713043451309204,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "For example, our graph-based approaches are trained from scratch, whereas the VLP-VinVL is heavily pre-trained with several multimodal task objectives on billions of image-text pairs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'VLP-VinVL' which is a method/model, not a dataset. No specific datasets are mentioned or used according to the given context.",
      "processing_time": 56.464993715286255,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202734445
    },
    {
      "context_text": "For example, our graph-based approaches are trained from scratch, whereas the VLP-VinVL is heavily pre-trained with several multimodal task objectives on billions of image-text pairs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'VLP-VinVL' which is a method/model, not a dataset. No specific datasets are mentioned or used according to the given context.",
      "processing_time": 56.464993715286255,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "While it is true that the VLP-VinVL baseline has better F1 scores across the board, we wish to stress here the complexity and resource requirements for this baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance and resource requirements of a model.",
      "processing_time": 54.522430181503296,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202734445
    },
    {
      "context_text": "While it is true that the VLP-VinVL baseline has better F1 scores across the board, we wish to stress here the complexity and resource requirements for this baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance and resource requirements of a model.",
      "processing_time": 54.522430181503296,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "The source retrieval baseline is finetuned on the VLP For a given question Q , every source S Q,i ‚àà S Q is fed into the baseline model one by one.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (VLP) which is not included as per instructions.",
      "processing_time": 55.298375606536865,
      "citing_paper_id": "275357711",
      "cited_paper_id": 202734445
    },
    {
      "context_text": "The authors propose a novel multimodal framework called MExBERT (Multimodal Extractive BERT) that uses joint attention over input textual and visual streams for extracting multimodal answers given a question.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MExBERT) and a reference to BERT. No verifiable datasets are identified.",
      "processing_time": 56.1748206615448,
      "citing_paper_id": "275357711",
      "cited_paper_id": 226096901
    },
    {
      "context_text": "For text, this encoder E T is a sentence level representation from BERT (Devlin et al., 2019).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 56.44574332237244,
      "citing_paper_id": "275357711",
      "cited_paper_id": 226096901
    },
    {
      "context_text": "Moreover, we can observe that the question contains some spanish words which may not be interpreted correctly in the embedding space of our BERT model which is trained in english.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a potential issue with Spanish words in the BERT model's embedding space.",
      "processing_time": 55.103715896606445,
      "citing_paper_id": "275357711",
      "cited_paper_id": 226096901
    },
    {
      "context_text": "MIMOQ (Singh et al., 2021) proposes a QA system that can reason and also respond in multiple modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MIMOQ, which is a method or system, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 55.10138010978699,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235097195
    },
    {
      "context_text": "However, the key contribution of MIMOQ is still to generate multimodal answers and not information aggregation across modalities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of the MIMOQ system. No verifiable resources are identified.",
      "processing_time": 55.68078804016113,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235097195
    },
    {
      "context_text": "however demonstrated VinVL (Zhang et al., 2021) to work best.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model (VinVL). The context is about the performance of the model, not the use of a dataset.",
      "processing_time": 56.37413835525513,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "For images, the baseline extracts ‚àº 100 region proposals using latest state-of-the-art model VinVL (Zhang et al., 2021).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (VinVL) but does not refer to a specific dataset. The citation is focused on the method used for extracting region proposals from images.",
      "processing_time": 56.369659423828125,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "Moreover, VinVL (Zhang et al., 2021) pre-training focuses on improving the object-centric visual representations that can be used by the downstream VLP-based multimodal fusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VinVL as a method for pre-training visual representations, not as a dataset. No specific dataset is mentioned.",
      "processing_time": 55.265092611312866,
      "citing_paper_id": "275357711",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "Bayesian Networks (BN) [15] and Multi-Entity Bayesian Networks (MEBN) [16, 17] model probabilistic dependencies using directed acyclic graphs and conditional probability distributions (CPDs).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.02815246582031,
      "citing_paper_id": "279802582",
      "cited_paper_id": 33389032
    },
    {
      "context_text": "Bayesian Networks (BN) [15] and Multi-Entity Bayesian Networks (MEBN) [16, 17] model probabilistic dependencies using directed acyclic graphs and conditional probability distributions (CPDs).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.02815246582031,
      "citing_paper_id": "279802582",
      "cited_paper_id": 40176582
    },
    {
      "context_text": "Bayesian Networks (BN) [15] and Multi-Entity Bayesian Networks (MEBN) [16, 17] model probabilistic dependencies using directed acyclic graphs and conditional probability distributions (CPDs).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.02815246582031,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Based on Description Logics (DL) [10], OWL uses axioms ( C , R , A ) to deÔ¨Åne concepts, relations, and individuals.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical constructs and standards. No verifiable resources are identified.",
      "processing_time": 54.83354640007019,
      "citing_paper_id": "279802582",
      "cited_paper_id": 35467721
    },
    {
      "context_text": "Bidirectional Encoder Representations from Transformers (BERT) revolutionized natural language understanding [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BERT but does not refer to it as a dataset. BERT is a model, not a dataset, and thus does not meet the criteria for inclusion.",
      "processing_time": 57.1232590675354,
      "citing_paper_id": "279802582",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Using self-attention mechanisms, BERT generates deeply contextualized word embeddings ( V = ( v 1 , ..., v n ) ), capturing nuances in text [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the BERT model. BERT is a method/model, not a dataset, and thus does not qualify for inclusion.",
      "processing_time": 56.77104330062866,
      "citing_paper_id": "279802582",
      "cited_paper_id": 59291975
    },
    {
      "context_text": "BERT and similar transformer models have shown promise in analyzing clinical notes or patient messages [6, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their potential applications. No verifiable resources are identified.",
      "processing_time": 55.09647274017334,
      "citing_paper_id": "279802582",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "BERT and similar transformer models have shown promise in analyzing clinical notes or patient messages [6, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their potential applications. No verifiable resources are identified.",
      "processing_time": 55.09647274017334,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional approaches, frequently reliant on static rules or explicit self-reporting, struggle to capture the full context and implicit cues within beneÔ¨Åciary situations, leading to potential mismatches in service allocation [1].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with traditional approaches in service allocation.",
      "processing_time": 54.495280027389526,
      "citing_paper_id": "279802582",
      "cited_paper_id": 207997778
    },
    {
      "context_text": "The OwlMEBN Jena API [19, 20] speciÔ¨Åcally integrates OWL ontologies with MEBN, allowing probabilistic assertions alongside ontological axioms [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods for integrating ontologies with MEBN.",
      "processing_time": 54.899516582489014,
      "citing_paper_id": "279802582",
      "cited_paper_id": 232061871
    },
    {
      "context_text": "The OwlMEBN Jena API [19, 20] speciÔ¨Åcally integrates OWL ontologies with MEBN, allowing probabilistic assertions alongside ontological axioms [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods for integrating ontologies with MEBN.",
      "processing_time": 54.899516582489014,
      "citing_paper_id": "279802582",
      "cited_paper_id": 263873151
    },
    {
      "context_text": "The OwlMEBN Jena API [19, 20] speciÔ¨Åcally integrates OWL ontologies with MEBN, allowing probabilistic assertions alongside ontological axioms [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods for integrating ontologies with MEBN.",
      "processing_time": 54.899516582489014,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Probabilistic graphical models are suitable for reasoning under uncertainty in health-care [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to reasoning under uncertainty in healthcare using probabilistic graphical models.",
      "processing_time": 55.363887786865234,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Previous iterations of the e-SAAD platform, for instance, utilized ontologies and rule-based inferences [2] or introduced Bayesian Networks for uncertainty [3], but faced limitations in deeply understanding unstructured communication and adaptively integrating diverse data sources.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodologies and limitations. There are no clear identifiers for datasets in the text.",
      "processing_time": 54.69092011451721,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "While recent studies have explored machine learning, including deep learning, for analyzing speciÔ¨Åc data modalities like sensor data [5] or text [6] in ambient assisted living (AAL), effectively integrating these with structured domain knowledge and robust probabilistic reasoning remains an open‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the integration of different data modalities and the challenges in ambient assisted living.",
      "processing_time": 56.400108337402344,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "While recent studies have explored machine learning, including deep learning, for analyzing speciÔ¨Åc data modalities like sensor data [5] or text [6] in ambient assisted living (AAL), effectively integrating these with structured domain knowledge and robust probabilistic reasoning remains an open‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the integration of different data modalities and the challenges in ambient assisted living.",
      "processing_time": 56.400108337402344,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Combining data from various sources (sensors, text, observations) often yields a more comprehensive understanding than using any single source [22, 23].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to generic data sources such as sensors, text, and observations.",
      "processing_time": 55.300689697265625,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶several key challenges: understanding implicit needs expressed through natural language, integrating heterogeneous data (structured proÔ¨Åles, sensor streams, unstructured notes), reasoning under the inherent uncertainty of real-world situations, and providing personalized, context-aware support [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses general challenges in a research area without referencing any particular dataset.",
      "processing_time": 56.077715158462524,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Fusion techniques range from simple concatenation to weighted sums or more complex attention mechanisms [24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only fusion techniques. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.767831325531006,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Fusion techniques range from simple concatenation to weighted sums or more complex attention mechanisms [24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only fusion techniques. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.767831325531006,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "The Web Ontology Language (OWL) provides a formal standard for knowledge representation, enabling structured deÔ¨Ånitions and automated reasoning [7, 8, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Web Ontology Language (OWL) which is a standard for knowledge representation. No datasets are referenced for use in research.",
      "processing_time": 57.439146280288696,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "The Web Ontology Language (OWL) provides a formal standard for knowledge representation, enabling structured deÔ¨Ånitions and automated reasoning [7, 8, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Web Ontology Language (OWL) which is a standard for knowledge representation. No datasets are referenced for use in research.",
      "processing_time": 57.439146280288696,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "In healthcare and AAL, OWL has been used for modeling patient data, care plans, and service descriptions [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to the use of OWL for modeling in healthcare and AAL, which is a method or approach rather than a dataset.",
      "processing_time": 57.501444816589355,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Multi-modal fusion is increasingly explored in AAL [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general trend in a field.",
      "processing_time": 55.203784465789795,
      "citing_paper_id": "279802582",
      "cited_paper_id": null
    },
    {
      "context_text": "Hybrid-LLM[12] proposes a two-stage decision-making process, in which a lightweight model is used to generate an initial response, and a self-validation module is used to determine whether to invoke a strong model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context describes a two-stage decision-making process involving models, not datasets.",
      "processing_time": 56.71505522727966,
      "citing_paper_id": "280001156",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "Hybrid-LLM[12] proposes a two-stage decision-making process, in which a lightweight model is used to generate an initial response, and a self-validation module is used to determine whether to invoke a strong model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context describes a two-stage decision-making process involving models, not datasets.",
      "processing_time": 56.71505522727966,
      "citing_paper_id": "280001156",
      "cited_paper_id": 259095695
    },
    {
      "context_text": "‚Ä¶the performance of MA-Router, four benchmark models were selected for comparison: 1) Traditional routing strategies (RouteLLM-MF[5], Hybrid-LLM[6]); 2) Cost-sensitive method (FrugalGPT[14]); 3) Attention enhancement model (GAT-Router[22]); 4) Reinforcement learning baseline (AutoRoute[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for comparison. No verifiable resources are identified.",
      "processing_time": 56.159517765045166,
      "citing_paper_id": "280001156",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "‚Ä¶the performance of MA-Router, four benchmark models were selected for comparison: 1) Traditional routing strategies (RouteLLM-MF[5], Hybrid-LLM[6]); 2) Cost-sensitive method (FrugalGPT[14]); 3) Attention enhancement model (GAT-Router[22]); 4) Reinforcement learning baseline (AutoRoute[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for comparison. No verifiable resources are identified.",
      "processing_time": 56.159517765045166,
      "citing_paper_id": "280001156",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "‚Ä¶the performance of MA-Router, four benchmark models were selected for comparison: 1) Traditional routing strategies (RouteLLM-MF[5], Hybrid-LLM[6]); 2) Cost-sensitive method (FrugalGPT[14]); 3) Attention enhancement model (GAT-Router[22]); 4) Reinforcement learning baseline (AutoRoute[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for comparison. No verifiable resources are identified.",
      "processing_time": 56.159517765045166,
      "citing_paper_id": "280001156",
      "cited_paper_id": 230435739
    },
    {
      "context_text": "‚Ä¶the performance of MA-Router, four benchmark models were selected for comparison: 1) Traditional routing strategies (RouteLLM-MF[5], Hybrid-LLM[6]); 2) Cost-sensitive method (FrugalGPT[14]); 3) Attention enhancement model (GAT-Router[22]); 4) Reinforcement learning baseline (AutoRoute[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for comparison. No verifiable resources are identified.",
      "processing_time": 56.159517765045166,
      "citing_paper_id": "280001156",
      "cited_paper_id": 231573431
    },
    {
      "context_text": "In order to comprehensively evaluate the performance of MA-Router, four benchmark models were selected for comparison: 1) Traditional routing strategies (RouteLLM-MF[5], Hybrid-LLM[6]); 2) Cost-sensitive method (FrugalGPT[14]); 3) Attention enhancement model (GAT-Router[22]); 4) Reinforcement learning baseline (AutoRoute[15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.993239402770996,
      "citing_paper_id": "280001156",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "‚Ä¶as RouteLLM[5], which uses matrix decomposition to model model-problem interaction and realizes binary classification decisions by presetting thresholds; (2) Cascading verification mechanism, such as Hybrid-LLM[6], determines whether to invoke large models through self-verification of small models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.72330355644226,
      "citing_paper_id": "280001156",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "The existing methods mainly evolve along two technical routes: (1) Routing strategies based on static rules, such as RouteLLM[5], which uses matrix decomposition to model model-problem interaction and realizes binary classification decisions by presetting thresholds; (2) Cascading verification mechanism, such as Hybrid-LLM[6], determines whether to invoke large models through self-verification of small models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 55.82744908332825,
      "citing_paper_id": "280001156",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "The existing methods mainly evolve along two technical routes: (1) Routing strategies based on static rules, such as RouteLLM[5], which uses matrix decomposition to model model-problem interaction and realizes binary classification decisions by presetting thresholds; (2) Cascading verification mechanism, such as Hybrid-LLM[6], determines whether to invoke large models through self-verification of small models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 55.82744908332825,
      "citing_paper_id": "280001156",
      "cited_paper_id": 236635565
    },
    {
      "context_text": "The existing methods mainly evolve along two technical routes: (1) Routing strategies based on static rules, such as RouteLLM[5], which uses matrix decomposition to model model-problem interaction and realizes binary classification decisions by presetting thresholds; (2) Cascading verification mechanism, such as Hybrid-LLM[6], determines whether to invoke large models through self-verification of small models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 55.82744908332825,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "The existing methods mainly evolve along two technical routes: (1) Routing strategies based on static rules, such as RouteLLM[5], which uses matrix decomposition to model model-problem interaction and realizes binary classification decisions by presetting thresholds; (2) Cascading verification mechanism, such as Hybrid-LLM[6], determines whether to invoke large models through self-verification of small models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 55.82744908332825,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "RouteLLM[17] tried to use it for query coding, but did not consider syntactic structure characteristics, resulting in an 18% increase in mathematical expression routing error rate [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RouteLLM) and its performance. No verifiable resources are identified.",
      "processing_time": 57.03378653526306,
      "citing_paper_id": "280001156",
      "cited_paper_id": 195657814
    },
    {
      "context_text": "RouteLLM[17] tried to use it for query coding, but did not consider syntactic structure characteristics, resulting in an 18% increase in mathematical expression routing error rate [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RouteLLM) and its performance. No verifiable resources are identified.",
      "processing_time": 57.03378653526306,
      "citing_paper_id": "280001156",
      "cited_paper_id": 236635565
    },
    {
      "context_text": "RouteLLM[17] tried to use it for query coding, but did not consider syntactic structure characteristics, resulting in an 18% increase in mathematical expression routing error rate [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RouteLLM) and its performance. No verifiable resources are identified.",
      "processing_time": 57.03378653526306,
      "citing_paper_id": "280001156",
      "cited_paper_id": 263227026
    },
    {
      "context_text": "RouteLLM[17] tried to use it for query coding, but did not consider syntactic structure characteristics, resulting in an 18% increase in mathematical expression routing error rate [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RouteLLM) and its performance. No verifiable resources are identified.",
      "processing_time": 57.03378653526306,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "RouteLLM[17] tried to use it for query coding, but did not consider syntactic structure characteristics, resulting in an 18% increase in mathematical expression routing error rate [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RouteLLM) and its performance. No verifiable resources are identified.",
      "processing_time": 57.03378653526306,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶mechanism in routing decision has been gradually paid attention to, mainly applied to feature representation and relationship modeling: DeBERTa-v3[16] enhanced semantic representation by decoupling attention mechanism, and improved the accuracy of text classification tasks by 4.2% compared with‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the attention mechanism and its application in feature representation and relationship modeling.",
      "processing_time": 57.63111853599548,
      "citing_paper_id": "280001156",
      "cited_paper_id": 203626972
    },
    {
      "context_text": "The potential of attention mechanism in routing decision has been gradually paid attention to, mainly applied to feature representation and relationship modeling: DeBERTa-v3[16] enhanced semantic representation by decoupling attention mechanism, and improved the accuracy of text classification tasks by 4.2% compared with BERT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance improvements. The context focuses on the attention mechanism and its application in feature representation and relationship modeling.",
      "processing_time": 57.95010328292847,
      "citing_paper_id": "280001156",
      "cited_paper_id": 203626972
    },
    {
      "context_text": "DeBERTa-v3's contextual semantic coding ability can capture the deep meaning of words [16], while GNN's modeling of syntactic dependent graphs follows the structural equivalence principle of graph representation learning [19], which can effectively represent syntactic relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.34133958816528,
      "citing_paper_id": "280001156",
      "cited_paper_id": 203626972
    },
    {
      "context_text": "DeBERTa-v3's contextual semantic coding ability can capture the deep meaning of words [16], while GNN's modeling of syntactic dependent graphs follows the structural equivalence principle of graph representation learning [19], which can effectively represent syntactic relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.34133958816528,
      "citing_paper_id": "280001156",
      "cited_paper_id": 261016305
    },
    {
      "context_text": "FrugalGPT[14] is further extended to a sequential call strategy, which invokes the model in ascending order of cost until the quality threshold is met, but its greedy decision pattern results in a 1.7-fold increase in the average number of calls in complex tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FrugalGPT) and its extension. The context is focused on the performance and strategy of the method, not on the use of datasets.",
      "processing_time": 59.65668082237244,
      "citing_paper_id": "280001156",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "The latest generation of models, such as GPT-4 and Claude 3, have reached near-human performance levels in complex reasoning, code generation and other t.asks [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There is no indication of a reusable resource or specific dataset usage.",
      "processing_time": 56.95907521247864,
      "citing_paper_id": "280001156",
      "cited_paper_id": 219182397
    },
    {
      "context_text": "However, its routing decision is only based on top-level semantic features, ignoring key indicators such as syntactic complexity, and the expert error rate in code generation tasks is as high as 23%[22].",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations and error rates in code generation tasks.",
      "processing_time": 56.19755816459656,
      "citing_paper_id": "280001156",
      "cited_paper_id": 230435739
    },
    {
      "context_text": "According to reference [22], the structural variation of dependency trees can simulate complex syntactic patterns (such as nested conditional sentences) in natural languages, thereby enhancing the model's adaptability to unseen grammatical patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the use of dependency trees for simulating syntactic patterns.",
      "processing_time": 56.024256467819214,
      "citing_paper_id": "280001156",
      "cited_paper_id": 230435739
    },
    {
      "context_text": "AutoRoute[15] designed a threshold adjustment strategy based on Q-learning to achieve 19% cost optimization under dynamic load environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (AutoRoute) and its performance improvement. The context is about a threshold adjustment strategy using Q-learning, which is not a dataset.",
      "processing_time": 59.38893389701843,
      "citing_paper_id": "280001156",
      "cited_paper_id": 231573431
    },
    {
      "context_text": "‚Ä¶statistics in 2024, more than 68% of queries in commercial-grade LLM applications belong to low-complexity tasks, but the existing system still generally adopts the extensive strategy of full call of a single model, resulting in an annual waste of more than $230 million in computing power [4].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on statistics about query complexity and computational costs in LLM applications.",
      "processing_time": 57.24992561340332,
      "citing_paper_id": "280001156",
      "cited_paper_id": 235417196
    },
    {
      "context_text": "Existing methods are mainly divided into the following three categories: RouteLLM[9] first proposed using human preference data to train binary routing models, modeling model-problem interaction ( 2 1 ( , ) ( ( )) M q ) through matrix decomposition, and combining static thresholds to achieve strong/weak model shingling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (RouteLLM) and its approach but does not reference any dataset by name.",
      "processing_time": 57.791194438934326,
      "citing_paper_id": "280001156",
      "cited_paper_id": 236635565
    },
    {
      "context_text": "Existing methods are mainly divided into the following three categories: RouteLLM[9] first proposed using human preference data to train binary routing models, modeling model-problem interaction ( 2 1 ( , ) ( ( )) M q ) through matrix decomposition, and combining static thresholds to achieve strong/weak model shingling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (RouteLLM) and its approach but does not reference any dataset by name.",
      "processing_time": 57.791194438934326,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing methods are mainly divided into the following three categories: RouteLLM[9] first proposed using human preference data to train binary routing models, modeling model-problem interaction ( 2 1 ( , ) ( ( )) M q ) through matrix decomposition, and combining static thresholds to achieve strong/weak model shingling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (RouteLLM) and its approach but does not reference any dataset by name.",
      "processing_time": 57.791194438934326,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶evaluate the performance of MA-Router, four benchmark models were selected for comparison: 1) Traditional routing strategies (RouteLLM-MF[5], Hybrid-LLM[6]); 2) Cost-sensitive method (FrugalGPT[14]); 3) Attention enhancement model (GAT-Router[22]); 4) Reinforcement learning baseline‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.965444564819336,
      "citing_paper_id": "280001156",
      "cited_paper_id": 236635565
    },
    {
      "context_text": "The existing methods mainly evolve along two technical routes: (1) Routing strategies based on static rules, such as RouteLLM[5], which uses matrix decomposition to model model-problem interaction and realizes binary classification decisions by presetting thresholds; (2) Cascading verification‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.62202739715576,
      "citing_paper_id": "280001156",
      "cited_paper_id": 236635565
    },
    {
      "context_text": "Similarly, LLM-BLENDER[11] adopted the integrated learning strategy to improve the quality through multi-model response fusion, but the inference delay increased by 3.8 times, which was difficult to meet the real-time requirements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LLM-BLENDER) and its performance issues. No verifiable resources are identified.",
      "processing_time": 57.55169129371643,
      "citing_paper_id": "280001156",
      "cited_paper_id": 237416585
    },
    {
      "context_text": "Finally, traditional feature representations ignore deep semantic cues such as grammatical structures, resulting in a routing error rate of 28.6% for complex mathematical reasoning tasks [8].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric for a task. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.09955048561096,
      "citing_paper_id": "280001156",
      "cited_paper_id": 239016062
    },
    {
      "context_text": "In recent years, Large Language Models (LLMs) have shown breakthrough progress in natural language processing, and their parameter scale has expanded from billions to trillions [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general progress of Large Language Models (LLMs).",
      "processing_time": 55.24856901168823,
      "citing_paper_id": "280001156",
      "cited_paper_id": 240420063
    },
    {
      "context_text": "‚Ä¶data relying on manual annotation is seriously sparse, resulting in a 37%-52% decline in routing accuracy in Out-of-Domain (OOD) scenarios [7]; Secondly, the static threshold setting is difficult to adapt to the dynamic load environment, which may lead to systematic decision-making bias‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with data sparsity and static threshold settings. No verifiable resources are identified.",
      "processing_time": 56.14568114280701,
      "citing_paper_id": "280001156",
      "cited_paper_id": 247618722
    },
    {
      "context_text": "However, the increase in model power is accompanied by an exponential increase in computing costs - a single GPT-4 inference consumes 120 times as much energy as Llama 3-8B [3], which leads to the dilemma of \"performance excess\" and \"cost runaway\" in practical applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their computational costs. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.66090512275696,
      "citing_paper_id": "280001156",
      "cited_paper_id": 256503897
    },
    {
      "context_text": "This method achieves 2.1-fold cost reduction on MT-Bench, but its performance is limited by the sparsity of preference data, and APGR decreases by 37% in OOD scenarios [10].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MT-Bench, which is a benchmark suite, and does not specify any particular dataset. The context focuses on performance metrics and limitations rather than dataset usage.",
      "processing_time": 57.22100591659546,
      "citing_paper_id": "280001156",
      "cited_paper_id": 256868484
    },
    {
      "context_text": "GraphSAGE[19] et al graph neural networks were used to model entity relationships and achieve a 15% click through rate increase in the recommendation system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of GraphSAGE and other graph neural networks. The context focuses on the methodology and performance improvement in a recommendation system.",
      "processing_time": 57.51875019073486,
      "citing_paper_id": "280001156",
      "cited_paper_id": 256868484
    },
    {
      "context_text": "GraphSAGE[19] et al graph neural networks were used to model entity relationships and achieve a 15% click through rate increase in the recommendation system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of GraphSAGE and other graph neural networks. The context focuses on the methodology and performance improvement in a recommendation system.",
      "processing_time": 57.51875019073486,
      "citing_paper_id": "280001156",
      "cited_paper_id": 261016305
    },
    {
      "context_text": "Graph feature coding adopts GraphSAGE[10] for inductive graph embedding learning: ( ) ( ) CONCAT( ,AGG({ , ( )})) l l v u h h v Where ( ) N v represents the neighborhood of node v , AGG is the mean aggregate function, and the syntactic feature 128 syn h ‚àà ÔÅí is obtained after 3 layers of propagation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GraphSAGE) for graph embedding learning. No verifiable datasets are referenced.",
      "processing_time": 56.32011675834656,
      "citing_paper_id": "280001156",
      "cited_paper_id": 256868484
    },
    {
      "context_text": "The cross-attention mechanism (Formula 3) realizes the optimal fusion of multi-modal features by maximizing the mutual information of semantic-syntactic features 21], and its theoretical validity has been verified in multi-task learning [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (cross-attention mechanism) and its application in multi-task learning. No verifiable resources are identified.",
      "processing_time": 56.865718841552734,
      "citing_paper_id": "280001156",
      "cited_paper_id": 260435365
    },
    {
      "context_text": "The cross-attention mechanism (Formula 3) realizes the optimal fusion of multi-modal features by maximizing the mutual information of semantic-syntactic features 21], and its theoretical validity has been verified in multi-task learning [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (cross-attention mechanism) and its application in multi-task learning. No verifiable resources are identified.",
      "processing_time": 56.865718841552734,
      "citing_paper_id": "280001156",
      "cited_paper_id": 270764307
    },
    {
      "context_text": "‚Ä¶still have the following core challenges: The coverage of preference data based on human annotations (e.g. Chatbot Arena) is less than 0.1%[23] and the lack of syntactic diversity enhancement results in a significant decline in the model's performance on previously unseen grammatical‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Chatbot Arena"
      ],
      "dataset_descriptions": {
        "Chatbot Arena": "Mentioned as having limited coverage of preference data based on human annotations, highlighting the challenge of insufficient syntactic diversity in training data."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Chatbot Arena' as an example of a dataset with limited coverage of preference data based on human annotations. However, it does not provide specific details on how this dataset is used in the research context.",
      "processing_time": 66.02463412284851,
      "citing_paper_id": "280001156",
      "cited_paper_id": 260435365
    },
    {
      "context_text": "Switch Transformer[21] proposes a sparsely activated MoE architecture to dynamically select expert modules through routers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Switch Transformer) and its architecture. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.28978657722473,
      "citing_paper_id": "280001156",
      "cited_paper_id": 270764307
    },
    {
      "context_text": "Existing methods are mainly divided into the following three categories: RouteLLM[9] first proposed using human preference data to train binary routing models, modeling model-problem interaction ( 2 1 ( , ) ( ( )) M q ) through matrix decomposition, and combining static thresholds to achieve‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method and its approach but does not reference any dataset by name.",
      "processing_time": 56.024269342422485,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing methods are mainly divided into the following three categories: RouteLLM[9] first proposed using human preference data to train binary routing models, modeling model-problem interaction ( 2 1 ( , ) ( ( )) M q ) through matrix decomposition, and combining static thresholds to achieve‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method and its approach but does not reference any dataset by name.",
      "processing_time": 56.024269342422485,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "The DeBERTa-v3 model [9] is used to extract the deep semantic features of the query, and the lexical sequence is obtained after the input query q is divided into words.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (DeBERTa-v3) which is not a dataset. The citation is focused on the methodology of using the model to extract semantic features.",
      "processing_time": 57.694451093673706,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "The DeBERTa-v3 model [9] is used to extract the deep semantic features of the query, and the lexical sequence is obtained after the input query q is divided into words.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (DeBERTa-v3) which is not a dataset. The citation is focused on the methodology of using the model to extract semantic features.",
      "processing_time": 57.694451093673706,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "The study of dynamic model selection strategy can be traced back to the early cascaded classifier system[9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to a cascaded classifier system. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.24569535255432,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "The study of dynamic model selection strategy can be traced back to the early cascaded classifier system[9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to a cascaded classifier system. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.24569535255432,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "This method saves 42% of computational costs in mathematical reasoning tasks, but the self-verification module relies on manual rule design, and the misjudgment rate in open domain dialogues is as high as 28%[13].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses computational costs and performance metrics.",
      "processing_time": 52.99622201919556,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "This method saves 42% of computational costs in mathematical reasoning tasks, but the self-verification module relies on manual rule design, and the misjudgment rate in open domain dialogues is as high as 28%[13].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses computational costs and performance metrics.",
      "processing_time": 52.99622201919556,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "In the GSM8K test, the misjudgment rate of the router using only semantic features for multi-step reasoning problems is as high as 31.4%, which is significantly higher than the 6.2% of human taggers [24].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and comparisons. The context is about the performance of a router using semantic features for reasoning problems.",
      "processing_time": 55.363242864608765,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "In the GSM8K test, the misjudgment rate of the router using only semantic features for multi-step reasoning problems is as high as 31.4%, which is significantly higher than the 6.2% of human taggers [24].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and comparisons. The context is about the performance of a router using semantic features for reasoning problems.",
      "processing_time": 55.363242864608765,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "Based on the knowledge transfer theory Reference [24], knowledge graph extension can effectively mitigate domain bias by introducing external entity association information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concept. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.58233118057251,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "Based on the knowledge transfer theory Reference [24], knowledge graph extension can effectively mitigate domain bias by introducing external entity association information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concept. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.58233118057251,
      "citing_paper_id": "280001156",
      "cited_paper_id": null
    },
    {
      "context_text": "A comprehensive review of state-of-the-art methods for anomaly detection using language modeling in the financial domain is given in paper [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of methods for anomaly detection in finance.",
      "processing_time": 52.6973021030426,
      "citing_paper_id": "266747200",
      "cited_paper_id": 201666793
    },
    {
      "context_text": "CTRL was developed to address this limitation by allowing users to control various aspects of text generation [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (CTRL) which is not included as per instructions.",
      "processing_time": 53.65736484527588,
      "citing_paper_id": "266747200",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Pretrained Chinese language models were revisited in paper [6] to see if the techniques in these state-of-the-art models generalize well to languages other than English.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pretrained models. The context focuses on the generalization of techniques across languages, not on the use of a particular dataset.",
      "processing_time": 55.73811602592468,
      "citing_paper_id": "266747200",
      "cited_paper_id": 216641856
    },
    {
      "context_text": "Leippold et al. explored whether financial text could be manipulated to fool machine readers for sentiment analysis [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to financial text. No clear, verifiable dataset is identified.",
      "processing_time": 53.46298336982727,
      "citing_paper_id": "266747200",
      "cited_paper_id": 256436290
    },
    {
      "context_text": "In recent years, one of the most promising NLP models for NLP is the Generative Pretrained Transformer (GPT) family, which includes GPT-2, GPT-3, and ChatGPT [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context focuses on the GPT family of models, which are excluded according to the instructions.",
      "processing_time": 54.31131339073181,
      "citing_paper_id": "266747200",
      "cited_paper_id": 256827430
    },
    {
      "context_text": "The paper [9] introduces the REFinD dataset and demonstrates a solution for relation extraction using the REFinD dataset by leveraging the OpenAI model within the contextual learning (ICL) framework.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REFinD"
      ],
      "dataset_descriptions": {
        "REFinD": "Used for financial relation extraction, specifically leveraging the OpenAI model within the in-context learning framework to improve relation extraction accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the REFinD dataset, which is used for relation extraction in a financial context. The dataset is leveraged within the ICL framework with the OpenAI model.",
      "processing_time": 61.906291246414185,
      "citing_paper_id": "266747200",
      "cited_paper_id": 259309148
    },
    {
      "context_text": "Linardatos et al. [2020] 8 the ability to explain or to present in understandable terms to a human Doshi-Velez and Kim [2017] that uses Weinberger et al. [2009] on top of HANS for the scene similarity search.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on explaining or presenting terms understandably, using a method on top of HANS for scene similarity search.",
      "processing_time": 55.96789312362671,
      "citing_paper_id": "251979712",
      "cited_paper_id": 291713
    },
    {
      "context_text": "Table 1: Scene similarity queries on top of HANS using feature hashing Weinberger et al. [2009].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'HANS' which could be a dataset, but there is insufficient context to confirm its nature or usage. The cited paper title does not help in disambiguating.",
      "processing_time": 55.85513949394226,
      "citing_paper_id": "251979712",
      "cited_paper_id": 291713
    },
    {
      "context_text": "To this end, we used the TMKG graph structure of each video frame to Ô¨Ånd similar frames using methods such as Weinberger et al. [2009] or Gamanayake et al. [2020].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to methods and graph structures but does not provide a clear identifier for a dataset.",
      "processing_time": 54.23785090446472,
      "citing_paper_id": "251979712",
      "cited_paper_id": 291713
    },
    {
      "context_text": "[2020] (8)the ability to explain or to present in understandable terms to a human Doshi-Velez and Kim [2017] that uses Weinberger et al. [2009] on top of HANS for the scene similarity search.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on interpretability and scene similarity search, but no datasets are explicitly named.",
      "processing_time": 55.18381905555725,
      "citing_paper_id": "251979712",
      "cited_paper_id": 11319376
    },
    {
      "context_text": "[2020] (8)the ability to explain or to present in understandable terms to a human Doshi-Velez and Kim [2017] that uses Weinberger et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to interpretable machine learning concepts.",
      "processing_time": 52.72363877296448,
      "citing_paper_id": "251979712",
      "cited_paper_id": 11319376
    },
    {
      "context_text": "SOTA reasoners that use whole graph structures to learn appropriate features from data can be used to obviate the scalability issues that symbolic methods generally suffer from Lin et al. [2005].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for learning in graph domains.",
      "processing_time": 53.05336141586304,
      "citing_paper_id": "251979712",
      "cited_paper_id": 20480879
    },
    {
      "context_text": "ITMo focuses on deriving actionable knowledge from a large array of networks of sensors deployed along highways, city roads and intersections, etc. Muppalla et al. [2017] use knowledge graph technology for trafÔ¨Åc congestion detection by proposing an ontology based on SSN Haller et al. [2019] to interpret the video feed from trafÔ¨Åc cameras and a heuristic reasoner on top of the ontology.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of an ontology based on SSN, which is a standard for sensor-related data. However, it does not mention a specific dataset.",
      "processing_time": 54.430389642715454,
      "citing_paper_id": "251979712",
      "cited_paper_id": 21688777
    },
    {
      "context_text": "‚Ä¶along highways, city roads and intersections, etc. Muppalla et al. [2017] use knowledge graph technology for trafÔ¨Åc congestion detection by proposing an ontology based on SSN Haller et al. [2019] to interpret the video feed from trafÔ¨Åc cameras and a heuristic reasoner on top of the ontology.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the SSN ontology but does not refer to it as a dataset. It is used to interpret video feeds from traffic cameras, which is more about methodology than a reusable dataset.",
      "processing_time": 55.407251834869385,
      "citing_paper_id": "251979712",
      "cited_paper_id": 21688777
    },
    {
      "context_text": "SOTA reasoners that use whole graph structures to learn appropriate features from data can be used to obviate the scalability issues that symbolic methods generally suffer from Lin et al. [2019]; Gamanayake et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and approaches. The context is about symbolic methods and scalability issues in reasoning systems.",
      "processing_time": 54.317288637161255,
      "citing_paper_id": "251979712",
      "cited_paper_id": 21698461
    },
    {
      "context_text": "[2017]; Lam et al. [2017]. Muppalla et al. [2017] use the deviation of each frame of video from the median as a core indicator of traffic congestion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a method for detecting traffic congestion using video frames but does not specify a dataset. The citation is focused on a method rather than a reusable dataset.",
      "processing_time": 54.99142622947693,
      "citing_paper_id": "251979712",
      "cited_paper_id": 21698461
    },
    {
      "context_text": "Linardatos et al. [2020] (8)the ability to explain or to present in understandable terms to a human Doshi-Velez and Kim [2017] that uses Weinberger et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and concepts. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 54.414939165115356,
      "citing_paper_id": "251979712",
      "cited_paper_id": 202540096
    },
    {
      "context_text": "[2019]; Gamanayake et al. [2020]; Scarselli et al. [2008]; Gori et al. [2005]. To this end, we used the TMKG graph structure of each video frame to find similar frames using methods such as Weinberger et al. [2009] or Gamanayake et al. [2020]. Table 1 presents two examples from the graph-based reasoning model",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TMKG graph structure' but does not specify it as a dataset. It is used to find similar frames in video, which is part of the methodological approach rather than a reusable dataset.",
      "processing_time": 56.952274799346924,
      "citing_paper_id": "251979712",
      "cited_paper_id": 212414835
    },
    {
      "context_text": "[2019]; Gamanayake et al. [2020]; Scarselli et al. [2008]; Gori et al. [2005]. To this end, we used the TMKG graph structure of each video frame to find similar frames using methods such as Weinberger et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a graph structure (TMKG) but does not specify a dataset name. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 56.96101212501526,
      "citing_paper_id": "251979712",
      "cited_paper_id": 212414835
    },
    {
      "context_text": "Inspired by the idea of generative adversarial networks [17, 44], we propose a Collaborative Modality Adversarial Training (abbreviated as CoMAT ) module to augment the modality embed-dings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (generative adversarial networks).",
      "processing_time": 52.798701763153076,
      "citing_paper_id": "270711106",
      "cited_paper_id": 623013
    },
    {
      "context_text": "The imbalance problem arises imbalanced distribution of modality information The diversity problem is caused by the diversity modalities that exist in the media of information dissemination nowadays.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses general problems related to multi-modal data, such as imbalance and diversity.",
      "processing_time": 54.681923151016235,
      "citing_paper_id": "270711106",
      "cited_paper_id": 623013
    },
    {
      "context_text": "‚Ä¢ DB15K [32] is an MMKG with image, text, and numerical information proposed by [32], which is a subset of DBpedia [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DB15K"
      ],
      "dataset_descriptions": {
        "DB15K": "Used as a multi-modal knowledge graph with image, text, and numerical information for reasoning tasks, serving as a subset of DBpedia."
      },
      "confidence_score": 1.0,
      "reasoning": "DB15K is identified as a multi-modal knowledge graph (MMKG) containing image, text, and numerical information, which is a subset of DBpedia. It is relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 64.42434549331665,
      "citing_paper_id": "270711106",
      "cited_paper_id": 1181640
    },
    {
      "context_text": "MMKRL [26]). high-frequency We evaluate the eÔøΩciency and performance of diÔøΩer-ent models with the same batch size of 1024 and dimension of 250 on the same device. relations that can raise each modality weight As shown in Figure 5, we can observe that NÔøΩÔøΩ ÔøΩÔøΩÔøΩE makes a good trade-oÔøΩ between eÔøΩciency‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model performance and efficiency. No clear identifiers for datasets are present.",
      "processing_time": 53.45531225204468,
      "citing_paper_id": "270711106",
      "cited_paper_id": 1181640
    },
    {
      "context_text": "Each P ùëö comprises a two-layer MLP with ReLU [16] as the activation function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ReLU activation function).",
      "processing_time": 52.20820617675781,
      "citing_paper_id": "270711106",
      "cited_paper_id": 2239473
    },
    {
      "context_text": "However, these methods neglect two vital problems Conventional KGC methods [3, 33, 36, 53] usually learn structural embeddings to model the triple structure and measure the triple plausibility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and problems in knowledge graph completion.",
      "processing_time": 52.74179482460022,
      "citing_paper_id": "270711106",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "However, these methods neglect two vital problems Conventional KGC methods [3, 33, 36, 53] usually learn structural embeddings to model the triple structure and measure the triple plausibility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and problems in knowledge graph completion.",
      "processing_time": 52.74179482460022,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Translation-based methods such as TransE [3], TransD [21], RotatE [40], OTE [41], and PairRE [6] modeling the triple structure as relational translation from the head entity to the tail entity, which design distance-based score functions as the plausibility measurement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.22988843917847,
      "citing_paper_id": "270711106",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "We employ 10 different MMKGC methods that consider both multi-modal information and the triple structuralinformationincluding IKRL [56], TBKGC [37], TransAE [54], RSME [48], MMKRL [33], VBKGC [67], OTKGE [5], IMF [27], QEB [52] and VISTA [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods but does not refer to any specific datasets. The context is focused on describing the methods used for multi-modal knowledge graph reasoning.",
      "processing_time": 54.80827522277832,
      "citing_paper_id": "270711106",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "We employ 10 different MMKGC methods that consider both multi-modal information and the triple structuralinformationincluding IKRL [56], TBKGC [37], TransAE [54], RSME [48], MMKRL [33], VBKGC [67], OTKGE [5], IMF [27], QEB [52] and VISTA [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods but does not refer to any specific datasets. The context is focused on describing the methods used for multi-modal knowledge graph reasoning.",
      "processing_time": 54.80827522277832,
      "citing_paper_id": "270711106",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "We employ 10 different MMKGC methods that consider both multi-modal information and the triple structuralinformationincluding IKRL [56], TBKGC [37], TransAE [54], RSME [48], MMKRL [33], VBKGC [67], OTKGE [5], IMF [27], QEB [52] and VISTA [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods but does not refer to any specific datasets. The context is focused on describing the methods used for multi-modal knowledge graph reasoning.",
      "processing_time": 54.80827522277832,
      "citing_paper_id": "270711106",
      "cited_paper_id": 252280329
    },
    {
      "context_text": "Existing MMKGC methods [27, 37, 48, 56] typically However, KGs usually face critical incompleteness issues as many unobserved factual knowledge is waiting to be discovered in the given KG. treat multi-modal information of entities as auxiliary multi-modal embeddings This phenomenon makes knowledge‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about multi-modal knowledge graph completion and the challenges of incomplete knowledge graphs.",
      "processing_time": 54.05040216445923,
      "citing_paper_id": "270711106",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "These methods [5, 8, 25, 35, 37, 48, 54, 56, 65] design elegant approaches to achieve multi-modal fusion in the same representation space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for multi-modal fusion. No dataset names are present in the text.",
      "processing_time": 53.857691287994385,
      "citing_paper_id": "270711106",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "‚Ä¶information Aiming to solve these key issues of The imbalance problem (a) MMKGC in the wild the multi-modal embeddings representations for the the diverse use of varied [34] data 1(a) MMKGs. designed for generalization from the implying that in the real-world entity-wise the Existing works [56], .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide specific names of datasets, only vague references to 'varied data' and 'real-world entity-wise data'. No clear, verifiable datasets are mentioned.",
      "processing_time": 55.65282988548279,
      "citing_paper_id": "270711106",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Of To prove this point, we apply CoMAT on more MMKGC models (IKRL [56], TBKGC [37], and VBKGC [67]) and demonstrate the results in Figure 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context focuses on applying CoMAT to various MMKGC models, which are methods, not datasets.",
      "processing_time": 55.18088483810425,
      "citing_paper_id": "270711106",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "The existing KGE models can be divided into two main categories: translation-based methods and semantic matching methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 53.62433218955994,
      "citing_paper_id": "270711106",
      "cited_paper_id": 11202498
    },
    {
      "context_text": "The existing KGE models can be divided into two main categories: translation-based methods and semantic matching methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 53.62433218955994,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "For example, we can employ BERT [14] as the textual encoder and VGG16 [38] as the image encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and VGG16 as encoders but does not refer to them as datasets. They are models or methods, not datasets.",
      "processing_time": 54.59138584136963,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "For example, we can employ BERT [14] as the textual encoder and VGG16 [38] as the image encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and VGG16 as encoders but does not refer to them as datasets. They are models or methods, not datasets.",
      "processing_time": 54.59138584136963,
      "citing_paper_id": "270711106",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We conduct link prediction [3] task on the five datasets, which is a significant task of KGC.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'five datasets' but does not specify their names. The citation is used to support the task of link prediction in knowledge graph completion, but no specific datasets are named.",
      "processing_time": 55.21253490447998,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Besides, the filter setting [3] is applied to remove the candidate triples already existing in the training set for fair comparisons.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for filtering candidate triples in a training set.",
      "processing_time": 51.972355127334595,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "This expansion consequently complicates the score function by considering multi-modal embedding integration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating multi-modal embeddings.",
      "processing_time": 51.560102224349976,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "In the KGC field, there are some methods [4, 33, 42, 49] employs a GAN-based framework to enhance the negative sampling [3] process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and frameworks. The context focuses on the use of GAN-based frameworks for enhancing negative sampling in knowledge graph completion.",
      "processing_time": 55.46782565116882,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "We select 5 state-of-the-art uni-modal KGC methods including TransE [3], DistMult [58], Com-plEx [43], RotatE [40], PairRE [6], which design elegant score functions and learn the structural embeddings of the given KG without any multi-modal information. ii) Multi-modal KGC methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods but does not refer to any specific datasets. The citation is focused on describing the methods used for knowledge graph completion.",
      "processing_time": 54.00083589553833,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "= The negative triple set can be denoted as ( ) ‚àà T ‚à© ‚àà E\\{ }}‚à™{( ) | ( ) ‚àà T ‚à© ‚àà E\\{ }} During the inference stage, the KGC model is usually evaluated with the link prediction task [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general evaluation task. No clear, verifiable resource names are provided.",
      "processing_time": 53.45236349105835,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "This phenomenon makes knowledge graph completion (KGC) [30] a significant task to automatically discover new knowledge in the given KGs. Conventional KGC methods [3, 40, 43, 58] generally emphasize learning structural embeddings to model the triple structure and measure the triple plausibility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and tasks related to knowledge graph completion.",
      "processing_time": 52.4995322227478,
      "citing_paper_id": "270711106",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "This technique is also called knowledge graph embedding (KGE) [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique called knowledge graph embedding (KGE).",
      "processing_time": 52.67728853225708,
      "citing_paper_id": "270711106",
      "cited_paper_id": 19135805
    },
    {
      "context_text": "Multi-modal knowledge graphs (MMKGs) [7, 11, 32] represent a rich extension of traditional knowledge graphs (KGs) [30, 50], enriching the structure triples in the form of (head entity, relation, tail entity) with comprehensive multi-modal entity attributes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graphs. No verifiable resources are identified.",
      "processing_time": 53.9713180065155,
      "citing_paper_id": "270711106",
      "cited_paper_id": 19135805
    },
    {
      "context_text": "‚Ä¶the different However, the existing MMKGC methods are primarily the prevalent However, these methods neglect two vital problems for MMKGC in real scenarios, which can be concluded as the diversity problem and the imbalance problem . image and text modalities, limiting their to diverse modalities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with multimodal knowledge graph construction methods.",
      "processing_time": 52.67033934593201,
      "citing_paper_id": "270711106",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "MMKGs are representative and have become the significant infrastructure in many AI fields such as recommender systems [51], computer vision [18], and nature language processing [10, 60, 64].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of MMKGs in various AI fields. No verifiable resources are identified.",
      "processing_time": 54.391300439834595,
      "citing_paper_id": "270711106",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "MMKGs are representative and have become the significant infrastructure in many AI fields such as recommender systems [51], computer vision [18], and nature language processing [10, 60, 64].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of MMKGs in various AI fields. No verifiable resources are identified.",
      "processing_time": 54.391300439834595,
      "citing_paper_id": "270711106",
      "cited_paper_id": null
    },
    {
      "context_text": "MMKGs are representative and have become the significant infrastructure in many AI fields such as recommender systems [51], computer vision [18], and nature language processing [10, 60, 64].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of MMKGs in various AI fields. No verifiable resources are identified.",
      "processing_time": 54.391300439834595,
      "citing_paper_id": "270711106",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶a pair of discriminator and generator by playing a min-max game between them and achieve better performance, which has been widely used in various fields such as computer vision [1, 22], natural language processing [12, 61], information retrieval [47, 62], and recommender systems [53, 55, 61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of adversarial methods across various fields.",
      "processing_time": 52.83737921714783,
      "citing_paper_id": "270711106",
      "cited_paper_id": 238419331
    },
    {
      "context_text": "‚Ä¶a pair of discriminator and generator by playing a min-max game between them and achieve better performance, which has been widely used in various fields such as computer vision [1, 22], natural language processing [12, 61], information retrieval [47, 62], and recommender systems [53, 55, 61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of adversarial methods across various fields.",
      "processing_time": 52.83737921714783,
      "citing_paper_id": "270711106",
      "cited_paper_id": null
    },
    {
      "context_text": "In existing works in other fields [1, 31], D is usually implemented with another two-layer MLP.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological implementation detail.",
      "processing_time": 51.679238080978394,
      "citing_paper_id": "270711106",
      "cited_paper_id": 248525030
    },
    {
      "context_text": "According to the properties of trigonometric inequalities, we have the following derivations: Besides, in the RotatE score function, the relation embeddings ùíì are limited with unit modulus [40] which means || ùíì || ‚â§ 1 for any ùëü ‚àà R .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical properties and constraints in the RotatE score function.",
      "processing_time": 52.74541711807251,
      "citing_paper_id": "270711106",
      "cited_paper_id": 248525030
    },
    {
      "context_text": "We report the MRR and Hit@1 results on the KVC16K/DB15K datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KVC16K",
        "DB15K"
      ],
      "dataset_descriptions": {
        "KVC16K": "Used to evaluate knowledge graph completion methods, focusing on multi-modal reasoning and performance metrics such as MRR and Hit@1.",
        "DB15K": "Used to evaluate knowledge graph completion methods, focusing on multi-modal reasoning and performance metrics such as MRR and Hit@1."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for reporting MRR and Hit@1 results, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 67.89911770820618,
      "citing_paper_id": "270711106",
      "cited_paper_id": 252280329
    },
    {
      "context_text": "Therefore, some methods [57, 63, 67] attempt to enhance the NS process and generate high-quality negative samples by utilizing the multi-modal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for enhancing the negative sampling process using multi-modal information.",
      "processing_time": 52.97780466079712,
      "citing_paper_id": "270711106",
      "cited_paper_id": 252280329
    },
    {
      "context_text": "Besides, when removing the gradient penalty loss or changing the adversarial loss to a vanilla GAN version with log-likelihoods [12], the model performance still decreases.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only changes in model components and their impact on performance.",
      "processing_time": 51.86491513252258,
      "citing_paper_id": "270711106",
      "cited_paper_id": null
    },
    {
      "context_text": "Semantic matching methods such as DistMult [58], ComplEx [43], TuckER [2] exploit similarity-based scoring functions based on tensor decomposition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.36914086341858,
      "citing_paper_id": "270711106",
      "cited_paper_id": null
    },
    {
      "context_text": "Among these methods, KBGAN [4] is an adversarial negative sampling method designed for conventional KGC, which applies reinforcement learning to optimize the models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called KBGAN. The context focuses on describing the method and its application in knowledge graph completion.",
      "processing_time": 54.98101568222046,
      "citing_paper_id": "270711106",
      "cited_paper_id": null
    },
    {
      "context_text": "Adversarial training (AT) (Kurakin et al., 2017; Goodfellow et al., 2014, 2015; Shafahi et al., 2019) is a powerful technology widely used in many machine learning fields (Mariani et al., 2018; Miyato et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only adversarial training methods and their applications. No verifiable resources are identified.",
      "processing_time": 53.93512940406799,
      "citing_paper_id": "267898006",
      "cited_paper_id": 6706414
    },
    {
      "context_text": "Adversarial training (AT) (Kurakin et al., 2017; Goodfellow et al., 2014, 2015; Shafahi et al., 2019) is a powerful technology widely used in many machine learning fields (Mariani et al., 2018; Miyato et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only adversarial training methods and their applications. No verifiable resources are identified.",
      "processing_time": 53.93512940406799,
      "citing_paper_id": "267898006",
      "cited_paper_id": 9059612
    },
    {
      "context_text": "‚Ä¶for comparisons and they can be divided into three categories: (i) Uni-modal KGC methods , including TransE (Bordes et al., 2013), TransD (Ji et al., 2015), Dist-Mult (Yang et al., 2015), ComplEx (Trouillon et al., 2017), RotatE (Sun et al., 2019), PairRE (Chao et al., 2021), and GC-OTE‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several knowledge graph embedding methods but does not refer to any specific datasets. The context is focused on describing uni-modal knowledge graph completion methods.",
      "processing_time": 54.984405755996704,
      "citing_paper_id": "267898006",
      "cited_paper_id": 11202498
    },
    {
      "context_text": "To demonstrate the effectiveness of our approach, we select several MMKGC methods as our base-lines for comparisons and they can be divided into three categories: (i) Uni-modal KGC methods , including TransE (Bordes et al., 2013), TransD (Ji et al., 2015), Dist-Mult (Yang et al., 2015), ComplEx (Trouillon et al., 2017), RotatE (Sun et al., 2019), PairRE (Chao et al., 2021), and GC-OTE (Tang et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The context is focused on describing the baseline methods for comparison.",
      "processing_time": 54.093119382858276,
      "citing_paper_id": "267898006",
      "cited_paper_id": 11202498
    },
    {
      "context_text": "In the knowledge graph community, a few meth-ods (Cai and Wang, 2018; Wang et al., 2018; Tang et al., 2022) employ AT to generate harder negative samples (Bordes et al., 2013) by reinforcement learning to enhance the embedding training process and improve the prediction results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the use of adversarial training (AT) and reinforcement learning to enhance the embedding training process in knowledge graphs.",
      "processing_time": 56.562650203704834,
      "citing_paper_id": "267898006",
      "cited_paper_id": 19131678
    },
    {
      "context_text": "‚Ä¶et al. (2019) further extend KGs with representative multi-modal information (e.g. textual descriptions, images, etc.) and have become the new infrastructure in many artificial intelligence tasks (Sun et al., 2020; Yasunaga et al., 2021; Cao et al., 2022a; Zhang et al., 2023b; Liang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about multi-modal information in knowledge graphs. No clear identifiers for datasets are present.",
      "processing_time": 54.48488092422485,
      "citing_paper_id": "267898006",
      "cited_paper_id": 224281034
    },
    {
      "context_text": "‚Ä¶et al. (2019) further extend KGs with representative multi-modal information (e.g. textual descriptions, images, etc.) and have become the new infrastructure in many artificial intelligence tasks (Sun et al., 2020; Yasunaga et al., 2021; Cao et al., 2022a; Zhang et al., 2023b; Liang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about multi-modal information in knowledge graphs. No clear identifiers for datasets are present.",
      "processing_time": 54.48488092422485,
      "citing_paper_id": "267898006",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "MANS (Zhang et al., 2023a) proposes a modality-aware negative sampling method to align the structural and multi-modal information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (MANS) rather than a dataset. There are no specific, verifiable datasets mentioned in the citation context.",
      "processing_time": 53.87196207046509,
      "citing_paper_id": "267898006",
      "cited_paper_id": 258298672
    },
    {
      "context_text": "MANS and MMRNS design new negative sampling strategies enhanced by the multi-modal information to generate high-quality negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for negative sampling in multi-modal knowledge graphs.",
      "processing_time": 52.723164796829224,
      "citing_paper_id": "267898006",
      "cited_paper_id": 258298672
    },
    {
      "context_text": "Such a result demonstrates that AdaMF can do better than the existing meth-ods to utilize the multi-modal information in the MMKGs. Additionally, we can observe that MAT can bring full range improvement of metrics by generating high-quality synthetic triples for positive-negative contrast during training, compared with other negative sampling methods like MANS and MMRNS 2 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The citation is focused on comparing methods for multi-modal knowledge graph embedding.",
      "processing_time": 54.47642683982849,
      "citing_paper_id": "267898006",
      "cited_paper_id": 258298672
    },
    {
      "context_text": "These methods utilize both the structural information and multi-modal information in the KGs. (iii) Negative sampling methods , including KB-GAN (Cai and Wang, 2018), MANS (Zhang et al., 2023a) and MMRNS (Xu et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions negative sampling methods for multi-modal knowledge graphs but does not specify any datasets. The focus is on methods rather than datasets.",
      "processing_time": 54.25580430030823,
      "citing_paper_id": "267898006",
      "cited_paper_id": 258298672
    },
    {
      "context_text": "Existing multi-modal knowledge graph completion (MMKGC) methods (Liang et al., 2022; Chen et al., 2024) are mainly concerned with extending the general knowledge graph embedding (KGE) meth-ods (Bordes et al., 2013; Yang et al., 2015) to the multi-modal scenario.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. There are no clear identifiers for datasets.",
      "processing_time": 53.87172484397888,
      "citing_paper_id": "267898006",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "Flickr30k (Young et al. 2014): It contains 31,783 images from Flickr website, and each image is described by five different sentences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "Flickr30k": "Used to train and evaluate models for generating textual descriptions from images, focusing on the alignment between visual content and natural language captions."
      },
      "confidence_score": 1.0,
      "reasoning": "Flickr30k is a well-known dataset in multi-modal learning, specifically for image captioning tasks. The context clearly identifies it as a dataset containing images and textual descriptions.",
      "processing_time": 62.062031507492065,
      "citing_paper_id": "268712378",
      "cited_paper_id": 3104920
    },
    {
      "context_text": "‚Ä¶learning methods have attracted broad attention which utilize the deep neural networks to extract global representations of both images and texts and then perform cross-modal alignment and fusion (Qian et al. 2021; Zhuo et al. 2020; Peng, Qi, and Yuan 2019; Wang et al. 2017; Wei et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles also do not provide clear evidence of specific datasets being used.",
      "processing_time": 55.60668992996216,
      "citing_paper_id": "268712378",
      "cited_paper_id": 8355505
    },
    {
      "context_text": "‚Ä¶learning methods have attracted broad attention which utilize the deep neural networks to extract global representations of both images and texts and then perform cross-modal alignment and fusion (Qian et al. 2021; Zhuo et al. 2020; Peng, Qi, and Yuan 2019; Wang et al. 2017; Wei et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles also do not provide clear evidence of specific datasets being used.",
      "processing_time": 55.60668992996216,
      "citing_paper_id": "268712378",
      "cited_paper_id": 13618178
    },
    {
      "context_text": "‚Ä¶learning methods have attracted broad attention which utilize the deep neural networks to extract global representations of both images and texts and then perform cross-modal alignment and fusion (Qian et al. 2021; Zhuo et al. 2020; Peng, Qi, and Yuan 2019; Wang et al. 2017; Wei et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles also do not provide clear evidence of specific datasets being used.",
      "processing_time": 55.60668992996216,
      "citing_paper_id": "268712378",
      "cited_paper_id": 235306301
    },
    {
      "context_text": "We compare CMGCH with 16 state-of-the-art methods on MSCOCO dataset by MAP, including twelve unsupervised approaches (UCCH(Hu et al. 2022), DGCPN(Yu et al. 2021), LSSH(Wang et al. 2020), UKD-SS(Hu et al. 2020) and DSAH(Tu et al. 2022), CMFH(Lu et al. 2019), UCH(Li et al. 2019), FSH(Hong et al. 2017",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate cross-modal hashing methods, specifically comparing CMGCH with 16 state-of-the-art methods using Mean Average Precision (MAP)."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the MSCOCO dataset for comparing CMGCH with other methods using MAP. The dataset is used for evaluating cross-modal hashing methods.",
      "processing_time": 62.414294958114624,
      "citing_paper_id": "268712378",
      "cited_paper_id": 13145195
    },
    {
      "context_text": "We compare CMGCH with 16 state-of-the-art methods on MSCOCO dataset by MAP, including twelve unsupervised approaches (UCCH(Hu et al. 2022), DGCPN(Yu et al. 2021), LSSH(Wang et al. 2020), UKD-SS(Hu et al. 2020) and DSAH(Tu et al. 2022), CMFH(Lu et al. 2019), UCH(Li et al. 2019), FSH(Hong et al. 2017",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate cross-modal hashing methods, specifically comparing CMGCH with 16 state-of-the-art methods using Mean Average Precision (MAP)."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the MSCOCO dataset for comparing CMGCH with other methods using MAP. The dataset is used for evaluating cross-modal hashing methods.",
      "processing_time": 62.414294958114624,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "We compare CMGCH with 16 state-of-the-art methods on MSCOCO dataset by MAP, including twelve unsupervised approaches (UCCH(Hu et al. 2022), DGCPN(Yu et al. 2021), LSSH(Wang et al. 2020), UKD-SS(Hu et al. 2020) and DSAH(Tu et al. 2022), CMFH(Lu et al. 2019), UCH(Li et al. 2019), FSH(Hong et al. 2017",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate cross-modal hashing methods, specifically comparing CMGCH with 16 state-of-the-art methods using Mean Average Precision (MAP)."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the MSCOCO dataset for comparing CMGCH with other methods using MAP. The dataset is used for evaluating cross-modal hashing methods.",
      "processing_time": 62.414294958114624,
      "citing_paper_id": "268712378",
      "cited_paper_id": 214743601
    },
    {
      "context_text": "The average MAP and Recall curves of different algorithms for MSCOCO and Flickr30k with different lengths of hash codes are shown in Figure 2 and Figure 3, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate different algorithms for generating hash codes, focusing on performance metrics like MAP and Recall across various hash code lengths.",
        "Flickr30k": "Used to evaluate different algorithms for generating hash codes, focusing on performance metrics like MAP and Recall across various hash code lengths."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the use of MSCOCO and Flickr30k datasets to evaluate different algorithms, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 68.41808271408081,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "The queue length of momentum encoder hyperparameter K is 8192 for Flickr30k and 65536 for MSCOCO, momentum encoder update hyperparameter m is 0.99, temperature coefficient œÑ is 0.07.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30k",
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "Flickr30k": "Used to set hyperparameters for a momentum encoder, focusing on queue length and update mechanisms in multi-modal learning.",
        "MSCOCO": "Used to set hyperparameters for a momentum encoder, focusing on queue length and update mechanisms in multi-modal learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets, Flickr30k and MSCOCO, which are used to set hyperparameters for a model. These are well-known image captioning and visual recognition datasets.",
      "processing_time": 69.21226215362549,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "Datasets and Evaluation Metrics MSCOCO (Lin et al. 2014): This dataset totally contains 123,287 images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to provide a large-scale image dataset for computer vision tasks, specifically containing 123,287 images for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MSCOCO dataset, which is a well-known image dataset used in various computer vision tasks. The dataset is explicitly named and referenced.",
      "processing_time": 61.24999284744263,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "Compared with previous best result, the proposed method achieves 16%, 17%, 19%, 31% increase in MAP for I ‚Üí T and 10%, 16%, 16%, 32% increase in MAP for T ‚Üí I on the MSCOCO separately in 16 bits, 32 bits, 64 bits and 128 bits.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate the proposed method's performance in cross-modal retrieval tasks, specifically measuring mean average precision (MAP) for image-to-text and text-to-image retrieval at various bit lengths."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MSCOCO' which is a well-known dataset used for multi-modal tasks such as image captioning and cross-modal retrieval. The dataset is used to evaluate the performance of the proposed method.",
      "processing_time": 64.90753436088562,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "Table 1 lists the results of MAP comparison of different algorithms with different lengths of hash codes on MSCOCO.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate different algorithms with varying hash code lengths, focusing on multi-modal reasoning and image captioning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MSCOCO' which is a well-known dataset in the field of computer vision and multi-modal learning. It is used for evaluating different algorithms with varying hash code lengths.",
      "processing_time": 61.72988724708557,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "We can see that CMGCH obtains higher MAP performance and R@K performance on the MSCOCO and Flickr30k for the cross-modal hashing retrieval tasks respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate cross-modal hashing retrieval tasks, focusing on image-text matching and retrieval performance using MAP and R@K metrics.",
        "Flickr30k": "Used to evaluate cross-modal hashing retrieval tasks, focusing on image-text matching and retrieval performance using MAP and R@K metrics."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of MSCOCO and Flickr30k datasets for evaluating cross-modal hashing retrieval tasks, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 69.08492112159729,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "In order to verify the effectiveness of different learning components, six types of CMGCH variants are compared and analyzed on MSCOCO.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to compare and analyze different variants of CMGCH, focusing on the effectiveness of various learning components in a multi-modal context."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MSCOCO' which is a well-known dataset in the field of computer vision and multi-modal learning. It is used here to compare and analyze different variants of CMGCH.",
      "processing_time": 62.081016063690186,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "The possible reason is that MSCOCO and Flickr30k datasets we used are feature vectors obtained by Faster RCNN using the BUTD mechanism, rather than the original dataset, so data augmentation of images may degrade intra-modal contrastive learning effect.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used for feature extraction with Faster RCNN and BUTD mechanism, investigating the impact of data augmentation on intra-modal contrastive learning.",
        "Flickr30k": "Used for feature extraction with Faster RCNN and BUTD mechanism, investigating the impact of data augmentation on intra-modal contrastive learning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for feature extraction, which are relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 66.54861998558044,
      "citing_paper_id": "268712378",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "Due to low storage cost and fast searching speed, cross-modal deep hashing methods have been increasingly popular (Chen et al. 2021; Xu et al. 2019; Wang et al. 2019; Zhang, Peng, and Yuan 2018a; Wendel et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only cross-modal deep hashing methods. No verifiable resources are identified.",
      "processing_time": 53.53020739555359,
      "citing_paper_id": "268712378",
      "cited_paper_id": 19153102
    },
    {
      "context_text": "‚Ä¶preserving network for unsupervised cross-modal hashing (DGCPN) (Yu et al. 2021), unsupervised generative adversarial cross-modal hashing (UGACH) (Zhang, Peng, and Yuan 2018a), unsupervised coupled cycle generation adversarial hashing method (UCH) (C, C, and L 2019), et al. In recent years,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper title confirms that the focus is on a method, not a dataset.",
      "processing_time": 54.87593078613281,
      "citing_paper_id": "268712378",
      "cited_paper_id": 19153102
    },
    {
      "context_text": "Repre-sentative methods include deep graph-neighbor coherence preserving network for unsupervised cross-modal hashing (DGCPN) (Yu et al. 2021), unsupervised generative adversarial cross-modal hashing (UGACH) (Zhang, Peng, and Yuan 2018a), unsupervised coupled cycle generation adversarial hashing method (UCH) (C, C, and L 2019), et al. In recent years, there are many impressive works which perform well on semantic feature learning by using contrastive learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The context focuses on describing various methods for unsupervised cross-modal hashing.",
      "processing_time": 54.51038122177124,
      "citing_paper_id": "268712378",
      "cited_paper_id": 19153102
    },
    {
      "context_text": "The time efficiency of the proposed CMGCH is higher than that of UGACH and LSSH, and comparable to that of UCCH, while delivering superior MAP performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only compares the performance of different methods.",
      "processing_time": 52.141196727752686,
      "citing_paper_id": "268712378",
      "cited_paper_id": 19153102
    },
    {
      "context_text": "The existing multimodal knowledge enhanced deep learning methods aim to incorporate multimodal knowledge into the networks, which have been utilized in visual question answering (Yang Ding and Wu 2022), and image-text retrieval (Fudong Nian and Xu 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions multimodal knowledge enhanced deep learning methods but does not specify any datasets. It refers to applications in visual question answering and image-text retrieval, but no specific datasets are named.",
      "processing_time": 56.130693197250366,
      "citing_paper_id": "268712378",
      "cited_paper_id": 27494872
    },
    {
      "context_text": "Nian et al. (Fudong Nian and Xu 2017) propose a multi-modal knowledge representation learning method that attempts to handle knowledge from both text and visual data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multi-modal knowledge representation learning.",
      "processing_time": 52.54289364814758,
      "citing_paper_id": "268712378",
      "cited_paper_id": 27494872
    },
    {
      "context_text": "In the text embedding learning network, Bert (Z. Lan and Soricut 2018) model is conducted to learn fine-grained word representation T = [ t 1 , ..., t L ] ‚àà R R L √ó D t , where t j ‚àà R D t denotes the representation of T ‚Äôs j -th word, L denotes the number of words, and D t denotes the dimension of‚Ä¶",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the BERT model, which is excluded as per instructions.",
      "processing_time": 53.10230803489685,
      "citing_paper_id": "268712378",
      "cited_paper_id": 202888986
    },
    {
      "context_text": "In the text embedding learning network, Bert (Z. Lan and Soricut 2018) model is conducted to learn fine-grained word representation T = [ t 1 , ..., t L ] ‚àà R R L √ó D t , where t j ‚àà R D t denotes the representation of T ‚Äôs j -th word, L denotes the number of words, and D t denotes the dimension of word embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the BERT model, which is excluded as per instructions.",
      "processing_time": 53.2871150970459,
      "citing_paper_id": "268712378",
      "cited_paper_id": 202888986
    },
    {
      "context_text": "He and Girshick 2020; T. Chen and Hinton 2020) and natural language processing (Z. Lan and Soricut 2018), little effort has been made on fine-grained cross-modal knowledge graph contrastive hash learning for cross-modal search.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general areas of research. No verifiable resources are identified.",
      "processing_time": 53.25184965133667,
      "citing_paper_id": "268712378",
      "cited_paper_id": 202888986
    },
    {
      "context_text": "He and Girshick 2020; T. Chen and Hinton 2020) and natural language processing (Z. Lan and Soricut 2018), little effort has been made on fine-grained cross-modal knowledge graph contrastive hash learning for cross-modal search.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general areas of research. No verifiable resources are identified.",
      "processing_time": 53.25184965133667,
      "citing_paper_id": "268712378",
      "cited_paper_id": 211096730
    },
    {
      "context_text": "For each input text, the basic version of the pre-trained Bert is leveraged to obtain the original word embeddings with dimension 768.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for obtaining word embeddings. BERT itself is a model, not a dataset.",
      "processing_time": 54.596513748168945,
      "citing_paper_id": "268712378",
      "cited_paper_id": 202888986
    },
    {
      "context_text": "Hinton et al. (T. Chen and Hinton 2020)propose a simple framework for contrastive learning of visual representations (SimCLR), which requires larger batch sizes to achieve superior performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SimCLR) for contrastive learning. The context focuses on the method's requirements and performance.",
      "processing_time": 54.927048683166504,
      "citing_paper_id": "268712378",
      "cited_paper_id": 211096730
    },
    {
      "context_text": "CLIP (Rad-ford et al. 2021) and ALIGN (Jia et al. 2021) are the recent proposed multi-modal pre-training models based on contrastive learning, which form a task-agnostic model by predicting which text matches which image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP and ALIGN as multi-modal pre-training models but does not refer to them as datasets. The focus is on their role as models in contrastive learning.",
      "processing_time": 55.71063208580017,
      "citing_paper_id": "268712378",
      "cited_paper_id": 231879586
    },
    {
      "context_text": "Dwibedi et al. (Dwibedi et al. 2021) propose a nearest-neighbor contrastive learning method of visual representations (NNCLR), which distinguishes the examples in the queue into positive examples and negative examples using the nearest neighbor method.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning visual representations.",
      "processing_time": 52.08937978744507,
      "citing_paper_id": "268712378",
      "cited_paper_id": 233444011
    },
    {
      "context_text": "A parametric contrastive learning (PaCo) method (Cui et al. 2021) is proposed to optimize the distinction between positive and negative samples in queue by using labels as a guide.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PaCo. The context focuses on the method's functionality and purpose.",
      "processing_time": 54.06890296936035,
      "citing_paper_id": "268712378",
      "cited_paper_id": 236428622
    },
    {
      "context_text": "TCL (Yang et al. 2022) uses intra-modal and inter-modal contrastive learning, using knowledge distillation to guide the learning process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the technique used in the paper rather than any particular dataset.",
      "processing_time": 54.7669894695282,
      "citing_paper_id": "268712378",
      "cited_paper_id": 247011309
    },
    {
      "context_text": "In the image embedding learning network, we detect salient regions with the Bottom-Up and Top-Down attention model (Peter Anderson 2020), which selects the top R ( R = 36) Regions of Interest (ROIs) with the highest class confidence scores.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for detecting salient regions using a model, but does not reference a dataset.",
      "processing_time": 54.06939196586609,
      "citing_paper_id": "268712378",
      "cited_paper_id": null
    },
    {
      "context_text": "The intra-modal text similarity matrix is denoted as A t with size n t ; the visual object similarity matrix is denoted as A o with size n o , and the feature space of the three is located as: (1) Entity representation: following MKVSE (Feng, He, and Peng 2023), GloVe is used in MKG as an encoder for text entities and Bottom-Up and Top-Down(BUTD) attention model is used as an encoder for image entities.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GloVe and BUTD, but these are models or methods, not datasets. No specific datasets are mentioned or used according to the given text.",
      "processing_time": 54.21112132072449,
      "citing_paper_id": "268712378",
      "cited_paper_id": null
    },
    {
      "context_text": "Visual object entities: Following MKVSE(Feng, He, and Peng 2023), we extract the visual object entities in the image as triple representation ( V i , T i , O i ) , where V i denotes the original image, T i denotes its corresponding text description, O i denotes the set of visual object objects in‚Ä¶",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for extracting visual object entities but does not reference a named dataset.",
      "processing_time": 54.39740777015686,
      "citing_paper_id": "268712378",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶similarity matrix is denoted as A o with size n o , and the feature space of the three is located as: (1) Entity representation: following MKVSE (Feng, He, and Peng 2023), GloVe is used in MKG as an encoder for text entities and Bottom-Up and Top-Down(BUTD) attention model is used as an encoder‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to methods and models such as GloVe and BUTD.",
      "processing_time": 54.131805658340454,
      "citing_paper_id": "268712378",
      "cited_paper_id": null
    },
    {
      "context_text": "Visual object entities: Following MKVSE(Feng, He, and Peng 2023), we extract the visual object entities in the image as triple representation ( V i , T i , O i ) , where V i denotes the original image, T i denotes its corresponding text description, O i denotes the set of visual object objects in each image, and the size of the triple tuples is N .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for extracting visual object entities from images, which is not a dataset but a process.",
      "processing_time": 55.2568793296814,
      "citing_paper_id": "268712378",
      "cited_paper_id": null
    },
    {
      "context_text": ", distance-based models [133], which consider that the head entity and tail entity of the same triplet should be close in the projection space, and translation-based models, TransE [134] and its variants [135], [136], [137], which should conform to the assumption: t ‚âà h + r.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 53.125584840774536,
      "citing_paper_id": "246823061",
      "cited_paper_id": 715463
    },
    {
      "context_text": ", distance-based models [133], which consider that the head entity and tail entity of the same triplet should be close in the projection space, and translation-based models, TransE [134] and its variants [135], [136], [137], which should conform to the assumption: t ‚âà h + r.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 53.125584840774536,
      "citing_paper_id": "246823061",
      "cited_paper_id": 11202498
    },
    {
      "context_text": "WordNet [3], BabelNet [4]), encyclopedia knowledge (e.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions WordNet and BabelNet, which are lexical databases, but does not indicate their use as datasets in the research context.",
      "processing_time": 54.46819496154785,
      "citing_paper_id": "246823061",
      "cited_paper_id": 1671874
    },
    {
      "context_text": "2) Multi-modal Translation learns to translate from a source instance in one modality to a target instance in another, including example-based [34] and generative translation models [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of models and approaches. No verifiable resources are identified.",
      "processing_time": 53.32352304458618,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2315434
    },
    {
      "context_text": "Typical multi-modal tasks with images and texts include image caption [32], visual question answering [33], and cross-modal retrieval [34], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks. No dataset names are present in the text.",
      "processing_time": 53.33035445213318,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2315434
    },
    {
      "context_text": "[67] aligns the situation graph [94] extracted from an image of an event to the abstract meaning representation graph (AMR graph) [98] representing the semantic structure of the caption of this event in terms of the semantic and categories of cross modal arguments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'situation graph' and 'AMR graph', which are not datasets but rather representations or structures used in the research. No specific, verifiable datasets are mentioned.",
      "processing_time": 56.03227710723877,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2424223
    },
    {
      "context_text": "Schemas deÔ¨Åned in datasets of situation recognition tasks, such as SituNet [101] and SWiG [102], can be used to train models in this task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SituNet",
        "SWiG"
      ],
      "dataset_descriptions": {
        "SituNet": "Used to train models in situation recognition tasks, focusing on visual semantic role labeling for image understanding.",
        "SWiG": "Used to train models in situation recognition tasks, focusing on visual semantic role labeling for image understanding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, SituNet and SWiG, which are used for training models in situation recognition tasks. These datasets are relevant to multi-modal knowledge graph reasoning as they involve visual and semantic information.",
      "processing_time": 68.48464941978455,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2424223
    },
    {
      "context_text": "The visual arguments detection relies on the cooccurrence [94], [95] and relative positions [96] of detected objects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for visual semantic role labeling.",
      "processing_time": 52.53478646278381,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2424223
    },
    {
      "context_text": "Some datasets, such as SituNet [94] and SWiG [95], have defined a lot of visual event schemas.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SituNet",
        "SWiG"
      ],
      "dataset_descriptions": {
        "SituNet": "Used to define visual event schemas, contributing to the development of situation recognition systems that understand image content through visual semantic role labeling.",
        "SWiG": "Used to define visual event schemas, contributing to the development of situation recognition systems that understand image content through visual semantic role labeling."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions two datasets, SituNet and SWiG, which are used to define visual event schemas. These datasets are relevant to multi-modal knowledge graph reasoning as they involve both visual and semantic information.",
      "processing_time": 69.53079843521118,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2424223
    },
    {
      "context_text": "In early studies, action is defined as a serial of poses of different parts of the body, and the discriminative features are mined by heuristics approaches [73], [74], [75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches for action recognition. No verifiable resources are identified.",
      "processing_time": 54.456902503967285,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2493017
    },
    {
      "context_text": "In early studies, action is defined as a serial of poses of different parts of the body, and the discriminative features are mined by heuristics approaches [73], [74], [75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches for action recognition. No verifiable resources are identified.",
      "processing_time": 54.456902503967285,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7352553
    },
    {
      "context_text": "methods mainly focus on some specific types of relation, such as spatial relation [71], [72] and action relation [73], [74], [75], [76], [77], [78].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of relations studied in various papers. No verifiable resources are identified.",
      "processing_time": 54.26521182060242,
      "citing_paper_id": "246823061",
      "cited_paper_id": 2493017
    },
    {
      "context_text": "methods mainly focus on some specific types of relation, such as spatial relation [71], [72] and action relation [73], [74], [75], [76], [77], [78].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of relations studied in various papers. No verifiable resources are identified.",
      "processing_time": 54.26521182060242,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7352553
    },
    {
      "context_text": "methods mainly focus on some specific types of relation, such as spatial relation [71], [72] and action relation [73], [74], [75], [76], [77], [78].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of relations studied in various papers. No verifiable resources are identified.",
      "processing_time": 54.26521182060242,
      "citing_paper_id": "246823061",
      "cited_paper_id": 8505367
    },
    {
      "context_text": "methods mainly focus on some specific types of relation, such as spatial relation [71], [72] and action relation [73], [74], [75], [76], [77], [78].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of relations studied in various papers. No verifiable resources are identified.",
      "processing_time": 54.26521182060242,
      "citing_paper_id": "246823061",
      "cited_paper_id": 10282227
    },
    {
      "context_text": "2: Examples of labeling images: (a) labeling components after image segmentation in Visipedia [58]; (b) labeling objects with bounding boxes in Visual Genome [59]; (c) labeling two objects where one is a part of the other in NEIL [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used to label objects with bounding boxes, connecting language and vision using crowdsourced dense image annotations."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Visual Genome' as an example of labeling objects with bounding boxes, which is a dataset used for connecting language and vision.",
      "processing_time": 59.610438108444214,
      "citing_paper_id": "246823061",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "and a MMKG Visual Genome [59] for visual data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used to connect language and vision using crowdsourced dense image annotations, specifically for visual data in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Visual Genome' as a dataset used for visual data in the context of multi-modal knowledge graphs.",
      "processing_time": 59.732412338256836,
      "citing_paper_id": "246823061",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "Image-text Matching Flickr30k [63] MSCOCO [62] Visual Genome [59] 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30k",
        "MSCOCO",
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Flickr30k": "Used for evaluating image-text matching, focusing on aligning captions with images using multi-modal representations.",
        "MSCOCO": "Utilized for training and evaluating image-text alignment models, emphasizing rich caption annotations and diverse visual content.",
        "Visual Genome": "Applied to connect language and vision through dense image annotations, supporting complex multi-modal reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for image-text matching, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 70.34421014785767,
      "citing_paper_id": "246823061",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "ThemethodsusingtherelationsinKGsshowa strongabilityoflogicalinferencebetweenimages,generating more Ô¨Çuent stories than non-KG methods, and the triples from Visual Genome perform better than those from OpenIE in this task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used to generate more fluent stories through logical inference between images, demonstrating superior performance compared to non-KG methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'triples from Visual Genome' which is a specific dataset used for connecting language and vision. It is used to generate more fluent stories compared to non-KG methods.",
      "processing_time": 61.773715019226074,
      "citing_paper_id": "246823061",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "For example, the explicit knowledge in [51] has four sources: has-Part triples from hasPart KB [162], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hasPart KB",
        "DBpedia",
        "ConceptNet",
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "hasPart KB": "Used to provide has-Part triples, contributing to the construction of a knowledge graph with part-whole relationships.",
        "DBpedia": "Used to extract hasPart/isA triples, enhancing the knowledge graph with hierarchical and part-whole relationships.",
        "ConceptNet": "Used to incorporate commonsense triples, enriching the knowledge graph with everyday reasoning and conceptual relationships.",
        "Visual Genome": "Used to supply location triples of visual objects, integrating spatial information into the knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used as sources of explicit knowledge, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 74.4235782623291,
      "citing_paper_id": "246823061",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "For example, the explicit knowledge in [51] has four sources: has-Part triples from hasPart KB [162], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hasPart KB",
        "DBpedia",
        "ConceptNet",
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "hasPart KB": "Used to provide has-Part triples, contributing to the construction of a knowledge graph with part-whole relationships.",
        "DBpedia": "Used to extract hasPart/isA triples, enhancing the knowledge graph with hierarchical and part-whole relationships.",
        "ConceptNet": "Used to incorporate commonsense triples, enriching the knowledge graph with everyday reasoning and conceptual relationships.",
        "Visual Genome": "Used to supply location triples of visual objects, integrating spatial information into the knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used as sources of explicit knowledge, which are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 74.4235782623291,
      "citing_paper_id": "246823061",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "[110] uses dynamic programming to search for the optimal sequence in an image graph, where the value of edges are a joint criterion combining diversity score and",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimizing relevance and diversity in image retrieval.",
      "processing_time": 53.672876834869385,
      "citing_paper_id": "246823061",
      "cited_paper_id": 6425394
    },
    {
      "context_text": "‚Ä¶by constructing scene graphs 3.enhance the reasoning capabilities of multi-modal data with graph-structured information Image Tagging NUS-WIDE [129] help disambiguation the concept and relate them better to images Image Captioning MSVD [130] MSCOCO [58] GoodNews [131] 1.enable the‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NUS-WIDE",
        "MSVD",
        "MSCOCO",
        "GoodNews"
      ],
      "dataset_descriptions": {
        "NUS-WIDE": "Used for image tagging to enhance reasoning capabilities by constructing scene graphs and disambiguating concepts in web images.",
        "MSVD": "Used for image captioning to improve the generation of textual descriptions by leveraging graph-structured information.",
        "MSCOCO": "Used for image captioning to enhance the accuracy and context of generated captions through graph-structured information.",
        "GoodNews": "Used for image captioning to improve the generation of captions for news images by incorporating graph-structured information."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for enhancing multi-modal reasoning through graph-structured information. These datasets are used for image tagging and captioning tasks.",
      "processing_time": 76.7341628074646,
      "citing_paper_id": "246823061",
      "cited_paper_id": 6483070
    },
    {
      "context_text": "Incorporating information of MMKG across different modalities can help solve the cold-start problem long existing in Collaborative Filtering (CF) based recommending strategies [167].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to collaborative filtering and knowledge base embedding. No verifiable resources are identified.",
      "processing_time": 55.3037474155426,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7062707
    },
    {
      "context_text": "VQA aims at generating a textual answer for a textual question with the help of a relevant image [32], [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of VQA which involves images and questions. No clear, verifiable dataset names are provided.",
      "processing_time": 55.72330927848816,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7483388
    },
    {
      "context_text": "VQA GQA [119] Visual7w [120] OK-VQA [56] FVQA [33] KVQA [121] KB-VQA [122] 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VQA",
        "GQA",
        "Visual7w",
        "OK-VQA",
        "FVQA",
        "KVQA",
        "KB-VQA"
      ],
      "dataset_descriptions": {
        "VQA": "Used for visual question answering, focusing on image-text pairs to test reasoning capabilities.",
        "GQA": "Employs complex questions requiring multi-step reasoning over images and knowledge graphs.",
        "Visual7w": "Utilizes a subset of VQA with wh-questions to evaluate fine-grained reasoning.",
        "OK-VQA": "Focuses on open-ended questions that require external knowledge beyond the image content.",
        "FVQA": "Used for fact-based visual question answering, integrating factual knowledge with visual information.",
        "KVQA": "Employs knowledge graphs to answer questions that require reasoning over structured data and images.",
        "KB-VQA": "Utilizes knowledge bases to enhance visual question answering with external information."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for visual question answering and knowledge-based reasoning, which are directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 86.51333403587341,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7483388
    },
    {
      "context_text": "VQA GQA [119] Visual7w [120] OK-VQA [56] FVQA [33] KVQA [121] KB-VQA [122] 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VQA",
        "GQA",
        "Visual7w",
        "OK-VQA",
        "FVQA",
        "KVQA",
        "KB-VQA"
      ],
      "dataset_descriptions": {
        "VQA": "Used for visual question answering, focusing on image-text pairs to test reasoning capabilities.",
        "GQA": "Employs complex questions requiring multi-step reasoning over images and knowledge graphs.",
        "Visual7w": "Utilizes a subset of VQA with wh-questions to evaluate fine-grained reasoning.",
        "OK-VQA": "Focuses on open-ended questions that require external knowledge beyond the image content.",
        "FVQA": "Used for fact-based visual question answering, integrating factual knowledge with visual information.",
        "KVQA": "Employs knowledge graphs to answer questions that require reasoning over structured data and images.",
        "KB-VQA": "Utilizes knowledge bases to enhance visual question answering with external information."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for visual question answering and knowledge-based reasoning, which are directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 86.51333403587341,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "The existing efforts either project the multiple modalities into a uniÔ¨Åed space [35], or represent every single modal in its own vector space which satisÔ¨Åes certain constraints like linear correlation [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for handling multi-modal data.",
      "processing_time": 53.61608600616455,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7732372
    },
    {
      "context_text": "[68] aligns the situation graph [99] extracted from the image and the abstract meaning representation graph (AMR graph) [102] extracted from the caption of an event in terms of the semantics and categories of cross-modal arguments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AMR graph, which is a structured representation of sentence meaning, but does not refer to a specific dataset. The citation is about aligning graphs, not using a dataset.",
      "processing_time": 56.21743369102478,
      "citing_paper_id": "246823061",
      "cited_paper_id": 7771402
    },
    {
      "context_text": "For example, NEIL [22] initially collects image datasets by retrieving images from search engines with ontologies of NELL [141] as queries and then extracts objects and relations in these images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of image datasets collected by NEIL using NELL ontologies as queries. However, no specific dataset names are provided.",
      "processing_time": 55.702391624450684,
      "citing_paper_id": "246823061",
      "cited_paper_id": 8423494
    },
    {
      "context_text": "For example, NEIL [22] initially collects image datasets by retrieving images from search engines with ontologies of NELL [142] as queries and then extracts objects and relations in these images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of image datasets collected by NEIL using NELL ontologies as queries. However, no specific dataset names are provided.",
      "processing_time": 55.69943046569824,
      "citing_paper_id": "246823061",
      "cited_paper_id": 8423494
    },
    {
      "context_text": "[76] grounds many gerunds to images through crowdsourcing, such as arguing with, wrestling with and dancing with.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method for grounding gerunds to images through crowdsourcing, which is not a dataset but a process.",
      "processing_time": 56.61703300476074,
      "citing_paper_id": "246823061",
      "cited_paper_id": 8505367
    },
    {
      "context_text": "In [103], [106], [107] tags are clustered based on semantic features and images are re-assigned into each cluster according to their tags‚Äô semantic clusters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for clustering tags and reassigning images based on semantic features, which is more about the methodology than a specific dataset.",
      "processing_time": 57.45142149925232,
      "citing_paper_id": "246823061",
      "cited_paper_id": 13296639
    },
    {
      "context_text": "In [103], [106], [107] tags are clustered based on semantic features and images are re-assigned into each cluster according to their tags‚Äô semantic clusters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for clustering tags and reassigning images based on semantic features, which is more about the methodology than a specific dataset.",
      "processing_time": 57.45142149925232,
      "citing_paper_id": "246823061",
      "cited_paper_id": 233375020
    },
    {
      "context_text": "There are two classic cross-modal retrieval tasks including searching for images through a text [34], [35], and searching for texts through an image [36], [37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general cross-modal retrieval tasks. No verifiable resources are identified.",
      "processing_time": 54.527184009552,
      "citing_paper_id": "246823061",
      "cited_paper_id": 13755946
    },
    {
      "context_text": "There are two classic cross-modal retrieval tasks including searching for images through a text [34], [35], and searching for texts through an image [36], [37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general cross-modal retrieval tasks. No verifiable resources are identified.",
      "processing_time": 54.527184009552,
      "citing_paper_id": "246823061",
      "cited_paper_id": 202565460
    },
    {
      "context_text": "Image Tagging NUS-WIDE [123] MIRFlickr [124] help disambiguation the concept and relate them better to images",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NUS-WIDE",
        "MIRFlickr"
      ],
      "dataset_descriptions": {
        "NUS-WIDE": "Used to disambiguate concepts and relate them better to images, enhancing multi-modal reasoning in image tagging tasks.",
        "MIRFlickr": "Used to disambiguate concepts and relate them better to images, enhancing multi-modal reasoning in image tagging tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two image tagging datasets, NUS-WIDE and MIRFlickr, which are used to disambiguate concepts and relate them better to images.",
      "processing_time": 69.30296063423157,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14040310
    },
    {
      "context_text": "Heuristic approaches (e.g., the Apriori algorithm) can be utilized to mine frequent visual image patches to Ô¨Ånd association rules for predicting the event type by visual patterns [96], [101].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to heuristic approaches and algorithms, which are methods, not datasets.",
      "processing_time": 55.136330127716064,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14217450
    },
    {
      "context_text": "For example, an ontology expert may consider Explosion and Weapon as important items in the schema of event Attack , but in some news corpus, these concepts are not discovered and Smoke and Police appears much more frequently, which is not expected in advance [101].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a 'news corpus' but does not specify a named dataset. The citation is used to illustrate a point about concept discovery in knowledge base construction.",
      "processing_time": 56.192951917648315,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14217450
    },
    {
      "context_text": "Probase [10]) and geographic knowledge (e.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide enough information to identify a specific, verifiable dataset. 'Probase' is mentioned but it is not clear if it is used as a dataset or a method. The citation intent and resource type are also unclear.",
      "processing_time": 58.33647918701172,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14775471
    },
    {
      "context_text": "However, without incorporating more knowledge of various modalities, it is not scalable to reason just over image-question-answer triplets by semantic parsing and matching and can hardly generalize to more complicated cases [122].",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of reasoning over image-question-answer triplets without additional knowledge.",
      "processing_time": 55.12813091278076,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14843884
    },
    {
      "context_text": "‚Ä¶possibility of existence of triple ( h , r , t ) by the calculation of h , r , t in vector space, and translational distance based models, TransE [146] and its variants [147], [148], [149], which should conform to the assumption: t ‚âà h + r . h , t , r is respectively the vector representation of‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on explaining the methodology of translational distance-based models.",
      "processing_time": 54.994999408721924,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "SpeciÔ¨Åcally, in the training stage, the embedding model learns an embedding for each entity or relation, for instance, with the training objective t ‚âà h + r deÔ¨Åned by TransE [146].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TransE, which is a method for modeling multi-relational data, not a dataset. No specific dataset is mentioned.",
      "processing_time": 54.818694829940796,
      "citing_paper_id": "246823061",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "However, since some salience methods are too sensitive to input changes to produce reliable results [64], [65], [66], thus attention-based methods [21], [56], [62], [67], [68] are more studied than saliency-based methods [57], [69] on locating visual objects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses methods and their reliability. No verifiable resources are identified.",
      "processing_time": 54.50526523590088,
      "citing_paper_id": "246823061",
      "cited_paper_id": 15397918
    },
    {
      "context_text": "However, since some salience methods are too sensitive to input changes to produce reliable results [64], [65], [66], thus attention-based methods [21], [56], [62], [67], [68] are more studied than saliency-based methods [57], [69] on locating visual objects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses methods and their reliability. No verifiable resources are identified.",
      "processing_time": 54.50526523590088,
      "citing_paper_id": "246823061",
      "cited_paper_id": 202782699
    },
    {
      "context_text": "However, since some salience methods are too sensitive to input changes to produce reliable results [64], [65], [66], thus attention-based methods [21], [56], [62], [67], [68] are more studied than saliency-based methods [57], [69] on locating visual objects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses methods and their reliability. No verifiable resources are identified.",
      "processing_time": 54.50526523590088,
      "citing_paper_id": "246823061",
      "cited_paper_id": 222291117
    },
    {
      "context_text": "While textual queries can be represented as structured data in the format of (subject, relation, object) through AMR graph [98], candidate images could also be structured into a scene graph [113].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'scene graph' but does not refer to a specific dataset. It is used to describe a method for structuring images.",
      "processing_time": 55.83451962471008,
      "citing_paper_id": "246823061",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "[113] assumes that if there are a relation between two entities, the relation is",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a general assumption about entity relations.",
      "processing_time": 55.108116149902344,
      "citing_paper_id": "246823061",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "A simple solution is that to only match objects and co-occurring relations in two graphs without predicting the relation types [113].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for image retrieval using scene graphs.",
      "processing_time": 54.39773678779602,
      "citing_paper_id": "246823061",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "Recommender System MovieLens [129] IntentBooks [130] Dianping [131] KKBOX [132] 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MovieLens"
      ],
      "dataset_descriptions": {
        "MovieLens": "Used to evaluate recommender system algorithms, specifically focusing on user-item interactions and rating predictions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MovieLens' which is a well-known dataset in the field of recommender systems. The other names (IntentBooks, Dianping, KKBOX) do not have clear identifiers or are not recognized as specific datasets.",
      "processing_time": 63.5489501953125,
      "citing_paper_id": "246823061",
      "cited_paper_id": 16619709
    },
    {
      "context_text": "Link prediction in MMKG [134], [136], [141], [142] aims to complete a triple (h, r, t) when one of the entities in h, r, t",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the task of link prediction in multi-modal knowledge graphs.",
      "processing_time": 55.01266598701477,
      "citing_paper_id": "246823061",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "task in multi-modal pretrained language models [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multi-modal pretrained language models.",
      "processing_time": 54.54062604904175,
      "citing_paper_id": "246823061",
      "cited_paper_id": 38582742
    },
    {
      "context_text": "score between the input image and text pair [114], [155], [156], [157], [158].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, and the context is too vague to infer any specific resource usage.",
      "processing_time": 57.09143900871277,
      "citing_paper_id": "246823061",
      "cited_paper_id": 49867191
    },
    {
      "context_text": "Image Captioning MSVD [125] MSCOCO [62] NYTimes800k [126] GoodNews [127] 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSVD",
        "MSCOCO",
        "NYTimes800k",
        "GoodNews"
      ],
      "dataset_descriptions": {
        "MSVD": "Used for evaluating image captioning models, focusing on video and image data with detailed annotations.",
        "MSCOCO": "Employed to train and evaluate image captioning models, providing a large set of images with diverse captions.",
        "NYTimes800k": "Utilized for training and evaluating entity-aware captioning models, containing news images with rich contextual information.",
        "GoodNews": "Applied to study context-driven entity-aware captioning for news images, focusing on generating captions that reflect the article content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions multiple datasets by name, which are used for image captioning. These datasets are specific and have clear identifiers.",
      "processing_time": 77.33959603309631,
      "citing_paper_id": "246823061",
      "cited_paper_id": 91184120
    },
    {
      "context_text": "For example, the OK-VQA dataset [49], which contains only questions that require external knowledge to answer, is built to test the reasoning capability of VQA models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OK-VQA"
      ],
      "dataset_descriptions": {
        "OK-VQA": "Used to test the reasoning capability of VQA models by providing questions that require external knowledge, focusing on the model's ability to integrate visual and textual information."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the OK-VQA dataset, which is a specific, verifiable resource used to test the reasoning capability of VQA models requiring external knowledge.",
      "processing_time": 62.5172860622406,
      "citing_paper_id": "246823061",
      "cited_paper_id": 173991173
    },
    {
      "context_text": "In most recent VQA benchmark datasets such as GQA [125], OK-VQA [49] and KVQA [127], many questions require visual reasoning combined with external knowledge.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GQA",
        "OK-VQA",
        "KVQA"
      ],
      "dataset_descriptions": {
        "GQA": "Used to evaluate visual reasoning combined with external knowledge, focusing on complex question answering in the VQA domain.",
        "OK-VQA": "Used to assess the need for external knowledge in visual question answering, emphasizing the integration of visual and textual information.",
        "KVQA": "Used to test visual reasoning capabilities requiring external knowledge, specifically designed for evaluating multi-modal understanding."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions specific VQA benchmark datasets that require visual reasoning combined with external knowledge, which aligns with multi-modal knowledge graph reasoning.",
      "processing_time": 71.47620892524719,
      "citing_paper_id": "246823061",
      "cited_paper_id": 173991173
    },
    {
      "context_text": "‚Äù, if the stuffed animal in the image is detected as ‚Äú Teddy Bear ‚Äù, the answer inferred through KG will be ‚Äú Theodore Roosevelt ‚Äù, who is often referred as ‚Äú Teddy Roosevelt ‚Äù, and after whom Teddy Bear is named [49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or example of how a knowledge graph can be used in visual question answering.",
      "processing_time": 55.667879819869995,
      "citing_paper_id": "246823061",
      "cited_paper_id": 173991173
    },
    {
      "context_text": "‚Ä¶capture the relationship among mentions and entities 3.learn distributed representations for each entity with multi-modal data VQA GQA [125] OK-VQA [49] FVQA [126] KVQA [127] KB-VQA [128] 1.provide knowledge about the named entities and their relations in the image, leading to deeper visual‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VQA",
        "GQA",
        "OK-VQA",
        "FVQA",
        "KVQA",
        "KB-VQA"
      ],
      "dataset_descriptions": {
        "VQA": "Used to train and evaluate models that answer questions about images, focusing on the integration of visual and textual information.",
        "GQA": "Used to assess the ability of models to reason about complex visual scenes and answer detailed questions, emphasizing compositional and logical reasoning.",
        "OK-VQA": "Used to benchmark models that require external knowledge to answer visual questions, enhancing the depth of understanding in multi-modal reasoning.",
        "FVQA": "Used to evaluate models that incorporate functional knowledge to answer questions about images, focusing on the functional aspects of visual elements.",
        "KVQA": "Used to test models that integrate knowledge bases with visual question answering, enhancing the accuracy and depth of responses.",
        "KB-VQA": "Used to assess the performance of models that combine knowledge graphs with visual question answering, focusing on the integration of structured knowledge."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several VQA datasets, which are relevant to multi-modal knowledge graph reasoning. These datasets are used to train and evaluate models that require external knowledge to answer questions about images.",
      "processing_time": 92.62799596786499,
      "citing_paper_id": "246823061",
      "cited_paper_id": 173991173
    },
    {
      "context_text": "Models have proven their effectiveness in improving many downstream tasks [48], [49], [50], [51], [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models and their effectiveness in downstream tasks. No verifiable resources are identified.",
      "processing_time": 55.86698818206787,
      "citing_paper_id": "246823061",
      "cited_paper_id": 201058752
    },
    {
      "context_text": "Models have proven their effectiveness in improving many downstream tasks [48], [49], [50], [51], [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models and their effectiveness in downstream tasks. No verifiable resources are identified.",
      "processing_time": 55.86698818206787,
      "citing_paper_id": "246823061",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "[74] limits the scale of independent concepts‚Äô labels by setting a small value of maximum extension level to avoid too many related images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to limit the scale of independent concepts' labels in a multi-modal knowledge graph.",
      "processing_time": 56.617128133773804,
      "citing_paper_id": "246823061",
      "cited_paper_id": 201066287
    },
    {
      "context_text": "The single-stream models, such as VLBERT [53] and ViLT [50], input all modal information into",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context focuses on describing single-stream models and their input modalities.",
      "processing_time": 55.86157178878784,
      "citing_paper_id": "246823061",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "For example, [172] constructs an MMKG called VTKB containing hierarchical concepts, linking concepts of original tags to images and linking images by the similarities of embed-dings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VTKB"
      ],
      "dataset_descriptions": {
        "VTKB": "Used to construct a visio-textual knowledge base linking concepts to images and images by embedding similarities, enhancing image tagging quality through hierarchical concept organization."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the construction of a visio-textual knowledge base (VTKB) that links concepts to images and images by embedding similarities, which is directly relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 64.62943768501282,
      "citing_paper_id": "246823061",
      "cited_paper_id": 201871273
    },
    {
      "context_text": "The cross-modal representations are usually fused by the mechanism of attention, so the disadvantage of global representation is lacking semantics of explicit finegrained relations [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for cross-modal representation fusion.",
      "processing_time": 54.88034510612488,
      "citing_paper_id": "246823061",
      "cited_paper_id": 202565460
    },
    {
      "context_text": "The Ô¨Ågure shows an example of relation grounding by graph matching in [119]. from the aggregations of their neighbors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for cross-modal scene graph matching.",
      "processing_time": 55.381458044052124,
      "citing_paper_id": "246823061",
      "cited_paper_id": 204402762
    },
    {
      "context_text": "[119] represents the two graphs by GCN, in which objects are updated from themselves and relation nodes are updated Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GCN) and a general description of graph representation. No verifiable resources are identified.",
      "processing_time": 57.13789367675781,
      "citing_paper_id": "246823061",
      "cited_paper_id": 204402762
    },
    {
      "context_text": "To combine the two conflicting scores, [108], [109] use MaxMin methods to choose candidates: assign higher score to images that are not similar to the selected set, and choose the",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for combining scores in image search.",
      "processing_time": 55.09334063529968,
      "citing_paper_id": "246823061",
      "cited_paper_id": 206751218
    },
    {
      "context_text": "‚Ä¶common sense knowledge (e.g., Cyc [1], ConceptNet [2]), lexical knowledge (e.g., WordNet [3], BabelNet [4]), encyclopedia knowledge (e.g., Freebase [5], DBpedia [6], YAGO [7], WikiData [8], CN-DBpedia [9]), taxonomic knowledge (e.g., Probase [10]) and geographic knowledge (e.g., GeoNames [11]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "DBpedia",
        "YAGO",
        "WikiData",
        "CN-DBpedia",
        "GeoNames"
      ],
      "dataset_descriptions": {
        "Freebase": "Mentioned as a collaboratively created graph database for structuring human knowledge, but not explicitly used in the research context.",
        "DBpedia": "Mentioned as an encyclopedia knowledge source, but not explicitly used in the research context.",
        "YAGO": "Mentioned as an encyclopedia knowledge source, but not explicitly used in the research context.",
        "WikiData": "Mentioned as an encyclopedia knowledge source, but not explicitly used in the research context.",
        "CN-DBpedia": "Mentioned as an encyclopedia knowledge source, but not explicitly used in the research context.",
        "GeoNames": "Mentioned as a geographic knowledge source, but not explicitly used in the research context."
      },
      "confidence_score": 0.5,
      "reasoning": "The context mentions several knowledge bases and databases, which are relevant to multi-modal knowledge graph reasoning. However, none of these are explicitly used in the research context provided.",
      "processing_time": 88.09375071525574,
      "citing_paper_id": "246823061",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "To tackle these problems, some recent efforts [164] resorts to a MMKG for help within a distill-enrich-generate three stage framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving a multi-modal knowledge graph (MMKG).",
      "processing_time": 56.78206753730774,
      "citing_paper_id": "246823061",
      "cited_paper_id": 208547698
    },
    {
      "context_text": "To get more fine-grained and informative unbiased predictions, [89] uses counterfactual causation instead of conven-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating unbiased scene graphs.",
      "processing_time": 55.775943756103516,
      "citing_paper_id": "246823061",
      "cited_paper_id": 211532586
    },
    {
      "context_text": "There are much other work focusing on detecting relations with few samples by transfer learning [86], fewshot learning [87] and contrastive learning [88], still being",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 56.17033505439758,
      "citing_paper_id": "246823061",
      "cited_paper_id": 213755659
    },
    {
      "context_text": "linking (MEL) uses both textual and visual information to map an ambiguous mention in the text to an entity in a knowledge base (KB) [150].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of using textual and visual information for multimodal entity linking.",
      "processing_time": 56.7733268737793,
      "citing_paper_id": "246823061",
      "cited_paper_id": 215746363
    },
    {
      "context_text": "the relationship among mentions and entities [150], [151], but the irrelevant part with images may also become noises and bring negative impact to the representation learning for both mentions and entities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the relationship between mentions and entities in multimodal settings, but does not reference any named datasets.",
      "processing_time": 57.92954182624817,
      "citing_paper_id": "246823061",
      "cited_paper_id": 215746363
    },
    {
      "context_text": "Furthermore, to improve Ô¨Åne-grained cross-modal understanding, some work also add cross-modal object align-JOURNAL [43], [44], [45], relation alignment [46], [47] tasks to optimize the pre-training process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.47515654563904,
      "citing_paper_id": "246823061",
      "cited_paper_id": 215754208
    },
    {
      "context_text": "Furthermore, to improve Ô¨Åne-grained cross-modal understanding, some work also add cross-modal object align-JOURNAL [43], [44], [45], relation alignment [46], [47] tasks to optimize the pre-training process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.47515654563904,
      "citing_paper_id": "246823061",
      "cited_paper_id": 244117525
    },
    {
      "context_text": "VL-PTMs: Recently, many large companies and research institutions including OpenMind [41], Microsoft [42], [43] and Huawei [44] etc. pay great efforts on training large VL-PTMs based on large-scale unsupervised multi-modal data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large-scale unsupervised multi-modal data' but does not specify any named datasets. The citation is more about the development of models rather than the use of specific datasets.",
      "processing_time": 58.193262815475464,
      "citing_paper_id": "246823061",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "The symbol grounding solutions are usually retrieval-based methods [20], [28], [29], [30], [140], [141] and the multi-modal data labeling solutions are usually classi-Ô¨Åcation and detection methods [21], [23], [24], [94], [136].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of methods and approaches. No verifiable resources are identified.",
      "processing_time": 56.31030297279358,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "For example, the heatmap values in GAIA [21] are similarities between image regions and entity mentions in a caption, and those in [68] are similarities between image regions and possible event argument role types.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or systems (GAIA) and their components. No clear, verifiable dataset names are present.",
      "processing_time": 57.69649028778076,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "Due to the rapid growth of applications‚Äô demand for multi-modal knowledge guidance, the multi-modalization of KGs and their applications has been booming in recent years [19], [20], [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in the field. No verifiable resources are named.",
      "processing_time": 56.478630781173706,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "‚Ä¶objects are duplicated instances of the same entities at different viewpoints, positions, poses and appearances, a common way to process is to cluster all the regions with recognized objects, and only the central one of each cluster will eventually be the output as a new visual entity [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses a method for processing recognized objects in multimedia content.",
      "processing_time": 56.2931854724884,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "These detectors are trained with supervised data from public images-text datasets [21] (such as MSCOCO [58], Flickr30k [59], Flick30k Entities [60] and Open Images [61]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO",
        "Flickr30k",
        "Flick30k Entities",
        "Open Images"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to train detectors on annotated images and captions, enhancing multi-modal understanding and reasoning capabilities.",
        "Flickr30k": "Utilized for training detectors on image-caption pairs, improving the system's ability to recognize and describe visual content accurately.",
        "Flick30k Entities": "Employed to enhance entity recognition in images and associated text, supporting fine-grained multimedia knowledge extraction.",
        "Open Images": "Leveraged for training on a diverse set of labeled images and annotations, expanding the system's multi-modal reasoning across various domains."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several image-text datasets used for training detectors. These datasets are specific and publicly accessible, making them relevant for multi-modal knowledge graph reasoning.",
      "processing_time": 81.61335754394531,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "These detectors are trained with supervised data from public images-text datasets [21] (such as MSCOCO [58], Flickr30k [59], Flick30k Entities [60] and Open Images [61]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO",
        "Flickr30k",
        "Flick30k Entities",
        "Open Images"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to train detectors on annotated images and captions, enhancing multi-modal understanding and reasoning capabilities.",
        "Flickr30k": "Utilized for training detectors on image-caption pairs, improving the system's ability to recognize and describe visual content accurately.",
        "Flick30k Entities": "Employed to enhance entity recognition in images and associated text, supporting fine-grained multimedia knowledge extraction.",
        "Open Images": "Leveraged for training on a diverse set of labeled images and annotations, expanding the system's multi-modal reasoning across various domains."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several image-text datasets used for training detectors. These datasets are specific and publicly accessible, making them relevant for multi-modal knowledge graph reasoning.",
      "processing_time": 81.61335754394531,
      "citing_paper_id": "246823061",
      "cited_paper_id": null
    },
    {
      "context_text": "The precision of the visual object extraction model used in GAIA is only 43% on the benchmark MSCOCO [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used to evaluate the precision of the visual object extraction model in GAIA, focusing on fine-grained multimedia knowledge extraction."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the benchmark MSCOCO, which is a well-known dataset used for evaluating visual object extraction models. The context indicates that the dataset is used to measure the precision of a model.",
      "processing_time": 66.009024143219,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "It requires much pre-processing work for Ô¨Åne-grained recognition [21], such as pre-deÔ¨Åned rules, pre-trained Ô¨Åne-grained detectors, etc. 2) Visual Grounding Methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about pre-processing work and visual grounding methods, which do not indicate the use of a specific dataset.",
      "processing_time": 58.63165998458862,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "The accuracy of visual grounding methods used in GAIA [21] is 69.2% on Flickr30k.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "Flickr30k": "Used to evaluate the accuracy of visual grounding methods in the GAIA system, focusing on fine-grained multimedia knowledge extraction."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the use of Flickr30k for evaluating the accuracy of visual grounding methods in the GAIA system.",
      "processing_time": 63.63305974006653,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "3: The heatmap for detected visual entities ( Soldier and Boats ) in two example images by visual grounding in GAIA [21], where the stronger the correlation between a pixel and a word, the warmer the color of the pixel.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'GAIA' but does not refer to it as a dataset. It is described as a system for fine-grained multimedia knowledge extraction, which suggests it is a method or tool rather than a dataset.",
      "processing_time": 59.19023895263672,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "However, since some salience meth-ods are too sensitive to input changes to produce reliable results [64], [65], [66], thus attention-based methods [21], [56], [62], [67], [68] are more studied than saliency-based methods [57], [69] on locating visual objects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.03255271911621,
      "citing_paper_id": "246823061",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "ment [43], [44], [45], relation alignment [46], [47] tasks to optimize the pre-training process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.194443702697754,
      "citing_paper_id": "246823061",
      "cited_paper_id": 220265934
    },
    {
      "context_text": "ment [43], [44], [45], relation alignment [46], [47] tasks to optimize the pre-training process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.194443702697754,
      "citing_paper_id": "246823061",
      "cited_paper_id": 244920947
    },
    {
      "context_text": "Besides, the explicit semantic knowledge preserved in MMKG help refine the answers with more interpretability and generality [154].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'explicit semantic knowledge' in MMKG. No clear, verifiable resource names are provided.",
      "processing_time": 58.264222860336304,
      "citing_paper_id": "246823061",
      "cited_paper_id": 221397171
    },
    {
      "context_text": "to perform a prediction [27], where various attention mechanism such as gated cross-modality attention [46], bottom up attention [47] etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only attention mechanisms. No verifiable resources are identified.",
      "processing_time": 56.24201726913452,
      "citing_paper_id": "246823061",
      "cited_paper_id": 221703022
    },
    {
      "context_text": "In this case, the symbol grounding way is preferred to construct the MMKG because the symbols in KGs have already been well Ô¨Åltered and reÔ¨Åned, such as the movie ontologies in recommendation systems [19], product ontologies in e-commerce dialogue systems [20] and paper ontologies in academic information retrieval and KBQA [137], [138].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'movie ontologies', 'product ontologies', and 'paper ontologies' but does not specify them as datasets. They are likely part of knowledge graphs or ontologies used in various systems.",
      "processing_time": 59.68424916267395,
      "citing_paper_id": "246823061",
      "cited_paper_id": 224291855
    },
    {
      "context_text": "and paper ontologies in academic information retrieval and KBQA [138], [139].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of paper ontologies in academic information retrieval and KBQA. No verifiable resources are identified.",
      "processing_time": 57.92102265357971,
      "citing_paper_id": "246823061",
      "cited_paper_id": 224291855
    },
    {
      "context_text": "[138] uses an academic MMKG about papers and codes to offer retrieval on the implementation level.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "academic MMKG"
      ],
      "dataset_descriptions": {
        "academic MMKG": "Used to offer retrieval on the implementation level for deep learning papers and code, enhancing the linking between academic publications and their corresponding implementations."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions an academic MMKG, which is a specific multimodal knowledge graph. The title confirms it is about papers and codes, making it a relevant dataset for multi-modal knowledge graph reasoning.",
      "processing_time": 66.0299243927002,
      "citing_paper_id": "246823061",
      "cited_paper_id": 224291855
    },
    {
      "context_text": "In addition to applications on movie recommender [19] or e-commerce KBQA systems [20], MMKGs are also applied in multi-modal tasks such as cross-modal retrieval, dialogue system and object detection in some domain applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of MMKGs. No clear, verifiable resources are identified.",
      "processing_time": 57.20509934425354,
      "citing_paper_id": "246823061",
      "cited_paper_id": 224291855
    },
    {
      "context_text": "Besides, MMKGs have advantages on providing long-tail knowledge, background knowledge, and Ô¨Åne-grained knowledge compared with VL-PTMs [51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between MMKGs and VL-PTMs. No verifiable resources are identified.",
      "processing_time": 57.631970167160034,
      "citing_paper_id": "246823061",
      "cited_paper_id": 229339845
    },
    {
      "context_text": "The model fusing explicit symbolic knowledge from the MMKG and implicit knowledge from VL-PTMs outperforms the pure VL-PTMs, and most of the knowledge in the MMKG is non-overlapping with the implicit knowledge in VL-PTMs [51].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions MMKG, which appears to be a multi-modal knowledge graph used in the research. However, it is not clear if MMKG is a specific, verifiable dataset or a method. The context does not provide enough detail to confirm its nature as a dataset.",
      "processing_time": 61.77289032936096,
      "citing_paper_id": "246823061",
      "cited_paper_id": 229339845
    },
    {
      "context_text": "For example, the explicit knowledge in [51] has four sources: hasPart triples from hasPart KB [163], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hasPart KB",
        "DBpedia",
        "ConceptNet",
        "Visual Genome"
      ],
      "dataset_descriptions": {
        "hasPart KB": "Used to provide hasPart triples, contributing to the construction of a knowledge graph for open-domain VQA.",
        "DBpedia": "Used to provide hasPart and isA triples, enhancing the knowledge graph with structured information.",
        "ConceptNet": "Used to provide commonsense triples, enriching the knowledge graph with everyday knowledge.",
        "Visual Genome": "Used to provide location triples of visual objects, integrating visual and textual information in the knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used as sources of explicit knowledge for a knowledge graph. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 77.73242568969727,
      "citing_paper_id": "246823061",
      "cited_paper_id": 229339845
    },
    {
      "context_text": "‚Ä¶and relation triples of different modal in MMKGs can be represented as nodes and edges in a heterogeneous graph and represented in a uniÔ¨Åed format, which facilitates explicit reasoning with heuristic rules, SPARQL queries [128] or weighted passing messages between GNN nodes [51], [128], [162].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for representing and reasoning with multi-modal knowledge graphs.",
      "processing_time": 56.74100160598755,
      "citing_paper_id": "246823061",
      "cited_paper_id": 229339845
    },
    {
      "context_text": "graph parser is employed to generate a scene graph with objects, attributes and relations from the caption of an image, then UNIMO [51] randomly replaces the object, attribute",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (UNIMO) and a general process of generating scene graphs and replacing objects and attributes.",
      "processing_time": 58.71887469291687,
      "citing_paper_id": "246823061",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "It is proved that VL-PTMs such as CLIP [41], trained on hundreds of millions of image-text data, can recognize many popular entities such as famous people and landmarks with high accuracy [73].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image-text data' but does not specify a named dataset. It refers to the capabilities of CLIP, which is a model, not a dataset.",
      "processing_time": 58.462124824523926,
      "citing_paper_id": "246823061",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "It is proved that VL-PTMs such as CLIP [41], trained on hundreds of millions of image-text data, can recognize many popular entities such as famous people and landmarks with high accuracy [73].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image-text data' but does not specify a named dataset. It refers to the capabilities of CLIP, which is a model, not a dataset.",
      "processing_time": 58.462124824523926,
      "citing_paper_id": "246823061",
      "cited_paper_id": 233296711
    },
    {
      "context_text": "A typical VL-PTM example is CLIP [41] trained on 400 million text-image pairs, which signiÔ¨Åcantly improves the performance of image classiÔ¨Åcation and cross-modal retrieval.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a large-scale dataset used for training CLIP, but does not specify a named dataset. The reference is to a method (CLIP) and its training data size, not a specific, named dataset.",
      "processing_time": 60.074660301208496,
      "citing_paper_id": "246823061",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "It is proved that multi-modal pre-trained language models, such as CLIP [68], trained on hundreds of millions of web-scale image-text data are highly accurate on famous figures and landmark buildings [69], which will reduce much workload on data collection and model training when constructing a MMKG of figures or buildings.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'web-scale image-text data' but does not provide a specific dataset name. It also references 'famous figures and landmark buildings' without naming a specific dataset. No clear, verifiable datasets are identified.",
      "processing_time": 60.173630714416504,
      "citing_paper_id": "246823061",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Some pre-trained vision transformer models already have strong capability of visual object segmentation and focus on the foreground object even in highly ambiguous situations, such as DINO [70], which will improve the performance of locating visual objects and aligning cross-modal knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained models. The citation is used to reference a method, not a dataset.",
      "processing_time": 56.85000038146973,
      "citing_paper_id": "246823061",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "Then in the prediction stage, the most matching h in (?, r, t) is found by ranking all candidate head entities h‚àó according to a score function like argmaxh‚àó œÜ(h ‚àó, r, t), where the score function is diverse in different embedding models [154].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general method for link prediction in knowledge graphs.",
      "processing_time": 56.20118069648743,
      "citing_paper_id": "246823061",
      "cited_paper_id": 234685749
    },
    {
      "context_text": "The symbol grounding solutions are usually retrieval-based methods [20], [28], [29], [30], [140], [141] and the multi-modal data labeling solutions are usually classification and detection methods [21], [23], [24], [94], [136].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific datasets, only general categories of methods. However, the cited paper titles suggest potential datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 58.26055860519409,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235270502
    },
    {
      "context_text": "The symbol grounding solutions are usually retrieval-based methods [20], [28], [29], [30], [140], [141] and the multi-modal data labeling solutions are usually classification and detection methods [21], [23], [24], [94], [136].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific datasets, only general categories of methods. However, the cited paper titles suggest potential datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 58.26055860519409,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235271284
    },
    {
      "context_text": "The symbol grounding solutions are usually retrieval-based methods [20], [28], [29], [30], [140], [141] and the multi-modal data labeling solutions are usually classification and detection methods [21], [23], [24], [94], [136].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific datasets, only general categories of methods. However, the cited paper titles suggest potential datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 58.26055860519409,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235503675
    },
    {
      "context_text": "The symbol grounding solutions are usually retrieval-based methods [20], [28], [29], [30], [140], [141] and the multi-modal data labeling solutions are usually classification and detection methods [21], [23], [24], [94], [136].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific datasets, only general categories of methods. However, the cited paper titles suggest potential datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 58.26055860519409,
      "citing_paper_id": "246823061",
      "cited_paper_id": 248779998
    },
    {
      "context_text": ", X-rays, CT and ultrasound) and textual description, improving the performance of doctor-patient dialogue systems of COVID-19 [141] and further reducing the risk of close contact.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions multi-modal data including X-rays, CT, ultrasound, and textual descriptions, which are used to improve doctor-patient dialogue systems for COVID-19. However, no specific dataset names are provided.",
      "processing_time": 60.190406799316406,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235270502
    },
    {
      "context_text": "In the medical Ô¨Åeld, MMKGs enrich the representation of entities with the help of images (e.g., X-rays, CT and ultrasound) and textual description, improving the performance of doctor-patient dialogue systems of COVID-19 [140] and further reducing the risk of close contact.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions the use of multi-modal knowledge graphs (MMKGs) in the medical field, specifically for enhancing doctor-patient dialogue systems related to COVID-19. However, no specific dataset names are mentioned.",
      "processing_time": 60.588695764541626,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235270502
    },
    {
      "context_text": "Therefore, [140] proposes a collection of extensive and high-qualified multi-modal benchmarks for precisely evaluating node classification tasks on MMKGs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'multi-modal benchmarks' but does not specify individual datasets. The cited paper title suggests a collection of datasets, but no specific names are provided in the citation context.",
      "processing_time": 59.05941581726074,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235271284
    },
    {
      "context_text": "However, [140] argues that this task in KGs cannot be solved purely by node embedding models, and the graph structures should also be considered.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general argument about the limitations of node embedding models in KGs.",
      "processing_time": 57.32584857940674,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235271284
    },
    {
      "context_text": "For instance, some efforts [140], [155] work on learning embeddings for entities and concepts from several different types of modalities and then encode them to a joint representation space.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to learning embeddings from multiple modalities.",
      "processing_time": 56.73392844200134,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235271284
    },
    {
      "context_text": "If the multi-modal data are treated as first-class citizens in some scenarios, the multi-modal data labeling way is more preferred to construct the MMKG, such as unearthed oracle bones‚Äô photos in oracle bones recognition system [136], teachers‚Äô class audios in educa-",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "unearthed oracle bones‚Äô photos"
      ],
      "dataset_descriptions": {
        "unearthed oracle bones‚Äô photos": "Used to construct a multi-modal knowledge graph for oracle bone recognition, focusing on integrating visual and textual data to enhance information processing."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'unearthed oracle bones‚Äô photos' which is a specific type of data used in the construction of a multi-modal knowledge graph for oracle bone recognition.",
      "processing_time": 67.62340450286865,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235503675
    },
    {
      "context_text": "In the archaeology field, MMKGs also contribute to oracle bones detection and recognition, not only taking into account edges, textures, cracks, scratches, splinters and background, but also offering relevant literature, location and institutions to assist decision making [136].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions the use of multi-modal knowledge graphs (MMKGs) in the archaeology field, specifically for oracle bones detection and recognition. However, no specific dataset name is mentioned.",
      "processing_time": 59.7754967212677,
      "citing_paper_id": "246823061",
      "cited_paper_id": 235503675
    },
    {
      "context_text": "In [18] the textual scene graph and visual scene graph extracted from the input article and images are aligned by the cross-modal entity matching module pre-trained on Wikipedia articles and images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia articles and images"
      ],
      "dataset_descriptions": {
        "Wikipedia articles and images": "Used to pre-train a cross-modal entity matching module, aligning textual and visual scene graphs extracted from input articles and images."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions the use of Wikipedia articles and images for pre-training a cross-modal entity matching module, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 66.2669563293457,
      "citing_paper_id": "246823061",
      "cited_paper_id": 236428934
    },
    {
      "context_text": "On the other hand, there is an increasing demand for the multi-modality of knowledge to break through the bottleneck of real-world applications [16], [17], [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend towards multi-modality in knowledge graphs.",
      "processing_time": 57.150333881378174,
      "citing_paper_id": "246823061",
      "cited_paper_id": 236428934
    },
    {
      "context_text": "Fortunately, some datasets [120], [121] of relation extraction based on textual named entities and visual relations could be helpful.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions datasets for multimodal relation extraction, which aligns with the topic of multi-modal knowledge graph reasoning. However, no specific dataset names are provided.",
      "processing_time": 58.90951180458069,
      "citing_paper_id": "246823061",
      "cited_paper_id": 239011558
    },
    {
      "context_text": "Recently, many large companies and research institutions including OpenMind [41], Microsoft [42], [43] and Huawei [44] etc. pay great efforts on training large VL-PTMs based on large-scale unsupervised multi-modal data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large-scale unsupervised multi-modal data' but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.99697279930115,
      "citing_paper_id": "246823061",
      "cited_paper_id": 244117525
    },
    {
      "context_text": "Recently, many large companies and research institutions including OpenMind [41], Microsoft [42], [43]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only companies and institutions. There is no clear indication of a reusable resource or dataset being used.",
      "processing_time": 57.84503674507141,
      "citing_paper_id": "246823061",
      "cited_paper_id": 244920947
    },
    {
      "context_text": "VL-PTMs bring new opportunities to nearly all cross-modal downstream tasks, including the detection of visual entities and concepts [70], [71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only cross-modal downstream tasks. No verifiable resources are identified.",
      "processing_time": 57.2129864692688,
      "citing_paper_id": "246823061",
      "cited_paper_id": 246634906
    },
    {
      "context_text": "‚Ä¶citizens in some scenarios, the multi-modal data labeling way is more preferred to construct the MMKG, such as unearthed oracle bones‚Äô photos in oracle bones recognition system [136], teachers‚Äô class audios in educational services [137] and the movies‚Äô videos in deep video understanding tasks [94].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions specific types of multi-modal data used in constructing MMKGs, including photos of oracle bones, class audios, and movie videos. However, no specific dataset names are provided.",
      "processing_time": 60.13517951965332,
      "citing_paper_id": "246823061",
      "cited_paper_id": 246938066
    },
    {
      "context_text": "With the development of deep learning, hidden features gotten from CNN [145], [150], [151] or Transformers [152]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.6368567943573,
      "citing_paper_id": "246823061",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "With the development of deep learning, hidden features gotten from CNN [144], [149], [150] or Transformers [151] are the main image embeddings used in MMKG representation, while other explicit visual features such as GHD, HOG, CLD can hardly be leveraged in MMKG representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of image embeddings from CNNs and Transformers in MMKG representation.",
      "processing_time": 58.782816648483276,
      "citing_paper_id": "246823061",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "MMEKG [24] N-MMKG event Wikipedia, BookCorpus, CC3M&CC12M, C4(news) WordNet event classification, object recognition, event relation extraction < 990K events, < 644 event relations < 863M instance events, < 934M instance events‚Äô relations (including textual and visual ones)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia",
        "BookCorpus",
        "CC3M",
        "CC12M",
        "C4(news)",
        "WordNet"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used for event classification and relation extraction, contributing to the construction of the MMEKG multi-modal event knowledge graph.",
        "BookCorpus": "Used for textual data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph.",
        "CC3M": "Used for visual data to support object recognition and event relation extraction in the MMEKG multi-modal event knowledge graph.",
        "CC12M": "Used for visual data to support object recognition and event relation extraction in the MMEKG multi-modal event knowledge graph.",
        "C4(news)": "Used for news-related textual data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph.",
        "WordNet": "Used for lexical data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for constructing and evaluating the MMEKG multi-modal event knowledge graph, including Wikipedia, BookCorpus, CC3M, CC12M, and C4(news). These datasets are used for various tasks such as event classification, object recognition, and event relation extraction.",
      "processing_time": 100.19110798835754,
      "citing_paper_id": "246823061",
      "cited_paper_id": 248779998
    },
    {
      "context_text": "Later, MMEKG [24] optimizes some modules and adapt to billion-scale universal events extraction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMEKG but does not specify a dataset. It appears to be a method or system rather than a dataset.",
      "processing_time": 57.68011665344238,
      "citing_paper_id": "246823061",
      "cited_paper_id": 248779998
    },
    {
      "context_text": "In MMEKG [24], the instance-level evaluation has a precision score of about 64% on visual events and cross-modal triples.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'instance-level evaluation' and 'visual events and cross-modal triples', which suggests the use of a specific dataset for evaluation, but no explicit dataset name is provided.",
      "processing_time": 59.75579237937927,
      "citing_paper_id": "246823061",
      "cited_paper_id": 248779998
    },
    {
      "context_text": "Markov random walk is also used for the optimal sequence in [106], [115], where [115] weights the values by Max-Min methods and [106] reassigns the visits values between nodes according to their source clusters by a two-layer graph model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of Markov random walks and graph models, which are methods rather than datasets.",
      "processing_time": 59.54389142990112,
      "citing_paper_id": "246823061",
      "cited_paper_id": 254054622
    },
    {
      "context_text": "For example, a word might be visualizable if the number of Google image hits is larger than that of Google web hits [106].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a general observation about Google image and web hits. No verifiable resource is identified.",
      "processing_time": 57.635263204574585,
      "citing_paper_id": "246823061",
      "cited_paper_id": 254054622
    },
    {
      "context_text": "In [106], [109], [110], tags are clustered based on semantic features and images are reassigned into each cluster according to their tags‚Äô semantic clusters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for clustering tags and reassigning images. No clear, verifiable dataset names are provided.",
      "processing_time": 58.40439796447754,
      "citing_paper_id": "246823061",
      "cited_paper_id": 254054622
    },
    {
      "context_text": "The experimental results show that the proposed method with MMKGs achieves higher mean average precision than the baselines without MMKGs. MMKGs help to generate more relevant candidate tags and are more capable of disambiguating them than ConceptNet, WebChild and ImageNet.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MMKGs, ConceptNet, WebChild, and ImageNet, but does not specify their usage as datasets. It compares the performance of MMKGs with these resources, indicating they are used as baselines.",
      "processing_time": 61.36662530899048,
      "citing_paper_id": "246823061",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "More and more knowledge graphs have been created, covering common sense knowledge (e.g., Cyc [1], ConceptNet [2]), lexical knowledge (e.g., WordNet [3], BabelNet [4]), encyclo-pedia knowledge (e.g., Freebase [5], DBpedia [6], YAGO [7], WikiData [8], CN-DBpedia [9]), taxonomic knowledge (e.g., Probase [10]) and geographic knowledge (e.g., GeoNames [11]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Cyc",
        "ConceptNet",
        "WordNet",
        "BabelNet",
        "Freebase",
        "DBpedia",
        "YAGO",
        "WikiData",
        "CN-DBpedia",
        "Probase",
        "GeoNames"
      ],
      "dataset_descriptions": {
        "Cyc": "Cyc is used as a source of common sense knowledge, providing a structured representation of everyday concepts and relationships.",
        "ConceptNet": "ConceptNet is utilized for commonsense reasoning, offering a large, multilingual knowledge graph of semantic relationships.",
        "WordNet": "WordNet serves as a lexical knowledge base, providing a structured vocabulary and semantic relations between words.",
        "BabelNet": "BabelNet is used for lexical knowledge, integrating WordNet and Wikipedia to provide a multilingual lexicalized semantic network.",
        "Freebase": "Freebase is employed as an encyclopedia knowledge graph, containing structured information from various sources.",
        "DBpedia": "DBpedia is used to extract structured information from Wikipedia, serving as a comprehensive knowledge base for encyclopedia knowledge.",
        "YAGO": "YAGO is utilized for encyclopedia knowledge, providing a large, multilingual knowledge graph derived from Wikipedia and other sources.",
        "WikiData": "WikiData is used as a collaborative, multilingual knowledge base, providing structured data for a wide range of topics.",
        "CN-DBpedia": "CN-DBpedia is employed for Chinese-specific encyclopedia knowledge, providing a structured representation of information from Chinese Wikipedia.",
        "Probase": "Probase is used for taxonomic knowledge, offering a large-scale taxonomy of concepts and their relationships.",
        "GeoNames": "GeoNames is utilized for geographic knowledge, providing a comprehensive database of place names and geographic features."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions several knowledge graphs, which are relevant to multi-modal knowledge graph reasoning. However, they are not explicitly used as datasets in the context provided.",
      "processing_time": 116.44709849357605,
      "citing_paper_id": "246823061",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "different kinds of knowledge for VQA, including both unimodal KGs such as DBpedia [6], ConceptNet [2] and hasPart KB [153] for categorical and commonsense knowledge,",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia",
        "ConceptNet",
        "hasPart KB"
      ],
      "dataset_descriptions": {
        "DBpedia": "Used to provide categorical knowledge for Visual Question Answering (VQA), enhancing the system's ability to understand and answer questions about images.",
        "ConceptNet": "Used to provide commonsense knowledge for VQA, enhancing the system's ability to reason about everyday concepts and situations.",
        "hasPart KB": "Used to provide part-whole relationships for VQA, enhancing the system's ability to understand compositional structures in images."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions unimodal KGs, which are knowledge bases, not datasets. However, they are specific, verifiable resources used for reasoning in VQA.",
      "processing_time": 77.87267184257507,
      "citing_paper_id": "246823061",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "Cyc [1], ConceptNet [2]), lexical knowledge (e.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific, verifiable datasets. It mentions Cyc and ConceptNet, which are knowledge bases or toolkits, not datasets.",
      "processing_time": 58.270026206970215,
      "citing_paper_id": "246823061",
      "cited_paper_id": 266028051
    },
    {
      "context_text": "‚Ä¶common sense knowledge (e.g., Cyc [1], ConceptNet [2]), lexical knowledge (e.g., WordNet [3], BabelNet [4]), encyclo-pedia knowledge (e.g., Freebase [5], DBpedia [6], YAGO [7], WikiData [8], CN-DBpedia [9]), taxonomic knowledge (e.g., Probase [10]) and geographic knowledge (e.g., GeoNames [11]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Cyc",
        "ConceptNet",
        "WordNet",
        "BabelNet",
        "Freebase",
        "DBpedia",
        "YAGO",
        "WikiData",
        "CN-DBpedia",
        "Probase",
        "GeoNames"
      ],
      "dataset_descriptions": {
        "Cyc": "Referenced as a common sense knowledge base, though its specific use in multi-modal reasoning is not detailed.",
        "ConceptNet": "Mentioned as a common sense knowledge base, but its application in the current research is not specified.",
        "WordNet": "Cited as a lexical knowledge base, without details on its integration into the research methodology.",
        "BabelNet": "Noted as a lexical knowledge base, but its role in the research is not elaborated.",
        "Freebase": "Identified as an encyclopedia knowledge base, though its specific contribution to the research is unclear.",
        "DBpedia": "Listed as an encyclopedia knowledge base, but its use in the research is not described.",
        "YAGO": "Cited as an encyclopedia knowledge base, with no specific mention of its application in the study.",
        "WikiData": "Mentioned as an encyclopedia knowledge base, but its role in the research is not detailed.",
        "CN-DBpedia": "Referenced as an encyclopedia knowledge base, though its specific use in the research is not specified.",
        "Probase": "Noted as a taxonomic knowledge base, but its application in the research is not described.",
        "GeoNames": "Identified as a geographic knowledge base, though its specific contribution to the research is not detailed."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several knowledge bases but does not specify their use in a multi-modal reasoning context. These are general references to types of knowledge bases.",
      "processing_time": 114.02296948432922,
      "citing_paper_id": "246823061",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, in [138] when we want to know the relations between geoscience academic papers and maps in them, the papers are Ô¨Årst-class citizens; when we want to know the relations between maps and regions pointed in these maps, the maps are Ô¨Årst-class citizens.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the concept of first-class citizens in a knowledge graph but does not reference any named datasets.",
      "processing_time": 57.91603374481201,
      "citing_paper_id": "246823061",
      "cited_paper_id": null
    },
    {
      "context_text": "In this paper, we obtain the raw triplet facts from DBpedia 1 (Lehmann et al. 2015).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia"
      ],
      "dataset_descriptions": {
        "DBpedia": "Used to extract raw triplet facts for constructing a knowledge graph, focusing on the integration of multi-modal data and reasoning over the extracted facts."
      },
      "confidence_score": 1.0,
      "reasoning": "DBpedia is a well-known knowledge base extracted from Wikipedia, which fits the criteria for a dataset. The context indicates it is used as a source of raw triplet facts.",
      "processing_time": 65.74219799041748,
      "citing_paper_id": "268697475",
      "cited_paper_id": 1181640
    },
    {
      "context_text": "That is, where To obtain x e text , we collect the first sentence (often the entity definition) of the Wikipedia entry of the entity e .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for collecting textual data from Wikipedia entries.",
      "processing_time": 57.264601707458496,
      "citing_paper_id": "268697475",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "The first is to obtain entity images from online encyclopedias like Wikipedia (Ferrada, Bustos, and Hogan 2017; Wang et al. 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions obtaining entity images from online encyclopedias like Wikipedia, which suggests the use of a dataset derived from Wikipedia. However, the specific dataset name is not provided in the context.",
      "processing_time": 60.10908627510071,
      "citing_paper_id": "268697475",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "MMKGs along this line include IMGpedia (Ferrada, Bustos, and Hogan 2017), Richpedia (Wang et al. 2020), and Visu-alSem (Alberts et al. 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used to create a linked dataset with content-based analysis of Wikimedia images, enhancing multi-modal knowledge graph reasoning through image-text integration."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions IMGpedia, Richpedia, and VisualSem as examples of multi-modal knowledge graphs (MMKGs). IMGpedia is a linked dataset with content-based analysis of Wikimedia images, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 68.65927004814148,
      "citing_paper_id": "268697475",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "(Chao et al. 2015) and HICO-DET (Chao et al. 2018) rely on manual labeling for the object bounding boxes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HICO-DET"
      ],
      "dataset_descriptions": {
        "HICO-DET": "Used to detect human-object interactions, focusing on manual labeling of object bounding boxes for training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions HICO-DET, which is a dataset used for detecting human-object interactions. The context indicates that the dataset relies on manual labeling for object bounding boxes.",
      "processing_time": 65.38839149475098,
      "citing_paper_id": "268697475",
      "cited_paper_id": 3608725
    },
    {
      "context_text": "In this paper, we define ‚â§ Eps } as the Eps -neighborhood of the image p ‚Ä≤ 1 , where P ‚Ä≤ is the remaining images for a given triplet fact and dist denotes the cosine similarity between the vectors of p ‚Ä≤ 1 and p ‚Ä≤ 2 obtained by the VGG al-gorithm (Simonyan and Zisserman 2014).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a method (VGG algorithm) and a concept (Eps-neighborhood) but does not cite a dataset.",
      "processing_time": 60.050631046295166,
      "citing_paper_id": "268697475",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "In addition, there are many datasets with images that embody both action and spatial relations, such as Scene Graph (Johnson et al. 2015) and VrR-VG (Liang et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Scene Graph",
        "VrR-VG"
      ],
      "dataset_descriptions": {
        "Scene Graph": "Used to retrieve images based on scene graphs, focusing on action and spatial relations within images.",
        "VrR-VG": "Used to refocus visually-relevant relationships in images, enhancing the representation of spatial and relational information."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Scene Graph' and 'VrR-VG', which are relevant to multi-modal knowledge graph reasoning involving images and spatial relations.",
      "processing_time": 70.39381146430969,
      "citing_paper_id": "268697475",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "In addition, there are many datasets with images that embody both action and spatial relations, such as Scene Graph (Johnson et al. 2015) and VrR-VG (Liang et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Scene Graph",
        "VrR-VG"
      ],
      "dataset_descriptions": {
        "Scene Graph": "Used to retrieve images based on scene graphs, focusing on action and spatial relations within images.",
        "VrR-VG": "Used to refocus visually-relevant relationships in images, enhancing the representation of spatial and relational information."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Scene Graph' and 'VrR-VG', which are relevant to multi-modal knowledge graph reasoning involving images and spatial relations.",
      "processing_time": 70.39381146430969,
      "citing_paper_id": "268697475",
      "cited_paper_id": 201881176
    },
    {
      "context_text": "For spatial relations, datasets such as SpatialSense (Yang, Rus-sakovsky, and Deng 2019) and SpatialVOC2K (Belz et al. 2018) are created using manual-based methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SpatialSense",
        "SpatialVOC2K"
      ],
      "dataset_descriptions": {
        "SpatialSense": "Used to study spatial relations between objects, focusing on manual annotations to understand human perception and reasoning about spatial configurations.",
        "SpatialVOC2K": "Applied to analyze multilingual spatial relations in images, emphasizing manual annotations and features to explore cross-linguistic variations in spatial understanding."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, SpatialSense and SpatialVOC2K, which are used for studying spatial relations between objects. Both datasets are created using manual-based methods.",
      "processing_time": 71.64964151382446,
      "citing_paper_id": "268697475",
      "cited_paper_id": 53235839
    },
    {
      "context_text": "The second involves using a Web search engine like Google and designing an entity-image matching model to select high-quality images from the returned results (O Àú noro-Rubio et al. 2017; Liu et al. 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method for selecting images using a web search engine and an entity-image matching model.",
      "processing_time": 58.298900842666626,
      "citing_paper_id": "268697475",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "‚Ä¶the input of experiment A 2 is h and r , while there are five kinds of inputs for experiment B 2 : 1) h , r , and the image p ‚Ä≤ h of h from MMKG (Liu et al. 2019), 2) h , r , and the image p ‚àó r 2 of ( h 1 , r, t 1 ) generated by VisualChatGPT (VCG), where h 1 Ã∏ = h and t 1 Ã∏ = t , 3) h , r ,‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMKG"
      ],
      "dataset_descriptions": {
        "MMKG": "Used to provide images for entities in multi-modal reasoning experiments, specifically integrating visual information into knowledge graph relations."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'MMKG' as a source of images for the experiments, which is a multi-modal knowledge graph. The cited paper title confirms it is a dataset.",
      "processing_time": 65.27020740509033,
      "citing_paper_id": "268697475",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "In this paper, we aim to construct a new large-scale MMKG by grounding triplet facts on images, where the images re-Method (Liu et al. 2019), VisualChatGPT (Wu et al. 2023a) and our ImgFact, respectively. flect not only entities but also their relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models, which are excluded according to the instructions.",
      "processing_time": 57.557310581207275,
      "citing_paper_id": "268697475",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Notably, the parameters of BERT, ResNet50, and ViLT are fixed and only classifier parameters are updated.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their parameters. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.55389332771301,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "We notice that both BERT+ResNet50 and ViLT with p outperform BERT, showing that the image encoded by the encoders is helpful for the task.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.834229469299316,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "In experiment B 1 , we employ (BERT+ResNet50)- and ViLT-based (Kim, Son, and Kim 2021) classifiers to predict t , respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of BERT+ResNet50 and ViLT-based classifiers for prediction tasks.",
      "processing_time": 59.381131410598755,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Then, we encode each image p i with ResNet (He et al. 2016) to obtain its representation p i .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only the use of ResNet for encoding images. No verifiable dataset is referenced.",
      "processing_time": 57.06861352920532,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "We observe that BERT outperforms both BERT+ResNet50 and ViLT with p noise , but it still un-derperforms the methods with p .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.22306442260742,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Specifically, the (BERT+ResNet50)-based classifier incorporating p t achieves an impressive 8.38% and 9.87% improvement over the classifier with p ‚Ä≤ t and p ‚àó t on the F1, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and performance metrics. The context focuses on the improvement of a classifier using BERT and ResNet50, which are models, not datasets.",
      "processing_time": 60.000710010528564,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "In experiments, we employ a BERT-based classifier for A 2 , while we utilize (BERT+ResNet50)- and ViLT-based classifiers for B 2 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.14960026741028,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Similar to link prediction, we employ BERT-, (BERT+ResNet50)-, and ViLT-based classifiers, where the input is ‚Äú h and t ‚Äù and the output is a relation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the methodology rather than the data used.",
      "processing_time": 57.28503203392029,
      "citing_paper_id": "268697475",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Similar to (Li and Liang 2021), we add several virtual tokens in front of the entity (pair) as the language prompts.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving virtual tokens and language prompts.",
      "processing_time": 55.52992248535156,
      "citing_paper_id": "268697475",
      "cited_paper_id": 230433941
    },
    {
      "context_text": "To compute the similarities, we employ a language-image pre-trained model named CLIP (Radford et al. 2021) as the matching model.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using CLIP, which is a model, not a dataset. No specific dataset is referenced in the context.",
      "processing_time": 56.94849514961243,
      "citing_paper_id": "268697475",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Specifically, we encode the entity (pair) and image with CLIP and output their similarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CLIP) for encoding entities and images. No verifiable datasets are referenced.",
      "processing_time": 57.29322147369385,
      "citing_paper_id": "268697475",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In this step, we employ a language-image pre-trained model (e.g., CLIP (Radford et al. 2021)) enhanced by language prompts to compute the similarity scores.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (CLIP) but does not refer to any specific dataset. The citation is about using a pre-trained model for computing similarity scores, not about a dataset.",
      "processing_time": 58.97022891044617,
      "citing_paper_id": "268697475",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "To keep a balance be-tween effectiveness and efficiency, we freeze the parameters of the CLIP model and only train the prompts.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the CLIP model, which is a method/model and not a dataset.",
      "processing_time": 56.2879536151886,
      "citing_paper_id": "268697475",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "The prompts with the entity (pair) are considered as the input of the text encoder in CLIP: T ( h,t ) = [ g 1 , ..., g a , h, and, t ] , where [ q 1 , ..., q a ] and [ g 1 , ..., g a ] are the prompts for the head-/tail entity and entity pair respectively, and a = 8 is the number of virtual tokens in prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the input format for a text encoder in CLIP, which is a method, not a dataset.",
      "processing_time": 58.13395404815674,
      "citing_paper_id": "268697475",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "This error may be attributed to that not all entities mentioned in the pre-training data (i.e., image-text pairs) of the CLIP model are explicitly depicted in images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to pre-training data of the CLIP model, which is a model, not a dataset.",
      "processing_time": 56.58624243736267,
      "citing_paper_id": "268697475",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Furthermore, designing triplet-image matching methods is challenging due to the difficulty faced by current language-image models in capturing the deep semantics of relations (e.g., wedding , pregnant , and family portrait for the relation Spouse ) (Zheng et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses challenges in designing triplet-image matching methods and mentions a specific relation 'Spouse' but does not explicitly mention any datasets. The cited paper title suggests a dataset, but it is not mentioned in the citation context.",
      "processing_time": 59.87433648109436,
      "citing_paper_id": "268697475",
      "cited_paper_id": 236273668
    },
    {
      "context_text": "However, the current focus of MMKG-related work mainly focuses on grounding entities to images (Wu et al. 2023b), without considering the visual semantics of relations, thus limiting their effectiveness in downstream tasks.",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation in the current work on MMKG. No verifiable resources are identified.",
      "processing_time": 56.15129828453064,
      "citing_paper_id": "268697475",
      "cited_paper_id": 265051720
    },
    {
      "context_text": "This is facilitated by a diagonal weight matrix [57] ùëä ùëî ‚àà R ùëë √ó ùëë for linear transformation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving a diagonal weight matrix for linear transformation.",
      "processing_time": 54.71947717666626,
      "citing_paper_id": "275694460",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "Early efforts, like IKRL [55], utilize multiple TransE-based scoring functions [2] for modal interaction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of TransE-based scoring functions for modal interaction, which is a methodological approach.",
      "processing_time": 58.243064165115356,
      "citing_paper_id": "275694460",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Early efforts, like IKRL [55], utilize multiple TransE-based scoring functions [2] for modal interaction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of TransE-based scoring functions for modal interaction, which is a methodological approach.",
      "processing_time": 58.243064165115356,
      "citing_paper_id": "275694460",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "‚Ä¶most research in MKGC focuses on Entity Prediction, also widely recognized as Link Prediction, with two main methods emerging: Embedding-based Approaches build on conventional Knowledge Graph Embedding (KGE) methods [2, 42], adapted to integrate multi-modal data, enhancing entity embed-dings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on embedding-based methods for multi-modal knowledge graph completion.",
      "processing_time": 56.14788484573364,
      "citing_paper_id": "275694460",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "‚Ä¶most research in MKGC focuses on Entity Prediction, also widely recognized as Link Prediction, with two main methods emerging: Embedding-based Approaches build on conventional Knowledge Graph Embedding (KGE) methods [2, 42], adapted to integrate multi-modal data, enhancing entity embed-dings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on embedding-based methods for multi-modal knowledge graph completion.",
      "processing_time": 56.14788484573364,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "MMRNS [56] enhances this with relation-based sample selection. exploit the world understanding capabilities of pre-trained Trans-former models like BERT [14] and VisualBERT [23] for MKGC.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and VisualBERT but does not indicate their use as datasets. They are referenced as pre-trained models, which are excluded according to the rules.",
      "processing_time": 57.14196300506592,
      "citing_paper_id": "275694460",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "MMRNS [56] enhances this with relation-based sample selection. exploit the world understanding capabilities of pre-trained Trans-former models like BERT [14] and VisualBERT [23] for MKGC.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and VisualBERT but does not indicate their use as datasets. They are referenced as pre-trained models, which are excluded according to the rules.",
      "processing_time": 57.14196300506592,
      "citing_paper_id": "275694460",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "Regarding surface embeddings, we leverage Sentence-BERT [38], a pre-trained textual encoder, to derive textual features from each entity‚Äôs description.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Sentence-BERT as a pre-trained textual encoder used to derive textual features from entity descriptions, but it does not refer to a specific dataset.",
      "processing_time": 56.442322731018066,
      "citing_paper_id": "275694460",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "For MMEA, following Yang et al. [58], we use bag-of-words features for relation ( ùë• ) and attribute ( ùë• ) representations of entities (detailed in ¬ß4.1.3) .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for using bag-of-words features. The cited paper title does not provide additional context to identify a dataset.",
      "processing_time": 57.01059556007385,
      "citing_paper_id": "275694460",
      "cited_paper_id": 202121966
    },
    {
      "context_text": "Multi-Modal Entity Alignment (MMEA) enhances this process by incorporating visual data, thereby improving alignment accuracy accuracy [4, 33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called MMEA. The context focuses on the method's capability to enhance entity alignment using visual data.",
      "processing_time": 56.87944412231445,
      "citing_paper_id": "275694460",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "EVA [32] applies an attention mechanism to modulate the importance of each modality and introduces an unsupervised approach that utilizes visual similarities for alignment, reducing reliance on gold-standard labels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EVA) and its approach to entity alignment using visual similarities.",
      "processing_time": 55.454065561294556,
      "citing_paper_id": "275694460",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "RSME [50] employs gates for selective modal information integration.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gates) for integrating modal information. The context is too limited to infer any dataset usage.",
      "processing_time": 56.35382795333862,
      "citing_paper_id": "275694460",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "OurstudyforMKGC, consistent with the domain practices [7, 25, 50, 53, 64], focuses exclusively on relation triples.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a focus on relation triples in the context of multi-modal knowledge graph construction.",
      "processing_time": 55.105666160583496,
      "citing_paper_id": "275694460",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "OurstudyforMKGC, consistent with the domain practices [7, 25, 50, 53, 64], focuses exclusively on relation triples.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a focus on relation triples in the context of multi-modal knowledge graph construction.",
      "processing_time": 55.105666160583496,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "(i) Modality Fusion methods [20, 21, 30, 49, 54] integrate multi-modal and structural embeddings to assess triple plausibility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for integrating multi-modal and structural embeddings.",
      "processing_time": 53.97018337249756,
      "citing_paper_id": "275694460",
      "cited_paper_id": 245904709
    },
    {
      "context_text": "(i) Modality Fusion methods [20, 21, 30, 49, 54] integrate multi-modal and structural embeddings to assess triple plausibility.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for integrating multi-modal and structural embeddings.",
      "processing_time": 53.97018337249756,
      "citing_paper_id": "275694460",
      "cited_paper_id": 251117920
    },
    {
      "context_text": "For example, MKGRL-MS [49] crafts distinct single-modal embeddings, using multi-head self-attention to evaluate the contribution of each modality to the semantic composition and summing the weighted multi-modal features for MMKG entity representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MKGRL-MS' which appears to be a method or model rather than a dataset. No specific dataset is mentioned or used in the described research context.",
      "processing_time": 56.91140818595886,
      "citing_paper_id": "275694460",
      "cited_paper_id": 251117920
    },
    {
      "context_text": "MSNEA [5] leverages visual cues to guide relational feature learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MSNEA) that leverages visual cues for relational feature learning.",
      "processing_time": 55.15064525604248,
      "citing_paper_id": "275694460",
      "cited_paper_id": 251518434
    },
    {
      "context_text": "DuMF [27] adopts a dual-track strategy, utilizing a bilinear layer for feature projection and an attention block for modality preference learning in each track, with a gate network to synthesize these features into a unified representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DuMF) and its components. The context focuses on the model architecture and its functionality.",
      "processing_time": 56.121875047683716,
      "citing_paper_id": "275694460",
      "cited_paper_id": 251684399
    },
    {
      "context_text": "WithinMKGC,weconsidertwometh-ods for entity representation as candidates: (i) ¬Ø ‚Ñé ùëî : Reflecting insights from previous research [8, 61], graph structure embedding emerges as crucial for model performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on entity representation methods and their importance in model performance.",
      "processing_time": 55.31536078453064,
      "citing_paper_id": "275694460",
      "cited_paper_id": 255340818
    },
    {
      "context_text": "In cases where entities lack corresponding image data, we synthesize random image features adhering to a normal distribution, parameterized by the mean and standard deviation observed across other entities‚Äô images [8, 9, 61].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for synthesizing random image features when image data is missing.",
      "processing_time": 55.19478678703308,
      "citing_paper_id": "275694460",
      "cited_paper_id": 255340818
    },
    {
      "context_text": "In cases where entities lack corresponding image data, we synthesize random image features adhering to a normal distribution, parameterized by the mean and standard deviation observed across other entities‚Äô images [8, 9, 61].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for synthesizing random image features when image data is missing.",
      "processing_time": 55.19478678703308,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "MEAformer [8] adjusts mutual modality preferences dynamically for entity-level modality fusion, addressing inconsistencies in entities‚Äô surrounding modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MEAformer) and its functionality. There are no clear identifiers for datasets in the citation context.",
      "processing_time": 56.22962284088135,
      "citing_paper_id": "275694460",
      "cited_paper_id": 255340818
    },
    {
      "context_text": "For MMEA, we follow [8, 9] to utilize the Graph Attention Network (GAT) [47], configured with two attention heads and two layers, to capture the structural information of G .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on the use of Graph Attention Network (GAT) for capturing structural information, which is not a dataset.",
      "processing_time": 57.419437646865845,
      "citing_paper_id": "275694460",
      "cited_paper_id": 255340818
    },
    {
      "context_text": "For MMEA, we follow [8, 9] to utilize the Graph Attention Network (GAT) [47], configured with two attention heads and two layers, to capture the structural information of G .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on the use of Graph Attention Network (GAT) for capturing structural information, which is not a dataset.",
      "processing_time": 57.419437646865845,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "In MMEA, following [8, 9], we adopt the Global Modality Integration (GMI) derived multi-modal features as the representations for entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Global Modality Integration) for deriving multi-modal features. No verifiable datasets are referenced.",
      "processing_time": 55.669910192489624,
      "citing_paper_id": "275694460",
      "cited_paper_id": 255340818
    },
    {
      "context_text": "In MMEA, following [8, 9], we adopt the Global Modality Integration (GMI) derived multi-modal features as the representations for entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Global Modality Integration) for deriving multi-modal features. No verifiable datasets are referenced.",
      "processing_time": 55.669910192489624,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "Similarly, Chen et al. [10] demonstrate that cross-modal masking and reconstruction can improve a model‚Äôs cross-modal alignment capabilities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on cross-modal masking and reconstruction, which are techniques rather than datasets.",
      "processing_time": 56.23644542694092,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "Similarly, Chen et al. [10] demonstrate that cross-modal masking and reconstruction can improve a model‚Äôs cross-modal alignment capabilities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on cross-modal masking and reconstruction, which are techniques rather than datasets.",
      "processing_time": 56.23644542694092,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "To integrate dynamic confidences into the training process and enhance multi-modal entity alignment, we adopt two specialized training objectives from UMAEA [9]: (ii) Explicit Confidence-augmented Intra-modal Alignment (ECIA): This objective modifies Equation (16) to incorporate explicit confidence‚Ä¶",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only specialized training objectives from a method called UMAEA. The context focuses on the methodology rather than the use of a dataset.",
      "processing_time": 56.60129499435425,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "Recent research in MMKG [9, 18, 61] suggests that models can maintain a certain level of noise without a noticeable decline in the expressive capability of multi-modal entity representations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general findings about multi-modal entity representations.",
      "processing_time": 53.474146604537964,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "As demonstrated by Chen et al. [9], the modality feature is often smoothed by the Transformer Layer in MMEA, potentially reducing entity distinction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMEA) and a potential issue with the Transformer Layer. No verifiable resources are identified.",
      "processing_time": 56.227744579315186,
      "citing_paper_id": "275694460",
      "cited_paper_id": 260334664
    },
    {
      "context_text": "On the other hand, Graph-level methods [17, 24] capitalize on the structural connections among entities in a global MMKG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only graph-level methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 54.806321144104004,
      "citing_paper_id": "275694460",
      "cited_paper_id": 262464639
    },
    {
      "context_text": "On the other hand, Graph-level methods [17, 24] capitalize on the structural connections among entities in a global MMKG.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only graph-level methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 54.806321144104004,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "Such advancements have the potential to make significant contributions to the community, especially with the rapid development of Large Language Models [60, 62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to advancements and Large Language Models. No verifiable resources are identified.",
      "processing_time": 54.8030047416687,
      "citing_paper_id": "275694460",
      "cited_paper_id": 265150374
    },
    {
      "context_text": "We apply the RotatE model [42] as our score function to assess the plausibility of triples.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (RotatE). The context is focused on the methodology rather than the data.",
      "processing_time": 55.43091082572937,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "The choice of dimensionality ùëë / 2 is informed by our use of the RotatE model [42] as the scoring function for assessing triple plausibility.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (RotatE) which is excluded according to the instructions.",
      "processing_time": 53.76431703567505,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "RotatE models relations as rotations in a complex space, requiring the relation embedding‚Äôs dimension to be half that of the entity embedding to account for the real and imaginary components of complex numbers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RotatE) and its requirements. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.82273864746094,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "To prioritize positive triples with higher scores, we optimize the embeddings using a sigmoid-based loss function [42].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimizing embeddings.",
      "processing_time": 51.90443181991577,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, Cuconasu et al. [12] observe that in the Retrieval-Augmented Generation (RAG) process of LLMs, filling up the retrieved context with irrelevant documents consistently improves model performance in realistic scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Retrieval-Augmented Generation) and its performance implications.",
      "processing_time": 54.61016821861267,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, Cuconasu et al. [12] observe that in the Retrieval-Augmented Generation (RAG) process of LLMs, filling up the retrieved context with irrelevant documents consistently improves model performance in realistic scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Retrieval-Augmented Generation) and its performance implications.",
      "processing_time": 54.61016821861267,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "As the field evolves, there is a promising avenue for integrating this unified representation into multi-modal knowledge pre-training, which could facilitate diverse downstream tasks, including but not limited to Multi-modal Knowledge Injection and Multi-modal Retrieval-Augmented Generation (RAG).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and potential applications. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.81844186782837,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "As the field evolves, there is a promising avenue for integrating this unified representation into multi-modal knowledge pre-training, which could facilitate diverse downstream tasks, including but not limited to Multi-modal Knowledge Injection and Multi-modal Retrieval-Augmented Generation (RAG).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and potential applications. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.81844186782837,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "MMRotatH [53] utilizes a gated encoder to merge textual and structural data, filtering irrelevant information within a rotational dynamics-based KGE framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMRotatH) and its components. No verifiable resources are identified.",
      "processing_time": 54.81486463546753,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "Triple-level methods [36] treat triples as standalone knowledge units, embedding the (head entity, relationship, tail entity) structure ‚Ä† Corresponding author. into Visual Language Model‚Äôs space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no clear identifiers for datasets in the text.",
      "processing_time": 54.84863352775574,
      "citing_paper_id": "275694460",
      "cited_paper_id": null
    },
    {
      "context_text": "Thus, we used MPEG [14] compression to reduce the data size signiÔ¨Åcantly.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MPEG compression but does not refer to it as a dataset, method, or reusable resource. It is used as a technique for reducing data size.",
      "processing_time": 55.84952449798584,
      "citing_paper_id": "271962901",
      "cited_paper_id": 652286
    },
    {
      "context_text": "Typical MMKGs [10, 27] are knowledge graphs (KGs) in which images are grounded to entities in the graph.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of multi-modal knowledge graphs (MMKGs). No verifiable resource names are provided.",
      "processing_time": 56.01777958869934,
      "citing_paper_id": "271962901",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "IMGpedia [10] is an MMKG that grounds Wikimedia Commons images into DBpedia [2] entities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used to ground Wikimedia Commons images into DBpedia entities, focusing on content-based analysis and linking images to structured data."
      },
      "confidence_score": 1.0,
      "reasoning": "IMGpedia is identified as a multi-modal knowledge graph that links images to DBpedia entities, which is directly relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 62.028717279434204,
      "citing_paper_id": "271962901",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "We employed BLEU [20], ROUGE-1 [15], and METEOR [3] as evaluation metrics to calculate the similarity between the short sentences output and the correct answers.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics but does not refer to any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 53.90145945549011,
      "citing_paper_id": "271962901",
      "cited_paper_id": 7164502
    },
    {
      "context_text": "Vi-sionKG [29] is an MMKG containing bounding boxes of objects extracted from various image datasets such as MS-COCO [16], CI-FAR [13], and PASCAL VOC [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO",
        "CI-FAR",
        "PASCAL VOC"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object detection and annotation in diverse images.",
        "CI-FAR": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on small-scale image recognition and object localization.",
        "PASCAL VOC": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object classification and segmentation in visual scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to create Vi-sionKG, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 74.14226365089417,
      "citing_paper_id": "271962901",
      "cited_paper_id": 18268744
    },
    {
      "context_text": "Vi-sionKG [29] is an MMKG containing bounding boxes of objects extracted from various image datasets such as MS-COCO [16], CI-FAR [13], and PASCAL VOC [9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO",
        "CI-FAR",
        "PASCAL VOC"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object detection and annotation in diverse images.",
        "CI-FAR": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on small-scale image recognition and object localization.",
        "PASCAL VOC": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object classification and segmentation in visual scenes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to create Vi-sionKG, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 74.14226365089417,
      "citing_paper_id": "271962901",
      "cited_paper_id": 262466164
    },
    {
      "context_text": "Use remote to turn on the TV. VirtualHome2KG structures the environmental information out-putby VirtualHomebasedon an ontologytoconstructevent-centric KGs (EKGs [11]), where nodes are events and entities, while edges are event-event relations, event-entity relations, and entity-entity relations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the construction of event-centric knowledge graphs using an ontology, but does not reference a named dataset.",
      "processing_time": 56.26181077957153,
      "citing_paper_id": "271962901",
      "cited_paper_id": 245634781
    },
    {
      "context_text": "In the future, we plantogenerate multi-agent and egocentric videosand link VHAKG to real visual dataset KGs [28, 29] for sim2real tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions linking VHAKG to real visual dataset KGs for sim2real tasks, but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 57.05799746513367,
      "citing_paper_id": "271962901",
      "cited_paper_id": 262466164
    },
    {
      "context_text": "In the future, we plantogenerate multi-agent and egocentric videosand link VHAKG to real visual dataset KGs [28, 29] for sim2real tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions linking VHAKG to real visual dataset KGs for sim2real tasks, but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 57.05799746513367,
      "citing_paper_id": "271962901",
      "cited_paper_id": null
    },
    {
      "context_text": "The GUI displays the speciÔ¨Åed videos and images and is provided as Docker Compose [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a GUI for displaying videos and images.",
      "processing_time": 52.92793846130371,
      "citing_paper_id": "271962901",
      "cited_paper_id": null
    },
    {
      "context_text": "In our previous work [8], we developed VirtualHome2KG, a framework for constructing KGs from Ô¨Åne-grained event data generated by Virtual-Home [21] simulator.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Virtual-Home' as a simulator used to generate fine-grained event data for constructing KGs, but it does not specify a dataset. The simulator itself is not a dataset.",
      "processing_time": 56.901814460754395,
      "citing_paper_id": "271962901",
      "cited_paper_id": null
    },
    {
      "context_text": "We manually created over 700 various daily activity scenarios (i.e., program data described in Section 2) with reference to the existing dataset [21] and simulated them using VirtualHome-AIST.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'existing dataset' but does not provide a specific name. The reference to 'VirtualHome-AIST' is a tool or environment, not a dataset.",
      "processing_time": 55.75223898887634,
      "citing_paper_id": "271962901",
      "cited_paper_id": null
    },
    {
      "context_text": "We integrate these data and transform them into a KG in RDF format based on the designed schema using Python scripts with RDFLib [25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to transforming data into a KG, which is too generic.",
      "processing_time": 54.006534576416016,
      "citing_paper_id": "271962901",
      "cited_paper_id": null
    },
    {
      "context_text": "In order to complete the missing triplet problem in knowledge graph, the link prediction problem is studied [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem in knowledge graphs.",
      "processing_time": 52.963395833969116,
      "citing_paper_id": "274705711",
      "cited_paper_id": 12161567
    },
    {
      "context_text": "ÔÇ∑ MKGC [20]:Multi-modal Knowledge Graph Completion (MKGC) model is a multi-modal embedding method that focuses on completing the knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method called MKGC. The context is about the model's functionality and not about a dataset.",
      "processing_time": 56.14539670944214,
      "citing_paper_id": "274705711",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "ÔÇ∑ MKBE [17]: MKBE (Multimodal Knowledge Base Embedding) model is a knowledge graph embedding method used to process multi-modal data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MKBE as a model, not a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 54.18318319320679,
      "citing_paper_id": "274705711",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "‚Ä¶Inferences endows the model with the ability to identify causal relationships from observational data [1-3], an increasing number of studies [4,23,24,25] have shown that causal inference is beneficial to various machine learning tasks.Causal inference for GNNs is an emerging field of‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and fields. There are no clear identifiers for datasets, and the context is focused on the benefits of causal inference in machine learning tasks.",
      "processing_time": 57.54920816421509,
      "citing_paper_id": "274705711",
      "cited_paper_id": 219530451
    },
    {
      "context_text": "Graph Data Augmentation refers to the process of applying various techniques to graph-structured data to generate new graph data or enhance the generalization ability of graph models[5, 6, 7, 8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general concept of graph data augmentation. No verifiable resources are identified.",
      "processing_time": 54.9851975440979,
      "citing_paper_id": "274705711",
      "cited_paper_id": 221655683
    },
    {
      "context_text": "‚Ä¶Inferences endows the model with the ability to identify causal relationships from observational data [1-3], an increasing number of studies [4,23,24,25] have shown that causal inference is beneficial to various machine learning tasks.Causal inference for GNNs is an emerging field of research,‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and fields of research. There are no clear identifiers for datasets, and the context is too vague to infer specific resource usage.",
      "processing_time": 57.348061323165894,
      "citing_paper_id": "274705711",
      "cited_paper_id": 221970271
    },
    {
      "context_text": "CGI [27] explores how to select trustworthy neighbors for GNN in the inference stage, and demonstrates its effectiveness in node classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for selecting trustworthy neighbors in GNNs.",
      "processing_time": 53.968623638153076,
      "citing_paper_id": "274705711",
      "cited_paper_id": 225040610
    },
    {
      "context_text": "‚Ä¶unlabeled learning method with adversarial data enhancement (PUDA), which is used to solve the false negative problem and data sparsity problem in KGC.Cashman et al. [10] proposed Metapath-Based data-augmentation for Knowledge Graphs to enhance triples to solve the problem of sparse Data method .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Metapath-Based data-augmentation for Knowledge Graphs' as a method to enhance triples in knowledge graphs, but does not refer to a specific dataset.",
      "processing_time": 56.5206458568573,
      "citing_paper_id": "274705711",
      "cited_paper_id": 260124505
    },
    {
      "context_text": "The applications of KGs are diverse, including question answering [10, 11, 37], recommender systems [51, 53] H @ 1 FB15K-YAGO15K MMEAMCLEA Figure 1: The percentage of seed entities, or pre-aligned entities, is important to the performance of the entity alignment on multi-modal knowledge graph.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15K-YAGO15K": "Used to evaluate the performance of entity alignment on multi-modal knowledge graphs, focusing on the impact of seed entities or pre-aligned entities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'FB15K-YAGO15K' which appears to be a dataset used in the context of multi-modal knowledge graph reasoning. No other specific datasets are mentioned.",
      "processing_time": 65.18933010101318,
      "citing_paper_id": "271961581",
      "cited_paper_id": 2887257
    },
    {
      "context_text": "The applications of KGs are diverse, including question answering [10, 11, 37], recommender systems [51, 53] H @ 1 FB15K-YAGO15K MMEAMCLEA Figure 1: The percentage of seed entities, or pre-aligned entities, is important to the performance of the entity alignment on multi-modal knowledge graph.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15K-YAGO15K": "Used to evaluate the performance of entity alignment on multi-modal knowledge graphs, focusing on the impact of seed entities or pre-aligned entities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'FB15K-YAGO15K' which appears to be a dataset used in the context of multi-modal knowledge graph reasoning. No other specific datasets are mentioned.",
      "processing_time": 65.18933010101318,
      "citing_paper_id": "271961581",
      "cited_paper_id": 30164212
    },
    {
      "context_text": "The generated data can potentially enhance model performance in the language [23, 58] and image domains [21, 48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generated data, which is too generic and lacks a clear identifier.",
      "processing_time": 55.33419370651245,
      "citing_paper_id": "271961581",
      "cited_paper_id": 4929980
    },
    {
      "context_text": "The generated data can potentially enhance model performance in the language [23, 58] and image domains [21, 48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generated data, which is too generic and lacks a clear identifier.",
      "processing_time": 55.33419370651245,
      "citing_paper_id": "271961581",
      "cited_paper_id": 252907242
    },
    {
      "context_text": "We use ResNet-152 [20] or VGG-16 [42], pre-trained visual models that has been trained on ImageNet [12], to learn image embeddings for entities adopted by the previous MMKG models [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to pre-train ResNet-152 and VGG-16 models for learning image embeddings in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a dataset used for pre-training visual models. No other datasets are mentioned.",
      "processing_time": 60.34837794303894,
      "citing_paper_id": "271961581",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "We use ResNet-152 [20] or VGG-16 [42], pre-trained visual models that has been trained on ImageNet [12], to learn image embeddings for entities adopted by the previous MMKG models [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to pre-train ResNet-152 and VGG-16 models for learning image embeddings in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a dataset used for pre-training visual models. No other datasets are mentioned.",
      "processing_time": 60.34837794303894,
      "citing_paper_id": "271961581",
      "cited_paper_id": null
    },
    {
      "context_text": "A Multi-Modal Knowledge Graph (MMKG) is an extension of a traditional knowledge graph that incorporates multiple modalities of information, such as text, images, and audio, which can provide rich and complementary information for entity representation and matching [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of Multi-Modal Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 54.359210729599,
      "citing_paper_id": "271961581",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "For cross-KG datasets, we use 20%, 50%, and 80% of seed entities as in [30].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions using percentages of seed entities from cross-KG datasets, but does not specify the name of the dataset. The context is too generic to identify a specific dataset.",
      "processing_time": 57.08382725715637,
      "citing_paper_id": "271961581",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "However, the current datasets of MMKGs, which have around 10K entities [30], are relatively small compared with the size of those in the image [15] and language domains [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'datasets of MMKGs' but does not provide specific names. It compares the size of MMKG datasets to those in image and language domains, but no specific datasets are named.",
      "processing_time": 57.84856081008911,
      "citing_paper_id": "271961581",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "However, the current datasets of MMKGs, which have around 10K entities [30], are relatively small compared with the size of those in the image [15] and language domains [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'datasets of MMKGs' but does not provide specific names. It compares the size of MMKG datasets to those in image and language domains, but no specific datasets are named.",
      "processing_time": 57.84856081008911,
      "citing_paper_id": "271961581",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "However, the current datasets of MMKGs, which have around 10K entities [30], are relatively small compared with the size of those in the image [15] and language domains [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'datasets of MMKGs' but does not provide specific names. It compares the size of MMKG datasets to those in image and language domains, but no specific datasets are named.",
      "processing_time": 57.84856081008911,
      "citing_paper_id": "271961581",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "The cross-KG datasets include two pairs of KGs: FB15K-DB15K and FB15K-YAGO15K [30], where FB15K, DB15K, and YAGO15K are subsets of Freebase [2], DBpedia [1], and YAGO [33], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-DB15K",
        "FB15K-YAGO15K",
        "FB15K",
        "DB15K",
        "YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15K-DB15K": "Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and DBpedia subsets.",
        "FB15K-YAGO15K": "Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and YAGO subsets.",
        "FB15K": "Subset of Freebase used for cross-KG reasoning experiments, focusing on entity and relation alignment.",
        "DB15K": "Subset of DBpedia used for cross-KG reasoning experiments, focusing on entity and relation alignment.",
        "YAGO15K": "Subset of YAGO used for cross-KG reasoning experiments, focusing on entity and relation alignment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for cross-KG experiments, which are subsets of larger knowledge graphs.",
      "processing_time": 87.06870722770691,
      "citing_paper_id": "271961581",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "The cross-KG datasets include two pairs of KGs: FB15K-DB15K and FB15K-YAGO15K [30], where FB15K, DB15K, and YAGO15K are subsets of Freebase [2], DBpedia [1], and YAGO [33], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-DB15K",
        "FB15K-YAGO15K",
        "FB15K",
        "DB15K",
        "YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15K-DB15K": "Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and DBpedia subsets.",
        "FB15K-YAGO15K": "Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and YAGO subsets.",
        "FB15K": "Subset of Freebase used for cross-KG reasoning experiments, focusing on entity and relation alignment.",
        "DB15K": "Subset of DBpedia used for cross-KG reasoning experiments, focusing on entity and relation alignment.",
        "YAGO15K": "Subset of YAGO used for cross-KG reasoning experiments, focusing on entity and relation alignment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for cross-KG experiments, which are subsets of larger knowledge graphs.",
      "processing_time": 87.06870722770691,
      "citing_paper_id": "271961581",
      "cited_paper_id": null
    },
    {
      "context_text": "Multi-modalEAmethodswithproperfusionandalignmentstrategy formulti-modaldataincludingMMEA[7],HMEA[17],PoE[30], EVA[28],MCLEA[26]andMSNEA[8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions multi-modal methods but does not specify any datasets. The cited papers' titles do not provide additional clarity on datasets.",
      "processing_time": 55.077253341674805,
      "citing_paper_id": "271961581",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Multi-modalEAmethodswithproperfusionandalignmentstrategy formulti-modaldataincludingMMEA[7],HMEA[17],PoE[30], EVA[28],MCLEA[26]andMSNEA[8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions multi-modal methods but does not specify any datasets. The cited papers' titles do not provide additional clarity on datasets.",
      "processing_time": 55.077253341674805,
      "citing_paper_id": "271961581",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "Multi-modalEAmethodswithproperfusionandalignmentstrategy formulti-modaldataincludingMMEA[7],HMEA[17],PoE[30], EVA[28],MCLEA[26]andMSNEA[8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions multi-modal methods but does not specify any datasets. The cited papers' titles do not provide additional clarity on datasets.",
      "processing_time": 55.077253341674805,
      "citing_paper_id": "271961581",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "This indicates that one may try to use the limited seed entities more effectively to improve the performance. and natural language understanding [29, 50, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to seed entities and natural language understanding. No verifiable resources are identified.",
      "processing_time": 55.136515378952026,
      "citing_paper_id": "271961581",
      "cited_paper_id": 158046772
    },
    {
      "context_text": "This indicates that one may try to use the limited seed entities more effectively to improve the performance. and natural language understanding [29, 50, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to seed entities and natural language understanding. No verifiable resources are identified.",
      "processing_time": 55.136515378952026,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202583325
    },
    {
      "context_text": "This indicates that one may try to use the limited seed entities more effectively to improve the performance. and natural language understanding [29, 50, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to seed entities and natural language understanding. No verifiable resources are identified.",
      "processing_time": 55.136515378952026,
      "citing_paper_id": "271961581",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "Auxiliary-enhanced methods incorporate auxiliary information such as names and descriptions to im-provethealignmentperformance,suchasMultiKE[61],HMAN[54], RDGCN[52],AttrGNN[31],BERT-INT[45]andERMC[55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 55.255324363708496,
      "citing_paper_id": "271961581",
      "cited_paper_id": 174802832
    },
    {
      "context_text": "Auxiliary-enhanced methods incorporate auxiliary information such as names and descriptions to im-provethealignmentperformance,suchasMultiKE[61],HMAN[54], RDGCN[52],AttrGNN[31],BERT-INT[45]andERMC[55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 55.255324363708496,
      "citing_paper_id": "271961581",
      "cited_paper_id": 198354047
    },
    {
      "context_text": "Auxiliary-enhanced methods incorporate auxiliary information such as names and descriptions to im-provethealignmentperformance,suchasMultiKE[61],HMAN[54], RDGCN[52],AttrGNN[31],BERT-INT[45]andERMC[55].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 55.255324363708496,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202121966
    },
    {
      "context_text": "Structure-based methods utilize only structural information of KGs to align entities, such as BootEA [43], MUGNN [4], KECG [25], NAEA [65], and AliNet [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (BootEA, MUGNN, KECG, NAEA, AliNet) but does not refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.447211503982544,
      "citing_paper_id": "271961581",
      "cited_paper_id": 196205749
    },
    {
      "context_text": "Structure-based methods utilize only structural information of KGs to align entities, such as BootEA [43], MUGNN [4], KECG [25], NAEA [65], and AliNet [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models (BootEA, MUGNN, KECG, NAEA, AliNet) but does not refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.447211503982544,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202770936
    },
    {
      "context_text": "Inspired by [54] and follow [26], we encode the relations, attributes, and names of entity ùíÜ as bag-of-words [62] features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on encoding relations, attributes, and names of entities using bag-of-words features.",
      "processing_time": 56.48012447357178,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202121966
    },
    {
      "context_text": "DBP15K ùëçùêª ‚àí ùê∏ùëÅ DBP15K ‚àí DBP15K ‚àí Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR w / o SF KECG [25] . the baselines even without surface forms, showing its robustness and efficiency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBP15K"
      ],
      "dataset_descriptions": {
        "DBP15K": "Used to evaluate entity alignment models, specifically comparing performance with and without surface forms, demonstrating robustness and efficiency."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'DBP15K' which appears to be a dataset used for entity alignment experiments. The citation indicates that the dataset is used to evaluate the performance of a model without surface forms.",
      "processing_time": 63.9457426071167,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202770936
    },
    {
      "context_text": "Each node ùíè has a hidden vector ùíâ in R , which is refined by gathering the states of its adjacent nodes (including itself) denoted as N : where ùëæ ‚àà R ùëë √ó ùëë is a diagonal matrix that linearly manipulates the states [25].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for refining node vectors in a graph. The cited paper title suggests a method for entity alignment, not a dataset.",
      "processing_time": 56.67677545547485,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202770936
    },
    {
      "context_text": "The graph embedding ùíâ for entity ùëí ùëñ is: where ùëæ ‚àà R ùëë √ó ùëë is a diagonal matrix that linearly manipulates the states [25], N is the neighbor node and ùúé is the ReLU activation function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for graph embedding. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 55.25260400772095,
      "citing_paper_id": "271961581",
      "cited_paper_id": 202770936
    },
    {
      "context_text": "As shown in Fig 1, the EA tasks on MMKGs [7, 26] heavily rely on seed entities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reliance on seed entities for entity alignment tasks in multi-modal knowledge graphs.",
      "processing_time": 54.912715911865234,
      "citing_paper_id": "271961581",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "MMEA [7] integrates the different modalities into a joint embedding to represent the entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMEA, which is a method for integrating different modalities into a joint embedding for entity representation. No specific dataset is mentioned.",
      "processing_time": 55.86235070228577,
      "citing_paper_id": "271961581",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "EVA [28] uses the visual feature to create the initial pre-aligned entities that guide the training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (EVA) that uses visual features for entity alignment.",
      "processing_time": 54.90706133842468,
      "citing_paper_id": "271961581",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "We also use character bigrams [26, 34] of entity names as complementary information for missing names.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to character bigrams as complementary information, which is a method, not a dataset.",
      "processing_time": 56.03306698799133,
      "citing_paper_id": "271961581",
      "cited_paper_id": 237420821
    },
    {
      "context_text": "Unsupervisedmethodsdonotrequireanyseedentityfor training,suchasRREA[36],MRAEA[35],EASY[16],andSEU[34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only unsupervised methods for entity alignment. No verifiable resources are identified.",
      "processing_time": 54.99121022224426,
      "citing_paper_id": "271961581",
      "cited_paper_id": 237420821
    },
    {
      "context_text": "Following the previous works, we use the character bigrams [26, 34] of entity names as complementary information for the missing name of some entities since proper nouns (e.g., person names) are missing.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method of using character bigrams of entity names.",
      "processing_time": 55.21719312667847,
      "citing_paper_id": "271961581",
      "cited_paper_id": 237420821
    },
    {
      "context_text": "Some existing works [5, 6, 40, 46, 57, 64] have proposed to augment KGs using adversarial learning, reverse relationships, mixup, rule-based methods, and counterfactual generation for knowledge graph data augmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for augmenting knowledge graphs. No verifiable resources are identified.",
      "processing_time": 55.216084241867065,
      "citing_paper_id": "271961581",
      "cited_paper_id": 252819516
    },
    {
      "context_text": "Some existing works [5, 6, 40, 46, 57, 64] propose augmenting KGs using adversarial learning, reverse relationships, mixup, rule-based methods, and counterfactual generation for knowledge graph data augmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for knowledge graph data augmentation.",
      "processing_time": 54.049187660217285,
      "citing_paper_id": "271961581",
      "cited_paper_id": 252819516
    },
    {
      "context_text": "It is a common technique for different data structures, like images [47], languages [24], and graphs [59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general data structures without naming any particular dataset.",
      "processing_time": 55.20967173576355,
      "citing_paper_id": "271961581",
      "cited_paper_id": null
    },
    {
      "context_text": "It is beneficial in various data structures, such as images [47], languages [24], and graphs [59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general data types (images, languages, graphs) without naming any particular datasets.",
      "processing_time": 56.60494661331177,
      "citing_paper_id": "271961581",
      "cited_paper_id": null
    },
    {
      "context_text": "Knowledge graphs (KGs) [1, 2, 33] are structured representations of real-world facts that are represented by entities and the connection between them.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general concept of knowledge graphs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 54.97542214393616,
      "citing_paper_id": "271961581",
      "cited_paper_id": null
    },
    {
      "context_text": "Diffusion models [56] have demonstrated impressive results in generating high-quality, diverse images, text, and audio.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only diffusion models and their capabilities.",
      "processing_time": 53.664337158203125,
      "citing_paper_id": "271961581",
      "cited_paper_id": null
    },
    {
      "context_text": "RDGCN is a Graph Convolutional Network-based models which uses Word2Vec embeddings [22] of entity names as the initial weights of entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Word2Vec embeddings but does not refer to a specific dataset. Word2Vec is a method for generating word embeddings, not a dataset.",
      "processing_time": 56.195658683776855,
      "citing_paper_id": "257020057",
      "cited_paper_id": 2407601
    },
    {
      "context_text": "For training, we employ the widely used [20, 21] margin-based alignment loss function L , which expects greater similarity between positive pairs in alignment mappings and less similarity between negative pairs generated.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a loss function used for training. The context is focused on the methodology rather than the data.",
      "processing_time": 55.998932123184204,
      "citing_paper_id": "257020057",
      "cited_paper_id": 53082628
    },
    {
      "context_text": "For training, we employ the widely used [20, 21] margin-based alignment loss function L , which expects greater similarity between positive pairs in alignment mappings and less similarity between negative pairs generated.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a loss function used for training. The context is focused on the methodology rather than the data.",
      "processing_time": 55.998932123184204,
      "citing_paper_id": "257020057",
      "cited_paper_id": 202712648
    },
    {
      "context_text": "We then evaluated four competitive unimodal embedding-based EA models for thier compatibility with visual modalities , including BootEA [10], MultiKE, RDGCN [11] and IMUSE [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models but does not refer to any specific datasets. The context is about evaluating models, not using datasets.",
      "processing_time": 55.15393137931824,
      "citing_paper_id": "257020057",
      "cited_paper_id": 129946212
    },
    {
      "context_text": "It plays a critical role in KGs application and NLP field [1, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to the importance of KGs in NLP.",
      "processing_time": 54.653878688812256,
      "citing_paper_id": "257020057",
      "cited_paper_id": 196172975
    },
    {
      "context_text": "For the original embedding-based models, we follow the setting in [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a setting from another paper. The cited paper title suggests it might involve datasets, but no specific names are provided in the citation context.",
      "processing_time": 57.27781963348389,
      "citing_paper_id": "257020057",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "In this work we first constructed a series of large-scale multimodal EA benchmarks named Multi-OpenEA with a high ratio of image equipped entities and multiple images per entity, based on the OpenEA benchmarks [4].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multi-OpenEA"
      ],
      "dataset_descriptions": {
        "Multi-OpenEA": "Used to construct large-scale multimodal entity alignment benchmarks with a high ratio of image-equipped entities and multiple images per entity, enhancing the evaluation of multimodal knowledge graph reasoning methods."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the construction of a new benchmark named Multi-OpenEA, which is based on the existing OpenEA benchmarks. However, OpenEA itself is not a dataset but a benchmark suite, so it is excluded.",
      "processing_time": 66.86678981781006,
      "citing_paper_id": "257020057",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "We consider BootEA, MultiKE, RDGCN and IMUSE for extension since they are very recent and typical embedding-based methods, often achieving state-of-the-art performance on many EA benchmarks without images [4, 5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and benchmarks. The context focuses on embedding-based methods for entity alignment in knowledge graphs.",
      "processing_time": 56.49787616729736,
      "citing_paper_id": "257020057",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "We report the higher of the results we reproduce and the results reported in [4].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of reproduced results and reported results. No verifiable resources are identified.",
      "processing_time": 55.455639123916626,
      "citing_paper_id": "257020057",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "Worse, they fail to perform logical deduction in the inference phase like some conventional non-embedding based methods, which has proved to be quite useful by recent study[4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to methods and their performance.",
      "processing_time": 54.076393365859985,
      "citing_paper_id": "257020057",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "With the development of representation learning [3], embedding-based EA methods have taken the dominant position in recent research [4, 5, 6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to embedding-based entity alignment methods.",
      "processing_time": 54.08409309387207,
      "citing_paper_id": "257020057",
      "cited_paper_id": 212737039
    },
    {
      "context_text": "In building domain KGs, such as goods KG in e-commerce [9], it‚Äôs also common to add images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'goods KG in e-commerce' which is likely referring to a specific knowledge graph or dataset used in e-commerce, but no specific dataset name is provided. The mention is too generic and lacks a clear identifier.",
      "processing_time": 59.170002698898315,
      "citing_paper_id": "257020057",
      "cited_paper_id": 214714259
    },
    {
      "context_text": "Hence, we recommend that future studies use more visual information and discard entity names, considering the name bias problems also mentioned in many recent works [30, 31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a recommendation for future studies and a reference to name bias problems.",
      "processing_time": 55.196187019348145,
      "citing_paper_id": "257020057",
      "cited_paper_id": 222177127
    },
    {
      "context_text": "With the development of representation learning [1], embedding-based EA methods have taken the dominant position in recent research [2, 3, 4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general trends in research methods.",
      "processing_time": 54.05593729019165,
      "citing_paper_id": "257020057",
      "cited_paper_id": 232404011
    },
    {
      "context_text": "They over-rely on the structure and literal (entity names) information [7, 8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general criticism of methods over-relying on structure and entity names.",
      "processing_time": 55.618404150009155,
      "citing_paper_id": "257020057",
      "cited_paper_id": 248227575
    },
    {
      "context_text": "We also use the latest multi-modal EA method EVA [10] and homochronous MSNEA [11] as baselines.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (EVA and MSNEA) but does not reference any specific datasets. The context focuses on comparing these methods as baselines.",
      "processing_time": 57.002373695373535,
      "citing_paper_id": "257020057",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "Although, some recent approaches [10, 11] utilize images for EA, they cannot effectively utilize multiple images but can only handle single images.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of certain approaches in handling multiple images.",
      "processing_time": 54.38583540916443,
      "citing_paper_id": "257020057",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "Although MSNEA and EVA are competitive, they are supervised while LODEME is unsupervised (self-supervised).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.61117696762085,
      "citing_paper_id": "257020057",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "We report the higher of the results we reproduce and the results reported in [4].4 We also use the latest multi-modal EA method EVA [10] and homochronous MSNEA [11] as baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and baselines. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.569586992263794,
      "citing_paper_id": "257020057",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "Moreover, the fusion of multi-modal knowledge is often accompanied by noise [58] [61].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept about multimodal knowledge fusion and noise.",
      "processing_time": 55.177900552749634,
      "citing_paper_id": "278959270",
      "cited_paper_id": 352650
    },
    {
      "context_text": "Then, using a transformer-based [33] approach, it encodes the obtained entity sequences from different modalities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer-based approach).",
      "processing_time": 54.606127977371216,
      "citing_paper_id": "278959270",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "‚Ä¶obtaining the token sequences of the two modalities after linear layer projection, TSAM uses the pre-trained language model based on transformer [33] to perform encoder processing on the sequences respectively: where g e represents the encoder layer based on the Trans-former [33] pre-trained‚Ä¶",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer) which is excluded. No verifiable resources are identified.",
      "processing_time": 56.00713801383972,
      "citing_paper_id": "278959270",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "‚Ä¶on transformer [33] to perform encoder processing on the sequences respectively: where g e represents the encoder layer based on the Trans-former [33] pre-trained language model, Pooling is the pooling operation, and [ENT] is similar to [CLS] in Bert [30], which is used to obtain the final‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of a transformer-based encoder and pooling operations.",
      "processing_time": 56.984235525131226,
      "citing_paper_id": "278959270",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "‚Ä¶we selected 13 different types of methods as base-line models for comparison, including 3 classic single-modal baseline models: TransE [14], RotatE [15], Tucker [34], as well as dozens of MMKGC models as demonstrated below: IKRL [47], AdaMF [4], OTKGE [42], VISTA [49], RSME [79], QEB [50], IMF‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 56.547571897506714,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Specifically, the basic formula of TransE is: For RotatE [15], its core idea is to model relations as rotations in a complex vector space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (TransE and RotatE). The citation is focused on explaining the methodological approach rather than using a specific dataset.",
      "processing_time": 58.05957508087158,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Traditional KGC methods [14] [15] [17] [16] [67] primarily focus on completing static KGs with a single modality and are unable to handle multi-modal KGs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about traditional KGC methods and their limitations. No verifiable resources are identified.",
      "processing_time": 56.843305587768555,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Traditional KGC methods [14] [15] [17] [16] [67] primarily focus on completing static KGs with a single modality and are unable to handle multi-modal KGs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about traditional KGC methods and their limitations. No verifiable resources are identified.",
      "processing_time": 56.843305587768555,
      "citing_paper_id": "278959270",
      "cited_paper_id": 236459932
    },
    {
      "context_text": "Traditional KGC methods [14] [15] [17] [16] [67] primarily focus on completing static KGs with a single modality and are unable to handle multi-modal KGs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about traditional KGC methods and their limitations. No verifiable resources are identified.",
      "processing_time": 56.843305587768555,
      "citing_paper_id": "278959270",
      "cited_paper_id": 259949942
    },
    {
      "context_text": "We adopt the TuckER [34], TransE [14] and RotatE [15] models to construct a structured encoder to achieve more accurate knowledge representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (TuckER, TransE, RotatE) but does not refer to any specific datasets. The context focuses on the use of these models for constructing a structured encoder for knowledge representation.",
      "processing_time": 58.880450963974,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "‚Ä¶in various vector spaces to constrain the distance between head and tail entities to optimize model representation, such as TransE [14], TransR [60], RotatE [15], HAKE [65], QIQE [66], WeightE [68], ConKGC [67], GIE [76], SpherE [69], MRME [70], RecPiece [71], ExpressivE [72] and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods for knowledge graph embedding but does not refer to any specific datasets. The context is focused on describing different embedding techniques rather than using a particular dataset.",
      "processing_time": 58.18658947944641,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "‚Ä¶in various vector spaces to constrain the distance between head and tail entities to optimize model representation, such as TransE [14], TransR [60], RotatE [15], HAKE [65], QIQE [66], WeightE [68], ConKGC [67], GIE [76], SpherE [69], MRME [70], RecPiece [71], ExpressivE [72] and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods for knowledge graph embedding but does not refer to any specific datasets. The context is focused on describing different embedding techniques rather than using a particular dataset.",
      "processing_time": 58.18658947944641,
      "citing_paper_id": "278959270",
      "cited_paper_id": 259949770
    },
    {
      "context_text": "‚Ä¶in various vector spaces to constrain the distance between head and tail entities to optimize model representation, such as TransE [14], TransR [60], RotatE [15], HAKE [65], QIQE [66], WeightE [68], ConKGC [67], GIE [76], SpherE [69], MRME [70], RecPiece [71], ExpressivE [72] and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods for knowledge graph embedding but does not refer to any specific datasets. The context is focused on describing different embedding techniques rather than using a particular dataset.",
      "processing_time": 58.18658947944641,
      "citing_paper_id": "278959270",
      "cited_paper_id": 259949942
    },
    {
      "context_text": "‚Ä¶in various vector spaces to constrain the distance between head and tail entities to optimize model representation, such as TransE [14], TransR [60], RotatE [15], HAKE [65], QIQE [66], WeightE [68], ConKGC [67], GIE [76], SpherE [69], MRME [70], RecPiece [71], ExpressivE [72] and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods for knowledge graph embedding but does not refer to any specific datasets. The context is focused on describing different embedding techniques rather than using a particular dataset.",
      "processing_time": 58.18658947944641,
      "citing_paper_id": "278959270",
      "cited_paper_id": 269457193
    },
    {
      "context_text": "‚Ä¶in various vector spaces to constrain the distance between head and tail entities to optimize model representation, such as TransE [14], TransR [60], RotatE [15], HAKE [65], QIQE [66], WeightE [68], ConKGC [67], GIE [76], SpherE [69], MRME [70], RecPiece [71], ExpressivE [72] and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods for knowledge graph embedding but does not refer to any specific datasets. The context is focused on describing different embedding techniques rather than using a particular dataset.",
      "processing_time": 58.18658947944641,
      "citing_paper_id": "278959270",
      "cited_paper_id": 276117175
    },
    {
      "context_text": "This modality is typically learned using knowledge graph embedding techniques (e.g., TransE [14], TuckER [34],RotatE [15]) and serves as the backbone of our entity representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is used to reference the method RotatE, which is not a dataset.",
      "processing_time": 57.42090153694153,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "In KGE models (e.g., TransE [14], TuckER [34], RotatE [15]), the embeddings for entities h , t ‚àà R d and relations r ‚àà R d are initialized as trainable vectors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 55.896453857421875,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "‚Ä¶of other modalities after fusion, thereby bringing them closer to the vector space and enhancing the effectiveness of MMKGC. Additionally, TSAM employs KGE models such as [14] [15] [34] to serve as scoring functions and capture structural-semantic relationships, obtaining structured embeddings.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only KGE models which are excluded according to the instructions.",
      "processing_time": 55.48595571517944,
      "citing_paper_id": "278959270",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "We use bert-base as the main transformer encoder and decoder of the model, and use bert-large, RoBERTa-base/large [31], LLaMA-7B [78] and DeBERTa-base/large [32] as variant models for cross-validation.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The citation is focused on describing the models used for cross-validation, not datasets.",
      "processing_time": 56.82747268676758,
      "citing_paper_id": "278959270",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "We use bert-base as the main transformer encoder and decoder of the model, and use bert-large, RoBERTa-base/large [31], LLaMA-7B [78] and DeBERTa-base/large [32] as variant models for cross-validation.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The citation is focused on describing the models used for cross-validation, not datasets.",
      "processing_time": 56.82747268676758,
      "citing_paper_id": "278959270",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "The FgMAF method first utilizes visual pre-trained models [29] [27] and text pre-trained models [30] [31] [32] to perform tokenization on the visual and textual modalities of entities in the MMKG, capturing fine-grained semantic token sequences for each modality.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models for visual and textual modalities but does not specify any datasets. The cited papers are pre-trained models, not datasets.",
      "processing_time": 57.56505298614502,
      "citing_paper_id": "278959270",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "The FgMAF method first utilizes visual pre-trained models [29] [27] and text pre-trained models [30] [31] [32] to perform tokenization on the visual and textual modalities of entities in the MMKG, capturing fine-grained semantic token sequences for each modality.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models for visual and textual modalities but does not specify any datasets. The cited papers are pre-trained models, not datasets.",
      "processing_time": 57.56505298614502,
      "citing_paper_id": "278959270",
      "cited_paper_id": 235436185
    },
    {
      "context_text": "‚Ä¶using KGE, we use the Transformer-based decoder to obtain the prediction result of the tail entity in the triple: where g d () represents the decoder layer with a Transformer-based pre-trained language model [30] [31] [32], h f represents the modality fusion representation of the head entity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer-based decoder) and a concept (modality fusion representation). The cited papers are about pre-trained language models, which are methods, not datasets.",
      "processing_time": 59.00297832489014,
      "citing_paper_id": "278959270",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "‚Ä¶in KG and achieve the purpose of KGC. Models based on natural language processing : By converting triples into text sequences, using transformer-based models to perform encoder-decoder operations to achieve prediction, such as SimKGC [17], KG-Bert [36], CSProm-KG [73], StAR [35], and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models but does not refer to any specific datasets. The focus is on describing models and their methodologies, not on datasets.",
      "processing_time": 57.02751803398132,
      "citing_paper_id": "278959270",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "Contrastive learning [54] [55] [26] [56] [80] has garnered significant attention across various fields, as it enhances the representation of similar samples by bringing them closer together while pushing dissimilar samples apart.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of contrastive learning. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.82603406906128,
      "citing_paper_id": "278959270",
      "cited_paper_id": 211096730
    },
    {
      "context_text": "Contrastive learning [54] [55] [26] [56] [80] has garnered significant attention across various fields, as it enhances the representation of similar samples by bringing them closer together while pushing dissimilar samples apart.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of contrastive learning. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.82603406906128,
      "citing_paper_id": "278959270",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Contrastive learning [54] [55] [26] [56] [80] has garnered significant attention across various fields, as it enhances the representation of similar samples by bringing them closer together while pushing dissimilar samples apart.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of contrastive learning. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.82603406906128,
      "citing_paper_id": "278959270",
      "cited_paper_id": 233296292
    },
    {
      "context_text": "Contrastive learning [54] [55] [26] [56] [80] has garnered significant attention across various fields, as it enhances the representation of similar samples by bringing them closer together while pushing dissimilar samples apart.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of contrastive learning. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.82603406906128,
      "citing_paper_id": "278959270",
      "cited_paper_id": 259145427
    },
    {
      "context_text": "Contrastive learning [54] [55] [26] [56] [80] has garnered significant attention across various fields, as it enhances the representation of similar samples by bringing them closer together while pushing dissimilar samples apart.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of contrastive learning. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.82603406906128,
      "citing_paper_id": "278959270",
      "cited_paper_id": null
    },
    {
      "context_text": "such as recommendation systems [3] [5] [6] [7], social media [8], object detection [9] and applications combined with large language models [10] [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general application areas. No dataset names are provided in the context or titles.",
      "processing_time": 56.45546221733093,
      "citing_paper_id": "278959270",
      "cited_paper_id": 226246289
    },
    {
      "context_text": "such as recommendation systems [3] [5] [6] [7], social media [8], object detection [9] and applications combined with large language models [10] [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general application areas. No dataset names are provided in the context or titles.",
      "processing_time": 56.45546221733093,
      "citing_paper_id": "278959270",
      "cited_paper_id": 265213194
    },
    {
      "context_text": "From a multi-modal perspective, tokenization [26] [27] [25] can be understood as a process of transforming data from different modalities into a unified representation of token sequences.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of tokenization across different modalities. No verifiable resources are identified.",
      "processing_time": 57.214401721954346,
      "citing_paper_id": "278959270",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Using pre-trained visual encoders (e.g., BEIT-V2 [29]), the images are tokenized into fine-grained visual tokens, which capture the semantic content and nuances of the visual information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained models and methods. The citation is used to reference a method, not a dataset.",
      "processing_time": 57.54917240142822,
      "citing_paper_id": "278959270",
      "cited_paper_id": 235436185
    },
    {
      "context_text": "Transformers and Cross-modal Integration: Transformer-based architectures like VISTA [49] and MKGformer [77] excel in joint image-text representation learning, decoding complex cross-modal interactions for state-of-the-art performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the performance of transformer-based architectures in cross-modal tasks.",
      "processing_time": 57.37982201576233,
      "citing_paper_id": "278959270",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Typical MMKGC methods, such as: OTKGE [42], MyGo [25], LAFA [43], MR-MKG [44], IMF [45], SGMPT [81], CMR [24], SGMPT [81], DySarl [82], MKG-former [77] and MGKsite [20], extend single-modality KGE approaches by integrating multi-modal embeddings, which are extracted via pre-trained models, to‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.77861452102661,
      "citing_paper_id": "278959270",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Typical MMKGC methods, such as: OTKGE [42], MyGo [25], LAFA [43], MR-MKG [44], IMF [45], SGMPT [81], CMR [24], SGMPT [81], DySarl [82], MKG-former [77] and MGKsite [20], extend single-modality KGE approaches by integrating multi-modal embeddings, which are extracted via pre-trained models, to‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.77861452102661,
      "citing_paper_id": "278959270",
      "cited_paper_id": 273164062
    },
    {
      "context_text": "Models based on graph neural networks(GNN): By using KG completely in the form of GNN as an encoder to perform link prediction tasks to achieve the purpose of KGC, such as CompGCN [38], CLGAT [39], NBFNet [37], InGram [74], MGTCA [75], and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on describing various GNN-based models for knowledge graph completion.",
      "processing_time": 57.289223432540894,
      "citing_paper_id": "278959270",
      "cited_paper_id": 252351757
    },
    {
      "context_text": "Models based on graph neural networks(GNN): By using KG completely in the form of GNN as an encoder to perform link prediction tasks to achieve the purpose of KGC, such as CompGCN [38], CLGAT [39], NBFNet [37], InGram [74], MGTCA [75], and other models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on describing various GNN-based models for knowledge graph completion.",
      "processing_time": 57.289223432540894,
      "citing_paper_id": "278959270",
      "cited_paper_id": 268692718
    },
    {
      "context_text": "(3) The effect of the large prediction model LLaMA-7B [78] on the MMKGC task did not achieve the expected effect, and it was even worse than most pre-trained language models.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (LLaMA-7B). The context focuses on the performance of the model in a multi-modal knowledge graph completion task.",
      "processing_time": 59.04178047180176,
      "citing_paper_id": "278959270",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "K NOWLEDGE Graphs (KG) [1] [2] [12] are a structured form of knowledge representation and currently one of the most popular research areas in the field of knowledge engineering.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to knowledge graphs. No verifiable resources are identified.",
      "processing_time": 56.9548978805542,
      "citing_paper_id": "278959270",
      "cited_paper_id": 264492774
    },
    {
      "context_text": "‚Ä¶types of methods as base-line models for comparison, including 3 classic single-modal baseline models: TransE [14], RotatE [15], Tucker [34], as well as dozens of MMKGC models as demonstrated below: IKRL [47], AdaMF [4], OTKGE [42], VISTA [49], RSME [79], QEB [50], IMF [45], MMRNS [51], MyGO [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.70660328865051,
      "citing_paper_id": "278959270",
      "cited_paper_id": 265213194
    },
    {
      "context_text": "Consistent with previous mainstream MMKGC works [42] [23] [25] [45], this paper also considers three types of modality information in MMKGC: visual, textual, and structural modalities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only modalities used in multi-modal knowledge graph completion.",
      "processing_time": 56.38899064064026,
      "citing_paper_id": "278959270",
      "cited_paper_id": 270711106
    },
    {
      "context_text": "For example, MMKGC models using siamese networks and multi-hop reason [19] and considering better integration of various modal information: [22] [23] [24] [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of siamese networks and multi-hop reasoning in multi-modal knowledge graph completion.",
      "processing_time": 58.80260372161865,
      "citing_paper_id": "278959270",
      "cited_paper_id": 270711106
    },
    {
      "context_text": "NativE [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation 'NativE [23]' does not provide enough context to identify a specific dataset. The title suggests a method or tool rather than a dataset.",
      "processing_time": 57.716997146606445,
      "citing_paper_id": "278959270",
      "cited_paper_id": 270711106
    },
    {
      "context_text": "SGMPT [81] and MMRNS [45] leverage contrastive learning with semantic-aware negative sampling, refining discriminative power in entity disambiguation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (SGMPT and MMRNS) and their application in multi-modal knowledge graph reasoning.",
      "processing_time": 58.02626585960388,
      "citing_paper_id": "278959270",
      "cited_paper_id": 273164062
    },
    {
      "context_text": "Bojing Hu et al. [13] suggested the Translation based knowledge graph improved Multi-task learning method (TransMKR) for recommendation system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TransMKR. The context is about a recommendation system, but no datasets are explicitly named.",
      "processing_time": 58.288896560668945,
      "citing_paper_id": "277500003",
      "cited_paper_id": 271104595
    },
    {
      "context_text": "Knowledge graph embedding (KGE) is a well-known technique for KGC, where entities and relations are represented as vectors in a continuous low-dimensional space [4, 5, 6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general techniques and methods for knowledge graph embedding.",
      "processing_time": 56.69997310638428,
      "citing_paper_id": "280003374",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "Knowledge graph embedding (KGE) is a well-known technique for KGC, where entities and relations are represented as vectors in a continuous low-dimensional space [4, 5, 6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general techniques and methods for knowledge graph embedding.",
      "processing_time": 56.69997310638428,
      "citing_paper_id": "280003374",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Knowledge graph embedding (KGE) is a well-known technique for KGC, where entities and relations are represented as vectors in a continuous low-dimensional space [4, 5, 6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general techniques and methods for knowledge graph embedding.",
      "processing_time": 56.69997310638428,
      "citing_paper_id": "280003374",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Semantic-based techniques, such as DistMult [6] and ComplEx [19], use score functions that are based on the similarity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (DistMult and ComplEx). The cited papers do not provide additional context to identify datasets.",
      "processing_time": 58.28440737724304,
      "citing_paper_id": "280003374",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "Semantic-based techniques, such as DistMult [6] and ComplEx [19], use score functions that are based on the similarity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (DistMult and ComplEx). The cited papers do not provide additional context to identify datasets.",
      "processing_time": 58.28440737724304,
      "citing_paper_id": "280003374",
      "cited_paper_id": 258298672
    },
    {
      "context_text": "Zhang et al. [9] also use the entity‚Äôs description and the pre-trained textual encoder PTE [15] to get the textual features of each entity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PTE) and a general approach to obtaining textual features. No verifiable datasets are referenced.",
      "processing_time": 58.27003741264343,
      "citing_paper_id": "280003374",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "Zhang et al. [9] also use the entity‚Äôs description and the pre-trained textual encoder PTE [15] to get the textual features of each entity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PTE) and a general approach to obtaining textual features. No verifiable datasets are referenced.",
      "processing_time": 58.27003741264343,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "Modal information extraction based techniques, such as IKRL [22], extract visual information from the images of entities and achieve a triple score by utilizing both visual information and strutural information through using TransE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions IKRL, which is a method, not a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 57.701191425323486,
      "citing_paper_id": "280003374",
      "cited_paper_id": 4336741
    },
    {
      "context_text": "The experiments are performed on three datasets, such as DB15K [17], MKG-W, and MKG-Y [18].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DB15K",
        "MKG-W",
        "MKG-Y"
      ],
      "dataset_descriptions": {
        "DB15K": "Used to evaluate multimodal representation learning, focusing on cross-modal entity alignment and reasoning in knowledge graphs.",
        "MKG-W": "Used to enhance representation learning with multimodal data, specifically for evaluating the integration of textual and visual information in knowledge graphs.",
        "MKG-Y": "Used to enhance representation learning with multimodal data, specifically for evaluating the integration of textual and visual information in knowledge graphs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three datasets by name, two of which are acronyms and one is a hyphenated name, all of which meet the criteria for inclusion.",
      "processing_time": 77.32006692886353,
      "citing_paper_id": "280003374",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "The experiments are performed on three datasets, such as DB15K [17], MKG-W, and MKG-Y [18].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DB15K",
        "MKG-W",
        "MKG-Y"
      ],
      "dataset_descriptions": {
        "DB15K": "Used to evaluate multimodal representation learning, focusing on cross-modal entity alignment and reasoning in knowledge graphs.",
        "MKG-W": "Used to enhance representation learning with multimodal data, specifically for evaluating the integration of textual and visual information in knowledge graphs.",
        "MKG-Y": "Used to enhance representation learning with multimodal data, specifically for evaluating the integration of textual and visual information in knowledge graphs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three datasets by name, two of which are acronyms and one is a hyphenated name, all of which meet the criteria for inclusion.",
      "processing_time": 77.32006692886353,
      "citing_paper_id": "280003374",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "Neural network-based techniques [20] utilize neural networks to extract features from entities and relations and assign scores to the triples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using neural networks.",
      "processing_time": 56.301143169403076,
      "citing_paper_id": "280003374",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "This limitation hinders the practical use of KGs and MMKGs. Knowledge graph completion (KGC) is a popular research topic that focuses on discovering unobserved data in KGs [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general research topic. No verifiable resources are identified.",
      "processing_time": 56.806049823760986,
      "citing_paper_id": "280003374",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "‚Ä¶function is represented as. where N , P and T are called the sigmoid function, the margin and the self-adversarial weight, respectively, for each negative triple (‚Ñé X , X , X ) generated by negative sampling [4]. weight (cid:4) (cid:5) The self-adversarial T is where [ is the temperature parameter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses components of a method for modeling multi-relational data.",
      "processing_time": 57.09513187408447,
      "citing_paper_id": "280003374",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Translation-based techniques, such as TransE [4] and its variations, represent triples as translations from the head entities to the tail entities using a distance-based scoring function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TransE, which is a method for modeling multi-relational data, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 58.039299964904785,
      "citing_paper_id": "280003374",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Zhang et al. [9] emplpyed a pre-trained visual encoder PVE [14] to derive the visual feature for each entity where the & , ) are the parameters of the visual project layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a pre-trained visual encoder. The citation is focused on the method rather than a reusable dataset.",
      "processing_time": 58.74209523200989,
      "citing_paper_id": "280003374",
      "cited_paper_id": 16407324
    },
    {
      "context_text": "Zhang et al. [9] emplpyed a pre-trained visual encoder PVE [14] to derive the visual feature for each entity where the & , ) are the parameters of the visual project layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a pre-trained visual encoder. The citation is focused on the method rather than a reusable dataset.",
      "processing_time": 58.74209523200989,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "KGs store information about the world in the form of organized triples that look like (‚Ñé, (cid:4), (cid:5)) , where the head entity ‚Ñé and the tail entity are connected by the relation [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general structure of knowledge graphs. No verifiable resources are identified.",
      "processing_time": 58.02301216125488,
      "citing_paper_id": "280003374",
      "cited_paper_id": 19135805
    },
    {
      "context_text": "The CLS token [11] is utilized to record the sentence-level textual characteristic .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (CLS token) used in BERT. The context is about using the CLS token to capture sentence-level textual characteristics.",
      "processing_time": 59.91210651397705,
      "citing_paper_id": "280003374",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Nguyen Thi et al. [10] have shown that the performance of the KGC model with transfer learning is increased by using the Bidirectional Encoder Representations from Transformer (BERT) model [11] for entity embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of BERT for entity embeddings in a KGC model, but BERT is a model, not a dataset. No specific dataset is mentioned.",
      "processing_time": 59.31549310684204,
      "citing_paper_id": "280003374",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Nguyen Thi et al. [10] have shown that the performance of the KGC model with transfer learning is increased by using the Bidirectional Encoder Representations from Transformer (BERT) model [11] for entity embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of BERT for entity embeddings in a KGC model, but BERT is a model, not a dataset. No specific dataset is mentioned.",
      "processing_time": 59.31549310684204,
      "citing_paper_id": "280003374",
      "cited_paper_id": 253790338
    },
    {
      "context_text": "Our main contributions in this study are as follows: ‚Ä¢ Regarding the textual embedding, instead of using the special token CLS [11] as in [9], we propose to use the ‚ÄúText-to-Text Transfer Transformer‚Äù (T5) introduced by Raffel et al. [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of T5 for textual embedding, which is a method, not a dataset.",
      "processing_time": 59.704834938049316,
      "citing_paper_id": "280003374",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Our main contributions in this study are as follows: ‚Ä¢ Regarding the textual embedding, instead of using the special token CLS [11] as in [9], we propose to use the ‚ÄúText-to-Text Transfer Transformer‚Äù (T5) introduced by Raffel et al. [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of T5 for textual embedding, which is a method, not a dataset.",
      "processing_time": 59.704834938049316,
      "citing_paper_id": "280003374",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "Our main contributions in this study are as follows: ‚Ä¢ Regarding the textual embedding, instead of using the special token CLS [11] as in [9], we propose to use the ‚ÄúText-to-Text Transfer Transformer‚Äù (T5) introduced by Raffel et al. [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of T5 for textual embedding, which is a method, not a dataset.",
      "processing_time": 59.704834938049316,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "They trained the embeddings with a sigmoid loss fucntion from [5].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for training embeddings.",
      "processing_time": 55.9028594493866,
      "citing_paper_id": "280003374",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "The RotatE model [5] is used as the score function to measure the triple plausibility.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the RotatE model but does not refer to any specific dataset. The model is used for measuring triple plausibility in a knowledge graph, which is a methodological detail rather than a dataset.",
      "processing_time": 60.54264187812805,
      "citing_paper_id": "280003374",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Others techniques use various mathematical strutures to represent triples including RotatE [5].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to a method called RotatE, which is not a dataset.",
      "processing_time": 58.030386209487915,
      "citing_paper_id": "280003374",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "On the other hand, TransAE [23], an extension of of TransE, extracts modal information with a multi-modal auto-encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransAE) which is an extension of TransE. The context focuses on the method's ability to extract modal information using a multi-modal auto-encoder.",
      "processing_time": 61.227283000946045,
      "citing_paper_id": "280003374",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Recently, transfer learning-based techniques introduced in [21] leverage transfer learning across various KGC datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'KGC datasets'. No specific, verifiable datasets are named.",
      "processing_time": 58.480628967285156,
      "citing_paper_id": "280003374",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "As mentioned earlier, we use the ‚ÄúText-to-Text Transfer Transformer‚Äù (T5) model introduced by Raffel et al. [12] to obtain the textual embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using the T5 model but does not reference any specific dataset. The T5 model is a method, not a dataset.",
      "processing_time": 58.757256507873535,
      "citing_paper_id": "280003374",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "‚Ä¢ For the visual encoding, we the Vision Transformer model [13] to get the visual feature for each entity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model (Vision Transformer). The context focuses on the use of the model for visual feature extraction.",
      "processing_time": 58.97428297996521,
      "citing_paper_id": "280003374",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Transformer architecture has been achieved the state of the art results in many natural language processing (NLP) tasks [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the success of the Transformer architecture in NLP tasks.",
      "processing_time": 57.63340401649475,
      "citing_paper_id": "280003374",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Dosovitskiy et al. [13] have proposed to apply a standard Transformer directly to images, with minimal modifications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer applied to images).",
      "processing_time": 57.431071758270264,
      "citing_paper_id": "280003374",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Currently, KGs and MMKGs are extensively used in AI activities such as question answering [2] and recommendation systems [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGs and MMKGs. No verifiable resources are identified.",
      "processing_time": 58.7557647228241,
      "citing_paper_id": "280003374",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "Currently, KGs and MMKGs are extensively used in AI activities such as question answering [2] and recommendation systems [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGs and MMKGs. No verifiable resources are identified.",
      "processing_time": 58.7557647228241,
      "citing_paper_id": "280003374",
      "cited_paper_id": 233481294
    },
    {
      "context_text": "Zhang et al. [9] have developed a modality adversarial training (MAT) mechanism for MMKGC using adversarial training [16] to enhance the unbalanced multi-modal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (modality adversarial training) and a general application area (multi-modal knowledge graph completion).",
      "processing_time": 59.48791313171387,
      "citing_paper_id": "280003374",
      "cited_paper_id": 237353134
    },
    {
      "context_text": "The authors in [7, 8] incorporated rich-semantic multi-modal information such as images and text descriptions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the incorporation of multi-modal information. No clear, verifiable datasets are identified.",
      "processing_time": 58.909167766571045,
      "citing_paper_id": "280003374",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "The authors in [7, 8] incorporated rich-semantic multi-modal information such as images and text descriptions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the incorporation of multi-modal information. No clear, verifiable datasets are identified.",
      "processing_time": 58.909167766571045,
      "citing_paper_id": "280003374",
      "cited_paper_id": 252918783
    },
    {
      "context_text": "Wang et al. proposed sophisticated techniques to achieve multi-modal fusion by integrating textual, visual, and structural information [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only techniques for multi-modal fusion. The context does not provide enough information to identify a verifiable dataset.",
      "processing_time": 59.48504900932312,
      "citing_paper_id": "280003374",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "One more example is MANS [24] , a modality-aware negative sampling method that coordinates multi-modal and structural information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MANS as a method, not a dataset. It describes a modality-aware negative sampling technique, which is not a reusable dataset.",
      "processing_time": 59.16933822631836,
      "citing_paper_id": "280003374",
      "cited_paper_id": 252783084
    },
    {
      "context_text": "These techniques have been enhanced by making the most of the benefits from the BERT model, as demonstrated by Nguyen Thi et al. [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the BERT model. The context is about enhancing techniques using BERT, which is a model, not a dataset.",
      "processing_time": 59.89159631729126,
      "citing_paper_id": "280003374",
      "cited_paper_id": 253790338
    },
    {
      "context_text": "Our proposed method is based on the AdaMF-MAT model that was introduced by Zhang et al. [9] as the MMKGC model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (AdaMF-MAT/MMKGC). No verifiable resources are identified.",
      "processing_time": 58.96475791931152,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "Recently, Zhang et al. [9] have introduced a new MMKGC framework that is called AdaMF-MF which stands for Adaptive Multi-modal Fusion and Modality Adversarial Training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation introduces a method (AdaMF-MF) rather than a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 59.412848234176636,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "Despite of the excellent performance of existing MMKC models, Zhang et al. [9] have shown that these models would neglect the imbalance of modality information among entities, which might be perceived from two different viewpoints.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the issue of imbalanced modality information in multi-modal knowledge graph completion models, but does not mention any specific datasets.",
      "processing_time": 58.56259751319885,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "Conversely, Xu et al. tried to improve the process of negative sampling by incorporating multi-modal data [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving negative sampling using multi-modal data.",
      "processing_time": 58.02723217010498,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "Zhang et al. [9] suggested an adaptive multi-modal fusion (AdaMF) technique to facilitate adaptive modality fusion.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called AdaMF. The context is about a technique for multi-modal knowledge graph completion, but no datasets are explicitly named.",
      "processing_time": 60.81266713142395,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "A main training objective is to minize ‚Ñí H\"I [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a training objective. The cited paper title suggests a focus on multi-modal knowledge graph completion but does not provide a dataset name.",
      "processing_time": 60.61635446548462,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "In order to tackle this issue, Zhang et al. [9] came up with a new MMKGC system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called 'MMKGC'. No verifiable resources are identified.",
      "processing_time": 59.38721036911011,
      "citing_paper_id": "280003374",
      "cited_paper_id": 267898006
    },
    {
      "context_text": "In order to effectively utilize side information to enhance recommendation, researchers have proposed integrating side information into CF, such as social networks [10], user/item attributes [11], images [12], and context [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to types of side information that can be integrated into collaborative filtering systems.",
      "processing_time": 59.96117925643921,
      "citing_paper_id": "259483117",
      "cited_paper_id": 5625629
    },
    {
      "context_text": "In order to effectively utilize side information to enhance recommendation, researchers have proposed integrating side information into CF, such as social networks [10], user/item attributes [11], images [12], and context [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to types of side information that can be integrated into collaborative filtering systems.",
      "processing_time": 59.96117925643921,
      "citing_paper_id": "259483117",
      "cited_paper_id": 20401422
    },
    {
      "context_text": "(2) For lyrics, we utilize a pre-trained Word2Vec [38] model to assign a vector representation to each word in the lyrics.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using a pre-trained Word2Vec model but does not reference any specific dataset. Word2Vec is a method/model, not a dataset.",
      "processing_time": 59.95834755897522,
      "citing_paper_id": "259483117",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "In this context, the development of music information retrieval [3] has become crucial in enhancing user experience and improving the proÔ¨Åtability of these platforms [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general field of music information retrieval. There are no verifiable resources or datasets mentioned.",
      "processing_time": 59.543129205703735,
      "citing_paper_id": "259483117",
      "cited_paper_id": 49298988
    },
    {
      "context_text": "In this context, the development of music information retrieval [3] has become crucial in enhancing user experience and improving the proÔ¨Åtability of these platforms [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general field of music information retrieval. There are no verifiable resources or datasets mentioned.",
      "processing_time": 59.543129205703735,
      "citing_paper_id": "259483117",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional content-based recommendation methods [4] usually only consider the features of the music itself, neglecting the potential relations between music and other entities, such as artists, albums, and playlists.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to content-based recommendation methods. No verifiable resources are identified.",
      "processing_time": 59.37860918045044,
      "citing_paper_id": "259483117",
      "cited_paper_id": 49298988
    },
    {
      "context_text": "In recent years, convolutional neural networks (CNNs) have shown impressive performance in the domains of video [17] and images [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the performance of CNNs in video and image domains. No verifiable resources are identified.",
      "processing_time": 59.742674112319946,
      "citing_paper_id": "259483117",
      "cited_paper_id": 51972201
    },
    {
      "context_text": "In recent years, convolutional neural networks (CNNs) have shown impressive performance in the domains of video [17] and images [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the performance of CNNs in video and image domains. No verifiable resources are identified.",
      "processing_time": 59.742674112319946,
      "citing_paper_id": "259483117",
      "cited_paper_id": 235829175
    },
    {
      "context_text": "However, entity-based MMKGs do not fuse multi-modal data and therefore limit the exploitation of multi-modal information [25,26].",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of entity-based MMKGs. No verifiable resources are identified.",
      "processing_time": 59.31838536262512,
      "citing_paper_id": "259483117",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "This paper proposes three classiÔ¨Åcations of existing MMKG-based recommender systems from the perspective of multi-modal feature fusion [27]: (1) The feature-fusion method, also known as the early-fusion method, concatenates features extracted from different modalities into a single high-dimensional‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing different types of feature fusion methods in MMKG-based recommender systems.",
      "processing_time": 60.37523937225342,
      "citing_paper_id": "259483117",
      "cited_paper_id": 169032532
    },
    {
      "context_text": "Following the previous work [22,32,43], the search spaces for embedding dimension d , neighbor sampling size k , and propagation layers l are {4,8,16,32,64,128,256}, {2,4,8,16,32}, and {1,2,3,4}.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only parameters for search spaces. No verifiable resources are identified.",
      "processing_time": 58.69620609283447,
      "citing_paper_id": "259483117",
      "cited_paper_id": 189761996
    },
    {
      "context_text": "We compare MKGCN with six baseline models: SVD [40], CKE [12], RippleNet [32], KGCN [22], MMGCN [29], and MKGAT [28].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several models but does not refer to any specific datasets. The context is focused on comparing different models, not on the use of datasets.",
      "processing_time": 60.378904581069946,
      "citing_paper_id": "259483117",
      "cited_paper_id": 201701022
    },
    {
      "context_text": "We compare MKGCN with six baseline models: SVD [40], CKE [12], RippleNet [32], KGCN [22], MMGCN [29], and MKGAT [28].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several models but does not refer to any specific datasets. The context is focused on comparing different models, not on the use of datasets.",
      "processing_time": 60.378904581069946,
      "citing_paper_id": "259483117",
      "cited_paper_id": 207168823
    },
    {
      "context_text": "We compare MKGCN with six baseline models: SVD [40], CKE [12], RippleNet [32], KGCN [22], MMGCN [29], and MKGAT [28].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several models but does not refer to any specific datasets. The context is focused on comparing different models, not on the use of datasets.",
      "processing_time": 60.378904581069946,
      "citing_paper_id": "259483117",
      "cited_paper_id": 224281034
    },
    {
      "context_text": "Following the previous work [22,28,29,32], for each dataset, we randomly select 80% of the interaction data as the training set, the remaining 20% as the test set, and then 20% from the training set as the validation set to determine the selection of hyperparameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context does not mention any specific dataset names, only generic references to 'interaction data'. However, given the topic and cited papers, it is likely that multi-modal datasets are used, but they are not explicitly named.",
      "processing_time": 62.82438921928406,
      "citing_paper_id": "259483117",
      "cited_paper_id": 201701022
    },
    {
      "context_text": "Following the previous work [22,28,29,32], for each dataset, we randomly select 80% of the interaction data as the training set, the remaining 20% as the test set, and then 20% from the training set as the validation set to determine the selection of hyperparameters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context does not mention any specific dataset names, only generic references to 'interaction data'. However, given the topic and cited papers, it is likely that multi-modal datasets are used, but they are not explicitly named.",
      "processing_time": 62.82438921928406,
      "citing_paper_id": "259483117",
      "cited_paper_id": 224281034
    },
    {
      "context_text": "For example, MMGCN [29] is based on a knowledge graph of three different modalities (text, image, and audio) and then performs user‚Äìitem interaction predictions on all three knowledge graphs simultaneously.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a method (MMGCN) that uses a multi-modal knowledge graph but does not specify a named dataset. The focus is on the method's capability to handle multiple modalities.",
      "processing_time": 61.45004224777222,
      "citing_paper_id": "259483117",
      "cited_paper_id": 201701022
    },
    {
      "context_text": "‚Ä¢ MMGCN [29] constructs bipartite graphs for each modality, representing the relationship between users and items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MMGCN) and its application. No verifiable datasets are referenced.",
      "processing_time": 59.859243392944336,
      "citing_paper_id": "259483117",
      "cited_paper_id": 201701022
    },
    {
      "context_text": "‚Ä¢ SVD [40] is a CF-based recommendation framework that models user interactions using inner products.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SVD) used in a recommendation framework.",
      "processing_time": 58.622575998306274,
      "citing_paper_id": "259483117",
      "cited_paper_id": 207168823
    },
    {
      "context_text": "‚Ä¶recommender systems, as the core technology of music information retrieval, can provide personalized music recommendations to users based on their preferences and behavioral patterns, thereby increasing user satisfaction, loyalty, and ultimately promoting revenue growth of the music platform [5,6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about recommender systems and music information retrieval.",
      "processing_time": 58.6452739238739,
      "citing_paper_id": "259483117",
      "cited_paper_id": 207178741
    },
    {
      "context_text": "LightGCN [23] proposes a lightweight GCN based on an interaction graph by learning users and linearly propagating embedding items on a user‚Äìitem bipartite graph, and using the weighted sum of embedding item learning across all levels as the Ô¨Ånal embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LightGCN). The context focuses on the method's functionality and application in recommendation systems.",
      "processing_time": 60.35026502609253,
      "citing_paper_id": "259483117",
      "cited_paper_id": 211043589
    },
    {
      "context_text": "Most existing recommendation systems employ random negative sampling strategies to generate negative samples, such as KGCN [22], RippleNet [32], and CKAN [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on negative sampling strategies in recommendation systems.",
      "processing_time": 59.39207935333252,
      "citing_paper_id": "259483117",
      "cited_paper_id": 220730030
    },
    {
      "context_text": "‚Ä¢ MKGAT [28] incorporates visual and textual modal information for each entity on the knowledge graph and aggregates it to the entity representation.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of visual and textual modal information in a knowledge graph, which aligns with multi-modal knowledge graph reasoning. However, no specific dataset is named.",
      "processing_time": 61.11059594154358,
      "citing_paper_id": "259483117",
      "cited_paper_id": 224281034
    },
    {
      "context_text": "For instance, MKGAT [28] Ô¨Årst performs separate feature representations of multi-modal data such as text, image, and triples, and then aggregates the embedding representation of the feature vectors from each modality to make recommendations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the use of multi-modal data in a recommender system, but does not specify a named dataset. The focus is on the method (MKGAT) rather than a specific dataset.",
      "processing_time": 61.29843235015869,
      "citing_paper_id": "259483117",
      "cited_paper_id": 224281034
    },
    {
      "context_text": "As a result, they fail to uncover deeper semantic information behind the music [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general limitation in uncovering deeper semantic information in music.",
      "processing_time": 60.02126884460449,
      "citing_paper_id": "259483117",
      "cited_paper_id": 245502839
    },
    {
      "context_text": "We use the Xavier initializer [41] to initialize the parameters of the MKGCN and optimize the MKGCN using the Adam optimizer [42].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Xavier initializer and Adam optimizer, which are methods, not datasets. No datasets are mentioned in the context.",
      "processing_time": 59.215184926986694,
      "citing_paper_id": "259483117",
      "cited_paper_id": null
    },
    {
      "context_text": "We also compared our method with three trans-former models that incorporate multi-modal information: (4) BERT (Devlin et al., 2019) is a pre-trained model to generate representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. BERT is referenced but it is a model, not a dataset.",
      "processing_time": 59.51379871368408,
      "citing_paper_id": "263829023",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We then perform a flatten operation to convert the matrix into a one-dimensional vector similar to word embeddings, which is similar to embeddings in BERT (Devlin et al., 2019) and concatenate them to form the image embedding vector, denoted as v v .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for converting matrices into vectors similar to word embeddings used in BERT. BERT itself is a model, not a dataset.",
      "processing_time": 60.9836962223053,
      "citing_paper_id": "263829023",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The textual attribute is initialized by BERT (Devlin et al., 2019).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BERT, which is a model, not a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 59.231918811798096,
      "citing_paper_id": "263829023",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We then perform a flatten operation to convert the matrix into a one-dimensional vector similar to word embeddings, which is similar to embeddings in BERT (Devlin et al., 2019) and concatenate them to form the image embedding vector, denoted as vv.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for converting matrices into vectors similar to word embeddings used in BERT. BERT itself is a model, not a dataset.",
      "processing_time": 61.23622727394104,
      "citing_paper_id": "263829023",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We further use three transformer models (BERT (Devlin et al., 2019), ViT (Dosovitskiy et al.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. No dataset names are present in the citation span.",
      "processing_time": 58.43282771110535,
      "citing_paper_id": "263829023",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The FB15K-DB15K dataset 2 is an entity alignment dataset of FB15K and DB15K MMKGs, while the latter is a dataset of FB15K and YAGO15K MMKGs. Consistent with prior works (Chen et al., 2020; Guo et al., 2021), we split each dataset into training and testing sets in proportions of 2:8, 5:5, and 8:2, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-DB15K",
        "DB15K"
      ],
      "dataset_descriptions": {
        "FB15K-DB15K": "Used for entity alignment in multi-modal knowledge graphs, specifically to align entities between FB15K and DB15K. The dataset is split into training and testing sets in proportions of 2:8, 5:5, and 8:2.",
        "DB15K": "Used for entity alignment in multi-modal knowledge graphs, specifically to align entities between FB15K and YAGO15K. The dataset is split into training and testing sets in proportions of 2:8, 5:5, and 8:2."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FB15K-DB15K and DB15K, which are used for entity alignment in multi-modal knowledge graphs. The datasets are split into training and testing sets in various proportions.",
      "processing_time": 88.48539328575134,
      "citing_paper_id": "263829023",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "‚Ä¶dataset 2 is an entity alignment dataset of FB15K and DB15K MMKGs, while the latter is a dataset of FB15K and YAGO15K MMKGs. Consistent with prior works (Chen et al., 2020; Guo et al., 2021), we split each dataset into training and testing sets in proportions of 2:8, 5:5, and 8:2, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K",
        "DB15K",
        "YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15K": "Used for entity alignment in multi-modal knowledge graphs, split into training and testing sets in various proportions to evaluate model performance.",
        "DB15K": "Used for entity alignment in multi-modal knowledge graphs, split into training and testing sets in various proportions to evaluate model performance.",
        "YAGO15K": "Used for entity alignment in multi-modal knowledge graphs, split into training and testing sets in various proportions to evaluate model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for entity alignment in multi-modal knowledge graphs, which are directly relevant to the research topic.",
      "processing_time": 79.86635422706604,
      "citing_paper_id": "263829023",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "(8) Chen et al. (Chen et al., 2020) designs a fusion module to integrate multi-modal attributes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating multi-modal attributes.",
      "processing_time": 57.46593117713928,
      "citing_paper_id": "263829023",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "Multi-modal entity alignment (Chen et al., 2020; Guo et al., 2021; Liu et al., 2021; Chen et al., 2022) aims to determine if two entities from different multi-modal knowledge graphs refer to the same real-world entity.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-modal entity alignment. No verifiable resources are named.",
      "processing_time": 59.34026837348938,
      "citing_paper_id": "263829023",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "More recent works (Chen et al., 2020; Guo et al., 2021) designed multi-modal fusion modules to better integrate attributes and entities, but still did not fully exploit the potential interactions among modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and approaches in multi-modal knowledge graph reasoning.",
      "processing_time": 57.942930698394775,
      "citing_paper_id": "263829023",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "Recent research suggests that the feed-forward layers within the transformer architecture store factual knowledge and can be regarded as unnormalized key-value memories (Yao et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (transformer architecture).",
      "processing_time": 58.53133463859558,
      "citing_paper_id": "263829023",
      "cited_paper_id": 246015673
    },
    {
      "context_text": "A.1 Multi-Modal Entity Alignment In the real-world, due to the multi-modal nature of KGs, there have been several works (Zhu et al., 2022; Wang et al., 2021; Jiang et al., 2021; Fang et al., 2022) that have started to focus on MMEA technology.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that focus on multi-modal entity alignment technology.",
      "processing_time": 58.83155608177185,
      "citing_paper_id": "263829023",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "A.1 Multi-Modal Entity Alignment In the real-world, due to the multi-modal nature of KGs, there have been several works (Zhu et al., 2022; Wang et al., 2021; Jiang et al., 2021; Fang et al., 2022) that have started to focus on MMEA technology.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that focus on multi-modal entity alignment technology.",
      "processing_time": 58.83155608177185,
      "citing_paper_id": "263829023",
      "cited_paper_id": 252519575
    },
    {
      "context_text": "The Transformer architecture, originally proposed for natural language processing tasks, has been applied to various knowledge graph tasks as well (Liu et al., 2022; Hu et al., 2022; Howard et al., 2022; Wang et al., 2023, 2022; Fang et al., 2023b,a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the application of the Transformer architecture to knowledge graph tasks. No verifiable resources are identified.",
      "processing_time": 59.824544191360474,
      "citing_paper_id": "263829023",
      "cited_paper_id": 251518175
    },
    {
      "context_text": "The Transformer architecture, originally proposed for natural language processing tasks, has been applied to various knowledge graph tasks as well (Liu et al., 2022; Hu et al., 2022; Howard et al., 2022; Wang et al., 2023, 2022; Fang et al., 2023b,a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the application of the Transformer architecture to knowledge graph tasks. No verifiable resources are identified.",
      "processing_time": 59.824544191360474,
      "citing_paper_id": "263829023",
      "cited_paper_id": 258714901
    },
    {
      "context_text": "MMEA (Liu et al., 2019; Li et al., 2023b; Liu et al., 2021; Lin et al., 2022) is challenging due to the heterogeneity of MMKGs ( e . g ., different neighbors, multi-modal attributes, distinct types), which makes it difficult to learn rich knowledge representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the challenges associated with Multi-modal Multi-entity Alignment (MMEA) and Multi-modal Knowledge Graphs (MMKGs).",
      "processing_time": 60.86043119430542,
      "citing_paper_id": "263829023",
      "cited_paper_id": 252070586
    },
    {
      "context_text": "One popular approach is to use embeddings to represent entities and their associated modalities (Cao et al., 2022; Yang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of embeddings for representing entities and modalities in multi-modal knowledge graphs.",
      "processing_time": 60.873188495635986,
      "citing_paper_id": "263829023",
      "cited_paper_id": 252782076
    },
    {
      "context_text": "Mosselly et al [28] and Pezeshkpour et al [12] use VGG [27] and GloVe [29] to separately extract visual and textual information and then fused them into multi-modal information.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG and GloVe, which are models/methods, not datasets. No specific datasets are mentioned or used according to the citation context.",
      "processing_time": 59.962366580963135,
      "citing_paper_id": "258298672",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Mosselly et al [28] and Pezeshkpour et al [12] use VGG [27] and GloVe [29] to separately extract visual and textual information and then fused them into multi-modal information.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG and GloVe, which are models/methods, not datasets. No specific datasets are mentioned or used according to the citation context.",
      "processing_time": 59.962366580963135,
      "citing_paper_id": "258298672",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "KBGAN [31] and IGAN [14] apply Generative Adversarial Networks (GANs) [32] to select harder negative samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions KBGAN and IGAN but does not refer to them as datasets. They are methods or models using GANs for knowledge graph reasoning.",
      "processing_time": 60.15215992927551,
      "citing_paper_id": "258298672",
      "cited_paper_id": 3401524
    },
    {
      "context_text": "IKRL [11] apply VGG [27] to extract visual information of entities‚Äô images and scoring a triple with both visual information and structure information using TransE [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of VGG for extracting visual information and TransE for scoring triples, but does not specify a dataset. The focus is on methods rather than a specific dataset.",
      "processing_time": 61.529945373535156,
      "citing_paper_id": "258298672",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "IKRL [11] apply VGG [27] to extract visual information of entities‚Äô images and scoring a triple with both visual information and structure information using TransE [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of VGG for extracting visual information and TransE for scoring triples, but does not specify a dataset. The focus is on methods rather than a specific dataset.",
      "processing_time": 61.529945373535156,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Marginrank loss is a general training objective extensively used in the MMKGE model [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'MMKGE model' but does not refer to any specific dataset. The context is about a training objective used in a model, not a dataset.",
      "processing_time": 60.73712491989136,
      "citing_paper_id": "258298672",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Marginrank loss is a general training objective extensively used in the MMKGE model [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'MMKGE model' but does not refer to any specific dataset. The context is about a training objective used in a model, not a dataset.",
      "processing_time": 60.73712491989136,
      "citing_paper_id": "258298672",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "Nevertheless, existing multimodal KGE (MMKGE) [11]‚Äì[13] methods design additional embeddings to represent the modal information, which would also participate in the score function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for multimodal knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 59.38191556930542,
      "citing_paper_id": "258298672",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Nevertheless, existing multimodal KGE (MMKGE) [11]‚Äì[13] methods design additional embeddings to represent the modal information, which would also participate in the score function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for multimodal knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 59.38191556930542,
      "citing_paper_id": "258298672",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "During training, we divide each dataset into 400 batches and apply IKRL [11] as the MMKGE model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method (IKRL) and a general concept (MMKGE model).",
      "processing_time": 60.302245140075684,
      "citing_paper_id": "258298672",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "IKRL [11] apply VGG [27] to extract visual information of entities‚Äô images and scoring a triple with both visual information and",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of VGG for extracting visual information. No clear dataset names are provided.",
      "processing_time": 59.25472807884216,
      "citing_paper_id": "258298672",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "The experiments are based on FB15K dataset and TransE base score function.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K"
      ],
      "dataset_descriptions": {
        "FB15K": "Used to evaluate the performance of TransE in modeling multi-relational data, focusing on link prediction tasks in knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "FB15K is a well-known dataset used for evaluating multi-relational data models, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 67.06318712234497,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "We evaluate the link prediction task by mean rank (MR) [7], mean reciprocal rank (MRR) [10] and Hit@K (K=1,3,10) [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics for the link prediction task. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 60.210113286972046,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "We evaluate the link prediction task by mean rank (MR) [7], mean reciprocal rank (MRR) [10] and Hit@K (K=1,3,10) [7].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics for the link prediction task. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 60.210113286972046,
      "citing_paper_id": "258298672",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "1) Tasks and Evaluation Protocol: We evaluate our method on two tasks, link prediction, and triple classification [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and evaluation protocols. The cited paper title suggests a method rather than a dataset.",
      "processing_time": 59.070719718933105,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Normal NS [7] randomly replaces the head or tail entity with another entity with the same probabilities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for modifying entities in multi-relational data.",
      "processing_time": 58.12573432922363,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Negative sampling (NS) [7] is a widely used technology for training KGE models, which aims to generate manual negative triples by randomly replacing entities for positive-negative contrast.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (negative sampling) used in training KGE models. The context does not provide information about a reusable dataset.",
      "processing_time": 60.41388392448425,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Knowledge graph embedding (KGE) [7]‚Äì[10] is a popular and universal approach for KGC, which represents entities and relations of KGs in a continuous low-dimension vector space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 59.04482412338257,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Knowledge graph embedding (KGE) [7]‚Äì[10] is a popular and universal approach for KGC, which represents entities and relations of KGs in a continuous low-dimension vector space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 59.04482412338257,
      "citing_paper_id": "258298672",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Besides, we follow the filter setting [7] which would remove candidate triples that have already appeared in the datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific dataset names, only a generic reference to 'datasets'. No clear, verifiable resource is identified.",
      "processing_time": 58.39715790748596,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "TransAE [13] also employs TransE as the score function and exact modal information with a multi-modal auto-encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.03997731208801,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "TransAE [13] also employs TransE as the score function and exact modal information with a multi-modal auto-encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.03997731208801,
      "citing_paper_id": "258298672",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "Translation-based methods like TransE [7] and TransH [21] modeling the triples as the translation from head to tail entities with a distance-based scoring function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TransE and TransH, which are methods for knowledge graph embedding, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 60.012847900390625,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Translation-based methods like TransE [7] and TransH [21] modeling the triples as the translation from head to tail entities with a distance-based scoring function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions TransE and TransH, which are methods for knowledge graph embedding, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 60.012847900390625,
      "citing_paper_id": "258298672",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "The overall score function consists of four parts, aiming to learn the embeddings in the same vector space, which can be denoted as: F(h, r, t) = f(hs, r, ts) + f(hv, r, tv) + f(hs, r, tv) + f(hv, r, ts), where f is the TransE score [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TransE, which is a method for modeling multi-relational data, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 59.667280435562134,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "structure information using TransE [7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (TransE) for modeling multi-relational data.",
      "processing_time": 58.32081699371338,
      "citing_paper_id": "258298672",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Semantic-based methods like DistMult [9] and ComplEx [8] use similarity-based scoring functions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DistMult and ComplEx, which are methods, not datasets. No specific datasets are mentioned in the context.",
      "processing_time": 58.80924868583679,
      "citing_paper_id": "258298672",
      "cited_paper_id": 16407324
    },
    {
      "context_text": "Besides, MANS performs particularly well in Hit@1 and MRR, which are sensitive to high-rank results [15].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics. The cited paper title 'RotatE' is a method, not a dataset.",
      "processing_time": 58.81445670127869,
      "citing_paper_id": "258298672",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Several KGE methods modeling triples with various mathematical structures, such as RotatE [10], ConE [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RotatE as a method, not a dataset. No datasets are explicitly mentioned or used in the given context.",
      "processing_time": 58.66750526428223,
      "citing_paper_id": "258298672",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Model FB15K DB15KMRR‚Üë MR‚Üì Hit@10‚Üë Hit@3‚Üë Hit@1‚Üë MRR‚Üë MR‚Üì Hit@10‚Üë Hit@3‚Üë Hit@1‚Üë\nNormal [7] 0.479 95 0.755 0.604 0.314 0.303 685 0.542 0.410 0.167 No-Samp [17] 0.109 1594 0.212 0.130 0.051 0.151 456 0.271 0.171 0.087 NSCaching [15] 0.329 121 0.526 0.374 0.224 0.291 835 0.471 0.344 0.192 SANS [16] 0.394 109 0.635 0.466 0.264 0.276 703 0.413 0.387 0.127 CAKE [18] 0.395 68 0.647 0.467 0.262 - - - - - EANS [19] 0.483 111 0.739 0.597 0.327 0.269 1036 0.489 0.353 0.141\nMANS-V 0.454 103 0.713 0.552 0.305 0.274 506 0.525 0.333 0.165 MANS-T 0.485 93 0.748 0.591 0.333 0.307 615 0.546 0.411 0.178 MANS-H 0.493 92 0.756 0.606 0.351 0.329 553 0.541 0.414 0.204 MANS-A 0.499 88 0.749 0.601 0.353 0.332 549 0.550 0.420 0.204\ndesign of MANS could benefit the MMKGE model in various KGC tasks such as link prediction and triple classification, which means that MANS is a universal approach for better KGC.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. The cited papers' titles do not help in identifying any datasets.",
      "processing_time": 59.13816428184509,
      "citing_paper_id": "258298672",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Model FB15K DB15KMRR‚Üë MR‚Üì Hit@10‚Üë Hit@3‚Üë Hit@1‚Üë MRR‚Üë MR‚Üì Hit@10‚Üë Hit@3‚Üë Hit@1‚Üë\nNormal [7] 0.479 95 0.755 0.604 0.314 0.303 685 0.542 0.410 0.167 No-Samp [17] 0.109 1594 0.212 0.130 0.051 0.151 456 0.271 0.171 0.087 NSCaching [15] 0.329 121 0.526 0.374 0.224 0.291 835 0.471 0.344 0.192 SANS [16] 0.394 109 0.635 0.466 0.264 0.276 703 0.413 0.387 0.127 CAKE [18] 0.395 68 0.647 0.467 0.262 - - - - - EANS [19] 0.483 111 0.739 0.597 0.327 0.269 1036 0.489 0.353 0.141\nMANS-V 0.454 103 0.713 0.552 0.305 0.274 506 0.525 0.333 0.165 MANS-T 0.485 93 0.748 0.591 0.333 0.307 615 0.546 0.411 0.178 MANS-H 0.493 92 0.756 0.606 0.351 0.329 553 0.541 0.414 0.204 MANS-A 0.499 88 0.749 0.601 0.353 0.332 549 0.550 0.420 0.204\ndesign of MANS could benefit the MMKGE model in various KGC tasks such as link prediction and triple classification, which means that MANS is a universal approach for better KGC.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. The cited papers' titles do not help in identifying any datasets.",
      "processing_time": 59.13816428184509,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "Some recent methods [25], [26] combine rule learning / analogical inference and KGE together to enhance the interpretability of KGE models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.00647258758545,
      "citing_paper_id": "258298672",
      "cited_paper_id": 84843937
    },
    {
      "context_text": "Some recent methods [25], [26] combine rule learning / analogical inference and KGE together to enhance the interpretability of KGE models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.00647258758545,
      "citing_paper_id": "258298672",
      "cited_paper_id": 255393926
    },
    {
      "context_text": "Nowadays, KGs and MMKGs have been widely used in AI-related tasks like question answering [3], recommendation systems [4], language modeling [5] and telecom fault analysis [6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGs and MMKGs. No verifiable resources are named.",
      "processing_time": 59.01268029212952,
      "citing_paper_id": "258298672",
      "cited_paper_id": 202583325
    },
    {
      "context_text": "Nowadays, KGs and MMKGs have been widely used in AI-related tasks like question answering [3], recommendation systems [4], language modeling [5] and telecom fault analysis [6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of KGs and MMKGs. No verifiable resources are named.",
      "processing_time": 59.01268029212952,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233481294
    },
    {
      "context_text": "Knowledge graphs (KGs) [1], [2] represent real-world knowledge in the form of triple (h, r, t), which indicates the entity h and the entity t are connected by the relation r.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of knowledge graphs and their structure.",
      "processing_time": 57.43804049491882,
      "citing_paper_id": "258298672",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "Besides, we have found in practice that NSCaching [15] and No-Samp [17] would consume lots of memory and GPU resources, which is 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the memory and GPU consumption of certain techniques.",
      "processing_time": 58.71605634689331,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "Besides, we have found in practice that NSCaching [15] and No-Samp [17] would consume lots of memory and GPU resources, which is 1.13√ó (NSCaching [15]) and 6.65√ó (NoSamp [17]) than MANS-A.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their resource consumption. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.29980969429016,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "Though NoSamp [17] is very fast, it fails to perform well in MMKGE according to the link prediction results in Table II.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NoSamp) and its performance in a task (MMKGE).",
      "processing_time": 58.38819432258606,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "They are No-Samp [17], NSCaching [15], SANS [16], CAKE [18], and EANS [19], which enhance the normal NS from their different perspectives.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or models. There are no clear identifiers for datasets within the text.",
      "processing_time": 58.44255185127258,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "NS-KGE [17] employs a unified square loss to avoid NS during training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NS-KGE) and its training approach. The context is focused on the methodological aspect of avoiding negative sampling during training.",
      "processing_time": 60.70552587509155,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "For baselines, we have taken full account of the parameter settings in the original paper [15], [17]‚Äì[19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to parameter settings in other papers.",
      "processing_time": 57.069796323776245,
      "citing_paper_id": "258298672",
      "cited_paper_id": 233347189
    },
    {
      "context_text": "Reinanda et al. [17] introduce the definition of entity \"aspect\", using the aspect catalog of entity types, and using aspect features to identify related documents of long-tail entities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions an 'aspect catalog' but does not specify a dataset name. The term 'catalog' is too generic without a specific identifier.",
      "processing_time": 59.06874322891235,
      "citing_paper_id": "260735729",
      "cited_paper_id": 12544231
    },
    {
      "context_text": "In addition, some researchers [9, 14, 17, 20, 28] extract information from large-scale query logs or perform entity analysis to generate salient content about entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general methods and approaches used by researchers.",
      "processing_time": 58.27330541610718,
      "citing_paper_id": "260735729",
      "cited_paper_id": 12544231
    },
    {
      "context_text": "In addition, some researchers [9, 14, 17, 20, 28] extract information from large-scale query logs or perform entity analysis to generate salient content about entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general methods and approaches used by researchers.",
      "processing_time": 58.27330541610718,
      "citing_paper_id": "260735729",
      "cited_paper_id": 14136367
    },
    {
      "context_text": "In addition, some researchers [9, 14, 17, 20, 28] extract information from large-scale query logs or perform entity analysis to generate salient content about entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general methods and approaches used by researchers.",
      "processing_time": 58.27330541610718,
      "citing_paper_id": "260735729",
      "cited_paper_id": 15810061
    },
    {
      "context_text": "(3) MMKG [11] is a collection of three knowledge graphs that incorporate both numerical features and image links for all entities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMKG"
      ],
      "dataset_descriptions": {
        "MMKG": "Used to incorporate both numerical features and image links for all entities in multi-modal knowledge graphs, enhancing the representation of entity information."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MMKG' as a collection of knowledge graphs, which fits the criteria for a dataset. It is used to incorporate both numerical features and image links for all entities.",
      "processing_time": 67.0866813659668,
      "citing_paper_id": "260735729",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "As shown in Table 2, compared with the existing mainstream MMKGs [1, 6, 11, 13, 27], our AspectMMKG is the only MMKG with fine-grained aspect information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with existing multi-modal knowledge graphs (MMKGs). The term 'MMKG' is used as a category, not a specific dataset.",
      "processing_time": 61.28645062446594,
      "citing_paper_id": "260735729",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "[2, 3] use chapters of Wikipedia articles to learn how to segment articles into different topics and identify answer paragraphs to biomedical questions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions using chapters of Wikipedia articles, which is a specific, identifiable resource. However, it does not specify a named dataset, only a type of data.",
      "processing_time": 59.2280969619751,
      "citing_paper_id": "260735729",
      "cited_paper_id": 211010432
    },
    {
      "context_text": "We follow the approach of Ramsdell et al [16]., using Wikipedia‚Äôs chapter hierarchy to define the aspect table of contents, where the names of the aspects are derived from the chapter titles.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions using Wikipedia‚Äôs chapter hierarchy, which is a specific, identifiable resource. However, it does not mention a specific dataset name, only a method of deriving aspect names from chapter titles.",
      "processing_time": 60.082817792892456,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "Following Ramsdell et al. [16], there are two aspect features can be used to calculate the similarity between the context and aspect: (a) only the aspect name; (b) the aspect content which is a description text containing aspect name provided by the EAL dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EAL dataset"
      ],
      "dataset_descriptions": {
        "EAL dataset": "Used to provide aspect content for calculating similarity between context and aspect, focusing on entity aspect linking in the research."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'EAL dataset' which is a specific, verifiable dataset used for entity aspect linking. The dataset is used to provide aspect content for calculating similarity.",
      "processing_time": 65.76320815086365,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "Ramsdell et al [16]. use the aspect catalog to define aspects for each entity instance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "aspect catalog"
      ],
      "dataset_descriptions": {
        "aspect catalog": "Used to define aspects for entity instances, enhancing the representation of entities in a knowledge graph by providing detailed attributes and relationships."
      },
      "confidence_score": 0.8,
      "reasoning": "The term 'aspect catalog' appears to be a specific resource used in the context of defining aspects for entity instances, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 65.73995280265808,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "We use the EAL dataset 6 provided by Ramsdell et al. [16] in the experiment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EAL dataset"
      ],
      "dataset_descriptions": {
        "EAL dataset": "Used in experiments for entity aspect linking, focusing on the performance of the proposed method in linking entities to their aspects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'EAL dataset' which is a specific, verifiable dataset used in the experiment. The title confirms it is a test collection for entity aspect linking.",
      "processing_time": 65.61894536018372,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "Complementing the work of Nanni et al [12]., Ramsdell et al [16]. provide a large-scale test set and provide strong baseline results, and a fine-grained feature set to stimulate more research in this area.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a 'large-scale test set' but does not provide a specific name for the dataset. The context does not specify how the dataset is used or its characteristics.",
      "processing_time": 59.08927845954895,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "We use the machine learning tool Rank-lips 5 from Ramsdell et al. [16] to combine information from multiple features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a tool (Rank-lips 5) rather than a dataset. No specific dataset is identified or used according to the citation context.",
      "processing_time": 58.40492558479309,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "The baseline model is proposed by Ramsdell et al. [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a baseline model. The title of the cited paper suggests a test collection, but it is not explicitly mentioned in the citation context.",
      "processing_time": 59.440640687942505,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "Experimental results on widely-used eal-dataset-2020 [16] show that with the help of our AspectMMKG, the original EAL model can achieve a new state-of-the-art performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "eal-dataset-2020"
      ],
      "dataset_descriptions": {
        "eal-dataset-2020": "Used to evaluate the AspectMMKG model, demonstrating state-of-the-art performance in entity aspect linking by comparing against the original EAL model."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'eal-dataset-2020' which appears to be a specific dataset used for evaluating the AspectMMKG model. The dataset is used to demonstrate improved performance over the original EAL model.",
      "processing_time": 70.65485453605652,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "Following previous work [12, 16, 21], we collect entities and their aspect labels from Wikipedia by regarding the section names in each entity‚Äôs Wikipedia page as its aspect labels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used to collect entities and their aspect labels by treating section names in Wikipedia pages as aspect labels, focusing on entity representation and aspect labeling."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions collecting entities and aspect labels from Wikipedia, which suggests the use of Wikipedia as a dataset. However, it does not specify a particular dataset or version of Wikipedia.",
      "processing_time": 66.22102189064026,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "Following previous work [12, 16, 21], we collect entities and their aspect labels from Wikipedia by regarding the section names in each entity‚Äôs Wikipedia page as its aspect labels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used to collect entities and their aspect labels by treating section names in Wikipedia pages as aspect labels, focusing on entity representation and aspect labeling."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions collecting entities and aspect labels from Wikipedia, which suggests the use of Wikipedia as a dataset. However, it does not specify a particular dataset or version of Wikipedia.",
      "processing_time": 66.22102189064026,
      "citing_paper_id": "260735729",
      "cited_paper_id": 244527692
    },
    {
      "context_text": "Following previous work [16] and the preliminary observation on Wikipedia, we pre-define 15 entity types which tend to have plenty of aspects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only entity types and aspects. No clear, verifiable resource is identified.",
      "processing_time": 57.18980956077576,
      "citing_paper_id": "260735729",
      "cited_paper_id": 224271811
    },
    {
      "context_text": "(6) PKG [10] is an MMKG for classical Chinese poetry.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PKG"
      ],
      "dataset_descriptions": {
        "PKG": "Used as a multi-modal knowledge graph for classical Chinese poetry, integrating textual and visual data to enhance understanding and reasoning about poetic works."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions PKG as a multi-modal knowledge graph for classical Chinese poetry, which is directly relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 64.31489253044128,
      "citing_paper_id": "260735729",
      "cited_paper_id": 256631072
    },
    {
      "context_text": "‚Ä¶to discover the unobserved triples in the KGs. Knowledge graph embedding (KGE) [17] is a mainstream approach towards KGC. General KGE methods [1,21,15,13] embed the entities and relations of KGs into low-dimensional vector spaces and modeling the triple structure with different score functions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses general approaches to knowledge graph completion and embedding, but does not mention any specific datasets.",
      "processing_time": 56.4007625579834,
      "citing_paper_id": "260887576",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "‚Ä¶to discover the unobserved triples in the KGs. Knowledge graph embedding (KGE) [17] is a mainstream approach towards KGC. General KGE methods [1,21,15,13] embed the entities and relations of KGs into low-dimensional vector spaces and modeling the triple structure with different score functions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses general approaches to knowledge graph completion and embedding, but does not mention any specific datasets.",
      "processing_time": 56.4007625579834,
      "citing_paper_id": "260887576",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Generative methods are designed to learn the data distribution and generate the missing modality information with generative frameworks such as GAN [7] and VAE [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative frameworks like GAN and VAE. These are methods, not datasets.",
      "processing_time": 57.68630409240723,
      "citing_paper_id": "260887576",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Generative methods are designed to learn the data distribution and generate the missing modality information with generative frameworks such as GAN [7] and VAE [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative frameworks like GAN and VAE. These are methods, not datasets.",
      "processing_time": 57.68630409240723,
      "citing_paper_id": "260887576",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "‚Ä¶we propose a M odality A dversarial and CO ntrastive (MACO for short) framework for modality-missing MMKGC. Leveraging the generative adversarial framework [7], we integrate a pair of generator and discriminator to generate missing visual features conditioned on the entity structural information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (generative adversarial framework).",
      "processing_time": 55.90031027793884,
      "citing_paper_id": "260887576",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "‚Ä¶be denoted as: min Cross-modal Contrastive Loss In the mentioned design, we utilized the de-sign concepts of generative adversarial networks (GANs) [7], however, the training of GAN models is unstable, and the quality of the generated features is difficult to control[10], potentially decreasing‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of GANs and their training challenges.",
      "processing_time": 57.85060238838196,
      "citing_paper_id": "260887576",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "We implement G with a two-layer feed-forward network (FFN), which could be denoted as: where W 1 , W 2 , b 1 , b 2 are the parameters of two feed-forward layers, Œ¥ is the LeakyReLU [20] activation function, z ‚àº N ( 0 , I ) is the random noise, and [; ] is the concentrate operation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LeakyReLU activation function).",
      "processing_time": 55.91718149185181,
      "citing_paper_id": "260887576",
      "cited_paper_id": 14083350
    },
    {
      "context_text": "Knowledge graph completion (KGC) [1] is a popular research topic that focuses on discovering unobserved knowledge in knowledge graphs (KGs) [17], which consist of massive entities and relations in the form of triple (head entity, relation, tail entity).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general concept of knowledge graphs and knowledge graph completion.",
      "processing_time": 56.094156980514526,
      "citing_paper_id": "260887576",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "To assign the positive triples with higher scores, we apply margin-rank loss [1] to train the KGC model, denoted as:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training a KGC model.",
      "processing_time": 54.87590765953064,
      "citing_paper_id": "260887576",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "General KGE methods [1,21,15,13] embed the entities and relations of KGs into low-dimensional vector spaces and modeling the triple structure with different score functions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for knowledge graph embedding. No dataset names are present in the text.",
      "processing_time": 57.23455047607422,
      "citing_paper_id": "260887576",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Besides, we follow the filter setting [1] which would remove the candidate triples that have already appeared in the training data to avoid their interference.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'training data'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 58.44546175003052,
      "citing_paper_id": "260887576",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Incomplete multimodal learning (IML) has attracted extensive attention in the research community as the modality-missings situation is common in practice [6,8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of incomplete multimodal learning. No verifiable resources are identified.",
      "processing_time": 57.0248806476593,
      "citing_paper_id": "260887576",
      "cited_paper_id": 51935625
    },
    {
      "context_text": "To assign the positive triples with higher scores, we apply margin-rank loss [1] to train the KGC model, denoted as: where Œª is the margin and p i is the self-adversarial weight of the negative samples proposed by [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (margin-rank loss) and a model (KGC). The cited paper title 'RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space' confirms that the focus is on methods and models, not datasets.",
      "processing_time": 62.91144561767578,
      "citing_paper_id": "260887576",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "We evaluate our method with mean reciprocal rank (MRR), and Hit@K (K=1,3,10) following [13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The cited paper title 'RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space' does not provide additional context to identify a dataset.",
      "processing_time": 60.300299882888794,
      "citing_paper_id": "260887576",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "Besides, we design a cross-modal contrastive loss [10] to enhance the quality of generated features and improve training stability [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (cross-modal contrastive loss) and its purpose (enhancing feature quality and training stability).",
      "processing_time": 57.723488569259644,
      "citing_paper_id": "260887576",
      "cited_paper_id": 220425719
    },
    {
      "context_text": "Besides, we design a cross-modal contrastive loss [10] to enhance the quality of generated features and improve training stability [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (cross-modal contrastive loss) and its purpose (enhancing feature quality and training stability).",
      "processing_time": 57.723488569259644,
      "citing_paper_id": "260887576",
      "cited_paper_id": 231592822
    },
    {
      "context_text": "‚Ä¶Contrastive Loss In the mentioned design, we utilized the de-sign concepts of generative adversarial networks (GANs) [7], however, the training of GAN models is unstable, and the quality of the generated features is difficult to control[10], potentially decreasing the generator‚Äôs performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the challenges of training GAN models, which is not directly related to datasets.",
      "processing_time": 58.17934274673462,
      "citing_paper_id": "260887576",
      "cited_paper_id": 220425719
    },
    {
      "context_text": "In practice, we apply inbatch sampling [22] to get the negative entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sampling negative entities.",
      "processing_time": 54.82312846183777,
      "citing_paper_id": "260887576",
      "cited_paper_id": 231592822
    },
    {
      "context_text": "Recent methods like OTKGE [2] and MoSE [23] make further steps in multi-modal fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (OTKGE, MoSE) but does not refer to any specific datasets. The context is about methodological advancements in multi-modal fusion.",
      "processing_time": 57.830272912979126,
      "citing_paper_id": "260887576",
      "cited_paper_id": 258509157
    },
    {
      "context_text": "Traditional knowledge representation methods can be divided into methods based on information propagation, such as GCN [7] and GAT [8], and methods based on spatial domain distance such as TransE [1], TransH [9], and TransR [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.133031606674194,
      "citing_paper_id": "259180790",
      "cited_paper_id": 2949428
    },
    {
      "context_text": "Traditional knowledge representation methods can be divided into methods based on information propagation, such as GCN [7] and GAT [8], and methods based on spatial domain distance such as TransE [1], TransH [9], and TransR [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.133031606674194,
      "citing_paper_id": "259180790",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "Finally, the effectiveness of EDET was veriÔ¨Åed by predicate classiÔ¨Åcation and scene description tasks on Visual Genome dataset [17] and Flickr30k dataset [18], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Genome",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "Visual Genome": "Used for predicate classification tasks to verify the effectiveness of EDET, focusing on connecting language and vision using dense image annotations.",
        "Flickr30k": "Used for scene description tasks to verify the effectiveness of EDET, focusing on generating descriptive captions for images."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the use of two specific datasets, Visual Genome and Flickr30k, for verifying the effectiveness of EDET through predicate classification and scene description tasks.",
      "processing_time": 70.9330472946167,
      "citing_paper_id": "259180790",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "Inspired by the translation invariance of word2vec [13], TransE was proposed to represent the vectors of nodes and links as satisfying the form h + l = t.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (word2vec) and a model (TransE).",
      "processing_time": 56.39671015739441,
      "citing_paper_id": "259180790",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "[22] 58.5 65.2 67.1 NODIS [23] 58.9 66.0 67.9 VC-Tree [24] 59.8 66.2 67.9 GPS-Net [25] 60 The results show that EDET can generate excellent scene parsing in the scene graph predicate classiÔ¨Åcation task.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model names and performance metrics. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.4383704662323,
      "citing_paper_id": "259180790",
      "cited_paper_id": 54459095
    },
    {
      "context_text": "[22] 58.5 65.2 67.1 NODIS [23] 58.9 66.0 67.9 VC-Tree [24] 59.8 66.2 67.9 GPS-Net [25] 60 The results show that EDET can generate excellent scene parsing in the scene graph predicate classiÔ¨Åcation task.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model names and performance metrics. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.4383704662323,
      "citing_paper_id": "259180790",
      "cited_paper_id": 214714330
    },
    {
      "context_text": "The established knowledge graphs include general knowledge graph FB15K [1], medical knowledge graph DiaKG [2], CORD-19 [3], etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K",
        "DiaKG",
        "CORD-19"
      ],
      "dataset_descriptions": {
        "FB15K": "Used as a general knowledge graph for evaluating multi-modal reasoning models, focusing on entity and relation prediction tasks.",
        "DiaKG": "Utilized as a medical knowledge graph to enhance multi-modal reasoning in healthcare applications, specifically for disease diagnosis and treatment recommendation.",
        "CORD-19": "Applied as a comprehensive dataset of COVID-19 research articles to support multi-modal reasoning in pandemic-related studies, focusing on information extraction and synthesis."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific knowledge graphs, which are considered datasets in the context of multi-modal knowledge graph reasoning.",
      "processing_time": 76.38917541503906,
      "citing_paper_id": "259180790",
      "cited_paper_id": 216056360
    },
    {
      "context_text": "‚Ä¶inter-knowledge connectivity, thereby posing challenges in meeting the demands of the transformation and upgrading of the manufacturing industry [1] Consequently, unifying the representation of knowledge and leveraging its connectivity to achieve efficient application has emerged as one of the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses general challenges and goals in intelligent manufacturing.",
      "processing_time": 56.22485637664795,
      "citing_paper_id": "277111689",
      "cited_paper_id": 116132630
    },
    {
      "context_text": "LLMs represented by BERT and GPT have achieved remarkable successes in various natural language processing tasks [6], laying a solid foundation for the processing of multi-modal data and broadening the application scope of such models in general domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (BERT and GPT). The context is about the success of these models in NLP tasks and their potential for multi-modal data processing.",
      "processing_time": 59.24671673774719,
      "citing_paper_id": "277111689",
      "cited_paper_id": 211204736
    },
    {
      "context_text": "They have been widely utilized in major fields such as electric power, finance, healthcare, and aviation [2]-[5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general fields of application. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 56.547987937927246,
      "citing_paper_id": "277111689",
      "cited_paper_id": 213801097
    },
    {
      "context_text": "‚Ä¶LLM to learn from zero-sample and few-sample data, and combined its learning ability to realize entity relationship extraction in medical notes; YAN [14] used a pre-trained BERT model to realize the relationship extraction and task reasoning, and to identify the entity-relationships in the graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of a pre-trained BERT model for entity relationship extraction and task reasoning.",
      "processing_time": 56.701096296310425,
      "citing_paper_id": "277111689",
      "cited_paper_id": 243865362
    },
    {
      "context_text": "In addition, in terms of combining LLMs to construct MMKGs, AGRAWAL [13] used the InstructGPT LLM to learn from zero-sample and few-sample data, and combined its learning ability to realize entity relationship extraction in medical notes; YAN [14] used a pre-trained BERT model to realize the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of InstructGPT and BERT, but neither are datasets. The context does not specify any datasets used for constructing MMKGs.",
      "processing_time": 57.905885219573975,
      "citing_paper_id": "277111689",
      "cited_paper_id": 249062918
    },
    {
      "context_text": "By virtue of their rich knowledge reserves and powerful reasoning capabilities [7], they have demonstrated impressive performance in natural language understanding and complex interactive tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to general capabilities of systems with rich knowledge reserves and reasoning capabilities.",
      "processing_time": 56.51100039482117,
      "citing_paper_id": "277111689",
      "cited_paper_id": null
    },
    {
      "context_text": "To demonstrate that MMEA framework outperforms the state-of-the-art entity alignment models, we compare it with the following methods: ‚Ä¢ TransE [1] is a typical translational method for knowledge graph embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransE) for knowledge graph embedding. The context is about comparing methods, not using datasets.",
      "processing_time": 57.719053983688354,
      "citing_paper_id": "221193809",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Under the relational data, we adopt the most representative translational distance model: TransE [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TransE, which is a method for modeling multi-relational data, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 56.33030891418457,
      "citing_paper_id": "221193809",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "‚Ä¢ MTransE [3] learns the translation matrix to map the aligned entities from different knowledge graphs in the unified space.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for aligning entities across different knowledge graphs.",
      "processing_time": 55.4716682434082,
      "citing_paper_id": "221193809",
      "cited_paper_id": 15912887
    },
    {
      "context_text": ", similarity-based methods [9,12] and embedding-based methods [3,20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.16060924530029,
      "citing_paper_id": "221193809",
      "cited_paper_id": 15912887
    },
    {
      "context_text": "[8] proposes a multi-modal translation-based method, which defines the energy of a knowledge graph triple as the sum of sub-energy functions that leverages structural, visual and linguistic knowledge representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for knowledge graph representation learning.",
      "processing_time": 55.095983266830444,
      "citing_paper_id": "221193809",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "The model pre-trained on the ILSVRC 2012 dataset derived from ImageNet [5] is applied in our model.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ILSVRC 2012"
      ],
      "dataset_descriptions": {
        "ILSVRC 2012": "Used for pre-training a model, focusing on large-scale image classification. The dataset provides a hierarchical structure of images for robust feature learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ILSVRC 2012 dataset, which is a specific version of the ImageNet dataset. The dataset is used for pre-training a model.",
      "processing_time": 66.75324392318726,
      "citing_paper_id": "221193809",
      "cited_paper_id": 57246310
    },
    {
      "context_text": "AttrE [15] uses a large number of attribute triples to generate character embeddings, and employs the relationship transitivity rule.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'attribute triples' but does not specify a named dataset. The term 'large number of attribute triples' is too generic and lacks a specific identifier.",
      "processing_time": 58.35817337036133,
      "citing_paper_id": "221193809",
      "cited_paper_id": 69930495
    },
    {
      "context_text": "Recently, in view of the rapid development of knowledge graph embedding, many researchers have attempted to utilize embedding-based models for the entity alignment problem [10,15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the general trend of using embedding-based models for entity alignment in knowledge graphs.",
      "processing_time": 56.589022397994995,
      "citing_paper_id": "221193809",
      "cited_paper_id": 69930495
    },
    {
      "context_text": "Recently, in view of the rapid development of knowledge graph embedding, many researchers have attempted to utilize embedding-based models for the entity alignment problem [10,15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the general trend of using embedding-based models for entity alignment in knowledge graphs.",
      "processing_time": 56.589022397994995,
      "citing_paper_id": "221193809",
      "cited_paper_id": 86510052
    },
    {
      "context_text": "However, these methods often suÔ¨Äer from the attribute heterogeneity, which makes the entity alignment error-prone [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with entity alignment methods.",
      "processing_time": 54.95756673812866,
      "citing_paper_id": "221193809",
      "cited_paper_id": 69930495
    },
    {
      "context_text": "‚Ä¢ SEA [10] served as a semi-supervised method realizes the adversarial training with an awareness of the degree difference and leverages both labeled entities and the abundant unlabeled entity information for the alignment.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SEA) and its application in semi-supervised entity alignment. No verifiable datasets are referenced.",
      "processing_time": 57.36298489570618,
      "citing_paper_id": "221193809",
      "cited_paper_id": 86510052
    },
    {
      "context_text": "SEA [10] utilizes an awareness of the degree diÔ¨Äerence in adversarial training and incorporates the unaligned entities to enhance the performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the technique used in the paper rather than a dataset.",
      "processing_time": 57.193347692489624,
      "citing_paper_id": "221193809",
      "cited_paper_id": 86510052
    },
    {
      "context_text": "Additionally, soft alignment and multi-step relation paths are utilized to align entities from diÔ¨Äerent KGs. ‚Ä¢ SEA [10] served as a semi-supervised method realizes the adversarial training with an awareness of the degree diÔ¨Äerence and leverages both labeled entities and the abundant unlabeled entity information for the alignment.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SEA) and its application in entity alignment. No verifiable datasets are referenced.",
      "processing_time": 56.74919676780701,
      "citing_paper_id": "221193809",
      "cited_paper_id": 86510052
    },
    {
      "context_text": "SEA [10] utilizes an awareness of the degree difference in adversarial training and incorporates the unaligned entities to enhance the performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the technique used in the paper rather than a dataset.",
      "processing_time": 57.186519622802734,
      "citing_paper_id": "221193809",
      "cited_paper_id": 86510052
    },
    {
      "context_text": "In addition to the conventional methods, there are also some models using neural networks to produce KGE with remarkable effects, such as R-GCN [29], ConvE [30], ConvKB [31], KBGAT [32], and A2N [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models (R-GCN, ConvE, ConvKB, KBGAT, A2N) but does not refer to any specific datasets. The context is focused on describing models and their effects.",
      "processing_time": 60.09380793571472,
      "citing_paper_id": "258509157",
      "cited_paper_id": 3882054
    },
    {
      "context_text": "In addition to the conventional methods, there are also some models using neural networks to produce KGE with remarkable effects, such as R-GCN [29], ConvE [30], ConvKB [31], KBGAT [32], and A2N [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models (R-GCN, ConvE, ConvKB, KBGAT, A2N) but does not refer to any specific datasets. The context is focused on describing models and their effects.",
      "processing_time": 60.09380793571472,
      "citing_paper_id": "258509157",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "In addition to the conventional methods, there are also some models using neural networks to produce KGE with remarkable effects, such as R-GCN [29], ConvE [30], ConvKB [31], KBGAT [32], and A2N [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models (R-GCN, ConvE, ConvKB, KBGAT, A2N) but does not refer to any specific datasets. The context is focused on describing models and their effects.",
      "processing_time": 60.09380793571472,
      "citing_paper_id": "258509157",
      "cited_paper_id": 174797737
    },
    {
      "context_text": "‚Ä¢ The linguistic embeddings of entities are learned by adopting the word2vec [43] technique.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the word2vec technique but does not refer to a specific dataset. It is used as a method for learning linguistic embeddings.",
      "processing_time": 56.51532864570618,
      "citing_paper_id": "258509157",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "For instance, some representative methods are RESCAL [24], DistMult [25], ComplEx [11], and HolE [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods but does not refer to any specific datasets. The context is focused on describing methods used in knowledge graph embeddings.",
      "processing_time": 56.484668493270874,
      "citing_paper_id": "258509157",
      "cited_paper_id": 6071257
    },
    {
      "context_text": "‚Ä¶next step is to fuse the aligned multi-modal embeddings ( ÀÜ E I and ÀÜ E V ) with the structural embedding E S utilizing the strategy inspired by [38]: where E F is the uniÔ¨Åed representation; W ( E i , E ) is the W pp distance between the distribution of E i and that of E ; Œª i represents the‚Ä¶",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing multi-modal embeddings. The cited paper title 'Barycenters in the Wasserstein Space' suggests a methodological approach rather than a dataset.",
      "processing_time": 59.20803618431091,
      "citing_paper_id": "258509157",
      "cited_paper_id": 8592977
    },
    {
      "context_text": "There includes multi-modal datasets: WN9-IMG [41] and FB-IMG [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG",
        "FB-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to evaluate multi-modal knowledge graph representation learning, focusing on image-embodied knowledge representation and translation-based approaches.",
        "FB-IMG": "Used to evaluate multi-modal knowledge graph representation learning, focusing on image-embodied knowledge representation and translation-based approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, WN9-IMG and FB-IMG, which are used in the context of multi-modal knowledge graph reasoning.",
      "processing_time": 69.94398593902588,
      "citing_paper_id": "258509157",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "There includes multi-modal datasets: WN9-IMG [41] and FB-IMG [19].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG",
        "FB-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to evaluate multi-modal knowledge graph representation learning, focusing on image-embodied knowledge representation and translation-based approaches.",
        "FB-IMG": "Used to evaluate multi-modal knowledge graph representation learning, focusing on image-embodied knowledge representation and translation-based approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, WN9-IMG and FB-IMG, which are used in the context of multi-modal knowledge graph reasoning.",
      "processing_time": 69.94398593902588,
      "citing_paper_id": "258509157",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "‚Ä¶embedding E I can be transformed into target-aligned embedding ÀÜ E I with the barycenter-based strategies [34, 35, 36]: Once this has been done, the transported linguistic embedding ÀÜ E I is in the same space as structural embedding E S , where ‚àÜ T is an adjustable transport parameter [37].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and strategies for transforming embeddings. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.73144268989563,
      "citing_paper_id": "258509157",
      "cited_paper_id": 11198605
    },
    {
      "context_text": "As for the WN9-IMG dataset, we take the VGG19 [45] model to learn visual embeddings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to learn visual embeddings using the VGG19 model, focusing on multi-modal reasoning tasks involving image and word pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for multi-modal learning tasks. The citation intent is to describe the use of a reusable resource.",
      "processing_time": 65.62261247634888,
      "citing_paper_id": "258509157",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "‚Ä¢ The visual embeddings of entities are learned by pre-trained VGG [45] models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained VGG models but does not specify a dataset. VGG is a model, not a dataset, and no other datasets are mentioned.",
      "processing_time": 57.26038599014282,
      "citing_paper_id": "258509157",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "We compare our method to some representative models, e.g. , multi-modal KGE methods including IKRL [16], TBKGE [19], TransAE [15] and MMKRL [20]; uni-modal KGE methods including TransE [9], DistMult [25] ComplEx [11], and RotatE [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.60488677024841,
      "citing_paper_id": "258509157",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "We compare our method to some representative models, e.g. , multi-modal KGE methods including IKRL [16], TBKGE [19], TransAE [15] and MMKRL [20]; uni-modal KGE methods including TransE [9], DistMult [25] ComplEx [11], and RotatE [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.60488677024841,
      "citing_paper_id": "258509157",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "To tackle this problem, [19] proposes to take the sum of sub-energy functions as the overall score function, which aims to utilize the textual information, visual information, and structured KGs simultaneously.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for combining textual, visual, and structured KG information.",
      "processing_time": 55.488365650177,
      "citing_paper_id": "258509157",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Notice that both structural embeddings and multi-modal embeddings of the entity are always assumed to obey the same true score function f ‚àó : Z ‚Üí Y in multi-modal KGE problems [18, 16, 19, 15, 20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the concept of multi-modal embeddings in knowledge graph representation learning.",
      "processing_time": 55.66894245147705,
      "citing_paper_id": "258509157",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Notice that both structural embeddings and multi-modal embeddings of the entity are always assumed to obey the same true score function f ‚àó : Z ‚Üí Y in multi-modal KGE problems [18, 16, 19, 15, 20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the concept of multi-modal embeddings in knowledge graph representation learning.",
      "processing_time": 55.66894245147705,
      "citing_paper_id": "258509157",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "To that end, the study of the multi-modal KGE raises a new wave, and a great many meaningful work [18, 16, 19, 15, 20] emerges and achieves remarkable success.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research. No verifiable resources are named.",
      "processing_time": 55.727996587753296,
      "citing_paper_id": "258509157",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "To that end, the study of the multi-modal KGE raises a new wave, and a great many meaningful work [18, 16, 19, 15, 20] emerges and achieves remarkable success.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research. No verifiable resources are named.",
      "processing_time": 55.727996587753296,
      "citing_paper_id": "258509157",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "In the past decades, with the emergence and development of numerous knowledge graphs (KGs) [1, 2], a spectrum of related applications has been widely facilitated, e.g. , question answering [3, 4], semantic search [5, 6], and recommendation systems [7, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of knowledge graphs. No verifiable resources are identified.",
      "processing_time": 55.716540813446045,
      "citing_paper_id": "258509157",
      "cited_paper_id": 62841652
    },
    {
      "context_text": "For instance, MMRFAN [17] and MKRL [14] adopt the uniÔ¨Åed space to learn the textual knowledge and visual knowledge of entities and thus realize the multi-modal KGE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MMRFAN and MKRL as methods for multi-modal knowledge graph embedding, but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 58.15845060348511,
      "citing_paper_id": "258509157",
      "cited_paper_id": 86505256
    },
    {
      "context_text": "For instance, MMRFAN [17] and MKRL [14] adopt the uniÔ¨Åed space to learn the textual knowledge and visual knowledge of entities and thus realize the multi-modal KGE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MMRFAN and MKRL as methods for multi-modal knowledge graph embedding, but does not specify any datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 58.15845060348511,
      "citing_paper_id": "258509157",
      "cited_paper_id": 222278475
    },
    {
      "context_text": "In recent years, researchers [14, 15, 16, 17] point out that knowledge extracted from such multi-modal data can signiÔ¨Åcantly improve the quality of the learned knowledge graph representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multi-modal data. No clear, verifiable resource names are provided.",
      "processing_time": 57.03492283821106,
      "citing_paper_id": "258509157",
      "cited_paper_id": 86505256
    },
    {
      "context_text": "In recent years, researchers [14, 15, 16, 17] point out that knowledge extracted from such multi-modal data can signiÔ¨Åcantly improve the quality of the learned knowledge graph representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multi-modal data. No clear, verifiable resource names are provided.",
      "processing_time": 57.03492283821106,
      "citing_paper_id": "258509157",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "Then TransAE [15] adds a multi-modal autoencoder based on TransE, which can achieve the goal to obtain the embeddings of multi-modal KGE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransAE) that enhances representation learning for knowledge graphs using multimodal data.",
      "processing_time": 56.73103213310242,
      "citing_paper_id": "258509157",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "In recent years, researchers [14, 15, 16, 17] point out that knowledge extracted from such multi-modal data can significantly improve the quality of the learned knowledge graph representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to multi-modal data. No clear, verifiable resource names are provided.",
      "processing_time": 57.028738260269165,
      "citing_paper_id": "258509157",
      "cited_paper_id": 222278475
    },
    {
      "context_text": "Unfortunately, these models such as MMRFAN are designed for some speciÔ¨Åc cases ( e.g., medical knowledge graphs), and they may be not suitable for other general graph structures [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMRFAN but does not refer to a specific dataset. It discusses the limitations of the model for general graph structures.",
      "processing_time": 56.59779596328735,
      "citing_paper_id": "258509157",
      "cited_paper_id": 222278475
    },
    {
      "context_text": "For instance, MMRFAN [17] and MKRL [14] adopt the unified space to learn the textual knowledge and visual knowledge of entities and thus realize the multi-modal KGE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMRFAN and MKRL as methods for multi-modal knowledge graph embedding, but does not refer to any specific datasets.",
      "processing_time": 56.41236639022827,
      "citing_paper_id": "258509157",
      "cited_paper_id": 222278475
    },
    {
      "context_text": "Neural network models [7, 15, 23, 45] encode embeddings of entities and relations with linear or bilinear neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neural network models and their use in encoding embeddings. No verifiable resources are identified.",
      "processing_time": 56.71162223815918,
      "citing_paper_id": "250118042",
      "cited_paper_id": 715463
    },
    {
      "context_text": "Neural network models [7, 15, 23, 45] encode embeddings of entities and relations with linear or bilinear neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neural network models and their use in encoding embeddings. No verifiable resources are identified.",
      "processing_time": 56.71162223815918,
      "citing_paper_id": "250118042",
      "cited_paper_id": 19370455
    },
    {
      "context_text": "SE [7] is the first neural network model to learn knowledge graph embedding based on linear neural network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (SE [7]). The context is about the model's capability to learn knowledge graph embeddings.",
      "processing_time": 57.20414113998413,
      "citing_paper_id": "250118042",
      "cited_paper_id": 715463
    },
    {
      "context_text": "The results are presented in Figure 7 and it can be observed that as the number of decomposition factors increases, the result is not sensitive.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the sensitivity of results to the number of decomposition factors.",
      "processing_time": 56.70329523086548,
      "citing_paper_id": "250118042",
      "cited_paper_id": 715463
    },
    {
      "context_text": "Convolutional neural network (CNN) -based models [3, 13, 27] combine entity and relation embeddings and use convolutional neural network which extracts the feature of factual triplets to describe the correlation between entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of CNN-based models for combining entity and relation embeddings.",
      "processing_time": 57.198474407196045,
      "citing_paper_id": "250118042",
      "cited_paper_id": 3882054
    },
    {
      "context_text": "ConvKB [27] is similar to ConvE, but changes the resizing operation in ConvE into concatenation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ConvKB) which is compared to another method (ConvE).",
      "processing_time": 56.691046476364136,
      "citing_paper_id": "250118042",
      "cited_paper_id": 3882054
    },
    {
      "context_text": "We evaluate our HRGAT on four benchmark datasets, which are FB15k-237 [38], WN18RR [13], DB15K [24], and YAGO15K [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237",
        "WN18RR",
        "DB15K",
        "YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15k-237": "Used to evaluate the HRGAT model, focusing on link prediction in knowledge graphs with complex relational structures.",
        "WN18RR": "Used to assess the performance of HRGAT on wordnet relations, emphasizing the model's ability to handle hierarchical and semantic relationships.",
        "DB15K": "Used to test the HRGAT model's effectiveness in cross-lingual knowledge graph alignment, specifically between English and Japanese.",
        "YAGO15K": "Used to evaluate the HRGAT model on a large-scale knowledge graph, focusing on entity and relation prediction in a diverse and richly structured dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions four specific datasets used to evaluate the HRGAT model. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 85.90099620819092,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5378837
    },
    {
      "context_text": "‚Ä¢ We conduct extensive experiments on four challenging datasets FB15K-237 [38], DB15K [24], YAGO15K [24] and WN18RR [6], which show that our method achieves the state-of-the-art and obtains a faster convergence speed on multi-modal knowledge graph.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K-237",
        "DB15K",
        "YAGO15K",
        "WN18RR"
      ],
      "dataset_descriptions": {
        "FB15K-237": "Used to evaluate the proposed method's performance on multi-modal knowledge graph reasoning, focusing on state-of-the-art results and faster convergence.",
        "DB15K": "Used to assess the effectiveness of the proposed method in multi-modal knowledge graph reasoning, emphasizing state-of-the-art performance and convergence speed.",
        "YAGO15K": "Used to test the proposed method's capabilities in multi-modal knowledge graph reasoning, highlighting state-of-the-art results and faster convergence.",
        "WN18RR": "Used to validate the proposed method's performance on multi-modal knowledge graph reasoning, demonstrating state-of-the-art results and improved convergence speed."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions four specific datasets used for evaluating the performance of the proposed method on multi-modal knowledge graph reasoning tasks.",
      "processing_time": 85.44296050071716,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5378837
    },
    {
      "context_text": "‚Ä¢ R-GCN [32]: R-GCN applies relational graph convolution for multi-relational data such as knowledge graph.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions R-GCN, which is a method for processing multi-relational data like knowledge graphs, but does not mention any specific dataset.",
      "processing_time": 56.55920052528381,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "We have followed the original division ratio in R-GCN [32] on datasets FB15k-237 and WN18RR, and split DB15K and YAGO15K with the same ratio as FB15k-237.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237",
        "WN18RR",
        "DB15K",
        "YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15k-237": "Used to train and evaluate relational graph convolutional networks, focusing on entity and relation prediction tasks with a specific division ratio.",
        "WN18RR": "Used to train and evaluate relational graph convolutional networks, focusing on entity and relation prediction tasks with a specific division ratio.",
        "DB15K": "Split using the same ratio as FB15k-237 to train and evaluate relational graph convolutional networks, focusing on entity and relation prediction tasks.",
        "YAGO15K": "Split using the same ratio as FB15k-237 to train and evaluate relational graph convolutional networks, focusing on entity and relation prediction tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training and evaluation in the context of graph convolutional networks. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 88.86756086349487,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Graph neural network (GNN)-based models [26, 32, 33, 40] consider the natural graph structure information and use graph neural network to update the representation of entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide additional context about datasets.",
      "processing_time": 56.83940076828003,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Graph neural network (GNN)-based models [26, 32, 33, 40] consider the natural graph structure information and use graph neural network to update the representation of entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide additional context about datasets.",
      "processing_time": 56.83940076828003,
      "citing_paper_id": "250118042",
      "cited_paper_id": 174797737
    },
    {
      "context_text": "Graph neural network (GNN)-based models [26, 32, 33, 40] consider the natural graph structure information and use graph neural network to update the representation of entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide additional context about datasets.",
      "processing_time": 56.83940076828003,
      "citing_paper_id": "250118042",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "Graph neural network (GNN)-based models [26, 32, 33, 40] consider the natural graph structure information and use graph neural network to update the representation of entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide additional context about datasets.",
      "processing_time": 56.83940076828003,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "Shang et al. [33] propose weighted GCN (WGCN) to simplify the aggregation operation in R-GCN.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WGCN) and a reference to R-GCN. No verifiable resources are identified.",
      "processing_time": 57.28172206878662,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Shang et al. [33] propose weighted GCN (WGCN) to simplify the aggregation operation in R-GCN.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WGCN) and a reference to R-GCN. No verifiable resources are identified.",
      "processing_time": 57.28172206878662,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "We use the decoder ConvE like the previous knowledge graph embedding method with graph neural network, which has been demonstrated to be the best simple decoder in [32] and [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvE) used in knowledge graph embedding. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 57.57841897010803,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "We use the decoder ConvE like the previous knowledge graph embedding method with graph neural network, which has been demonstrated to be the best simple decoder in [32] and [33].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ConvE) used in knowledge graph embedding. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 57.57841897010803,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "R-GCN [32] is the first graph neural network for knowledge graph embedding which achieves individual relation embedding during graph convolution operation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (R-GCN). The context focuses on the method's capability in knowledge graph embedding.",
      "processing_time": 57.04913258552551,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Relational-graph attention network [26] adds relational attention into R-GCN and learns important score of neighbor nodes to the target node.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (relational-graph attention network) and its application to graph convolutional networks.",
      "processing_time": 56.60615563392639,
      "citing_paper_id": "250118042",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "Relational-graph attention network [26] adds relational attention into R-GCN and learns important score of neighbor nodes to the target node.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (relational-graph attention network) and its application to graph convolutional networks.",
      "processing_time": 56.60615563392639,
      "citing_paper_id": "250118042",
      "cited_paper_id": 174797737
    },
    {
      "context_text": "Early multi-modal knowledge graph embedding methods [44, 51] utilize concatenation or attention to capture the multi-modal information, which ignore the intra-modality and inter-modality dynamics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods for multi-modal knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 56.91585183143616,
      "citing_paper_id": "250118042",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "However, the above multi-modal knowledge graph embedding methods still cannot deal with the following challenges: (1) Entities with multi-modal information have intra-modal and intermodal dynamics, but traditional methods [44, 51] with simple concatenation or attention cannot deal with this property.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and challenges. There are no clear identifiers for datasets in the text.",
      "processing_time": 56.60170221328735,
      "citing_paper_id": "250118042",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "IKLR [44] proposes a knowledge representation learning model based on image embedding to realize cross-modal knowledge graph representation learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (IKLR) that uses image embedding for cross-modal knowledge graph representation learning, but does not specify a dataset.",
      "processing_time": 57.56199383735657,
      "citing_paper_id": "250118042",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "[44] firstly introduce visual modal to knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only the introduction of visual modality to knowledge graphs.",
      "processing_time": 56.30013084411621,
      "citing_paper_id": "250118042",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Note that, although there are some other multi-modal knowledge graph embedding methods such as IKLR [44] and MKHAN [51], they are all domain-specific methods, and all benchmark datasets we have used come from common knowledge graph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to domain-specific methods and a general statement about benchmark datasets from common knowledge graphs.",
      "processing_time": 57.552735805511475,
      "citing_paper_id": "250118042",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "The embedding space can also be complex space [39], multi-dimensional Gaussian distribution space [43], hyperbolic space [49], and manifold space [16].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to different types of embedding spaces, which are methods or approaches rather than datasets.",
      "processing_time": 57.72906827926636,
      "citing_paper_id": "250118042",
      "cited_paper_id": 11091552
    },
    {
      "context_text": "The embedding space can also be complex space [39], multi-dimensional Gaussian distribution space [43], hyperbolic space [49], and manifold space [16].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to different types of embedding spaces, which are methods or approaches rather than datasets.",
      "processing_time": 57.72906827926636,
      "citing_paper_id": "250118042",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "The embedding space can also be complex space [39], multi-dimensional Gaussian distribution space [43], hyperbolic space [49], and manifold space [16].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to different types of embedding spaces, which are methods or approaches rather than datasets.",
      "processing_time": 57.72906827926636,
      "citing_paper_id": "250118042",
      "cited_paper_id": 238244608
    },
    {
      "context_text": "Specifically, translation-based models [6, 16, 22, 37, 39, 43] map entities to different representation spaces, and then use mathematical operations in the space, such as summation, translation, and rotation, to describe the relationship between entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.25711750984192,
      "citing_paper_id": "250118042",
      "cited_paper_id": 11091552
    },
    {
      "context_text": "Specifically, translation-based models [6, 16, 22, 37, 39, 43] map entities to different representation spaces, and then use mathematical operations in the space, such as summation, translation, and rotation, to describe the relationship between entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.25711750984192,
      "citing_paper_id": "250118042",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "VGG-16 is a convolutional neural network model that contains a feature module and a classification module.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG-16, which is a model, not a dataset. No datasets are mentioned in the citation span.",
      "processing_time": 57.038963079452515,
      "citing_paper_id": "250118042",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Note that the VGG-16 and BERT we used are both based on the pre-trained models.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG-16 and BERT, which are models, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 57.63416862487793,
      "citing_paper_id": "250118042",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Note that the VGG-16 and BERT we used are both based on the pre-trained models.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGG-16 and BERT, which are models, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 57.63416862487793,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We use VGG-16 [34] for visual information decoding.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions VGG-16, which is a model, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 57.032756090164185,
      "citing_paper_id": "250118042",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "The input to VGG-16 is of fixed size 224 √ó 224 RGB images and we get the 4096-dimensional embeddings e v for visual information of entities.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the input size and output embeddings of the VGG-16 model.",
      "processing_time": 57.588189363479614,
      "citing_paper_id": "250118042",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "ANALOGY [23] conducts analogical inference in a differentiable manner to explicitly model analogical structure in knowledge graph embedding.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for analogical inference in knowledge graphs.",
      "processing_time": 55.78325319290161,
      "citing_paper_id": "250118042",
      "cited_paper_id": 19370455
    },
    {
      "context_text": "An et al. [1] introduce sentence-level textual description to knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing knowledge graphs with textual descriptions.",
      "processing_time": 56.072981119155884,
      "citing_paper_id": "250118042",
      "cited_paper_id": 44113572
    },
    {
      "context_text": "An et al. [1] and Yao et al. [46] add sentence-level description to entities for better representation and use BERT [14] to encode the text modal information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of BERT for encoding text modal information, but BERT is a model, not a dataset. No specific datasets are mentioned.",
      "processing_time": 57.95889616012573,
      "citing_paper_id": "250118042",
      "cited_paper_id": 44113572
    },
    {
      "context_text": "An et al. [1] and Yao et al. [46] add sentence-level description to entities for better representation and use BERT [14] to encode the text modal information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of BERT for encoding text modal information, but BERT is a model, not a dataset. No specific datasets are mentioned.",
      "processing_time": 57.95889616012573,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "[46] add sentence-level description to entities for better representation and use BERT [14] to encode the text modal information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using BERT for encoding text modal information, but BERT is a model, not a dataset. No specific datasets are mentioned.",
      "processing_time": 57.572160482406616,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Inspired by MMEA [9], we treat the numerical information as (key,value) pairs, and use BERT and a fully-connected layer to encode them.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT, which is a model, and does not mention any specific datasets. The citation is primarily about methodological inspiration.",
      "processing_time": 57.41203999519348,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Inspired by MMEA [9], we treat the numerical information as (key,value) pairs, and use BERT and a fully-connected layer to encode them.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT, which is a model, and does not mention any specific datasets. The citation is primarily about methodological inspiration.",
      "processing_time": 57.41203999519348,
      "citing_paper_id": "250118042",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "Recently, pre-trained language models such as bidirectional encoder representations from transformers (BERT) [14] have shown great success in natural language processing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT).",
      "processing_time": 55.02054738998413,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Firstly, we use BERT to get the 768-dimensional embeddings of attribute key k and use a 768-dimensional fully-connected layer to get the embeddings of numerical value (cid:2) v .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for embedding generation. BERT itself is a model, not a dataset.",
      "processing_time": 57.70291829109192,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "KG-BERT [46] treats the factual triplets as textual sequences and encodes the representation of knowledge graph by transformers (BERT) [14].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of BERT for encoding knowledge graphs.",
      "processing_time": 57.323094606399536,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The pre-trained BERT in our work is achieved by Sentence-BERT [31].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and Sentence-BERT, which are models, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 57.3164336681366,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The original English-language BERT model is a 110MB parameter neural network architecture that has 12 transformer layers, 768 hidden units and 12 multi-heads self-attention units.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the BERT model architecture, which is a method, not a dataset.",
      "processing_time": 57.87062382698059,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We collect the textual information for entities of FB15k-237, WN18RR, DB15K, and YAGO15K from KG-BERT [46].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237",
        "WN18RR",
        "DB15K",
        "YAGO15K"
      ],
      "dataset_descriptions": {
        "FB15k-237": "Used to collect textual information for entities, focusing on improving knowledge graph reasoning through multi-modal data integration.",
        "WN18RR": "Used to collect textual information for entities, focusing on improving knowledge graph reasoning through multi-modal data integration.",
        "DB15K": "Used to collect textual information for entities, focusing on improving knowledge graph reasoning through multi-modal data integration.",
        "YAGO15K": "Used to collect textual information for entities, focusing on improving knowledge graph reasoning through multi-modal data integration."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used to collect textual information for entities. These datasets are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 82.62706327438354,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "These models can learn contextualized information from sentences so we choose the pre-trained BERT as our textual decoder for the sentence-level entities description.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not indicate it is used as a dataset. It is referred to as a pre-trained model for textual decoding.",
      "processing_time": 57.41266942024231,
      "citing_paper_id": "250118042",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "CompGCN [40] jointly embeds both nodes and relations with a variety of composition operations in a relational graph.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CompGCN). The context focuses on the method's functionality and does not indicate the use of a dataset.",
      "processing_time": 58.472795486450195,
      "citing_paper_id": "250118042",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "Therefore, we reproduce some classic algorithm to verify the effectiveness of our method, and we utilize DGL-KE [53] and classical multi-modal knowledge graph method KBLRN [17] as our baselines.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DGL-KE but does not indicate it is used as a dataset. It is referenced as a baseline method for training knowledge graph embeddings.",
      "processing_time": 58.06648397445679,
      "citing_paper_id": "250118042",
      "cited_paper_id": 215828509
    },
    {
      "context_text": "MDCN [52] introduces a multi-scale dynamic convolutional network model for knowledge graph embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (MDCN) rather than a dataset. There are no specific, verifiable datasets mentioned in the citation context.",
      "processing_time": 57.57464599609375,
      "citing_paper_id": "250118042",
      "cited_paper_id": 226601508
    },
    {
      "context_text": "‚Ä¢ MDCN [52]: MDCN is a CNN-based model, which generates the weights of dynamic convolution filters related to each relation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model called MDCN. The context focuses on describing the model's functionality rather than its application to a particular dataset.",
      "processing_time": 58.74254751205444,
      "citing_paper_id": "250118042",
      "cited_paper_id": 226601508
    },
    {
      "context_text": "Besides, graph neural networks [11, 12, 20, 47] have been used for modeling different types of relations on graph edges and nodes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to graph neural networks and their applications.",
      "processing_time": 55.94592618942261,
      "citing_paper_id": "250118042",
      "cited_paper_id": 232417873
    },
    {
      "context_text": "‚Ä¢ JointE [54]: JointE utilizes 1D and 2D convolution operations jointly to achieve knowledge graph embedding effectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called JointE. The context focuses on the methodology and its effectiveness in knowledge graph embedding.",
      "processing_time": 57.92020869255066,
      "citing_paper_id": "250118042",
      "cited_paper_id": 245788482
    },
    {
      "context_text": "JointE [54] utilizes 1D and 2D convolution operations jointly to capture correlation between entities and relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (JointE) and its operations. No verifiable resources are identified.",
      "processing_time": 57.389620780944824,
      "citing_paper_id": "250118042",
      "cited_paper_id": 245788482
    },
    {
      "context_text": "[33] propose weighted GCN (WGCN) to simplify the aggregation operation in R-GCN.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WGCN). The context is about simplifying the aggregation operation in R-GCN, which is a method, not a dataset.",
      "processing_time": 59.245694398880005,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "MRR of WGCN and ConvE is only 0.091 and 0.062, respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance metrics. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.752479791641235,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "From the results of the first ten baselines (TransE to WGCN), HRGAT outperforms all traditional knowledge graph embedding methods.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance among various knowledge graph embedding methods.",
      "processing_time": 56.70757699012756,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "We also find out that our method gets a faster convergence speed compared with the CNN-based model ConvE [13] and the GNN-based model WGCN [33] on the FB15k-237 dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k-237"
      ],
      "dataset_descriptions": {
        "FB15k-237": "Used to compare convergence speeds of the proposed method with CNN-based ConvE and GNN-based WGCN models, focusing on knowledge graph reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the FB15k-237 dataset for comparing convergence speeds of different models. The dataset is a well-known benchmark for knowledge graph reasoning.",
      "processing_time": 68.14120197296143,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "KBLRN is only worse than the GNN-based method WGCN in baselines, which verifies the effect of multi-modal information again.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between KBLRN and WGCN methods. No verifiable resources are identified.",
      "processing_time": 58.42675161361694,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "We set the hyper-parameters of decoder ConvE as the same with in WGCN [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (ConvE) and a model (WGCN). The context is about setting hyper-parameters, which is not indicative of dataset usage.",
      "processing_time": 59.81483721733093,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "‚Ä¢ WGCN [33]: WGCN weighs the different types of relations differently when aggregating and the weights are adaptively learned during the training of the network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WGCN) and its functionality. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.73452877998352,
      "citing_paper_id": "250118042",
      "cited_paper_id": 262690390
    },
    {
      "context_text": "We employ the dataset split of the GoodNews dataset in [12], where the training, validation and testing splits contain 445,259 images, 19,448 images and 24,461 images, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoodNews"
      ],
      "dataset_descriptions": {
        "GoodNews": "Used to train, validate, and test entity-aware image captioning models for news images, focusing on context-driven caption generation with specific splits for training, validation, and testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'GoodNews dataset' which is a specific, verifiable dataset used for image caption generation in news contexts. The dataset is clearly identified and its splits are described.",
      "processing_time": 66.5890040397644,
      "citing_paper_id": "236428934",
      "cited_paper_id": 1055111
    },
    {
      "context_text": "We employ the dataset split of the GoodNews dataset in [12], where the training, validation and testing splits contain 445,259 images, 19,448 images and 24,461 images, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoodNews"
      ],
      "dataset_descriptions": {
        "GoodNews": "Used to train, validate, and test entity-aware image captioning models for news images, focusing on context-driven caption generation with specific splits for training, validation, and testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'GoodNews dataset' which is a specific, verifiable dataset used for image caption generation in news contexts. The dataset is clearly identified and its splits are described.",
      "processing_time": 66.5890040397644,
      "citing_paper_id": "236428934",
      "cited_paper_id": 91184120
    },
    {
      "context_text": "We employ the dataset split of the GoodNews dataset in [12], where the training, validation and testing splits contain 445,259 images, 19,448 images and 24,461 images, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoodNews"
      ],
      "dataset_descriptions": {
        "GoodNews": "Used to train, validate, and test entity-aware image captioning models for news images, focusing on context-driven caption generation with specific splits for training, validation, and testing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'GoodNews dataset' which is a specific, verifiable dataset used for image caption generation in news contexts. The dataset is clearly identified and its splits are described.",
      "processing_time": 66.5890040397644,
      "citing_paper_id": "236428934",
      "cited_paper_id": 222278716
    },
    {
      "context_text": "The images, captions and news articles in the GoodNews dataset and the NYTimes800 k dataset are collected from the New York Times, and each image is annotated with one ground-truth caption.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoodNews",
        "NYTimes800k"
      ],
      "dataset_descriptions": {
        "GoodNews": "Used to collect images, captions, and news articles from the New York Times, focusing on the annotation of images with ground-truth captions.",
        "NYTimes800k": "Used to collect images, captions, and news articles from the New York Times, focusing on the annotation of images with ground-truth captions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GoodNews and NYTimes800k, both of which are used for collecting images, captions, and news articles from the New York Times.",
      "processing_time": 74.88694310188293,
      "citing_paper_id": "236428934",
      "cited_paper_id": 1055111
    },
    {
      "context_text": "To reduce the ambiguity in the original text, we perform entity linking [40] to connect the recognized named entities in the news article to Wikipedia entries.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (entity linking) applied to connect named entities in news articles to Wikipedia entries.",
      "processing_time": 58.72010779380798,
      "citing_paper_id": "236428934",
      "cited_paper_id": 2724321
    },
    {
      "context_text": "Attention mechanism is used to selectively focus on certain objects or regions in the image at each time step [3], [14], [15], [16] or model the visual context that evolves over time [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of attention mechanisms in image captioning. No verifiable resources are identified.",
      "processing_time": 58.53833270072937,
      "citing_paper_id": "236428934",
      "cited_paper_id": 3120635
    },
    {
      "context_text": "Attention mechanism is used to selectively focus on certain objects or regions in the image at each time step [3], [14], [15], [16] or model the visual context that evolves over time [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of attention mechanisms in image captioning. No verifiable resources are identified.",
      "processing_time": 58.53833270072937,
      "citing_paper_id": "236428934",
      "cited_paper_id": 108294843
    },
    {
      "context_text": "The loss function used to train the cross-modal entity matching module is formulated as where the pairs ( e (cid:2) , v ) and ( e, v (cid:2) ) denote negative samples, sim( e, v ) denotes the similarity between e and v , and ( x ) + = max( x, 0) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a loss function and training methodology. No verifiable resources are identified.",
      "processing_time": 57.72316217422485,
      "citing_paper_id": "236428934",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Tang et al. [36] improve DTN by adding representation-shared and parameter-shared constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DTN) and improvements to it.",
      "processing_time": 57.410237073898315,
      "citing_paper_id": "236428934",
      "cited_paper_id": 8454173
    },
    {
      "context_text": "D IFFERENT from the conventional image captioning [1], [2], [3], [4], [5], [6], [7] that describes common objects and their relationships, entity-aware image captioning focuses on generating informative descriptions of named entities and speciÔ¨Åc events presented in the images by utilizing the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a distinction between conventional image captioning and entity-aware image captioning. No verifiable resources are named.",
      "processing_time": 58.85853695869446,
      "citing_paper_id": "236428934",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "D IFFERENT from the conventional image captioning [1], [2], [3], [4], [5], [6], [7] that describes common objects and their relationships, entity-aware image captioning focuses on generating informative descriptions of named entities and speciÔ¨Åc events presented in the images by utilizing the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a distinction between conventional image captioning and entity-aware image captioning. No verifiable resources are named.",
      "processing_time": 58.85853695869446,
      "citing_paper_id": "236428934",
      "cited_paper_id": 233689007
    },
    {
      "context_text": "For instance, some methods [31], [32] incorporate images as additional features of the entities in the knowledge graph to learn better entity representations for knowledge graph completion and triple classiÔ¨Åcation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions incorporating images as additional features in knowledge graphs, which is relevant to multi-modal knowledge graph reasoning. However, no specific dataset names are provided.",
      "processing_time": 58.98348665237427,
      "citing_paper_id": "236428934",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Ideally, the image corresponding to a named entity should be representative, i.e. the most salient object in the images reÔ¨Çects the entity.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general requirement for images to be representative of named entities.",
      "processing_time": 57.7131073474884,
      "citing_paper_id": "236428934",
      "cited_paper_id": 10585115
    },
    {
      "context_text": "Weusestandard image captioning metrics, including Bleu-4 [49], METEOR [50], ROUGE-L [51] and CIDEr [52] to evaluate the similarity of the generated captions to the ground-truth captions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics but does not refer to any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 57.7055561542511,
      "citing_paper_id": "236428934",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "V T , E T , where V T and E T denote the entities in the news article and the edges that connects them, we perform information extraction and coreference resolution by utilizing the Stanford CoreNLP toolkit [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Stanford CoreNLP toolkit, which is a software tool, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 58.97966146469116,
      "citing_paper_id": "236428934",
      "cited_paper_id": 14068874
    },
    {
      "context_text": "Graph neural networks [18], [19], [20] are used to model the spatial and semantic relationships between objects in the image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of graph neural networks for modeling spatial and semantic relationships in images.",
      "processing_time": 58.16904830932617,
      "citing_paper_id": "236428934",
      "cited_paper_id": 52304560
    },
    {
      "context_text": "Graph neural networks [18], [19], [20] are used to model the spatial and semantic relationships between objects in the image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of graph neural networks for modeling spatial and semantic relationships in images.",
      "processing_time": 58.16904830932617,
      "citing_paper_id": "236428934",
      "cited_paper_id": 54460890
    },
    {
      "context_text": "The generated multi-modal knowledge graph G M , the input image I and the associated news article T are encoded by the encoder of the entity-aware captioning model.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the components of a model but does not reference any named datasets.",
      "processing_time": 58.625826597213745,
      "citing_paper_id": "236428934",
      "cited_paper_id": 91184120
    },
    {
      "context_text": "The generated multi-modal knowledge graph G M , the input image I and the associated news article T are encoded by the encoder of the entity-aware captioning model.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the components of a model but does not reference any named datasets.",
      "processing_time": 58.625826597213745,
      "citing_paper_id": "236428934",
      "cited_paper_id": 215814392
    },
    {
      "context_text": "Index Terms ‚ÄîImage captioning, named entity, knowledge graph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general terms like 'image captioning' and 'knowledge graph'. No verifiable resources are identified.",
      "processing_time": 58.71138095855713,
      "citing_paper_id": "236428934",
      "cited_paper_id": 91184120
    },
    {
      "context_text": "Index Terms ‚ÄîImage captioning, named entity, knowledge graph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general terms like 'image captioning' and 'knowledge graph'. No verifiable resources are identified.",
      "processing_time": 58.71138095855713,
      "citing_paper_id": "236428934",
      "cited_paper_id": 215814392
    },
    {
      "context_text": "Index Terms ‚ÄîImage captioning, named entity, knowledge graph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general terms like 'image captioning' and 'knowledge graph'. No verifiable resources are identified.",
      "processing_time": 58.71138095855713,
      "citing_paper_id": "236428934",
      "cited_paper_id": 222278716
    },
    {
      "context_text": "Boosting Entity-Aware Image Captioning With Multi-Modal Knowledge Graph Wentian Zhao and Xinxiao Wu , Member, IEEE Abstract ‚ÄîEntity-aware image captioning aims to describe named entities and events related to the image by utilizing the background knowledge in the associated article.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of entity-aware image captioning. No clear, verifiable resources are identified.",
      "processing_time": 58.96330380439758,
      "citing_paper_id": "236428934",
      "cited_paper_id": 201070367
    },
    {
      "context_text": "Zhang et al. [28] uses CLIP [29] to transfer the visual concepts to linguistic space, and Ô¨Åne-tunes BART [30] with multi-modal prompts to generate the entity-aware caption.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of BART and CLIP, but these are models, not datasets. No specific datasets are mentioned or used according to the citation context.",
      "processing_time": 59.52369213104248,
      "citing_paper_id": "236428934",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "Zhang et al. [28] uses CLIP [29] to transfer the visual concepts to linguistic space, and Ô¨Åne-tunes BART [30] with multi-modal prompts to generate the entity-aware caption.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of BART and CLIP, but these are models, not datasets. No specific datasets are mentioned or used according to the citation context.",
      "processing_time": 59.52369213104248,
      "citing_paper_id": "236428934",
      "cited_paper_id": 252782756
    },
    {
      "context_text": "‚ÄúNewsMEP‚Äù designs a contextual promptsmoduletoselectnamedentities,andutilizesthedecoder ofBART[30]togenerateentity-awarecaptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BART) and a tool (NewsMEP). No verifiable datasets are referenced.",
      "processing_time": 59.09674263000488,
      "citing_paper_id": "236428934",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "The embedding is the face embedding extracted by pre-trained FaceNet model [46], and W f ‚àà R 2048 √ó 512 is a learnable parameter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a pre-trained FaceNet model, which is a method, not a dataset.",
      "processing_time": 59.332042932510376,
      "citing_paper_id": "236428934",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "‚Ä¶using the pre-trained ResNet-152, and the output before the last pooling layer is taken and Ô¨Çattened into a matrix The news article T is encoded by the pre-trained RoBERTa [47] into a sequence of subword units { w 1 , w 2 , . . ., w L T } , where L T denotes the length of the subword unit sequence.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 57.425188064575195,
      "citing_paper_id": "236428934",
      "cited_paper_id": 215814392
    },
    {
      "context_text": "SpeciÔ¨Åcally, the Spacy toolkit [53] is used to recognize the named entities in both ground-truth sentences and generated sentences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the Spacy toolkit for named entity recognition but does not refer to any specific dataset. The citation is about a method or tool, not a dataset.",
      "processing_time": 59.507370471954346,
      "citing_paper_id": "236428934",
      "cited_paper_id": 222278716
    },
    {
      "context_text": "Following [12], we count the exact string matches be-tween all the named entities in the ground-truth captions and the generated captions, to calculate the F1 score for the generated named entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for evaluating named entity recognition in image captioning.",
      "processing_time": 57.824567794799805,
      "citing_paper_id": "236428934",
      "cited_paper_id": 222278716
    },
    {
      "context_text": "Kannan et al. [33] construct multi-modal knowledge base to excavate the facts in deep learning literatures.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions constructing a multi-modal knowledge base, which aligns with the topic of multi-modal knowledge graph reasoning. However, the name 'multi-modal knowledge base' is too generic and lacks a specific identifier.",
      "processing_time": 61.546191930770874,
      "citing_paper_id": "236428934",
      "cited_paper_id": 224291855
    },
    {
      "context_text": "Transformer [21] is also employed by recent methods to encode the pair-wise relationship between visual features [5], [7], [22], [23] or syntactic structure of the sentence [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.081177711486816,
      "citing_paper_id": "236428934",
      "cited_paper_id": 231632752
    },
    {
      "context_text": "Transformer [21] is also employed by recent methods to encode the pair-wise relationship between visual features [5], [7], [22], [23] or syntactic structure of the sentence [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.081177711486816,
      "citing_paper_id": "236428934",
      "cited_paper_id": 233689007
    },
    {
      "context_text": "To evaluate the effect of object detection model and face detection model on entity-aware captioning, we conduct additional experiments by using the object detector from VinVL [55] and face detector TinaNet [56] to construct the image sub-graph.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of VinVL and TinaNet, but these are models, not datasets. No specific datasets are mentioned or used according to the given context.",
      "processing_time": 60.1085991859436,
      "citing_paper_id": "236428934",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "More recently, a few end-to-end methods [12], [13], [25], [26] are proposed to generate entity-aware captions in one pass.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to end-to-end methods for generating entity-aware captions.",
      "processing_time": 58.73398518562317,
      "citing_paper_id": "236428934",
      "cited_paper_id": 237431500
    },
    {
      "context_text": "We compare our method with end-to-end entity aware cap-tioning methods, including ‚ÄúICECAP‚Äù [13], ‚ÄúTransform and Tell‚Äù [12], ‚ÄúJoGANIC‚Äù [26] and ‚ÄúNewsMEP‚Äù [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models but does not refer to any specific datasets. The cited papers' titles suggest a focus on news image captioning, which is relevant to multi-modal learning but do not introduce specific datasets.",
      "processing_time": 62.0393168926239,
      "citing_paper_id": "236428934",
      "cited_paper_id": 237431500
    },
    {
      "context_text": "We compare our method with end-to-end entity aware cap-tioning methods, including ‚ÄúICECAP‚Äù [13], ‚ÄúTransform and Tell‚Äù [12], ‚ÄúJoGANIC‚Äù [26] and ‚ÄúNewsMEP‚Äù [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods/models but does not refer to any specific datasets. The cited papers' titles suggest a focus on news image captioning, which is relevant to multi-modal learning but do not introduce specific datasets.",
      "processing_time": 62.0393168926239,
      "citing_paper_id": "236428934",
      "cited_paper_id": 252782756
    },
    {
      "context_text": "Yang et al. [26] integrate templates that describe the key elements of the caption, i.e. who, what, when, where, why and how, to the entity-aware captioning model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating templates into an entity-aware captioning model.",
      "processing_time": 58.59582710266113,
      "citing_paper_id": "236428934",
      "cited_paper_id": 237431500
    },
    {
      "context_text": "[26], AND ‚ÄúBART‚Äù D ENOTES U SING THE D ECODER IN [28]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models or methods (BART, decoder). There are no clear identifiers for datasets.",
      "processing_time": 59.53524875640869,
      "citing_paper_id": "236428934",
      "cited_paper_id": 237431500
    },
    {
      "context_text": "[26], AND ‚ÄúBART‚Äù D ENOTES U SING THE D ECODER IN [28]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models or methods (BART, decoder). There are no clear identifiers for datasets.",
      "processing_time": 59.53524875640869,
      "citing_paper_id": "236428934",
      "cited_paper_id": 252782756
    },
    {
      "context_text": "We also use the decoder of NewsMEP [28] that is initialized by the parameters of BART, denoted as ‚ÄúOurs+BART‚Äù.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NewsMEP) and a model (BART). No verifiable datasets are referenced.",
      "processing_time": 59.7818922996521,
      "citing_paper_id": "236428934",
      "cited_paper_id": 252782756
    },
    {
      "context_text": ":3) e hi , e ri , e ti , we create two directed edges e h i ‚Üí e r i and e r i ‚Üí e t i .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or other resources. It appears to be describing a method or process for creating directed edges in a graph.",
      "processing_time": 60.02039432525635,
      "citing_paper_id": "236428934",
      "cited_paper_id": null
    },
    {
      "context_text": "Du et al. [37] perform Ô¨Åne-grained visual recognition by learning how the object parts interact with each other in multi-granularity fashion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fine-grained visual recognition.",
      "processing_time": 58.23899292945862,
      "citing_paper_id": "236428934",
      "cited_paper_id": null
    },
    {
      "context_text": "Knowledge integration is based on the thought of the translation methods [25, 35, 37], using multiple plausibilities to achieve the integration of heterogeneous knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of knowledge integration and translation methods. No verifiable resources are identified.",
      "processing_time": 59.47828245162964,
      "citing_paper_id": "244222941",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "Knowledge integration is based on the thought of the translation methods [25, 35, 37], using multiple plausibilities to achieve the integration of heterogeneous knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of knowledge integration and translation methods. No verifiable resources are identified.",
      "processing_time": 59.47828245162964,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Knowledge integration is based on the thought of the translation methods [25, 35, 37], using multiple plausibilities to achieve the integration of heterogeneous knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of knowledge integration and translation methods. No verifiable resources are identified.",
      "processing_time": 59.47828245162964,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "Knowledge integration is based on the thought of the translation methods [25, 35, 37], using multiple plausibilities to achieve the integration of heterogeneous knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of knowledge integration and translation methods. No verifiable resources are identified.",
      "processing_time": 59.47828245162964,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "WN9-IMG This dataset constructed by Xie et al. [37] is the subset of WN18 [4], which comes from the large lexical knowledge base WordNet [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to construct an image-embodied knowledge representation, focusing on integrating visual and textual information for multi-modal reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'WN9-IMG', which is a specific dataset derived from WN18, a subset of WordNet. It is clearly identified and used in the research.",
      "processing_time": 68.13689661026001,
      "citing_paper_id": "244222941",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "IKRL [37], [25], TransAE [32], and MMKRL have better performance compared to the TransE [5] and TransR [19], which demonstrates that textual representations and visual representations can improve the performance of triple classification.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance of different models in triple classification.",
      "processing_time": 59.75882530212402,
      "citing_paper_id": "244222941",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "IKRL [37], [25], TransAE [32], and MMKRL have better performance compared to the TransE [5] and TransR [19], which demonstrates that textual representations and visual representations can improve the performance of triple classification.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance of different models in triple classification.",
      "processing_time": 59.75882530212402,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "The visual representations of entities are obtained by pre-trained VGG [27] models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions VGG models but does not refer to any specific dataset. VGG is a model, not a dataset, and there is no mention of a dataset being used.",
      "processing_time": 61.02400469779968,
      "citing_paper_id": "244222941",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "For the FB-IMG dataset, textual representations can be obtained by pre-trained word2vec and visual representations can be obtained by using embeddings of the VGG-m-128CNN [6] model.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-IMG"
      ],
      "dataset_descriptions": {
        "FB-IMG": "Used to obtain textual and visual representations for multi-modal reasoning, combining pre-trained word2vec for text and VGG-m-128CNN embeddings for images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the FB-IMG dataset, which is used to obtain textual and visual representations using pre-trained models. The dataset is clearly identified and used for multi-modal reasoning.",
      "processing_time": 69.65636324882507,
      "citing_paper_id": "244222941",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Furthermore, we used the VGG19 [27] model to obtain visual embeddings.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using the VGG19 model, which is a method, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 59.955652952194214,
      "citing_paper_id": "244222941",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Second, the link prediction performance of multi-modal KRL models [25, 32, 35, 36] is obviously better than the single-modal KRL models [5, 12, 19, 33, 43].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a comparison between multi-modal and single-modal KRL models. No verifiable resources are identified.",
      "processing_time": 59.97902250289917,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "Second, the link prediction performance of multi-modal KRL models [25, 32, 35, 36] is obviously better than the single-modal KRL models [5, 12, 19, 33, 43].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a comparison between multi-modal and single-modal KRL models. No verifiable resources are identified.",
      "processing_time": 59.97902250289917,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Second, the link prediction performance of multi-modal KRL models [25, 32, 35, 36] is obviously better than the single-modal KRL models [5, 12, 19, 33, 43].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a comparison between multi-modal and single-modal KRL models. No verifiable resources are identified.",
      "processing_time": 59.97902250289917,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "Translation methods [33, 36] are one of the most valuable intuitions for KRL because of their high efficiency and fewer parameters required.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only translation methods for knowledge graph reasoning. The context is about the value of these methods, not the datasets used.",
      "processing_time": 60.39375114440918,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "Translation methods [33, 36] are one of the most valuable intuitions for KRL because of their high efficiency and fewer parameters required.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only translation methods for knowledge graph reasoning. The context is about the value of these methods, not the datasets used.",
      "processing_time": 60.39375114440918,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "‚Ä¢ The single-modal models, such as TransE [5], TransH [33], TransR [19], TransD [12] and ModE [43], only MMKRL: A robust embedding approach for multi-modal knowledge graph representation learning 7487",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on describing single-modal models for knowledge graph embedding.",
      "processing_time": 59.793349742889404,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "‚Ä¢ The single-modal models, such as TransE [5], TransH [33], TransR [19], TransD [12] and ModE [43], only\nuse structured knowledge graphs to accomplish the knowledge representation learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing single-modal models and their use of structured knowledge graphs.",
      "processing_time": 60.38396334648132,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "Wang et al. [33] proposed TransH, which allows an entity to have different representations under different relationships.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TransH. The context focuses on the method's capability to represent entities differently under different relationships.",
      "processing_time": 60.64986872673035,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "[33] proposed TransH, which allows an entity to have different representations under different relationships.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TransH. The context focuses on the method's capability to represent entities differently under different relationships.",
      "processing_time": 60.646023750305176,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "We attribute this to two points: i) Although our work builds upon the translation methods like other single-model methods [5, 12, 19, 33, 43], we introduce a knowledge alignment scheme and an adversarial training strategy to represent structured KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the introduction of a knowledge alignment scheme and adversarial training strategy.",
      "processing_time": 60.458794832229614,
      "citing_paper_id": "244222941",
      "cited_paper_id": 15027084
    },
    {
      "context_text": "‚Ä¶not able to replicate them, thus we select several available state-of-the-art link prediction algorithms and one variation of MMKRL as the baselines: ‚Ä¢ DKRL [36] [25] defines the energy of a KG triple as the sum of sub-energy functions that use both multi-modal embeddings and structured embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the baseline algorithms used for comparison.",
      "processing_time": 59.432788610458374,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Knowledge integration is Knowledge integratiton Current multi-modal KRL models, such as [25, 32, 35, 36], combine TransE [5] with multi-modal KGs to obtain multi-source representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.76612305641174,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Knowledge integration is Knowledge integratiton Current multi-modal KRL models, such as [25, 32, 35, 36], combine TransE [5] with multi-modal KGs to obtain multi-source representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.76612305641174,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "Some algorithms [32, 36] tend to integrate multi-modal knowledge graphs by translation thought directly; however, they ignore reconstruction errors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to integrating multi-modal knowledge graphs. No verifiable resources are identified.",
      "processing_time": 59.588505268096924,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on raw Hits@10."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
      "processing_time": 67.80933117866516,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on raw Hits@10."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
      "processing_time": 67.80933117866516,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN9-IMG"
      ],
      "dataset_descriptions": {
        "WN9-IMG": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on raw Hits@10."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
      "processing_time": 67.80933117866516,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "Translation methods generally measure a multi-modal triple‚Äôs plausibility by computing the distance between the two entities after a translation of the corresponding relationship, e.g., DKRL [36], TransAE [32] and IKRL [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for measuring multi-modal triples' plausibility but does not mention any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 61.09829235076904,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Translation methods generally measure a multi-modal triple‚Äôs plausibility by computing the distance between the two entities after a translation of the corresponding relationship, e.g., DKRL [36], TransAE [32] and IKRL [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for measuring multi-modal triples' plausibility but does not mention any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 61.09829235076904,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "Translation methods generally measure a multi-modal triple‚Äôs plausibility by computing the distance between the two entities after a translation of the corresponding relationship, e.g., DKRL [36], TransAE [32] and IKRL [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for measuring multi-modal triples' plausibility but does not mention any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 61.09829235076904,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "In particular, the MMKRL proposed in this paper is different from the existing models [25, 32, 35, 36], which means MMKRL can reduce the reconstruction errors of multi-source knowledge after multi-source knowledge integration and enhance the model‚Äôs robustness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the proposed MMKRL model with existing models.",
      "processing_time": 59.84245848655701,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Xie et al. [36] proposed DKRL, which considers the textual information to learn the representations of KGs. DKRL can improve the distinguishing ability of entity representations and realize the representations of new entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DKRL, which is a method for learning representations of knowledge graphs using textual information. No specific dataset is mentioned.",
      "processing_time": 59.419671058654785,
      "citing_paper_id": "244222941",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Following [25], we sampled incorrect triples based on the locally closedworld assumption.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is too vague to infer any specific dataset usage.",
      "processing_time": 59.47667217254639,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Third, the flexibility of existing models [25, 35] is limited because the expressiveness of their models depends on the number of modalities.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the limitations of existing models in handling multiple modalities.",
      "processing_time": 58.674887895584106,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Third, the flexibility of existing models [25, 35] is limited because the expressiveness of their models depends on the number of modalities.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the limitations of existing models in handling multiple modalities.",
      "processing_time": 58.674887895584106,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "[25] is the subset of FB15K [5], which consists of triples extracted from Freebase [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K"
      ],
      "dataset_descriptions": {
        "FB15K": "Used as a subset for training and evaluating knowledge graph representation models, focusing on multimodal translation-based approaches."
      },
      "confidence_score": 0.9,
      "reasoning": "FB15K is a well-known dataset derived from Freebase, commonly used in knowledge graph research. The context indicates it is a subset of FB15K, likely used for training or evaluation.",
      "processing_time": 68.51182150840759,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Knowledge integratiton Current multi-modal KRL models, such as [25, 32, 35, 36], combine TransE [5] with multimodal KGs to obtain multi-source representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the integration of multi-modal knowledge graph representation learning models.",
      "processing_time": 60.15018439292908,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "To integrate the structured knowledge and multi-modal knowledge in a unified space, we follow the proposal of [25] and define the cross-modal plausibility PC as PC = ‚à• ‚à• ‚à•h m + r s ‚àí t s ‚à• ‚à• ‚à• + ‚à• ‚à• ‚à•h s + r s ‚àí t m ‚à• ‚à• ‚à• (11)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating structured and multi-modal knowledge in a unified space.",
      "processing_time": 58.91568970680237,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "[25] proposed a method that regards the total plausibilities as the sum of sub-energy functions that use textual information, visual information, and structured KGs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method that uses textual, visual, and structured KG information.",
      "processing_time": 58.910298109054565,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "Knowledge alignment Several KRL models [25, 35] tend to integrate multi-source knowledge by simply using translation methods; however, they ignore reconstruction errors after knowledge integration, making KRL models challenging to maintain the inherent characteristics of multi-modal KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the limitations of existing KRL models in maintaining the characteristics of multi-modal KGs.",
      "processing_time": 61.44117832183838,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "‚Ä¢ [25] defines the energy of a KG triple as the sum of subenergy functions that use both multi-modal embeddings and structured embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for defining the energy of a KG triple using multi-modal and structured embeddings.",
      "processing_time": 59.81353545188904,
      "citing_paper_id": "244222941",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "According to the WordNet hierarchy, images of entities are obtained from ImageNet [8], which are built on a large image database.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to obtain images of entities based on the WordNet hierarchy, focusing on building a large-scale hierarchical image database."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ImageNet as a large image database used to obtain images of entities based on the WordNet hierarchy. The cited paper title confirms ImageNet is a dataset.",
      "processing_time": 66.70454812049866,
      "citing_paper_id": "244222941",
      "cited_paper_id": 57246310
    },
    {
      "context_text": "‚Ä¢ IKRL [37] considers the visual knowledge to learn the representations of KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or approach (IKRL) that considers visual knowledge for learning KG representations.",
      "processing_time": 59.65718650817871,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "However, DKRL and IKRL either only use textual knowledge or only visual knowledge, ignoring the simultaneous use of two or more multimodal knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (DKRL and IKRL) and their limitations in using multimodal knowledge.",
      "processing_time": 59.739755392074585,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "‚Ä¢ IKRL [37] considers the visual knowledge to learn the representations of KGs. IKRL can learn two separate representations for each entity, one based on structured KGs and the other based on visual images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IKRL) that uses visual knowledge and structured knowledge graphs. No clear, verifiable dataset names are provided.",
      "processing_time": 60.601009130477905,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "Especially in TransAE [32], IKRL [35] and [25], the improvements in MR are pronounced.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There is no indication of a reusable resource being used.",
      "processing_time": 59.3782160282135,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "Following DKRL, IKRL [35] has been proposed to integrate the visual knowledge and structured knowledge with the TransE model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on integrating visual and structured knowledge with the TransE model.",
      "processing_time": 59.97420835494995,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "Following DKRL, IKRL [35] has been proposed to integrate the visual knowledge and structured knowledge with the TransE model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on integrating visual and structured knowledge with the TransE model.",
      "processing_time": 59.97420835494995,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "Different from IKRL [25], MMKRL and [35] can effectively integrate visual knowledge and textual knowledge with structured KGs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 59.57352924346924,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "[37] is the subset of WN18 [4], which comes from the large lexical knowledge base WordNet [21].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WN18",
        "WordNet"
      ],
      "dataset_descriptions": {
        "WN18": "Used as a subset of WordNet for evaluating link prediction in knowledge graphs, focusing on relational patterns and entity types.",
        "WordNet": "Serves as a large lexical knowledge base for constructing and evaluating knowledge graphs, providing a rich structure of semantic relations."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions WN18, which is derived from WordNet. Both are specific, verifiable datasets used in knowledge graph research.",
      "processing_time": 71.64994502067566,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "It can reflect in the link prediction results from Table 3; the performance of MMKRL and [35] are better than IKRL [25], which indicates that more multi-source knowledge can contribute to representing multi-modal KGs.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. No verifiable resources are identified.",
      "processing_time": 58.728530406951904,
      "citing_paper_id": "244222941",
      "cited_paper_id": 67700681
    },
    {
      "context_text": "However, there is still a lot of multi-modal knowledge [2] that has not been effectively used, such as textual descriptions and visual images.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to the general concept of multi-modal knowledge, which is too generic.",
      "processing_time": 60.0873122215271,
      "citing_paper_id": "244222941",
      "cited_paper_id": 85566873
    },
    {
      "context_text": "Multi-modal knowledge graphs in this paper include not only structured knowledge [24] but also multi-modal knowledge [2], e.g., textual entities and visual entities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts of multi-modal knowledge graphs and entities.",
      "processing_time": 58.35987687110901,
      "citing_paper_id": "244222941",
      "cited_paper_id": 85566873
    },
    {
      "context_text": "Most KRL models [22, 40, 43] only use structured knowledge [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'structured knowledge'. No verifiable resources are identified.",
      "processing_time": 59.64163374900818,
      "citing_paper_id": "244222941",
      "cited_paper_id": 203704895
    },
    {
      "context_text": "Most KRL models [22, 40, 43] only use structured knowledge [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'structured knowledge'. No verifiable resources are identified.",
      "processing_time": 59.64163374900818,
      "citing_paper_id": "244222941",
      "cited_paper_id": 228971082
    },
    {
      "context_text": "However, most KRL methods [22, 31, 40, 43] only use structured knowledge for KRL.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that most KRL methods use structured knowledge. No verifiable resources are identified.",
      "processing_time": 60.10525321960449,
      "citing_paper_id": "244222941",
      "cited_paper_id": 203704895
    },
    {
      "context_text": "However, most KRL methods [22, 31, 40, 43] only use structured knowledge for KRL.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that most KRL methods use structured knowledge. No verifiable resources are identified.",
      "processing_time": 60.10525321960449,
      "citing_paper_id": "244222941",
      "cited_paper_id": 228971082
    },
    {
      "context_text": "The most representative KGs, e.g., DBpedia [15], Freebase [3], and WordNet [21], benefit various kinds of downstream tasks, such as semantic analysis [29], question-answer systems [14], and machine comprehension [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia",
        "Freebase",
        "WordNet"
      ],
      "dataset_descriptions": {
        "DBpedia": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided.",
        "Freebase": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided.",
        "WordNet": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided."
      },
      "confidence_score": 0.5,
      "reasoning": "The context mentions DBpedia, Freebase, and WordNet as representative KGs, but does not specify their usage in a particular research context. These are general references to well-known knowledge graphs.",
      "processing_time": 78.02803492546082,
      "citing_paper_id": "244222941",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "The most representative KGs, e.g., DBpedia [15], Freebase [3], and WordNet [21], benefit various kinds of downstream tasks, such as semantic analysis [29], question-answer systems [14], and machine comprehension [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia",
        "Freebase",
        "WordNet"
      ],
      "dataset_descriptions": {
        "DBpedia": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided.",
        "Freebase": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided.",
        "WordNet": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided."
      },
      "confidence_score": 0.5,
      "reasoning": "The context mentions DBpedia, Freebase, and WordNet as representative KGs, but does not specify their usage in a particular research context. These are general references to well-known knowledge graphs.",
      "processing_time": 78.02803492546082,
      "citing_paper_id": "244222941",
      "cited_paper_id": 213399202
    },
    {
      "context_text": "FB-IMG This dataset constructed by Hatem et al. [25] is the subset of FB15K [5], which consists of triples extracted from Freebase [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-IMG"
      ],
      "dataset_descriptions": {
        "FB-IMG": "Used to construct a subset of FB15K, focusing on triples extracted from Freebase for multi-modal knowledge graph reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'FB-IMG', which is a specific dataset derived from FB15K, a well-known knowledge graph dataset. The dataset is described as a subset of FB15K, containing triples extracted from Freebase.",
      "processing_time": 70.98747420310974,
      "citing_paper_id": "244222941",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "Existing studies [28, 32, 35, 41] show multi-modal knowledge can significantly improve the quality of learned knowledge graph representations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that multi-modal knowledge improves knowledge graph representations.",
      "processing_time": 58.34798240661621,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "Existing studies [28, 32, 35, 41] show multi-modal knowledge can significantly improve the performance of KRL models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that multi-modal knowledge improves KRL models. No verifiable resources are identified.",
      "processing_time": 59.76003456115723,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "The process can be defined as Knowledge alignment Several KRL models [25, 35] tend to integrate multi-source knowledge by simply using translation methods; however, they ignore reconstruction errors after knowledge integration, making KRL models challenging to maintain the inherent characteristics‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and challenges in knowledge representation learning.",
      "processing_time": 58.21364665031433,
      "citing_paper_id": "244222941",
      "cited_paper_id": 211137418
    },
    {
      "context_text": "Previous work [34] has primarily applied adversarial training to cross-modal retrieval and relation extraction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (adversarial training) applied to cross-modal retrieval and relation extraction.",
      "processing_time": 59.930864334106445,
      "citing_paper_id": "244222941",
      "cited_paper_id": 216535344
    },
    {
      "context_text": "Recently, knowledge representation learning (KRL) [11, 42] has become the most vital task in the KGs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to knowledge representation learning in knowledge graphs.",
      "processing_time": 58.67897582054138,
      "citing_paper_id": "244222941",
      "cited_paper_id": 219071361
    },
    {
      "context_text": "Besides, Wang et al. [30] proposed a complex internal logic model by integrating the semantic information into the knowledge representation framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the integration of semantic information into a knowledge representation framework.",
      "processing_time": 60.32436537742615,
      "citing_paper_id": "244222941",
      "cited_paper_id": 219330527
    },
    {
      "context_text": "Link prediction [30] is essentially a knowledge graph completion (KGC) task, i.e., predicting an entity that has a specific relationship with another given entity.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task description for link prediction in knowledge graphs.",
      "processing_time": 58.946531534194946,
      "citing_paper_id": "244222941",
      "cited_paper_id": 219330527
    },
    {
      "context_text": "Knowledge Graphs (KGs) [9, 18] describe entities and relationships in the objective world in a structured form.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 59.82617497444153,
      "citing_paper_id": "244222941",
      "cited_paper_id": 220543075
    },
    {
      "context_text": "We follow the idea of entity alignment [7] and introduce a component alignment scheme by using norm constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept related to entity alignment in multi-modal knowledge graphs.",
      "processing_time": 59.396106481552124,
      "citing_paper_id": "244222941",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "We reconstruct the multi-modal knowledge graphs in the knowledge reconstruction module by integrating and aligning the various knowledge embeddings [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general process of integrating and aligning knowledge embeddings.",
      "processing_time": 58.74347972869873,
      "citing_paper_id": "244222941",
      "cited_paper_id": 225287688
    },
    {
      "context_text": "Adversarial training [26, 38] is widely used to enhance the model‚Äôs robustness and generalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only adversarial training methods. No verifiable resources are identified.",
      "processing_time": 59.02945876121521,
      "citing_paper_id": "244222941",
      "cited_paper_id": 232023070
    },
    {
      "context_text": "Compared with the previous methods of generating sample images, FGN [17] directly generates feature representations for unseen class samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FGN) for generating feature representations. No verifiable resources are identified.",
      "processing_time": 59.98491835594177,
      "citing_paper_id": "258220916",
      "cited_paper_id": 179895
    },
    {
      "context_text": "These semantic features of classes include attributes, word vectors or sentence descriptions [4], [6]‚Äì[9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general types of semantic features such as attributes, word vectors, or sentence descriptions.",
      "processing_time": 59.3668999671936,
      "citing_paper_id": "258220916",
      "cited_paper_id": 261138
    },
    {
      "context_text": "Explicit correlation methods ConSE [22] Devise [6] SYNC [19] SE-GZSL [23] Gaussian-Ort [21] GCNZ [11] SGCN [12] DGP [",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models but does not refer to any specific datasets. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.91191625595093,
      "citing_paper_id": "258220916",
      "cited_paper_id": 261138
    },
    {
      "context_text": "Explicit correlation methods ConSE [22] Devise [6] SYNC [19] SE-GZSL [23] Gaussian-Ort [21] GCNZ [11] SGCN [12] DGP [",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several methods/models but does not refer to any specific datasets. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.91191625595093,
      "citing_paper_id": "258220916",
      "cited_paper_id": 1926319
    },
    {
      "context_text": "The implicit correlation baselines include ConSE [22], EXEM [20], SYNC [19], Devise [6], SE-GZSL [23], Gaussian-Ort [21], while explicit correlation baselines include GCNZ [11], SGCN [12], DGP [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not refer to any specific datasets. The context is focused on comparing different baselines for zero-shot learning and multi-modal reasoning.",
      "processing_time": 61.024672508239746,
      "citing_paper_id": "258220916",
      "cited_paper_id": 261138
    },
    {
      "context_text": "The implicit correlation baselines include ConSE [22], EXEM [20], SYNC [19], Devise [6], SE-GZSL [23], Gaussian-Ort [21], while explicit correlation baselines include GCNZ [11], SGCN [12], DGP [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not refer to any specific datasets. The context is focused on comparing different baselines for zero-shot learning and multi-modal reasoning.",
      "processing_time": 61.024672508239746,
      "citing_paper_id": "258220916",
      "cited_paper_id": 1926319
    },
    {
      "context_text": "The implicit correlation baselines include ConSE [22], EXEM [20], SYNC [19], Devise [6], SE-GZSL [23], Gaussian-Ort [21], while explicit correlation baselines include GCNZ [11], SGCN [12], DGP [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not refer to any specific datasets. The context is focused on comparing different baselines for zero-shot learning and multi-modal reasoning.",
      "processing_time": 61.024672508239746,
      "citing_paper_id": "258220916",
      "cited_paper_id": 44075854
    },
    {
      "context_text": "(GCN) [13] is used to extend the classifier for seen-classes to unseen-class samples with correlations as bridge.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GCN) used for extending classifiers. No verifiable resources are identified.",
      "processing_time": 59.57251262664795,
      "citing_paper_id": "258220916",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "Transfer learning has been widely used in CV, NLP, and other Ô¨Åelds [1]‚Äì[3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general fields where transfer learning is applied.",
      "processing_time": 57.78386902809143,
      "citing_paper_id": "258220916",
      "cited_paper_id": 7079167
    },
    {
      "context_text": "Most methods with implicit correlation use generative adversarial network (GAN) [14], [15] to construct samples for unseen-classes, and then use these samples and seen-classes to jointly train the classiÔ¨Åer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (GANs). There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 59.37589383125305,
      "citing_paper_id": "258220916",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Implicit correlation methods Explicit correlation methods ConSE [22] Devise [6] SYNC [19] SE-GZSL [23] Gaussian-Ort [21] GCNZ [11] SGCN [12] DGP [12] Our AWA2 44.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The term 'AWA2' appears but is not clearly identified as a dataset within the given context.",
      "processing_time": 60.69851279258728,
      "citing_paper_id": "258220916",
      "cited_paper_id": 44075854
    },
    {
      "context_text": "constructed based on WordNet [10] in [11], [12] to represent",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to WordNet, which is a lexical database and not a dataset in the context of multi-modal knowledge graph reasoning.",
      "processing_time": 60.47921967506409,
      "citing_paper_id": "258220916",
      "cited_paper_id": 44075854
    },
    {
      "context_text": "To address the redundant Laplace smoothing in GCNZ, a GCN network with only two layers is used in [12], and proposes a DGP method to aggregate a wider range of information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 59.05644512176514,
      "citing_paper_id": "258220916",
      "cited_paper_id": 44075854
    },
    {
      "context_text": "RESULTS DENOTED BY *, ‚Ä†, AND ‚Ä° ARE FROM [19], [20], AND [11], AND RESULTS OF SGCN AND DGP ARE FROM THEIR PUBLIC CODES [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other papers and their public codes. There are no clear identifiers for datasets.",
      "processing_time": 59.29987049102783,
      "citing_paper_id": "258220916",
      "cited_paper_id": 44075854
    },
    {
      "context_text": "FOR AWA2, THE RESULTS OF SE-GZSL, GCNZ, AND DGP ARE FROM [12], AND THE RESULTS OF SGCN ARE GENERATED BY OUR REPRODUCTION OF [12].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AWA2"
      ],
      "dataset_descriptions": {
        "AWA2": "Used to evaluate zero-shot learning models, specifically comparing the performance of SE-GZSL, GCNZ, DGP, and SGCN methods in the context of zero-shot learning."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions AWA2, which is a known dataset used in zero-shot learning research. However, the primary focus is on comparing results from different methods rather than describing the dataset's usage in detail.",
      "processing_time": 70.70390391349792,
      "citing_paper_id": "258220916",
      "cited_paper_id": 44075854
    },
    {
      "context_text": "To improve the quality of synthetic samples, Oral-GMN [16] proposes a new gradient matching loss, which is used to measure the difference between real samples and gradient vectors obtained by synthetic samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Oral-GMN) and its proposed loss function. No verifiable resources are identified.",
      "processing_time": 59.82155776023865,
      "citing_paper_id": "258220916",
      "cited_paper_id": 195779694
    },
    {
      "context_text": "Nowadays, multi-modal data (mainly images) is introduced into Knowledge Graph to enrich the representation of concepts, and increasing efforts are focused on grounding entities or concepts with their corresponding images to construct image-based Multi-modal Knowledge Graph (MMKG) [6‚Äì10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the general trend of integrating multi-modal data into Knowledge Graphs.",
      "processing_time": 59.60703206062317,
      "citing_paper_id": "248435608",
      "cited_paper_id": 1900911
    },
    {
      "context_text": "LEVAN [3] proposed to recognize ‚Äúvisual salient‚Äù words during constructing image dataset.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention a specific dataset name, only a general concept of constructing an image dataset. The context is too vague to identify a specific, verifiable dataset.",
      "processing_time": 60.50200581550598,
      "citing_paper_id": "248435608",
      "cited_paper_id": 7748515
    },
    {
      "context_text": "[3] proposed to filter out non-visualizable concepts based on the visually salient score of concepts.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for filtering concepts based on visual saliency.",
      "processing_time": 58.03851270675659,
      "citing_paper_id": "248435608",
      "cited_paper_id": 7748515
    },
    {
      "context_text": "TinyImage [13] is a MMKG constructed based on WordNet and contains 75k noun concepts with 1,052 images per concept in average.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TinyImage"
      ],
      "dataset_descriptions": {
        "TinyImage": "Used as a multi-modal knowledge graph containing 75k noun concepts with an average of 1,052 images per concept, constructed based on WordNet for nonparametric object and scene recognition."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'TinyImage' as a multi-modal knowledge graph (MMKG) constructed based on WordNet, which is relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 69.56887578964233,
      "citing_paper_id": "248435608",
      "cited_paper_id": 267802767
    },
    {
      "context_text": "For example, TinyImage [13] (which is a MMKG built on WordNet [4]) simply eliminated all abstract concepts in the hierarchical taxonomy of WordNet while it is rough and inaccurate according to our experiments in Section 3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TinyImage"
      ],
      "dataset_descriptions": {
        "TinyImage": "Used to evaluate the accuracy of a multi-modal knowledge graph built on WordNet, focusing on the elimination of abstract concepts and its impact on hierarchical taxonomy."
      },
      "confidence_score": 0.85,
      "reasoning": "TinyImage is mentioned as a multi-modal knowledge graph (MMKG) built on WordNet, but it is described as rough and inaccurate based on the authors' experiments. WordNet is referenced but not as a dataset.",
      "processing_time": 69.79391884803772,
      "citing_paper_id": "248435608",
      "cited_paper_id": 267802767
    },
    {
      "context_text": "For example, TinyImage [13] (which is a MMKG built on WordNet [4]) simply eliminated all abstract concepts in the hierarchical taxonomy of WordNet while it is rough and inaccurate according to our experiments in Section 3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TinyImage"
      ],
      "dataset_descriptions": {
        "TinyImage": "Used to evaluate the accuracy of a multi-modal knowledge graph built on WordNet, focusing on the elimination of abstract concepts and its impact on hierarchical taxonomy."
      },
      "confidence_score": 0.85,
      "reasoning": "TinyImage is mentioned as a multi-modal knowledge graph (MMKG) built on WordNet, but it is described as rough and inaccurate based on the authors' experiments. WordNet is referenced but not as a dataset.",
      "processing_time": 69.79391884803772,
      "citing_paper_id": "248435608",
      "cited_paper_id": null
    },
    {
      "context_text": "We compare our method with 3 baselines: Full set As most previous work of the MMKG construction ignore to distinguish VC and NVC, we design a baseline approach that regards all concepts as VC. TinyImage [13] TinyImage is a MMKG that is constructed based on WordNet hierachy.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TinyImage"
      ],
      "dataset_descriptions": {
        "TinyImage": "Used as a baseline for comparison in constructing a Multi-Modal Knowledge Graph (MMKG), focusing on the distinction between visible and non-visible concepts."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'TinyImage' as a Multi-Modal Knowledge Graph (MMKG) constructed based on WordNet hierarchy. It is used as a baseline for comparison in the research.",
      "processing_time": 68.20806884765625,
      "citing_paper_id": "248435608",
      "cited_paper_id": 267802767
    },
    {
      "context_text": "First, the crowdsourcing-based fact-checking problem has been widely studied by researchers in the HCI community [41, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that the problem of crowdsourcing-based fact-checking has been studied. No verifiable resources are named.",
      "processing_time": 59.87312412261963,
      "citing_paper_id": "253460330",
      "cited_paper_id": 144167
    },
    {
      "context_text": "First, the crowdsourcing-based fact-checking problem has been widely studied by researchers in the HCI community [41, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that the problem of crowdsourcing-based fact-checking has been studied. No verifiable resources are named.",
      "processing_time": 59.87312412261963,
      "citing_paper_id": "253460330",
      "cited_paper_id": 195347125
    },
    {
      "context_text": "Gardner [12] proposed a node-similarity estimation framework to fact-check the relationship between two entities in the graph through a random walk process.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fact-checking relationships in a knowledge graph using a random walk process.",
      "processing_time": 58.807071924209595,
      "citing_paper_id": "253460330",
      "cited_paper_id": 577805
    },
    {
      "context_text": "However, the above knowledge graph based fact-checking solutions [7, 12] can not explain their detection results because they only output a scalar value based on the information stored in knowledge graphs (e.g., total length of edges between two entities).",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of existing knowledge graph-based fact-checking solutions.",
      "processing_time": 58.16622233390808,
      "citing_paper_id": "253460330",
      "cited_paper_id": 577805
    },
    {
      "context_text": "However, the knowledge graphs that are used in current misinformation detection solutions are mainly text-based, which cannot be directly applied to detect and explain multi-modal fauxtography posts [9, 12, 42].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of current knowledge graphs in misinformation detection.",
      "processing_time": 57.124272108078,
      "citing_paper_id": "253460330",
      "cited_paper_id": 577805
    },
    {
      "context_text": "However, the knowledge graphs that are used in current misinformation detection solutions are mainly text-based, which cannot be directly applied to detect and explain multi-modal fauxtography posts [9, 12, 42].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of current knowledge graphs in misinformation detection.",
      "processing_time": 57.124272108078,
      "citing_paper_id": "253460330",
      "cited_paper_id": 245934348
    },
    {
      "context_text": "Recently, a good amount of fact-checking methods have been proposed to leverage different types of knowledge graphs to detect misleading information [7, 9, 11, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to fact-checking methods using knowledge graphs. No verifiable resources are identified.",
      "processing_time": 58.77554488182068,
      "citing_paper_id": "253460330",
      "cited_paper_id": 577805
    },
    {
      "context_text": "Recently, a good amount of fact-checking methods have been proposed to leverage different types of knowledge graphs to detect misleading information [7, 9, 11, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to fact-checking methods using knowledge graphs. No verifiable resources are identified.",
      "processing_time": 58.77554488182068,
      "citing_paper_id": "253460330",
      "cited_paper_id": 53597419
    },
    {
      "context_text": "FaceNet is a stateof-the-art deep learning based face recognition model in the computer vision community, which generates robust face features for both seen and unseen human faces in the wild [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions FaceNet as a model, not a dataset. No specific dataset is referenced in the context.",
      "processing_time": 57.60639190673828,
      "citing_paper_id": "253460330",
      "cited_paper_id": 4951598
    },
    {
      "context_text": ", Amazon MTurk, MicroWorkers) [10, 56, 60, 63] to infer the implicit and complex relations between different humans and non-human subjects in the posts.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only platforms for data collection. No verifiable resources are identified.",
      "processing_time": 57.44229507446289,
      "citing_paper_id": "253460330",
      "cited_paper_id": 4973991
    },
    {
      "context_text": "To solve the problem, we focus on interacting with crowd workers from online crowdsourcing systems (e.g., Amazon MTurk, MicroWorkers) [10, 56, 60, 63] to infer the implicit and complex relations between different humans and non-human subjects in the posts.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to online crowdsourcing systems, which are platforms, not datasets.",
      "processing_time": 58.31608510017395,
      "citing_paper_id": "253460330",
      "cited_paper_id": 4973991
    },
    {
      "context_text": "To solve the problem, we focus on interacting with crowd workers from online crowdsourcing systems (e.g., Amazon MTurk, MicroWorkers) [10, 56, 60, 63] to infer the implicit and complex relations between different humans and non-human subjects in the posts.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to online crowdsourcing systems, which are platforms, not datasets.",
      "processing_time": 58.31608510017395,
      "citing_paper_id": "253460330",
      "cited_paper_id": 207756951
    },
    {
      "context_text": "Examples of popular crowdsourcing systems include Amazon MTurk, MicroWorkers 3 , and Elance 4 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions crowdsourcing systems but does not refer to any specific, verifiable datasets. The context is about describing platforms, not using them for data collection or analysis.",
      "processing_time": 59.72613453865051,
      "citing_paper_id": "253460330",
      "cited_paper_id": 4973991
    },
    {
      "context_text": "While several language algorithms [13, 34] are proposed to explore the implicit relations of different semantic subjects in natural language documents, they are not applicable to explainable fauxtography detection problem because the fauxtography posts usually contain implicit relations between‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and their limitations in the context of fauxtography detection.",
      "processing_time": 57.7044894695282,
      "citing_paper_id": "253460330",
      "cited_paper_id": 7284112
    },
    {
      "context_text": "Crowdsourcing has been leveraged to address research problems in various application domains [6, 22, 25, 52‚Äì55 , 65].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general concept of crowdsourcing. No verifiable resources are identified.",
      "processing_time": 58.038490295410156,
      "citing_paper_id": "253460330",
      "cited_paper_id": 8350875
    },
    {
      "context_text": "Crowdsourcing has been leveraged to address research problems in various application domains [6, 22, 25, 52‚Äì55 , 65].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general concept of crowdsourcing. No verifiable resources are identified.",
      "processing_time": 58.038490295410156,
      "citing_paper_id": "253460330",
      "cited_paper_id": 203598995
    },
    {
      "context_text": "[24] developed a fact-checking service to",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a fact-checking service. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.98986315727234,
      "citing_paper_id": "253460330",
      "cited_paper_id": 14928728
    },
    {
      "context_text": "Our paper is closely related to the research of crowdsourcing and human-AI in human-computer interaction (HCI) community [15, 29, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to related research in crowdsourcing and human-AI in the HCI community.",
      "processing_time": 58.133078813552856,
      "citing_paper_id": "253460330",
      "cited_paper_id": 36593804
    },
    {
      "context_text": "‚Ä¢ EANN [57]: EANN is a recent fake news detection scheme that handles multi-modal content with convolution filters and applies an adversarial loss function to make the model event-invariant.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EANN) for detecting fake news using multi-modal content.",
      "processing_time": 57.91986680030823,
      "citing_paper_id": "253460330",
      "cited_paper_id": 46990556
    },
    {
      "context_text": "FakeNewsNet contains thousands of multi-modal celebrity-related news articles collected from GossipCop 5 while Fakeddit consists of millions of fauxtography posts from Reddit 6 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FakeNewsNet",
        "Fakeddit"
      ],
      "dataset_descriptions": {
        "FakeNewsNet": "Used to study multi-modal celebrity-related news articles, focusing on news content, social context, and dynamic information for fake news detection.",
        "Fakeddit": "Used to analyze millions of fauxtography posts from Reddit, focusing on visual and textual elements to understand the spread of manipulated images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FakeNewsNet and Fakeddit, which are used to study fake news and fauxtography, respectively. Both are relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 73.2077009677887,
      "citing_paper_id": "253460330",
      "cited_paper_id": 52164698
    },
    {
      "context_text": "In particular, we extract all the celebrity names from the articles in FakeNewsNet and utilize them to collect related fauxtography posts from Fakeddit.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FakeNewsNet",
        "Fakeddit"
      ],
      "dataset_descriptions": {
        "FakeNewsNet": "Used to extract celebrity names from news articles, focusing on fake news content and social context on social media platforms.",
        "Fakeddit": "Utilized to collect related fauxtography posts, specifically those associated with the celebrity names extracted from FakeNewsNet."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions using FakeNewsNet and Fakeddit, which are specific datasets. FakeNewsNet is used to extract celebrity names, and Fakeddit is used to collect related fauxtography posts.",
      "processing_time": 72.1464216709137,
      "citing_paper_id": "253460330",
      "cited_paper_id": 52164698
    },
    {
      "context_text": "We create a real-world celebrity-related social media dataset based on the previous public dataset FakeNewsNet [45] and Fakeddit [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FakeNewsNet",
        "Fakeddit"
      ],
      "dataset_descriptions": {
        "FakeNewsNet": "Used as a basis for creating a real-world celebrity-related social media dataset, providing news content, social context, and dynamic information for studying fake news.",
        "Fakeddit": "Used as a basis for creating a real-world celebrity-related social media dataset, contributing additional data for the study of fake news on social media."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the creation of a new dataset based on two existing datasets, FakeNewsNet and Fakeddit. These are specific, verifiable resources used as a basis for the new dataset.",
      "processing_time": 74.10758566856384,
      "citing_paper_id": "253460330",
      "cited_paper_id": 52164698
    },
    {
      "context_text": "While some recent fact-checking approaches [9, 11] try to retrieve explanations from the knowledge graphs, they can only explain their results from the text-based knowledge graphs, which cannot be easily applied to our explainable fauxtography detection problem with multiple data modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework (ExFaKT) and a general reference to knowledge graphs and text. No verifiable datasets are identified.",
      "processing_time": 59.51837372779846,
      "citing_paper_id": "253460330",
      "cited_paper_id": 53597419
    },
    {
      "context_text": "For example, Zhang et al. [62] developed FauxBuster, a content-free fauxtography detection framework by exploring the user comments of online social media posts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FauxBuster) and its application. The context focuses on the development and use of the method rather than a dataset.",
      "processing_time": 59.800989866256714,
      "citing_paper_id": "253460330",
      "cited_paper_id": 57880229
    },
    {
      "context_text": "There has been some initial research focusing on fauxtography detection for online social media posts [43, 62, 67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research on fauxtography detection. No verifiable resources are identified.",
      "processing_time": 57.529478788375854,
      "citing_paper_id": "253460330",
      "cited_paper_id": 57880229
    },
    {
      "context_text": "There has been some initial research focusing on fauxtography detection for online social media posts [43, 62, 67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research on fauxtography detection. No verifiable resources are identified.",
      "processing_time": 57.529478788375854,
      "citing_paper_id": "253460330",
      "cited_paper_id": 201698324
    },
    {
      "context_text": "Online social media has been popular for recent years and generates billions of online posts on a daily basis, some of which lead to the proliferation of misleading content, such as fake news [44], synthetic audio [38], deepfake faces [14], and fauxtography [62].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various types of misleading content but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.20032572746277,
      "citing_paper_id": "253460330",
      "cited_paper_id": 57880229
    },
    {
      "context_text": "Online social media has been popular for recent years and generates billions of online posts on a daily basis, some of which lead to the proliferation of misleading content, such as fake news [44], synthetic audio [38], deepfake faces [14], and fauxtography [62].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various types of misleading content but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.20032572746277,
      "citing_paper_id": "253460330",
      "cited_paper_id": 209415079
    },
    {
      "context_text": "There have been several recent efforts to study the problem of fauxtography detection in social sensing, crowdsourcing, and social network communities [43, 62, 67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general efforts in fauxtography detection. No verifiable resources are identified.",
      "processing_time": 57.58263659477234,
      "citing_paper_id": "253460330",
      "cited_paper_id": 57880229
    },
    {
      "context_text": "There have been several recent efforts to study the problem of fauxtography detection in social sensing, crowdsourcing, and social network communities [43, 62, 67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general efforts in fauxtography detection. No verifiable resources are identified.",
      "processing_time": 57.58263659477234,
      "citing_paper_id": "253460330",
      "cited_paper_id": 201698324
    },
    {
      "context_text": "To ensure the quality of responses from workers, we adopt several quality control strategies that are widely used in crowdsourcing research [1, 2, 17, 61].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to quality control strategies in crowdsourcing research.",
      "processing_time": 57.8491096496582,
      "citing_paper_id": "253460330",
      "cited_paper_id": 60600755
    },
    {
      "context_text": "considers the information across both entities and their relations to generate an integrated triplelevel feature, which is widely applied in graph neural networks in graph-based deep learning community [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for generating integrated triple-level features in graph neural networks.",
      "processing_time": 57.575183391571045,
      "citing_paper_id": "253460330",
      "cited_paper_id": 88523916
    },
    {
      "context_text": "Second, several recent efforts have been made in HCI research to model human intelligence in a graph structure [20, 49, 58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general efforts in HCI research to model human intelligence in a graph structure.",
      "processing_time": 58.71294832229614,
      "citing_paper_id": "253460330",
      "cited_paper_id": 159041743
    },
    {
      "context_text": "Second, several recent efforts have been made in HCI research to model human intelligence in a graph structure [20, 49, 58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general efforts in HCI research to model human intelligence in a graph structure.",
      "processing_time": 58.71294832229614,
      "citing_paper_id": "253460330",
      "cited_paper_id": 210873091
    },
    {
      "context_text": "A key challenge of this direction is how to control the number and quality of the selected non-human subjects as visual graph entities given the fact that there exist a large number of non-human subjects in different images [59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge related to non-human subjects in images.",
      "processing_time": 56.32604265213013,
      "citing_paper_id": "253460330",
      "cited_paper_id": 195441316
    },
    {
      "context_text": "Using the word embeddings from the word-level feature encoder, we design a bi-directional gated recurrent unit (biGRU) [50] to encode the entire text description of the post.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (biGRU) which is not a dataset. The citation is used to reference the method, not a dataset.",
      "processing_time": 59.49731492996216,
      "citing_paper_id": "253460330",
      "cited_paper_id": 195697699
    },
    {
      "context_text": "For example, compared to the multi-modal fake news detection schemes without knowledge graph (i.e., SpotFake, FCMF, ExFaux, SAFE, EANN, att-RNN), CrowdGraph is able to achieve 15 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing different approaches to multi-modal fake news detection.",
      "processing_time": 58.15299701690674,
      "citing_paper_id": "253460330",
      "cited_paper_id": 201698324
    },
    {
      "context_text": "Among them, human-centric fauxtography detection is a particularly challenging misinformation detection problem due to its diversified types and the multi-modal nature of its content [67].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem statement about human-centric fauxtography detection.",
      "processing_time": 57.13680076599121,
      "citing_paper_id": "253460330",
      "cited_paper_id": 201698324
    },
    {
      "context_text": "[67] developed a content based fauxtography detection model that utilizes the meta information of images (e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model that uses meta information of images. No verifiable resources are identified.",
      "processing_time": 57.136573791503906,
      "citing_paper_id": "253460330",
      "cited_paper_id": 201698324
    },
    {
      "context_text": "‚Ä¢ FCMF [67]: FCMF is a fauxtography detector that identifies fauxtography posts by exploring the image URLs and hand-crafted text features of the posts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method (FCMF) rather than a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 57.86843967437744,
      "citing_paper_id": "253460330",
      "cited_paper_id": 201698324
    },
    {
      "context_text": "‚Ä¢ GUpdater [48]: GUpdater is a knowledge graph updating framework that utilizes relational graph neural network to update relations in the knowledge graph based on new information from news articles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes GUpdater, a method for updating knowledge graphs using news articles. No specific dataset is mentioned.",
      "processing_time": 56.978336572647095,
      "citing_paper_id": "253460330",
      "cited_paper_id": 202786261
    },
    {
      "context_text": "While some solutions from other application domains (e.g., question-answering applications [64], recommender systems [47]) have developed knowledge graphs that contain knowledge in other modalities (e.g., image, audio), they only treat the non-text-based knowledge in the knowledge graphs as‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general application domains. No clear, verifiable datasets are identified.",
      "processing_time": 57.240936517715454,
      "citing_paper_id": "253460330",
      "cited_paper_id": 204837691
    },
    {
      "context_text": "The encoder is developed based on the ResNet model [16] as ResNet consists of multiple connected deep convolutional blocks that can encode the images efficiently and robustly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the ResNet model but does not refer to any specific dataset. The citation is about the model architecture, not a dataset.",
      "processing_time": 58.1104462146759,
      "citing_paper_id": "253460330",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Also, we compared the linear projection of patches with ResNet50 on image feature extraction when using ViT as the image encoder in our approach.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.75443911552429,
      "citing_paper_id": "237513894",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "We compared our cross-modal matching (aka CMM) approach with Pixel-BERT [4], which employs ResNet50 [3] to extract image features, and feeds the text and visual features into a single-stream Transformer network for image-text matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on comparing methodologies rather than using a particular dataset.",
      "processing_time": 58.28272843360901,
      "citing_paper_id": "237513894",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "During item search, we employ FLAT [6], a light-weight NER model to identify pre-defined semantic types from the query ùëû , and calculate an enhanced similarity score between ùëû and each item based on their tokens and identified semantic types.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using FLAT, which is a model, not a dataset. There are no specific datasets mentioned in the context.",
      "processing_time": 57.57196879386902,
      "citing_paper_id": "237513894",
      "cited_paper_id": 216144439
    },
    {
      "context_text": "For image input (the left branch), we employ vision Transformer (ViT) [2] as the feature extractor.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (vision Transformer) used for feature extraction. No dataset names are present in the citation span.",
      "processing_time": 58.41533541679382,
      "citing_paper_id": "237513894",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "On observing the limitation of pre-defined and mismatching categories of region detection [4], and the effectiveness of directly applying Transformer to images [2], we choose to learn from image patches rather than bounding boxes, to represent an image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited paper title confirms the focus on methodological aspects.",
      "processing_time": 57.93712639808655,
      "citing_paper_id": "237513894",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "‚Ä¶recommend that physicians communicate with patients (i.e., doctor-patient dialogues) before performing radiological examinations and obtain the patient‚Äôs past medical history, current medical history, etc. Doctor-patient dialogue is one of the most common forms of consultation [25, 75, 28, 62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of doctor-patient dialogues. No verifiable resources are identified.",
      "processing_time": 57.75129437446594,
      "citing_paper_id": "235270502",
      "cited_paper_id": 2633248
    },
    {
      "context_text": "From the above observation, for traditional graph embedding methods, ESim performs better than metapath2vec. e Besides, it is known that ESim is able to take multiple meta-paths. r In general, graph neural network-based methods combine feature and structure information, such as GCN, GAT, and MAGNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing graph embedding methods and graph neural networks.",
      "processing_time": 57.93383717536926,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Here we report the best performance and test all the meta-paths for GATNE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of meta-paths for GATNE. No verifiable resources are identified.",
      "processing_time": 58.087817430496216,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Here we report the best performance and test all the meta-paths for GATNE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of meta-paths for GATNE. No verifiable resources are identified.",
      "processing_time": 58.087817430496216,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": "SpeciÔ¨Åcally, Our is 0.1347, 0.1322, 0.1311, 0.1287, 0.1144, 0.1140, 0.1120, 0.1094, 0.1074, 0.1045, 0.0474, 0.0206, and 0.0177 higher than Ours(DeepWalk), Ours(Esim), Ours(metapath2vec), Ours(HE-Rec), Ours(GCN), Ours(GAT), Ours(MAGNN),Ours (RGCN), Ours(GATNE),Ours (HGAN),Ours(HetGNN), Ours(HGT), and Ours (MMGCN), in term of accuracy, respectively. o f In terms of precision, sensitivity, speciÔ¨Åcity, F1-Score, AUC, there are similar scenarios as the above. o On the other hand, from Table 7, we compare the model size of our knowledge attention mechanism with the model size of others: in the case of best performance, the model size of our knowledge attention mechanism does not exceed 40 M. r Compared to the heaviest model (i.e., Ours (HGAN)), our model parameters are about 8 M smaller, and the performance of our model increases all by almost 0 .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only compares performance metrics and model sizes of various methods, including DeepWalk, Esim, metapath2vec, HE-Rec, GCN, GAT, MAGNN, RGCN, GATNE, HGAN, HetGNN, HGT, and MMGCN. These are models or methods, not datasets.",
      "processing_time": 67.33176112174988,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "SpeciÔ¨Åcally, Our is 0.1347, 0.1322, 0.1311, 0.1287, 0.1144, 0.1140, 0.1120, 0.1094, 0.1074, 0.1045, 0.0474, 0.0206, and 0.0177 higher than Ours(DeepWalk), Ours(Esim), Ours(metapath2vec), Ours(HE-Rec), Ours(GCN), Ours(GAT), Ours(MAGNN),Ours (RGCN), Ours(GATNE),Ours (HGAN),Ours(HetGNN), Ours(HGT), and Ours (MMGCN), in term of accuracy, respectively. o f In terms of precision, sensitivity, speciÔ¨Åcity, F1-Score, AUC, there are similar scenarios as the above. o On the other hand, from Table 7, we compare the model size of our knowledge attention mechanism with the model size of others: in the case of best performance, the model size of our knowledge attention mechanism does not exceed 40 M. r Compared to the heaviest model (i.e., Ours (HGAN)), our model parameters are about 8 M smaller, and the performance of our model increases all by almost 0 .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only compares performance metrics and model sizes of various methods, including DeepWalk, Esim, metapath2vec, HE-Rec, GCN, GAT, MAGNN, RGCN, GATNE, HGAN, HetGNN, HGT, and MMGCN. These are models or methods, not datasets.",
      "processing_time": 67.33176112174988,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": ", graph attention network [25]), which capture entities (i.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Graph Attention Networks).",
      "processing_time": 55.945300579071045,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Here we report the best performance and test all the meta-paths for RGCN. ‚òÖ GATNE [11] classiÔ¨Åes all node embeddings of heterogeneous graphs into three categories: base embeddings, edge embeddings, and attribute embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the classification of node embeddings in heterogeneous graphs.",
      "processing_time": 58.08565855026245,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Here we report the best performance and test all the meta-paths for RGCN. ‚òÖ GATNE [11] classiÔ¨Åes all node embeddings of heterogeneous graphs into three categories: base embeddings, edge embeddings, and attribute embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the classification of node embeddings in heterogeneous graphs.",
      "processing_time": 58.08565855026245,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": "Compared to some advanced models, including RGCN, GATNE, HGAN, HetGNN, HGT, and MMGCN, our attention mechanism can capture the cross-modalities information successfully and shows its superiority.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance of different models, particularly in capturing cross-modalities information.",
      "processing_time": 58.64266395568848,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Compared to some advanced models, including RGCN, GATNE, HGAN, HetGNN, HGT, and MMGCN, our attention mechanism can capture the cross-modalities information successfully and shows its superiority.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance of different models, particularly in capturing cross-modalities information.",
      "processing_time": 58.64266395568848,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": "‚òÖ GAT [25] considers the attention mechanism on the graphs and is semi-supervised neural network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GAT).",
      "processing_time": 55.591153383255005,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "‚òÖ GCN[47]designs for thegraphs andis asemi-supervised graph convolutional network. p Here we report the best performance and test all the meta-paths for GCN. ‚òÖ GAT [89] considers the attention mechanism on the graphs and is a semi-supervised neural network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GCN and GAT). The context focuses on the methodologies and performance of these models.",
      "processing_time": 58.08118677139282,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "DiÔ¨Äerent from the proposed graph attention mechanism that focuses on the relationship between data modalities, GATNE focuses on the relationship between diÔ¨Äerent node embeddings and diÔ¨Äerent attributes between nodes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodologies and their differences.",
      "processing_time": 55.58380627632141,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "DiÔ¨Äerent from the proposed graph attention mechanism that focuses on the relationship between data modalities, GATNE focuses on the relationship between diÔ¨Äerent node embeddings and diÔ¨Äerent attributes between nodes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodologies and their differences.",
      "processing_time": 55.58380627632141,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": "In this sub-sub section, Ours(DeepWalk) means a variant of Ours, which only using DeepWalk and not using our graph attention mechanisms; Ours(ESim) means a variant of Ours, which only using ESim and not using our graph attention mechanisms; Ours(metapath2vec) means a variant of Ours, which only using metapath2vec and not using our graph attention mechanisms; Ours(HERec) means a variant of Ours, which only using HERec and not using our graph attention mechanisms; Ours(GCN) means a variant of Ours, which only using GCN and not using our graph attention mechanisms; Ours(GAT) means a variant of Ours, which only using GAT and not using our graph attention mechanisms; Ours(MAGNN) means a variant of Ours, which only using MAGNN and not using our graph attention mechanisms; Ours(RGCN) means a variant of Ours, which only using RGCN and not using our graph attention mechanisms; Ours(GATNE) means a variant of Ours, which only us-ing GATNE and not using our graph attention mechanisms; Ours(HGAN) means a variant of Ours, which only using HGANandnot usingour graphattention mechanisms; Ours(Het-GNN) means a variant of Ours, which only using HetGNN and not using our graph attention mechanisms; Ours(HGT) means a variant of Ours, which only using HGT and not using our graph attention mechanisms; Ours(MMGCN) means a variant of Ours, which only using MMGCN and not using our graph attention mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes various variants of the authors' method, none of which mention specific datasets. The focus is on comparing different methods, not on the use of datasets.",
      "processing_time": 58.4906210899353,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "In this sub-sub section, Ours(DeepWalk) means a variant of Ours, which only using DeepWalk and not using our graph attention mechanisms; Ours(ESim) means a variant of Ours, which only using ESim and not using our graph attention mechanisms; Ours(metapath2vec) means a variant of Ours, which only using metapath2vec and not using our graph attention mechanisms; Ours(HERec) means a variant of Ours, which only using HERec and not using our graph attention mechanisms; Ours(GCN) means a variant of Ours, which only using GCN and not using our graph attention mechanisms; Ours(GAT) means a variant of Ours, which only using GAT and not using our graph attention mechanisms; Ours(MAGNN) means a variant of Ours, which only using MAGNN and not using our graph attention mechanisms; Ours(RGCN) means a variant of Ours, which only using RGCN and not using our graph attention mechanisms; Ours(GATNE) means a variant of Ours, which only us-ing GATNE and not using our graph attention mechanisms; Ours(HGAN) means a variant of Ours, which only using HGANandnot usingour graphattention mechanisms; Ours(Het-GNN) means a variant of Ours, which only using HetGNN and not using our graph attention mechanisms; Ours(HGT) means a variant of Ours, which only using HGT and not using our graph attention mechanisms; Ours(MMGCN) means a variant of Ours, which only using MMGCN and not using our graph attention mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes various variants of the authors' method, none of which mention specific datasets. The focus is on comparing different methods, not on the use of datasets.",
      "processing_time": 58.4906210899353,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": "In this way, it is simple and efficient compared to the conventional LSTM [69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to LSTM. No verifiable resources are identified.",
      "processing_time": 56.67532300949097,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3389583
    },
    {
      "context_text": "In this way, it is simple and eÔ¨Écient compared to the conventional LSTM [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to LSTM. No verifiable resources are identified.",
      "processing_time": 56.8519606590271,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3389583
    },
    {
      "context_text": "‚òÖ CE-LSTM [52] is an event-Ô¨Çow serializing method to learn the representation from heterogeneous spatial-temporal graph through encoding the evolution of dynamic heterogeneous graph into a special language pattern such as word sequence in a corpus.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (CE-LSTM) and its application to learning representations from a heterogeneous spatial-temporal graph, but does not reference a specific dataset.",
      "processing_time": 60.42208003997803,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3389583
    },
    {
      "context_text": "‚òÖ CE-LSTM [52] is an event-Ô¨Çow serializing method to learn the representation from heterogeneous spatial-temporal graph through encoding the evolution of dynamic heterogeneous graph into a special language pattern such as word sequence in a corpus.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (CE-LSTM) and its application to learning representations from a heterogeneous spatial-temporal graph, but does not reference a specific dataset.",
      "processing_time": 60.42208003997803,
      "citing_paper_id": "235270502",
      "cited_paper_id": 70038990
    },
    {
      "context_text": "SpeciÔ¨Åcally, ours is 0.0802 and 0.0649 higher than DyHAN and CE-LSTM, in terms of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 56.794514179229736,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3389583
    },
    {
      "context_text": "SpeciÔ¨Åcally, ours is 0.0802 and 0.0649 higher than DyHAN and CE-LSTM, in terms of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons with other methods. No verifiable resources are identified.",
      "processing_time": 56.794514179229736,
      "citing_paper_id": "235270502",
      "cited_paper_id": 70038990
    },
    {
      "context_text": "‚òÖ metapath2vec [99] performs a random walk based on a metaath and uses skip-gram to embed knowledge graphs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It describes a method (metapath2vec) and its functionality.",
      "processing_time": 56.789727210998535,
      "citing_paper_id": "235270502",
      "cited_paper_id": 3919301
    },
    {
      "context_text": ", too much dye has been used for contrast) [115].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to an issue in histopathology images, which is not a dataset but rather a methodological concern.",
      "processing_time": 58.76806020736694,
      "citing_paper_id": "235270502",
      "cited_paper_id": 9815689
    },
    {
      "context_text": ", 70 ms per image) is not the optimal one, but it can satisfy the requirements of real-time applications (less than 100 ms to process each image [112]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance requirement for real-time applications.",
      "processing_time": 55.76742911338806,
      "citing_paper_id": "235270502",
      "cited_paper_id": 13838003
    },
    {
      "context_text": "Besides, if a certain diagnose contains images, we extract image features from a 16 -layer pre-trained VGGNet [84] to represent the images, similar to [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using a pre-trained VGGNet to extract image features, but does not refer to a specific dataset. The context is about methodology, not a reusable dataset.",
      "processing_time": 58.76287603378296,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "‚Ä¶on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3]. u r",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not explicitly refer to any specific, verifiable datasets. The context focuses on model performance rather than dataset usage.",
      "processing_time": 58.02216386795044,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "‚Ä¶COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM [55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3], in the light of accuracy, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various models and architectures but does not refer to any specific datasets. The context is focused on comparing the performance of different models.",
      "processing_time": 57.46603226661682,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Looking at the Table 5, it‚Äô s patently apparent that our method is superior to others. r Particularly, ours is 0.2416, 0.2397, 0.2364, 0.2344, 0.2324, 0.2292, 0.2258, 0.2203, 0.2197, 0.2147, 0.2781, 0.2656, 0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106], COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM [55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3], in the light of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various models and methods. The cited papers are also about models and methods, not datasets.",
      "processing_time": 57.63720512390137,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Looking at the Table 5, it‚Äô s patently apparent that our method is superior to others. r Particularly, ours is 0.2416, 0.2397, 0.2364, 0.2344, 0.2324, 0.2292, 0.2258, 0.2203, 0.2197, 0.2147, 0.2781, 0.2656, 0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106], COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM [55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3], in the light of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various models and methods. The cited papers are also about models and methods, not datasets.",
      "processing_time": 57.63720512390137,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Looking at the Table 5, it‚Äô s patently apparent that our method is superior to others. r Particularly, ours is 0.2416, 0.2397, 0.2364, 0.2344, 0.2324, 0.2292, 0.2258, 0.2203, 0.2197, 0.2147, 0.2781, 0.2656, 0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106], COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM [55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3], in the light of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various models and methods. The cited papers are also about models and methods, not datasets.",
      "processing_time": 57.63720512390137,
      "citing_paper_id": "235270502",
      "cited_paper_id": 211252408
    },
    {
      "context_text": "To represent the images, we employ the last hidden layer of VGG-16[84] as an embedding layer, which is pre-trained by ImageNet [18].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using VGG-16, which is a model, and ImageNet, which is a dataset. However, the primary focus is on the model's usage, not the dataset itself.",
      "processing_time": 60.153786420822144,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3]. u r",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.19205141067505,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3]. u r",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.19205141067505,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3]. u r",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.19205141067505,
      "citing_paper_id": "235270502",
      "cited_paper_id": 211252408
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55], LM3FT [59], MMCL [30], MEC [42], and GMMF [3]. u r",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.19205141067505,
      "citing_paper_id": "235270502",
      "cited_paper_id": 218487793
    },
    {
      "context_text": "Moreover, in the COVID-19 epidemic, most patients have a past medical history, and chronic medical history [23,24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to past medical history and chronic medical history. No verifiable resources are identified.",
      "processing_time": 57.46208643913269,
      "citing_paper_id": "235270502",
      "cited_paper_id": 14298338
    },
    {
      "context_text": "Moreover, in the COVID-19 epidemic, most patients have a past medical history, and chronic medical history [23,24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to past medical history and chronic medical history. No verifiable resources are identified.",
      "processing_time": 57.46208643913269,
      "citing_paper_id": "235270502",
      "cited_paper_id": 218528456
    },
    {
      "context_text": "We use the weight of the single-level modality attention to describe the similarity [59] of transformed embedding, denoted as a multiple-level modality attention vector q.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for describing similarity using attention mechanisms.",
      "processing_time": 55.9430410861969,
      "citing_paper_id": "235270502",
      "cited_paper_id": 29944424
    },
    {
      "context_text": "ùêñ ùêñ It is worth mentioning that this kind method follows Ref.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a reference to a method described in another paper.",
      "processing_time": 56.286540508270264,
      "citing_paper_id": "235270502",
      "cited_paper_id": 49901898
    },
    {
      "context_text": "We take advantage of the training data in different dataset and contitute the knowledge graph GOurs by [61,62].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'training data in different dataset' but does not specify any particular dataset names. The reference to 'GOurs' suggests a custom knowledge graph, which is not a verifiable resource.",
      "processing_time": 60.018609046936035,
      "citing_paper_id": "235270502",
      "cited_paper_id": 52183483
    },
    {
      "context_text": "‚òÖ HetGNN [105] Ô¨Årst samples a Ô¨Åxed number of neighbors in the vicinity of an object via random walk with a restart.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (HetGNN) and a general approach to sampling neighbors in a graph.",
      "processing_time": 57.12319040298462,
      "citing_paper_id": "235270502",
      "cited_paper_id": 67748132
    },
    {
      "context_text": "Besides, if a certain diagnose contains images, we extract mage features from a 16-layer pre-trained VGGNet [66] to represent he images, similar to [80].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VGGNet) for extracting image features. No verifiable datasets are referenced.",
      "processing_time": 57.285881996154785,
      "citing_paper_id": "235270502",
      "cited_paper_id": 69990369
    },
    {
      "context_text": "‚òÖ CE-LSTM [114] is an event-flow serializing method to learn he representation from heterogeneous spatial‚Äìtemporal graph through ncoding the evolution of dynamic heterogeneous graph into a special anguage pattern such as word sequence in a corpus.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (CE-LSTM) for learning representations from a heterogeneous spatial-temporal graph.",
      "processing_time": 57.11654758453369,
      "citing_paper_id": "235270502",
      "cited_paper_id": 70038990
    },
    {
      "context_text": "‚òÖ GATNE [104] classifies all node embeddings of heterogeneous graphs into three categories: base embeddings, edge embeddings, and attribute embeddings.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for classifying node embeddings in heterogeneous graphs.",
      "processing_time": 56.11141657829285,
      "citing_paper_id": "235270502",
      "cited_paper_id": 146121289
    },
    {
      "context_text": "‚òÖ HetGNN [106] first samples a fixed number of neighbours in the vicinity of an object via random walk with a restart.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HetGNN) and a general approach (random walk with restart).",
      "processing_time": 57.27587819099426,
      "citing_paper_id": "235270502",
      "cited_paper_id": 198952485
    },
    {
      "context_text": "Inspired by the success of temporal convolutional network (TCN) [68, 107, 58] and the self-attention mechanism [88], we design a novel convolutional network, named Temporal Convolutional Self-Attention Network (TCSAN). f Similar to most competitive neural sequence transduction models [88, 98, 110],‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.85419511795044,
      "citing_paper_id": "235270502",
      "cited_paper_id": 203056539
    },
    {
      "context_text": "‚òÖ SA-TCN [110] is a TCN-based model embedded with a temporal elf-attention block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (SA-TCN). The context focuses on the model's architecture and its application, not on any particular dataset.",
      "processing_time": 58.80233883857727,
      "citing_paper_id": "235270502",
      "cited_paper_id": 204766590
    },
    {
      "context_text": "The output is passed to the function. activation ‚òÖ SA-TCN [17] is a TCN-based model embedded with a temporal self-attention block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (SA-TCN). The context focuses on the model's architecture and functionality.",
      "processing_time": 57.57491731643677,
      "citing_paper_id": "235270502",
      "cited_paper_id": 204766590
    },
    {
      "context_text": "Jo SpeciÔ¨Åcally, ours is 0.1131, 0.1022, 0.0956, and 0.0940 higher than Ours (TCN), Ours (TrellisNet), Ours (SA-TCN), and Ours (TCAN), in term of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between different model variants. No verifiable resources are identified.",
      "processing_time": 56.683175802230835,
      "citing_paper_id": "235270502",
      "cited_paper_id": 204766590
    },
    {
      "context_text": "In this subsection, Ours(TCN) means a variant of Ours, which only using TCN and not using our temporal convolution networks; Ours(TrellisNet) means a variant of Ours, which only using TrellisNet and not using our temporal convolution networks; u r Ours(SA-TCN) means a variant of Ours, which only using SA-TCN and not using our temporal convolution networks; Ours(TCAN) means a variant of Ours, which only using TCAN and not using our temporal convolution networks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only variants of the authors' own models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.56972289085388,
      "citing_paper_id": "235270502",
      "cited_paper_id": 204766590
    },
    {
      "context_text": "1683 higher than CAAD [35], COVID-CAPS [82], COVID-DA [83], COVID-ResNet [84], DCSL [85], COVNet [86], AnoDet [87], DLS [88], DeCovNet [89], DLQCTM [90], DenseNet-161 [91], ResNet-34 [70], VGG-19 [66], ResNet-18 [70], VGG-16 [66], DGLM [92], LM3FT [93], MMCL [94], MEC [95], and GMMF [96], in the light of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context lists various models and methods, none of which are datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 57.23165440559387,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "1683 higher than CAAD [35], COVID-CAPS [82], COVID-DA [83], COVID-ResNet [84], DCSL [85], COVNet [86], AnoDet [87], DLS [88], DeCovNet [89], DLQCTM [90], DenseNet-161 [91], ResNet-34 [70], VGG-19 [66], ResNet-18 [70], VGG-16 [66], DGLM [92], LM3FT [93], MMCL [94], MEC [95], and GMMF [96], in the light of accuracy, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context lists various models and methods, none of which are datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 57.23165440559387,
      "citing_paper_id": "235270502",
      "cited_paper_id": 211252408
    },
    {
      "context_text": "], here we perform COVID-19 classiÔ¨Åcation with ResNet-34 clas-siÔ¨Åer [36]. r The individual state of the knowledge attention mechanism and TCSAN implementation is as follows: Temporal Self-Attention Network We utilize 10 layers of temporal convolutional networks [68] to serve as our network architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (ResNet-34) and a network architecture (temporal convolutional networks).",
      "processing_time": 58.21121668815613,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Finally, we use the classifier (in this paper, we use the ResNet-34 [70]) to gain the labels, i.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (ResNet-34). No verifiable resources are identified.",
      "processing_time": 56.784334659576416,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "We perform COVID9 classification with ResNet-34 classifier [70].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model (ResNet-34) which is excluded according to the rules.",
      "processing_time": 57.001564025878906,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Baselines on Our Dataset We compare against various stateof-the-art baselines on our dataset, including CAAD [35], COVIDCAPS [82], COVID-DA [83], COVID-ResNet [84], DCSL [85], COVNet [86], AnoDet [87], DLS [88], DeCovNet [89], DLQCTM [90], DenseNet-161[91], ResNet-34[70], VGG-19[66], ResNet-18[70], VGG16[66], DGLM [92], LM3FT [93], MMCL [94], MEC [95], and GMMF [96].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' but does not provide a specific, identifiable name for the dataset. The cited papers do not help in identifying a specific dataset.",
      "processing_time": 58.38248586654663,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "Baselines on Our Dataset We compare against various stateof-the-art baselines on our dataset, including CAAD [35], COVIDCAPS [82], COVID-DA [83], COVID-ResNet [84], DCSL [85], COVNet [86], AnoDet [87], DLS [88], DeCovNet [89], DLQCTM [90], DenseNet-161[91], ResNet-34[70], VGG-19[66], ResNet-18[70], VGG16[66], DGLM [92], LM3FT [93], MMCL [94], MEC [95], and GMMF [96].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' but does not provide a specific, identifiable name for the dataset. The cited papers do not help in identifying a specific dataset.",
      "processing_time": 58.38248586654663,
      "citing_paper_id": "235270502",
      "cited_paper_id": 211252408
    },
    {
      "context_text": "Finally, we use the classiÔ¨Åer (in this paper, we use the ResNet-34 [36]) to gain the labels, i.e., ÃÇùë¶ ùëù .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (ResNet-34). The citation is used to reference the model, not a dataset.",
      "processing_time": 58.56183195114136,
      "citing_paper_id": "235270502",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "‚òÖ TCAN [35] is also combines temporal convolutional network and attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TCAN).",
      "processing_time": 55.75774431228638,
      "citing_paper_id": "235270502",
      "cited_paper_id": 211572518
    },
    {
      "context_text": "Looking at the Table 5, it‚Äô s patently apparent that our method is superior to others. r Particularly, ours is 0.2416, 0.2397, 0.2364, 0.2344, 0.2324, 0.2292, 0.2258, 0.2203, 0.2197, 0.2147, 0.2781, 0.2656, 0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106],‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance comparisons. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.32795286178589,
      "citing_paper_id": "235270502",
      "cited_paper_id": 214802049
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55],‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.35978078842163,
      "citing_paper_id": "235270502",
      "cited_paper_id": 214802049
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55],‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.35978078842163,
      "citing_paper_id": "235270502",
      "cited_paper_id": 216553689
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55],‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.35978078842163,
      "citing_paper_id": "235270502",
      "cited_paper_id": 218487793
    },
    {
      "context_text": "We compare against various state-of-the-art baselines on our dataset, including CAAD [106], COVID-CAPS [2], COVID-DA[109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM[55],‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name. No other specific datasets are mentioned.",
      "processing_time": 58.35978078842163,
      "citing_paper_id": "235270502",
      "cited_paper_id": 227191214
    },
    {
      "context_text": "‚òÖ DyHAN [113] is a dynamic heterogeneous graph embedding ethod with hierarchical attention that learns node embeddings leverging both structural heterogeneity and temporal evolution.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DyHAN. No verifiable resources are identified.",
      "processing_time": 56.56489014625549,
      "citing_paper_id": "235270502",
      "cited_paper_id": 215746373
    },
    {
      "context_text": "[52,53] create an image collection containing 329 images from 183 patients, most of which are chest X-ray images for COVID-19.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a specific image collection created for COVID-19 detection, but it does not provide a clear, identifiable name for the dataset.",
      "processing_time": 58.16702175140381,
      "citing_paper_id": "235270502",
      "cited_paper_id": 216553528
    },
    {
      "context_text": "‚Ä¶0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106], COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84], ResNet-18[36], VGG-16[84], DGLM‚Ä¶",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.61120820045471,
      "citing_paper_id": "235270502",
      "cited_paper_id": 216553689
    },
    {
      "context_text": "‚Ä¶0.2197, 0.2147, 0.2781, 0.2656, 0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106], COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83], DenseNet-161[39], ResNet-34[36], VGG-19[84],‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.60655426979065,
      "citing_paper_id": "235270502",
      "cited_paper_id": 218487793
    },
    {
      "context_text": "It is fast and easy to operate and has become a widely used diagnostic tool [5,6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general diagnostic tool. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 56.779475927352905,
      "citing_paper_id": "235270502",
      "cited_paper_id": 218599990
    },
    {
      "context_text": "Researchers are studying how to distinguish COVID-19 from chest X-ray, ultrasound images, or CT scans to solve the lack of reagents [41, 91, 97, 15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to imaging modalities used for COVID-19 classification. No clear, verifiable dataset names are provided.",
      "processing_time": 58.574904441833496,
      "citing_paper_id": "235270502",
      "cited_paper_id": 219471191
    },
    {
      "context_text": "The great success of deep learning methods in pneumonia diagnosis tasks has inspired many researchers [82, 7, 63, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods in pneumonia diagnosis. No verifiable resources are identified.",
      "processing_time": 57.73240900039673,
      "citing_paper_id": "235270502",
      "cited_paper_id": 220541874
    },
    {
      "context_text": "For patients with a certain type of disease, their disease characteristics are statistical characteristics [20, 66, 23].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to statistical characteristics of disease characteristics, which is too generic.",
      "processing_time": 56.73834156990051,
      "citing_paper_id": "235270502",
      "cited_paper_id": 221355139
    },
    {
      "context_text": "‚Ä¶0.2324, 0.2292, 0.2258, 0.2203, 0.2197, 0.2147, 0.2781, 0.2656, 0.2596, 0.2532, 0.2445, 0.1741, 0.1700, 0.1690, 0.1688,and 0.1683 higher than CAAD [106], COVID-CAPS [2], COVID-DA [109], COVID-ResNet [26], DCSL [51], COVNet [50], AnoDet [71], DLS [100], DeCovNet [92], DLQCTM [83],‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 57.517183780670166,
      "citing_paper_id": "235270502",
      "cited_paper_id": 227191214
    },
    {
      "context_text": "Radiological diagnosis is a convenient medical technique for patients with COVID-19 who are suspected of urgently needing a risk area diagnosis [21, 116].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to radiological diagnosis for COVID-19. No verifiable resources are identified.",
      "processing_time": 58.201619148254395,
      "citing_paper_id": "235270502",
      "cited_paper_id": 235689895
    },
    {
      "context_text": "‚Ä¶network, named Temporal Convolutional Self-Attention Network (TCSAN). f Similar to most competitive neural sequence transduction models [88, 98, 110], we use the encoder and decoder structure in our network. o Encoder maps the input data ùê± = { ‚àí ‚àí , , , ùë° ùëù ùëá ùëù ‚ãÖ =1 ùëíùëõùëêùëúùëëùëíùëü‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a network architecture. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.75352621078491,
      "citing_paper_id": "235270502",
      "cited_paper_id": null
    },
    {
      "context_text": "Inspired by the success of graph-based attention models (e.g., graph attention network [89]), which capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other, we focus on the strategy of graph-based attention models to solve these above two issues [49, 110, 98].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only graph-based attention models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.56348466873169,
      "citing_paper_id": "235270502",
      "cited_paper_id": null
    },
    {
      "context_text": "Jo Also, medical COVID-19 data [101, 1, 117, 34] consists of chest X-ray images, ultrasound images, and CT images (i.e., slices) and mostly multi-modal.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'medical COVID-19 data' which is described as multi-modal, including chest X-ray images, ultrasound images, and CT images. However, no specific dataset name is provided.",
      "processing_time": 59.8710572719574,
      "citing_paper_id": "235270502",
      "cited_paper_id": null
    },
    {
      "context_text": "Much progress has been made in recent years in the area of extracting semantic content from the different modalities of individual videos, such as object detection [12], text extraction [18] or scene captioning [16] in still images, pose-tracking [3], and action recognition [9] in video or speech-transcription [4] in audio.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various tasks and modalities but does not specify any particular datasets. The cited papers are methods or models, not datasets.",
      "processing_time": 58.396278858184814,
      "citing_paper_id": "222278396",
      "cited_paper_id": 706860
    },
    {
      "context_text": "Much progress has been made in recent years in the area of extracting semantic content from the different modalities of individual videos, such as object detection [12], text extraction [18] or scene captioning [16] in still images, pose-tracking [3], and action recognition [9] in video or speech-transcription [4] in audio.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various tasks and modalities but does not specify any particular datasets. The cited papers are methods or models, not datasets.",
      "processing_time": 58.396278858184814,
      "citing_paper_id": "222278396",
      "cited_paper_id": 1055111
    },
    {
      "context_text": "Much progress has been made in recent years in the area of extracting semantic content from the different modalities of individual videos, such as object detection [12], text extraction [18] or scene captioning [16] in still images, pose-tracking [3], and action recognition [9] in video or speech-transcription [4] in audio.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various tasks and modalities but does not specify any particular datasets. The cited papers are methods or models, not datasets.",
      "processing_time": 58.396278858184814,
      "citing_paper_id": "222278396",
      "cited_paper_id": 4714433
    },
    {
      "context_text": "Much progress has been made in recent years in the area of extracting semantic content from the different modalities of individual videos, such as object detection [12], text extraction [18] or scene captioning [16] in still images, pose-tracking [3], and action recognition [9] in video or speech-transcription [4] in audio.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various tasks and modalities but does not specify any particular datasets. The cited papers are methods or models, not datasets.",
      "processing_time": 58.396278858184814,
      "citing_paper_id": "222278396",
      "cited_paper_id": 9505704
    },
    {
      "context_text": "Much progress has been made in recent years in the area of extracting semantic content from the different modalities of individual videos, such as object detection [12], text extraction [18] or scene captioning [16] in still images, pose-tracking [3], and action recognition [9] in video or speech-transcription [4] in audio.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various tasks and modalities but does not specify any particular datasets. The cited papers are methods or models, not datasets.",
      "processing_time": 58.396278858184814,
      "citing_paper_id": "222278396",
      "cited_paper_id": 206592218
    },
    {
      "context_text": "For the labelling of locations, we use the last layer of a ResNet152 [15] pre-trained on Imagenet [6] as a feature encoder to extract a semantic representation of both the provided examples as well as all representative key-frames generated during shot-segmentation.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using a pre-trained ResNet152 on Imagenet, but Imagenet is not explicitly referred to as a dataset in the citation span. The focus is on the model and its application.",
      "processing_time": 60.69137716293335,
      "citing_paper_id": "222278396",
      "cited_paper_id": 1023605
    },
    {
      "context_text": "On these segments we perform sentiment analysis [8] and compute the mean word2vec embedding [10] of their words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (sentiment analysis and word2vec embeddings).",
      "processing_time": 56.95008087158203,
      "citing_paper_id": "222278396",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "The challenge uses the recently introduced High Level Video Understanding (HLVU) dataset [5] which consists of 10 movies released under creative commons licenses with a total combined duration of 681 minutes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HLVU"
      ],
      "dataset_descriptions": {
        "HLVU": "Used to test deep understanding of movies, focusing on high-level video understanding with 10 Creative Commons licensed movies totaling 681 minutes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the HLVU dataset, which is a specific, verifiable resource used in the context of testing deep understanding of movies.",
      "processing_time": 64.75263524055481,
      "citing_paper_id": "222278396",
      "cited_paper_id": 218470438
    },
    {
      "context_text": "Figure 1 shows an examples in the Turtle syntax [1]. mechanism.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets or resources. It only refers to an example in Turtle syntax, which is not a dataset.",
      "processing_time": 58.643654108047485,
      "citing_paper_id": "222278396",
      "cited_paper_id": null
    },
    {
      "context_text": "In addition, a number of researchers jointly employed multiple cross-modal datasets to obtain excellent alignment be-tween visual and linguistic information and perform VCR [18], [19],[20],[21],[22],[23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'multiple cross-modal datasets' but does not specify any particular dataset names. The context suggests the use of datasets for vision and language tasks, particularly Visual Question Answering (VCR).",
      "processing_time": 60.51885175704956,
      "citing_paper_id": "258902254",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "In addition, a number of researchers jointly employed multiple cross-modal datasets to obtain excellent alignment be-tween visual and linguistic information and perform VCR [18], [19],[20],[21],[22],[23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'multiple cross-modal datasets' but does not specify any particular dataset names. The context suggests the use of datasets for vision and language tasks, particularly Visual Question Answering (VCR).",
      "processing_time": 60.51885175704956,
      "citing_paper_id": "258902254",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "In addition, a number of researchers jointly employed multiple cross-modal datasets to obtain excellent alignment be-tween visual and linguistic information and perform VCR [18], [19],[20],[21],[22],[23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'multiple cross-modal datasets' but does not specify any particular dataset names. The context suggests the use of datasets for vision and language tasks, particularly Visual Question Answering (VCR).",
      "processing_time": 60.51885175704956,
      "citing_paper_id": "258902254",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "[18]andSuetal.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets or resources. It only mentions 'Suetal.' which is likely an author reference and not a dataset.",
      "processing_time": 59.27351999282837,
      "citing_paper_id": "258902254",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "[23]introducedexternalknowl-edgebaseintothepre-trainedVL-BERTmodel[19]tosupply associationpriorsforVCR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (VL-BERT) and a task (VCR). No verifiable datasets are referenced.",
      "processing_time": 58.99376106262207,
      "citing_paper_id": "258902254",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "[19]fedcom-binationsofvisualandlinguisticdatafromseveralcross-modal datasetsintoBERTmodeltolearncross-modalalignment,then Ô¨Åne-tunedthemodelontheVCRdataset[8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VCR"
      ],
      "dataset_descriptions": {
        "VCR": "Fine-tuned the BERT model on the VCR dataset to learn cross-modal alignment, focusing on visual-linguistic reasoning tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'cross-modal datasets' and 'VCR dataset'. VCR is a specific dataset used for fine-tuning the model, while 'cross-modal datasets' is too generic.",
      "processing_time": 66.67226457595825,
      "citing_paper_id": "258902254",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "However, the heterogeneous graph in [13] was just constructed based on the answer representations attended over visual objects or questions, which is not capable of modeling intra-modal and inter-modal correlations sufÔ¨Åciently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a heterogeneous graph. The context focuses on the limitations of the graph construction method.",
      "processing_time": 58.80942511558533,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "Secondly, these methods [12], [13] only utilize one layer of graph convolution network [16], [17], and the performance declines as the number of layers increases due to over-smoothing of graph learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.52477526664734,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "Secondly, these methods [12], [13] only utilize one layer of graph convolution network [16], [17], and the performance declines as the number of layers increases due to over-smoothing of graph learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.52477526664734,
      "citing_paper_id": "258902254",
      "cited_paper_id": 209376561
    },
    {
      "context_text": "The Ô¨Ånal node representation of the evolutionary reasoning graph is required to be pooled as a vector for the classi-Ô¨Åcation network, which is a two-layer MLP with the ReLU activation function adopted in [8], [10], [12], [13] as well.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of a two-layer MLP with ReLU activation for classification. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.96080803871155,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "The Ô¨Ånal node representation of the evolutionary reasoning graph is required to be pooled as a vector for the classi-Ô¨Åcation network, which is a two-layer MLP with the ReLU activation function adopted in [8], [10], [12], [13] as well.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of a two-layer MLP with ReLU activation for classification. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.96080803871155,
      "citing_paper_id": "258902254",
      "cited_paper_id": 207758781
    },
    {
      "context_text": "To better represent the association between visual and linguistic data, several graph-learning-based methods [11], [12], [13] were proposed for VCR and obtained better performances.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance improvements for Visual Commonsense Reasoning.",
      "processing_time": 57.187185764312744,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "Different from the homogeneous graphs used above, the heterogeneous graphs [14], [15] can simultaneously represent data from different domains; and Yu et al. [13] introduced the heterogeneous graph into VCR to build cross-modal correlations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions the use of heterogeneous graphs in VCR for building cross-modal correlations, but does not specify a named dataset.",
      "processing_time": 57.442320823669434,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "‚Ä¶with these methods, the capability of the proposed framework to model the correlation between the complex response and the question or image can be evaluated); (3) VCR methods, including R2C [8], CKRM [9], TAB-VCR [10], CCN [11], HGL [13], ECMR [12], VC R-CNN [40], CL-VCR [41] and JAE [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various methods and models for visual commonsense reasoning but does not explicitly reference any datasets. The context focuses on describing different approaches and their capabilities.",
      "processing_time": 59.126415491104126,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "‚Ä¶with these methods, the capability of the proposed framework to model the correlation between the complex response and the question or image can be evaluated); (3) VCR methods, including R2C [8], CKRM [9], TAB-VCR [10], CCN [11], HGL [13], ECMR [12], VC R-CNN [40], CL-VCR [41] and JAE [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various methods and models for visual commonsense reasoning but does not explicitly reference any datasets. The context focuses on describing different approaches and their capabilities.",
      "processing_time": 59.126415491104126,
      "citing_paper_id": "258902254",
      "cited_paper_id": 207758781
    },
    {
      "context_text": "‚Ä¶calculated by the softmax function based on the four predictions as Typically, such a four-way classiÔ¨Åcation problem is optimized by the cross-entropy loss function [8], [10], [12], [13], which can be formulated as where n is the number of samples, k is the sample index, and y is the ground-truth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses loss functions and classification problems.",
      "processing_time": 56.80863642692566,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "‚Ä¶calculated by the softmax function based on the four predictions as Typically, such a four-way classiÔ¨Åcation problem is optimized by the cross-entropy loss function [8], [10], [12], [13], which can be formulated as where n is the number of samples, k is the sample index, and y is the ground-truth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses loss functions and classification problems.",
      "processing_time": 56.80863642692566,
      "citing_paper_id": "258902254",
      "cited_paper_id": 207758781
    },
    {
      "context_text": "Yu et al. [13] adopted heterogeneous graphs which didn‚Äôt consider intra-modal correlations for reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions the use of heterogeneous graphs but does not specify a dataset. The context does not provide enough information to identify a specific, verifiable dataset.",
      "processing_time": 58.68235230445862,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "To tackle the VCR task, various models using holistic attention [8], [9], [10] and graph-based networks [11], [12], [13] are proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.83324146270752,
      "citing_paper_id": "258902254",
      "cited_paper_id": 202785103
    },
    {
      "context_text": "To tackle the VCR task, various models using holistic attention [8], [9], [10] and graph-based networks [11], [12], [13] are proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.83324146270752,
      "citing_paper_id": "258902254",
      "cited_paper_id": 207758781
    },
    {
      "context_text": "At the stage of feature extraction for original visual and linguistic data, each object in the given image is extracted as a 512-dimensional vector by the ResNet101-based backbone [32], which was proposed in TAB-VCR [10] and also used in ECMR [12].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of ResNet101 for feature extraction but does not specify a dataset. The cited papers do not introduce datasets either.",
      "processing_time": 59.350805044174194,
      "citing_paper_id": "258902254",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "At the stage of feature extraction for original visual and linguistic data, each object in the given image is extracted as a 512-dimensional vector by the ResNet101-based backbone [32], which was proposed in TAB-VCR [10] and also used in ECMR [12].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of ResNet101 for feature extraction but does not specify a dataset. The cited papers do not introduce datasets either.",
      "processing_time": 59.350805044174194,
      "citing_paper_id": "258902254",
      "cited_paper_id": 207758781
    },
    {
      "context_text": "Later, Lin et al. [10] utilized an extra detector to capture object attributions to enhance visual features in VCR. Wen et al. [9] transferred commonsense knowledge learnt from an additional linguistic knowledge base to the VCR task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of a linguistic knowledge base for transferring commonsense knowledge to the VCR task, but does not specify a named dataset.",
      "processing_time": 59.11472535133362,
      "citing_paper_id": "258902254",
      "cited_paper_id": 207758781
    },
    {
      "context_text": "[26]de- velopedÔ¨Çat-latticetransformerforChinesenamedentityrecog-nitionwithexcellentparallelizationability,whichconverted spansfromlatticestructuretoÔ¨Çatstructure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Flat-Lattice Transformer) for Chinese named entity recognition.",
      "processing_time": 58.10087990760803,
      "citing_paper_id": "258902254",
      "cited_paper_id": 216144439
    },
    {
      "context_text": "Transformer [24] is a self-attention-based model, which is Ô¨Årstly proposed for machine translation and rapidly applied to multiple natural language processing tasks [25], [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their applications. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.24347758293152,
      "citing_paper_id": "258902254",
      "cited_paper_id": 216144439
    },
    {
      "context_text": "Later,Transformeris modiÔ¨Åed and applied in many computer vision tasks such as image classiÔ¨Åcation [27] and object detection [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.24108147621155,
      "citing_paper_id": "258902254",
      "cited_paper_id": 218889832
    },
    {
      "context_text": "Carion et al. [28] proposed the DETR model with a Transformer encoder-decoder architecture to detect objects by bipartite matching between predictions and ground-truths, and then to reason the object relations and generate the Ô¨Ånal prediction set.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the DETR model and its architecture. The context focuses on the method and its application in object detection and relation reasoning.",
      "processing_time": 59.673879623413086,
      "citing_paper_id": "258902254",
      "cited_paper_id": 218889832
    },
    {
      "context_text": "Digital Object IdentiÔ¨Åer 10.1109/TMM.2023.3279691 referring expressions [1], visual question answering (VQA) [2], [3], visual grounding [4], [5], image and video captioning [6], [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and tasks. No verifiable resources are identified.",
      "processing_time": 57.89256143569946,
      "citing_paper_id": "258902254",
      "cited_paper_id": 218923747
    },
    {
      "context_text": "[21]proposedacross-modalcontrastivelearning methodtoenhancedataandlearnmoregeneralizablerepresen-tations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a cross-modal contrastive learning method. The context is too vague to infer any dataset usage.",
      "processing_time": 58.922534465789795,
      "citing_paper_id": "258902254",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "We think the method based on similarity computation or representation learning [56] may improve our method and discover more new links in the domain knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and potential improvements to the current approach.",
      "processing_time": 57.46909046173096,
      "citing_paper_id": "201066287",
      "cited_paper_id": 393948
    },
    {
      "context_text": "MMKG [11] extracts data from DBpedia and Wikipedia to construct knowledge graph.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia",
        "Wikipedia"
      ],
      "dataset_descriptions": {
        "DBpedia": "Used to extract structured data for constructing a knowledge graph, focusing on the integration of multi-modal information from web sources.",
        "Wikipedia": "Used to extract textual content for constructing a knowledge graph, focusing on the integration of multi-modal information from web sources."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions DBpedia and Wikipedia as sources for constructing a knowledge graph, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 70.58790326118469,
      "citing_paper_id": "201066287",
      "cited_paper_id": 588863
    },
    {
      "context_text": "One is to build knowledge graph based on structured or semi-structured information in Wikipedia or other existing knowledge base (e.g., MMKG [11], Babel-Net [38], and WordNetGraph [39]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMKG",
        "Babel-Net",
        "WordNetGraph"
      ],
      "dataset_descriptions": {
        "MMKG": "Used to generate a metallic materials knowledge graph, integrating structured information from DBpedia and Wikipedia.",
        "Babel-Net": "Used to build a multilingual knowledge graph, providing a large-scale lexicalized semantic network.",
        "WordNetGraph": "Used to construct a knowledge graph based on WordNet, focusing on lexical and semantic relationships."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions MMKG, Babel-Net, and WordNetGraph as examples of knowledge bases used to build knowledge graphs. These are specific, verifiable resources.",
      "processing_time": 75.60148644447327,
      "citing_paper_id": "201066287",
      "cited_paper_id": 588863
    },
    {
      "context_text": "One is to build knowledge graph based on structured or semi-structured information in Wikipedia or other existing knowledge base (e.g., MMKG [11], Babel-Net [38], and WordNetGraph [39]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMKG",
        "Babel-Net",
        "WordNetGraph"
      ],
      "dataset_descriptions": {
        "MMKG": "Used to generate a metallic materials knowledge graph, integrating structured information from DBpedia and Wikipedia.",
        "Babel-Net": "Used to build a multilingual knowledge graph, providing a large-scale lexicalized semantic network.",
        "WordNetGraph": "Used to construct a knowledge graph based on WordNet, focusing on lexical and semantic relationships."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions MMKG, Babel-Net, and WordNetGraph as examples of knowledge bases used to build knowledge graphs. These are specific, verifiable resources.",
      "processing_time": 75.60148644447327,
      "citing_paper_id": "201066287",
      "cited_paper_id": 21715202
    },
    {
      "context_text": "DBpedia is one of the largest domain knowledge graphs in the world, which contains a large amount of knowledge, such as related knowledge in the metallic materials domain [11].",
      "catation_intent": "research work",
      "resource_type": "knowledge base",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DBpedia as a knowledge graph containing domain-specific knowledge, but it does not specify a dataset. DBpedia is a knowledge base, not a dataset, and the context does not indicate its use as a reusable resource.",
      "processing_time": 62.29055428504944,
      "citing_paper_id": "201066287",
      "cited_paper_id": 588863
    },
    {
      "context_text": "And most of the text resources are associated with DBpedia [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBpedia"
      ],
      "dataset_descriptions": {
        "DBpedia": "Mentioned as a large-scale, multilingual knowledge base extracted from Wikipedia, but the specific usage in the research context is not detailed."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DBpedia, which is a well-known knowledge base. However, it does not specify how it is used in the research context.",
      "processing_time": 65.24532651901245,
      "citing_paper_id": "201066287",
      "cited_paper_id": 1181640
    },
    {
      "context_text": "WordNet [12] is a wide range of English lexical semantic network, organized into different synonym networks according to different parts of speech, and different synonym networks are linked by a variety of relationships.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNet"
      ],
      "dataset_descriptions": {
        "WordNet": "Described as a lexical database for English, organized into synonym networks linked by various relationships, but not used in a specific research context."
      },
      "confidence_score": 0.6,
      "reasoning": "WordNet is described as a lexical database, which fits the criteria for a dataset. However, it is not used in a specific research context within the given citation.",
      "processing_time": 66.58981966972351,
      "citing_paper_id": "201066287",
      "cited_paper_id": 1671874
    },
    {
      "context_text": "IMGpedia [9] is a large multi-modal knowledge graph which includes two types of relationships: image-to-image and image-to-text.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMGpedia"
      ],
      "dataset_descriptions": {
        "IMGpedia": "Used as a large multi-modal knowledge graph with image-to-image and image-to-text relationships, supporting research in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "IMGpedia is identified as a multi-modal knowledge graph, which fits the topic of multi-modal knowledge graph reasoning. It is used as a reusable resource.",
      "processing_time": 65.1647572517395,
      "citing_paper_id": "201066287",
      "cited_paper_id": 3117929
    },
    {
      "context_text": "The image feature acquisition can be divided into two categories: hand-crafted features extraction method [15]‚Äì[17] and deep learning extraction method [18]‚Äì[20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for feature extraction. No verifiable resources are identified.",
      "processing_time": 58.426997900009155,
      "citing_paper_id": "201066287",
      "cited_paper_id": 3582486
    },
    {
      "context_text": "For example, Thomas et al. [49], [50] used Inception-v3 [51] to get the vector of visual features, Word2vec [52] to get the vector of word embedding, and TransE [53] to get KG-Entity Embedding, and then used vector splicing to achieve multi-modal fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and methods (Inception-v3, Word2vec, TransE) but does not reference any specific datasets. The context focuses on the methodologies used for multi-modal fusion.",
      "processing_time": 61.15808963775635,
      "citing_paper_id": "201066287",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "However, HDSKG [44] extracts relational triples from Web pages and then uses a pre-trained SVM classiÔ¨Åer and domain dictionary to determine the domain relevance of the extracted triples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HDSKG, which is a method for extracting domain-specific knowledge graphs from web pages, not a dataset. No specific dataset is mentioned.",
      "processing_time": 59.2445342540741,
      "citing_paper_id": "201066287",
      "cited_paper_id": 7547770
    },
    {
      "context_text": "2) We use DBpedia-Spotlight [13] to annotate the image association text to obtain the context semantic labels of the image, which can represent the semantic summarized from the text description of the image using the entities derived from DBpedia resources.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of DBpedia-Spotlight for annotating image association text, but it does not refer to a specific dataset. DBpedia-Spotlight is a tool or method, not a dataset.",
      "processing_time": 61.833420753479004,
      "citing_paper_id": "201066287",
      "cited_paper_id": 8722811
    },
    {
      "context_text": "‚Ä¶there is also a large amount of visual knowledge residing in images, which can not only be used as auxiliary information to improve the effect of knowledge graph construction (e.g., IKRL model [47]), but also be used to construct multi-modal knowledge graph with the knowledge derived from images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of images to construct multi-modal knowledge graphs, but does not specify any named datasets. The reference to 'IKRL model' is a method, not a dataset.",
      "processing_time": 60.95038723945618,
      "citing_paper_id": "201066287",
      "cited_paper_id": 9909815
    },
    {
      "context_text": "‚Ä¶graph can be clas-siÔ¨Åed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB [37].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "Knowledge Vault",
        "Microsoft Concept Graph",
        "SIDER",
        "MusicBrainz",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as a general knowledge graph to provide structured information about entities and their relationships.",
        "YAGO": "Used as a general knowledge graph to enhance semantic understanding and reasoning capabilities.",
        "Knowledge Vault": "Used as a large-scale knowledge graph to integrate diverse sources of information.",
        "Microsoft Concept Graph": "Used as a general knowledge graph to support concept-based search and recommendation systems.",
        "SIDER": "Used as a domain-specific knowledge graph to study drug-side effect relationships in medical contexts.",
        "MusicBrainz": "Used as a domain-specific knowledge graph to provide structured information about musical artists and recordings.",
        "ImageNet": "Used to train and evaluate multi-modal reasoning models, focusing on image and label relationships."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs, including Freebase, YAGO, Knowledge Vault, Microsoft Concept Graph, SIDER, MusicBrainz, and IMDB. These are all specific, verifiable datasets or knowledge bases.",
      "processing_time": 83.08274173736572,
      "citing_paper_id": "201066287",
      "cited_paper_id": 10442573
    },
    {
      "context_text": "The purpose of relation extraction is to judge whether two entities are related from a sentence [29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of relation extraction. No verifiable resources are identified.",
      "processing_time": 58.62694811820984,
      "citing_paper_id": "201066287",
      "cited_paper_id": 10910955
    },
    {
      "context_text": "The knowledge graph can be clas-siÔ¨Åed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "Knowledge Vault",
        "Microsoft Concept Graph",
        "SIDER",
        "MusicBrainz",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as an example of a general knowledge graph, structured collaboratively to represent human knowledge.",
        "YAGO": "Used as an example of a general knowledge graph, focusing on linking entities to Wikipedia and GeoNames.",
        "Knowledge Vault": "Used as an example of a general knowledge graph, emphasizing large-scale extraction and integration of facts.",
        "Microsoft Concept Graph": "Used as an example of a general knowledge graph, derived from web-scale data to represent concepts and their relationships.",
        "SIDER": "Used as an example of a domain-specific knowledge graph, focusing on drug side effects and indications.",
        "MusicBrainz": "Used as an example of a multi-modal knowledge graph, specifically for reasoning tasks involving music and multimedia data.",
        "IMDB": "Used as an example of a domain-specific knowledge graph, focusing on movies and their attributes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs, which are specific, verifiable resources. These are used as examples of general and domain-specific knowledge graphs.",
      "processing_time": 83.5743522644043,
      "citing_paper_id": "201066287",
      "cited_paper_id": 14775471
    },
    {
      "context_text": "The knowledge graph can be clas-siÔ¨Åed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB‚Ä¶",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "YAGO",
        "Knowledge Vault",
        "Microsoft Concept Graph",
        "SIDER",
        "MusicBrainz",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as an example of a general knowledge graph, structured collaboratively to represent human knowledge.",
        "YAGO": "Used as an example of a general knowledge graph, focusing on linking entities to Wikipedia and GeoNames.",
        "Knowledge Vault": "Used as an example of a general knowledge graph, emphasizing large-scale extraction and integration of facts.",
        "Microsoft Concept Graph": "Used as an example of a general knowledge graph, derived from web-scale data to represent concepts and their relationships.",
        "SIDER": "Used as an example of a domain-specific knowledge graph, focusing on drug side effects and indications.",
        "MusicBrainz": "Used as an example of a multi-modal knowledge graph, specifically for reasoning tasks involving music and multimedia data.",
        "IMDB": "Used as an example of a domain-specific knowledge graph, focusing on movies and their attributes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs, which are specific, verifiable resources. These are used as examples of general and domain-specific knowledge graphs.",
      "processing_time": 83.5743522644043,
      "citing_paper_id": "201066287",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "WordNetGraph [39] is based on the conceptual structure deÔ¨Åned by WordNet and use the classiÔ¨Åer to automatically label the terms deÔ¨Åned by natural language to construct the knowledge graph.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WordNetGraph"
      ],
      "dataset_descriptions": {
        "WordNetGraph": "Used to construct a knowledge graph by automatically labeling terms defined by natural language, focusing on interpretable text entailment recognition."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'WordNetGraph' which is a specific dataset derived from WordNet. It is used to construct a knowledge graph by labeling terms defined by natural language.",
      "processing_time": 66.58682107925415,
      "citing_paper_id": "201066287",
      "cited_paper_id": 21715202
    },
    {
      "context_text": "In order to further enhance the effect of multi-modal knowledge graph construction, Mousselly et al. [54] embed visual and linguistic to complete the multi-modal knowledge graph under the veriÔ¨Å-cation of structured knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions embedding visual and linguistic data to construct a multi-modal knowledge graph, but does not specify a named dataset.",
      "processing_time": 58.434271574020386,
      "citing_paper_id": "201066287",
      "cited_paper_id": 44145776
    },
    {
      "context_text": "At present, the main methods of entity extraction are rule-based methods [21]‚Äì[23], unsupervised methods [24], [25], neural network methods [26]‚Äì[28] and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for entity extraction. There are no clear identifiers for datasets within the text.",
      "processing_time": 59.461811780929565,
      "citing_paper_id": "201066287",
      "cited_paper_id": 73728465
    },
    {
      "context_text": "At present, the main methods of entity extraction are rule-based methods [21]‚Äì[23], unsupervised methods [24], [25], neural network methods [26]‚Äì[28] and so on.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for entity extraction. There are no clear identifiers for datasets within the text.",
      "processing_time": 59.461811780929565,
      "citing_paper_id": "201066287",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶of domain-speciÔ¨Åc data generally focuses on text data or structured data, such as transforming structured databases into knowledge graph to provide semantic query services [1] and extracting knowledge from unstructured source data to build new ontologies (e.g., STSM [2], X. Zhang [3]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'domain-specific data' and 'structured databases', but does not specify any named datasets. It refers to methods and tools for transforming and integrating data, which are not considered datasets.",
      "processing_time": 61.332234621047974,
      "citing_paper_id": "201066287",
      "cited_paper_id": 103802785
    },
    {
      "context_text": "For each modality of nodes, (a) in ùêæùê∫ ùê∏ , entity F ‚àà R ùëõ ùê∏ √ó ùëë ùê∏ is initialized by the TransE model [1], with ùëë ùê∏ as its dimension; (b) in ùêæùê∫ ùëá , text attribute F ‚àà R ùëõ ùëá √ó ùëë ùëá is initialized by the BERT [21], with ùëë ùëá as its dimension; (c) in ùêæùê∫ ùêº , image attribute F ‚àà R ùëõ ùêº √ó ùëë ùêº is initialized by the VGG16 [18], with ùëë ùêº as its dimension.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the initialization of different modalities in a multi-modal knowledge graph using various models. No specific datasets are mentioned, only models and methods.",
      "processing_time": 59.63477849960327,
      "citing_paper_id": "257921377",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "For each modality of nodes, (a) in ùêæùê∫ ùê∏ , entity F ‚àà R ùëõ ùê∏ √ó ùëë ùê∏ is initialized by the TransE model [1], with ùëë ùê∏ as its dimension; (b) in ùêæùê∫ ùëá , text attribute F ‚àà R ùëõ ùëá √ó ùëë ùëá is initialized by the BERT [21], with ùëë ùëá as its dimension; (c) in ùêæùê∫ ùêº , image attribute F ‚àà R ùëõ ùêº √ó ùëë ùêº is initialized by the VGG16 [18], with ùëë ùêº as its dimension.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the initialization of different modalities in a multi-modal knowledge graph using various models. No specific datasets are mentioned, only models and methods.",
      "processing_time": 59.63477849960327,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "For each modality of nodes, (a) in KGE , entity FE ‚àà RnE√ódE is initialized by the TransE model [1], with dE as its dimension; (b) in KGT , text attribute FT ‚àà RT √ódT is initialized by the BERT [21], with dT as its dimension; (c) in KGI , image attribute FI ‚àà RnI√ódI is initialized by the VGG16 [18], with dI as its dimension.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods used for initializing different modalities in a multi-modal knowledge graph.",
      "processing_time": 59.04789185523987,
      "citing_paper_id": "257921377",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The BERT version is bert-base-uncased in huggingface 3 for text attributes initialization and VGG version is VGG16 4 for image attributes initialization.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and frameworks which are excluded according to the instructions.",
      "processing_time": 58.24550127983093,
      "citing_paper_id": "257921377",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 15912887
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 49299019
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 208176414
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 224281034
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 248108992
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 252070586
    },
    {
      "context_text": "[0-4] [5-9] [10-14] [15-19] [20-24] Difference of Attribute Number 55 60 65 70 75 80",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The provided context does not contain any specific, verifiable dataset names or clear usage descriptions. The context appears to be a table or figure caption, which does not provide enough information to identify datasets.",
      "processing_time": 61.160096645355225,
      "citing_paper_id": "257921377",
      "cited_paper_id": 254564635
    },
    {
      "context_text": "TransE [1] IPTransE [32] GCN-align [23] SEA [16] IMUSE [8] AttrGNN [15] 11.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.644388914108276,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "TransE [1] IPTransE [32] GCN-align [23] SEA [16] IMUSE [8] AttrGNN [15] 11.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.644388914108276,
      "citing_paper_id": "257921377",
      "cited_paper_id": 28913990
    },
    {
      "context_text": "TransE [1] IPTransE [32] GCN-align [23] SEA [16] IMUSE [8] AttrGNN [15] 13.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.64752125740051,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "TransE [1] IPTransE [32] GCN-align [23] SEA [16] IMUSE [8] AttrGNN [15] 13.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.64752125740051,
      "citing_paper_id": "257921377",
      "cited_paper_id": 28913990
    },
    {
      "context_text": "= ÔøΩ 1 of entity ÔøΩ ‚àà E in EÔøΩ , where ÔøΩ ‚àà {ÔøΩ, ÔøΩ }, and ÔøΩÔøΩ is the TransE model [1], with ÔøΩÔøΩ as its dimension; (b) in ÔøΩÔøΩÔøΩ , text at- number of attribute in hÔøΩ,ÔøΩ .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (TransE). The context is too vague to infer any dataset usage.",
      "processing_time": 58.889185428619385,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Thus, we define the representations of entity relations R ( 0 ) and attribute relations R ( 0 ) are calculated as follows respectively: where R , R ‚àà R ùëõ ùëü √ó ùëë ùê∏ are the initial representations of relations, which are calculated from TransE.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the method TransE for calculating relation representations.",
      "processing_time": 57.841156005859375,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Here, we introduce the image attributes initialized by VGG16 for entity representation with the same aggregation manner of text attributes: (1) TransE [1] assumes that the entity embedding ùë£ should be close to the attribute embedding ùëé plus their relation ùëü .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (TransE) and a model (VGG16). No verifiable datasets are referenced.",
      "processing_time": 60.02428364753723,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Here, we introduce the image attributes initialized by VGG16 for entity representation with the same aggregation manner of text attributes: (1) TransE [1] assumes that the entity embedding ÔøΩ should be close to the attribute embedding ÔøΩ plus their relation ÔøΩ .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is used to explain the methodological approach.",
      "processing_time": 59.4114453792572,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "(2) IPTransE [32] is a translation-based method to jointly optimize entities and relations representation in knowledge graphs with an iterative and parameter sharing strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a method (IPTransE) rather than a dataset. No specific dataset is mentioned or used in the described research.",
      "processing_time": 59.21475338935852,
      "citing_paper_id": "257921377",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "(2) IPTransE [32] is a translation-based method to jointly optimize entities and relations representation in knowledge graphs with an iterative and parameter sharing strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a method (IPTransE) rather than a dataset. No specific dataset is mentioned or used in the described research.",
      "processing_time": 59.21475338935852,
      "citing_paper_id": "257921377",
      "cited_paper_id": 28913990
    },
    {
      "context_text": "Following previous MMEA studies [3, 7, 13], this paper focuses on the latter one.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous studies. There is no indication of a reusable resource or dataset being used.",
      "processing_time": 59.2156298160553,
      "citing_paper_id": "257921377",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "The multi-modal entity alignment task [3, 7, 13] is to identify whether a pair of entities in two multimodal knowledge graphs is equivalent or not.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task definition. No verifiable resources are identified.",
      "processing_time": 58.80537557601929,
      "citing_paper_id": "257921377",
      "cited_paper_id": 221995513
    },
    {
      "context_text": ", recommendation systems [19, 28] and question answering systems [11, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 59.99397397041321,
      "citing_paper_id": "257921377",
      "cited_paper_id": 224281034
    },
    {
      "context_text": ", recommendation systems [19, 28] and question answering systems [11, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 59.99397397041321,
      "citing_paper_id": "257921377",
      "cited_paper_id": 254564635
    },
    {
      "context_text": ", recommendation systems [19, 28] and question answering systems [11, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general application areas. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 59.99397397041321,
      "citing_paper_id": "257921377",
      "cited_paper_id": 257697222
    },
    {
      "context_text": "To make the entity representation have fault tolerance ability for the generated ACKGs, we adopt random dropouts [29] on neighbors to improve the robustness of entity representation, which assumes that missing part of entities does not affect the semantic meaning of ACKG.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving robustness in graph representations.",
      "processing_time": 57.98425006866455,
      "citing_paper_id": "257921377",
      "cited_paper_id": 225076220
    },
    {
      "context_text": "In general, the entity alignment task includes cross-language entity alignment and multisource entity alignment [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of the entity alignment task. No verifiable resources are identified.",
      "processing_time": 59.63632249832153,
      "citing_paper_id": "257921377",
      "cited_paper_id": 234950071
    },
    {
      "context_text": "Furthermore, because of the multi-modal nature of KGs in real-world, there are several works [10, 22, 33] beginning to focus on the MMEA technology.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to multi-modal knowledge graph technology.",
      "processing_time": 59.020957469940186,
      "citing_paper_id": "257921377",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "The LSTM network [54] has been widely used for aggregating few-shot support entity pairs [45] [24], which can model the interactions and capture the similarities among few-shot support entity pairs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The LSTM network is referenced but not used as a dataset.",
      "processing_time": 59.96953058242798,
      "citing_paper_id": "271405981",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "During the test period, we build the candidate entity set C based on the entity type constraint [48] [22] to predict facts of new relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for building a candidate entity set. No verifiable resources are identified.",
      "processing_time": 59.81415271759033,
      "citing_paper_id": "271405981",
      "cited_paper_id": 2127100
    },
    {
      "context_text": "To generate the text embeddings, we utilize the Doc2Vec model [56], an improved version of the Word2Vec framework, to pre-train the text data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using the Doc2Vec model for pre-training text data, but does not specify a dataset. The citation is focused on the method rather than a specific dataset.",
      "processing_time": 61.382898569107056,
      "citing_paper_id": "271405981",
      "cited_paper_id": 2407601
    },
    {
      "context_text": "ConvE [39] utilizes a convolutions neural network to jointly capture the features of entities and relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only describes the method ConvE, which uses a convolutional neural network to capture features of entities and relations.",
      "processing_time": 60.70514893531799,
      "citing_paper_id": "271405981",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "Traditional KGE models are categorized into three groups: translation-based models [31] [32] [34], bilinear-based models [35] [36] [37] and neural network models [38] [39] [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 59.84845948219299,
      "citing_paper_id": "271405981",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "Traditional KGE models are categorized into three groups: translation-based models [31] [32] [34], bilinear-based models [35] [36] [37] and neural network models [38] [39] [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 59.84845948219299,
      "citing_paper_id": "271405981",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Traditional KGE models are categorized into three groups: translation-based models [31] [32] [34], bilinear-based models [35] [36] [37] and neural network models [38] [39] [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 59.84845948219299,
      "citing_paper_id": "271405981",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "Traditional KGE models are categorized into three groups: translation-based models [31] [32] [34], bilinear-based models [35] [36] [37] and neural network models [38] [39] [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of knowledge graph embedding models. No verifiable resources are identified.",
      "processing_time": 59.84845948219299,
      "citing_paper_id": "271405981",
      "cited_paper_id": 210695009
    },
    {
      "context_text": "A promising approach is knowledge graph embedding [31] [32] [33] which embeds entities and relation into a low continuous vector space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 59.8477201461792,
      "citing_paper_id": "271405981",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "A promising approach is knowledge graph embedding [31] [32] [33] which embeds entities and relation into a low continuous vector space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of knowledge graph embedding. No verifiable resources are identified.",
      "processing_time": 59.8477201461792,
      "citing_paper_id": "271405981",
      "cited_paper_id": 258333674
    },
    {
      "context_text": "The well-known model of this work TransE [31] interprets each relation as a translation from a head to a tail.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model (TransE). The context focuses on the interpretation of relations in the model, not on the use of a dataset.",
      "processing_time": 61.53886342048645,
      "citing_paper_id": "271405981",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "We employ three widely used uni-modal models including TransE [31], DistMult [36], and ComplEx [37].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (TransE, DistMult, ComplEx) but does not refer to any specific datasets. The citation intent is to reference reusable models, not datasets.",
      "processing_time": 61.82351994514465,
      "citing_paper_id": "271405981",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "We employ three widely used uni-modal models including TransE [31], DistMult [36], and ComplEx [37].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (TransE, DistMult, ComplEx) but does not refer to any specific datasets. The citation intent is to reference reusable models, not datasets.",
      "processing_time": 61.82351994514465,
      "citing_paper_id": "271405981",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "ComplEx [37] argues that the real space is inadequate to model interactions between entities and extends bilinear model to the complex space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ComplEx) and its theoretical extension to complex space.",
      "processing_time": 59.61186456680298,
      "citing_paper_id": "271405981",
      "cited_paper_id": 15150247
    },
    {
      "context_text": "With the development of multi-modal knowledge graphs (MMKGs), some multi-modal KGC methods [15] [16] [17] [18] have been proposed to utilize multi-modal data, such as image features [19] [20] and text descriptions [21], to further improve KGC performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions multi-modal data sources like image features and text descriptions but does not specify any named datasets. The cited papers do not provide additional specific dataset names.",
      "processing_time": 61.17136836051941,
      "citing_paper_id": "271405981",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "With the development of multi-modal knowledge graphs (MMKGs), some multi-modal KGC methods [15] [16] [17] [18] have been proposed to utilize multi-modal data, such as image features [19] [20] and text descriptions [21], to further improve KGC performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions multi-modal data sources like image features and text descriptions but does not specify any named datasets. The cited papers do not provide additional specific dataset names.",
      "processing_time": 61.17136836051941,
      "citing_paper_id": "271405981",
      "cited_paper_id": 207930212
    },
    {
      "context_text": "With the development of multi-modal knowledge graphs (MMKGs), some multi-modal KGC methods [15] [16] [17] [18] have been proposed to utilize multi-modal data, such as image features [19] [20] and text descriptions [21], to further improve KGC performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions multi-modal data sources like image features and text descriptions but does not specify any named datasets. The cited papers do not provide additional specific dataset names.",
      "processing_time": 61.17136836051941,
      "citing_paper_id": "271405981",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "With the development of multi-modal knowledge graphs (MMKGs), some multi-modal KGC methods [15] [16] [17] [18] have been proposed to utilize multi-modal data, such as image features [19] [20] and text descriptions [21], to further improve KGC performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions multi-modal data sources like image features and text descriptions but does not specify any named datasets. The cited papers do not provide additional specific dataset names.",
      "processing_time": 61.17136836051941,
      "citing_paper_id": "271405981",
      "cited_paper_id": 248779998
    },
    {
      "context_text": "SpeciÔ¨Åcally, MMSN-Bert&ViT initial-izes text features by Bert [57] and image features by ViT [58], respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and ViT but does not refer to them as datasets. They are models used for initializing text and image features, respectively.",
      "processing_time": 60.366472244262695,
      "citing_paper_id": "271405981",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "SpeciÔ¨Åcally, MMSN-Bert&ViT initial-izes text features by Bert [57] and image features by ViT [58], respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and ViT but does not refer to them as datasets. They are models used for initializing text and image features, respectively.",
      "processing_time": 60.366472244262695,
      "citing_paper_id": "271405981",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "Most existing few-shot KGC models employ a meta-learning framework [42] [43] [44] and seek to enhance entity representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodology of few-shot KGC models and their use of meta-learning frameworks.",
      "processing_time": 61.33763289451599,
      "citing_paper_id": "271405981",
      "cited_paper_id": 67413369
    },
    {
      "context_text": "To automatically complete KGs, knowledge graph completion (KGC) [11] [12] [13] [14] is proposed to discover potential facts and has achieved great contributions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of knowledge graph completion (KGC). The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.709057092666626,
      "citing_paper_id": "271405981",
      "cited_paper_id": 67855617
    },
    {
      "context_text": "To automatically complete KGs, knowledge graph completion (KGC) [11] [12] [13] [14] is proposed to discover potential facts and has achieved great contributions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of knowledge graph completion (KGC). The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.709057092666626,
      "citing_paper_id": "271405981",
      "cited_paper_id": 240070688
    },
    {
      "context_text": "To automatically complete KGs, knowledge graph completion (KGC) [11] [12] [13] [14] is proposed to discover potential facts and has achieved great contributions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of knowledge graph completion (KGC). The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.709057092666626,
      "citing_paper_id": "271405981",
      "cited_paper_id": 249538526
    },
    {
      "context_text": "In addition, the image embeddings of FB-Img-Few and DB-Img-Few are provided from [17] and [55], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-Img-Few",
        "DB-Img-Few"
      ],
      "dataset_descriptions": {
        "FB-Img-Few": "Used to provide image embeddings for multi-modal knowledge graph reasoning, focusing on few-shot learning scenarios.",
        "DB-Img-Few": "Used to provide image embeddings for multi-modal knowledge graph reasoning, focusing on few-shot learning scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'FB-Img-Few' and 'DB-Img-Few' as sources of image embeddings, which are likely datasets used in the context of multi-modal knowledge graphs.",
      "processing_time": 74.22812056541443,
      "citing_paper_id": "271405981",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FreeBase",
        "Wikidata",
        "YAGO"
      ],
      "dataset_descriptions": {
        "FreeBase": "Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information.",
        "Wikidata": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data.",
        "YAGO": "Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
      "processing_time": 77.49904990196228,
      "citing_paper_id": "271405981",
      "cited_paper_id": 212827903
    },
    {
      "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FreeBase",
        "Wikidata",
        "YAGO"
      ],
      "dataset_descriptions": {
        "FreeBase": "Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information.",
        "Wikidata": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data.",
        "YAGO": "Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
      "processing_time": 77.49904990196228,
      "citing_paper_id": "271405981",
      "cited_paper_id": 254221022
    },
    {
      "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FreeBase",
        "Wikidata",
        "YAGO"
      ],
      "dataset_descriptions": {
        "FreeBase": "Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information.",
        "Wikidata": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data.",
        "YAGO": "Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
      "processing_time": 77.49904990196228,
      "citing_paper_id": "271405981",
      "cited_paper_id": 258333655
    },
    {
      "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FreeBase",
        "Wikidata",
        "YAGO"
      ],
      "dataset_descriptions": {
        "FreeBase": "Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information.",
        "Wikidata": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data.",
        "YAGO": "Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
      "processing_time": 77.49904990196228,
      "citing_paper_id": "271405981",
      "cited_paper_id": 258333851
    },
    {
      "context_text": "Notably, the image initialization methods are provided by [18].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for image initialization.",
      "processing_time": 57.72965669631958,
      "citing_paper_id": "271405981",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "Traditional KGC models [11] [13] [14] aim at completing potential facts for seen relations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about knowledge graph completion models.",
      "processing_time": 58.58215260505676,
      "citing_paper_id": "271405981",
      "cited_paper_id": 240070688
    },
    {
      "context_text": "Traditional KGC models [11] [13] [14] aim at completing potential facts for seen relations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about knowledge graph completion models.",
      "processing_time": 58.58215260505676,
      "citing_paper_id": "271405981",
      "cited_paper_id": 249538526
    },
    {
      "context_text": "We employ three widely used models, including IKRL [28], TransAE [29] and MKGFormer [41], as baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not reference any specific datasets. The context is focused on describing the models used as baselines.",
      "processing_time": 59.77607822418213,
      "citing_paper_id": "271405981",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "MKGFormer [41] is a hybrid Transformer network for multimodal knowledge graph completion, which presents a multi-modal encoder to model image-text incorporated entity representations with multi-level fusion at the last several layers of ViT and BERT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a method (MKGFormer) and does not mention any specific datasets. The focus is on the architecture and functionality of the model.",
      "processing_time": 60.495726585388184,
      "citing_paper_id": "271405981",
      "cited_paper_id": 248524814
    },
    {
      "context_text": "Additionally, different modalities of an entity are dependent, leading to inter-modal interactions between multiple modalities [16] [26] [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of inter-modal interactions. The cited papers' titles suggest a focus on multi-modal learning but do not provide specific dataset names.",
      "processing_time": 61.857662200927734,
      "citing_paper_id": "271405981",
      "cited_paper_id": 248779998
    },
    {
      "context_text": "Additionally, different modalities of an entity are dependent, leading to inter-modal interactions between multiple modalities [16] [26] [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of inter-modal interactions. The cited papers' titles suggest a focus on multi-modal learning but do not provide specific dataset names.",
      "processing_time": 61.857662200927734,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252070586
    },
    {
      "context_text": "Unlike traditional models that primarily handle seen relations, few-shot KGC models aim to complete potential facts involving unseen relations [22] [24] [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to few-shot KGC models. No verifiable resources are identified.",
      "processing_time": 59.982988595962524,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252518772
    },
    {
      "context_text": "MULTIFORM [25] is the most relevant study speciÔ¨Åcally designed to handle few-shot MMKGC.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MULTIFORM but does not refer to it as a dataset. It is described as a study, which suggests it is a method or research work rather than a dataset.",
      "processing_time": 61.51391792297363,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252518772
    },
    {
      "context_text": "To tackle this challenge, several meta-learning approaches [23] [24] [25] have been proposed to conduct few-shot knowledge graph completion (FKGC).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only meta-learning approaches for few-shot knowledge graph completion.",
      "processing_time": 58.90363121032715,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252518772
    },
    {
      "context_text": "MULTI-FORM [25] is the state-of-the-art MFKGC model, which focuses on pre-training a variety of modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (MULTI-FORM) rather than a dataset. No specific dataset is referenced in the context.",
      "processing_time": 59.576542139053345,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252518772
    },
    {
      "context_text": "Following the constructed rule for few-shot relational benchmarks [22] [25] [46], the relations associated with less than 500 but more than 50 are selected as few-shot relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for selecting few-shot relations. No verifiable resource names are provided.",
      "processing_time": 59.666324853897095,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252518772
    },
    {
      "context_text": "To address this problem, MUL-TIFORM [25] concatenates the pre-trained representations of multiple modalities to further enhance entity representations and improve FKGC performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MUL-TIFORM but does not refer to it as a dataset. It is described as a method or model for enhancing entity representations and improving FKGC performance.",
      "processing_time": 60.97768521308899,
      "citing_paper_id": "271405981",
      "cited_paper_id": 252518772
    },
    {
      "context_text": "4) Although MULTIFORM explores multi-modal data to enhance entity representations and achieves the best performance among the baselines on FB-Img-Few, its improvements are limited to a simple concatenation operation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB-Img-Few"
      ],
      "dataset_descriptions": {
        "FB-Img-Few": "Used to evaluate the performance of the MULTIFORM method, specifically focusing on the enhancement of entity representations through multi-modal data."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'FB-Img-Few' as a dataset used to evaluate the performance of the MULTIFORM method. However, the context does not provide details on how the dataset is used beyond this evaluation.",
      "processing_time": 68.7996416091919,
      "citing_paper_id": "271405981",
      "cited_paper_id": 258333851
    },
    {
      "context_text": "In addition, as the representations of images are continuously updated during model training, at the end of each epoch, we leverage Faiss (Johnson et al., 2019) to retrieve top-k images to update the visual prototype representations of the matched concepts.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Faiss, which is a tool for similarity search, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 58.365607023239136,
      "citing_paper_id": "259370889",
      "cited_paper_id": 926364
    },
    {
      "context_text": "Here, we suggest that image-text pairs of products have unique characteristics that are different from the general domain (such as MS-COCO (Lin et al., 2014), Flickr30k (Young et al., 2014) and Conceptual Captions (Sharma et al., 2018)), with examples shown in Figure 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO",
        "Flickr30k",
        "Conceptual Captions"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to highlight differences between general domain image-text pairs and product-specific pairs, emphasizing unique characteristics in product data.",
        "Flickr30k": "Used to highlight differences between general domain image-text pairs and product-specific pairs, emphasizing unique characteristics in product data.",
        "Conceptual Captions": "Used to highlight differences between general domain image-text pairs and product-specific pairs, emphasizing unique characteristics in product data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for image-text pairs, which are relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 75.85932278633118,
      "citing_paper_id": "259370889",
      "cited_paper_id": 3104920
    },
    {
      "context_text": "Here, we suggest that image-text pairs of products have unique characteristics that are different from the general domain (such as MS-COCO (Lin et al., 2014), Flickr30k (Young et al., 2014) and Conceptual Captions (Sharma et al., 2018)), with examples shown in Figure 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO",
        "Flickr30k",
        "Conceptual Captions"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to highlight differences between general domain image-text pairs and product-specific pairs, emphasizing unique characteristics in product data.",
        "Flickr30k": "Used to highlight differences between general domain image-text pairs and product-specific pairs, emphasizing unique characteristics in product data.",
        "Conceptual Captions": "Used to highlight differences between general domain image-text pairs and product-specific pairs, emphasizing unique characteristics in product data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for image-text pairs, which are relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 75.85932278633118,
      "citing_paper_id": "259370889",
      "cited_paper_id": 51876975
    },
    {
      "context_text": "Image-text retrieval, which finds correlated texts (or images) for a given image (or text) (Karpathy and Fei-Fei, 2015; Faghri et al., 2017), is a popular cross-modal task with strong practical values in a wide range of industrial applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications. No verifiable resources are identified.",
      "processing_time": 58.85678243637085,
      "citing_paper_id": "259370889",
      "cited_paper_id": 6095318
    },
    {
      "context_text": "Image-text retrieval, which finds correlated texts (or images) for a given image (or text) (Karpathy and Fei-Fei, 2015; Faghri et al., 2017), is a popular cross-modal task with strong practical values in a wide range of industrial applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications. No verifiable resources are identified.",
      "processing_time": 58.85678243637085,
      "citing_paper_id": "259370889",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "To ensure that both similarity and diversity of visual representations for each concept are considered, we slightly expand the range of image candidates (using a larger k ), and employ the MMR algorithm (Carbonell and Goldstein, 1998) to improve the diversity of the selected images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the MMR algorithm but does not refer to a specific dataset. The citation is about a method for improving diversity in selected images, not a dataset.",
      "processing_time": 60.20986342430115,
      "citing_paper_id": "259370889",
      "cited_paper_id": 6334682
    },
    {
      "context_text": "VLP models can be categorized into single-stream models (Chen et al., 2020; Li et al., 2020a; Gan et al., 2020), which first concatenate multi-modal inputs for interactions, and dual-stream models (Jia et al., 2021; Radford et al., 2021; Yao et al., 2021; Li et al., 2020b), which obtain the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of VLP models. No verifiable resources are identified.",
      "processing_time": 58.84786820411682,
      "citing_paper_id": "259370889",
      "cited_paper_id": 201058752
    },
    {
      "context_text": "VLP models can be categorized into single-stream models (Chen et al., 2020; Li et al., 2020a; Gan et al., 2020), which first concatenate multi-modal inputs for interactions, and dual-stream models (Jia et al., 2021; Radford et al., 2021; Yao et al., 2021; Li et al., 2020b), which obtain the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of VLP models. No verifiable resources are identified.",
      "processing_time": 58.84786820411682,
      "citing_paper_id": "259370889",
      "cited_paper_id": 219573512
    },
    {
      "context_text": "VLP models can be categorized into single-stream models (Chen et al., 2020; Li et al., 2020a; Gan et al., 2020), which first concatenate multi-modal inputs for interactions, and dual-stream models (Jia et al., 2021; Radford et al., 2021; Yao et al., 2021; Li et al., 2020b), which obtain the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of VLP models. No verifiable resources are identified.",
      "processing_time": 58.84786820411682,
      "citing_paper_id": "259370889",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "For models in different sizes, we refer to (Turc et al., 2019) to set the attention width and number of attention heads of the text encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model configurations. There are no verifiable resources that meet the criteria.",
      "processing_time": 58.88586640357971,
      "citing_paper_id": "259370889",
      "cited_paper_id": 202889175
    },
    {
      "context_text": "‚Ä¶semantic level interactions of images and texts, some works improve the similarity strategy by calculating between the image patch and the text token (Yao et al., 2021) or leverage fine-grained image information through object detectors (Li et al., 2020c,b; Gan et al., 2020; Zeng et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 58.886815786361694,
      "citing_paper_id": "259370889",
      "cited_paper_id": 219573512
    },
    {
      "context_text": "‚Ä¶semantic level interactions of images and texts, some works improve the similarity strategy by calculating between the image patch and the text token (Yao et al., 2021) or leverage fine-grained image information through object detectors (Li et al., 2020c,b; Gan et al., 2020; Zeng et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 58.886815786361694,
      "citing_paper_id": "259370889",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "‚Ä¶semantic level interactions of images and texts, some works improve the similarity strategy by calculating between the image patch and the text token (Yao et al., 2021) or leverage fine-grained image information through object detectors (Li et al., 2020c,b; Gan et al., 2020; Zeng et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 58.886815786361694,
      "citing_paper_id": "259370889",
      "cited_paper_id": 244117525
    },
    {
      "context_text": "Others introduces structured scene graphs for semantic knowledge (Yu et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (ERNIE-ViL) that uses scene graphs for enhancing vision-language representations.",
      "processing_time": 59.55532693862915,
      "citing_paper_id": "259370889",
      "cited_paper_id": 220265934
    },
    {
      "context_text": "In the general domain, fine-grained interactions could be achieved with object detection (Li et al., 2020c; Tan and Bansal, 2019), scene graph parsing (Cui et al., 2021), or semantic analysis (Yu et al., 2021; Li et al., 2020b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.38990879058838,
      "citing_paper_id": "259370889",
      "cited_paper_id": 220265934
    },
    {
      "context_text": "In the general domain, fine-grained interactions could be achieved with object detection (Li et al., 2020c; Tan and Bansal, 2019), scene graph parsing (Cui et al., 2021), or semantic analysis (Yu et al., 2021; Li et al., 2020b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.38990879058838,
      "citing_paper_id": "259370889",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "*Corresponding author. pre-trained (VLP) models (Yao et al., 2021; Zeng et al., 2021; Li et al., 2020c) has greatly improved the representation learning across data of different modalities, leading to significant performance improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained models and their impact on multi-modal representation learning.",
      "processing_time": 58.273815870285034,
      "citing_paper_id": "259370889",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "*Corresponding author. pre-trained (VLP) models (Yao et al., 2021; Zeng et al., 2021; Li et al., 2020c) has greatly improved the representation learning across data of different modalities, leading to significant performance improvement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained models and their impact on multi-modal representation learning.",
      "processing_time": 58.273815870285034,
      "citing_paper_id": "259370889",
      "cited_paper_id": 244117525
    },
    {
      "context_text": "‚Ä¶et al., 2020a; Gan et al., 2020), which first concatenate multi-modal inputs for interactions, and dual-stream models (Jia et al., 2021; Radford et al., 2021; Yao et al., 2021; Li et al., 2020b), which obtain the representations of the image and text respectively and learn the alignment afterwards.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 58.27162027359009,
      "citing_paper_id": "259370889",
      "cited_paper_id": 229924402
    },
    {
      "context_text": "‚Ä¶et al., 2020a; Gan et al., 2020), which first concatenate multi-modal inputs for interactions, and dual-stream models (Jia et al., 2021; Radford et al., 2021; Yao et al., 2021; Li et al., 2020b), which obtain the representations of the image and text respectively and learn the alignment afterwards.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 58.27162027359009,
      "citing_paper_id": "259370889",
      "cited_paper_id": 244117525
    },
    {
      "context_text": "We can see that with similar performance (CLIP and FashionKLIP-M), our model has much lower RT and higher QPS.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (CLIP and FashionKLIP-M). The context focuses on comparing model performance, not on dataset usage.",
      "processing_time": 60.18937587738037,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We train a CLIP-style model to learn the global representations of image-text pairs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general training approach using image-text pairs.",
      "processing_time": 57.239388942718506,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "‚Ä¶2020; Li et al., 2020a; Gan et al., 2020), which first concatenate multi-modal inputs for interactions, and dual-stream models (Jia et al., 2021; Radford et al., 2021; Yao et al., 2021; Li et al., 2020b), which obtain the representations of the image and text respectively and learn the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 59.14899945259094,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "‚Ä¶we utilize FashionKLIP-B model to compare against other baseline models, which uses ViT-B/32 (Dosovitskiy et al., 2020) as the image encoder, and adopts a 12-layer 512 wide Text Transformer as the text encoder as (Radford et al., 2021), in 63M parameter with 8 attention heads each layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their components. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 59.319575786590576,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Image-to-Text Text-to-Image R@1 R@5 R@10 R@1 R@5 R@10 CLIP Table 2: Retrieval results on FashionGen (Full).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FashionGen"
      ],
      "dataset_descriptions": {
        "FashionGen": "Used to evaluate image-to-text and text-to-image retrieval performance, focusing on visual and textual alignment in fashion-related content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'FashionGen' as a dataset used for retrieval results, which is relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 65.47058725357056,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "On full evaluation results in Table 2, FashionKLIP-B shows a remarkable increase of 11-15% compared to EI-CLIP (Ma et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance comparisons. No verifiable resources are identified.",
      "processing_time": 58.170490980148315,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "On full evaluation results in Table 2, FashionKLIP-B shows a remarkable increase of 11-15% compared to EI-CLIP (Ma et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance comparisons. No verifiable resources are identified.",
      "processing_time": 58.170490980148315,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "EI-CLIP (Ma et al., 2022) defines the entity-aware retrieval task from the linguistic perspective by introducing a causal model to concatenate different meta-data as e-commerce entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EI-CLIP) and its application to entity-aware retrieval. No verifiable datasets are referenced.",
      "processing_time": 60.03718280792236,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "EI-CLIP (Ma et al., 2022) defines the entity-aware retrieval task from the linguistic perspective by introducing a causal model to concatenate different meta-data as e-commerce entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EI-CLIP) and its application to entity-aware retrieval. No verifiable datasets are referenced.",
      "processing_time": 60.03718280792236,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "Utilizing the generalization ability of a pre-trained CLIP-style model, we retrieve product images from the image set I , with the query formulated as \"A photo of { concept } \" as in (Radford et al., 2021; Yao et al., 2021; Gu et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a pre-trained CLIP-style model and a generic image set 'I', which do not meet the criteria for inclusion.",
      "processing_time": 61.341838121414185,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Utilizing the generalization ability of a pre-trained CLIP-style model, we retrieve product images from the image set I , with the query formulated as \"A photo of { concept } \" as in (Radford et al., 2021; Yao et al., 2021; Gu et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a pre-trained CLIP-style model and a generic image set 'I', which do not meet the criteria for inclusion.",
      "processing_time": 61.341838121414185,
      "citing_paper_id": "259370889",
      "cited_paper_id": 244117525
    },
    {
      "context_text": "Utilizing the generalization ability of a pre-trained CLIP-style model, we retrieve product images from the image set I , with the query formulated as \"A photo of { concept } \" as in (Radford et al., 2021; Yao et al., 2021; Gu et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a pre-trained CLIP-style model and a generic image set 'I', which do not meet the criteria for inclusion.",
      "processing_time": 61.341838121414185,
      "citing_paper_id": "259370889",
      "cited_paper_id": 246823486
    },
    {
      "context_text": "Image-to-Product Text-to-Product R@1 R@5 R@10 R@20 R@1 R@5 R@10 R@20 CLIP We conduct zero-shot experiments for T2P and I2P on FashionKLIP-B and FashionKLIP-M and compare it with the baseline CLIP (Radford et al., 2021), as shown in Table 6.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FashionKLIP-B",
        "FashionKLIP-M"
      ],
      "dataset_descriptions": {
        "FashionKLIP-B": "Used for zero-shot text-to-product retrieval experiments, comparing performance with CLIP. Focuses on evaluating the transferability of visual models using natural language supervision.",
        "FashionKLIP-M": "Used for zero-shot image-to-product retrieval experiments, comparing performance with CLIP. Focuses on evaluating the transferability of visual models using natural language supervision."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'FashionKLIP-B' and 'FashionKLIP-M' as specific datasets used for zero-shot experiments in image-to-product and text-to-product retrieval tasks. These datasets are relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 74.92961001396179,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "For smaller settings such as FashionKLIP-M, the retrieval performance is also competitive and closer to CLIP.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between models. No verifiable resources are identified.",
      "processing_time": 57.978312492370605,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "For models of the same size, we can see that FashionKLIP-B greatly outperforms CLIP on Recall@1-20 and particularly achieves an improvement of 3~5% on both tasks for R@1.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics on Recall@1-20 and R@1. The context focuses on model performance comparison rather than dataset usage.",
      "processing_time": 60.49837255477905,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "As smaller pre-trained CLIP weights",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a reference to pre-trained CLIP weights. No verifiable resources are identified.",
      "processing_time": 58.863598108291626,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We initialize FashionKLIP from CLIP pre-trained weights and continually pre-train the model based on our in-house dataset for MMKG construction (as described previously), only using the contrastive learning process over image-text pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'in-house dataset' which is too generic and lacks a specific identifier. No other datasets are mentioned.",
      "processing_time": 58.69919490814209,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Recently, several domain-specific VLP models including FashionBERT (Gao et al., 2020), Kalei-149 doBERT (Zhuge et al., 2021), CommerceMM (Yu et al., 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al., 2022) are proposed based on e-commence image-text pairs, which greatly improve the performance of e-commerce image-text retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several domain-specific VLP models but does not refer to any specific datasets. The models are discussed in the context of e-commerce image-text pairs, but no named datasets are explicitly mentioned.",
      "processing_time": 61.46819806098938,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Recently, several domain-specific VLP models including FashionBERT (Gao et al., 2020), Kalei-149 doBERT (Zhuge et al., 2021), CommerceMM (Yu et al., 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al., 2022) are proposed based on e-commence image-text pairs, which greatly improve the performance of e-commerce image-text retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several domain-specific VLP models but does not refer to any specific datasets. The models are discussed in the context of e-commerce image-text pairs, but no named datasets are explicitly mentioned.",
      "processing_time": 61.46819806098938,
      "citing_paper_id": "259370889",
      "cited_paper_id": 250627337
    },
    {
      "context_text": "Recently, several domain-specific VLP models including FashionBERT (Gao et al., 2020), Kalei-149 doBERT (Zhuge et al., 2021), CommerceMM (Yu et al., 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al., 2022) are proposed based on e-commence image-text pairs, which greatly improve the performance of e-commerce image-text retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several domain-specific VLP models but does not refer to any specific datasets. The models are discussed in the context of e-commerce image-text pairs, but no named datasets are explicitly mentioned.",
      "processing_time": 61.46819806098938,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "The Fashion-MMKG is later incorporated as the prior cross-modal fashion knowledge in training a CLIP-style model to support e-commerce image-text retrieval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Fashion-MMKG"
      ],
      "dataset_descriptions": {
        "Fashion-MMKG": "Used to train a CLIP-style model for e-commerce image-text retrieval, incorporating cross-modal fashion knowledge to enhance the model's performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Fashion-MMKG' as a specific dataset used for training a CLIP-style model for e-commerce image-text retrieval. The dataset is clearly identified and its usage is described.",
      "processing_time": 67.39729523658752,
      "citing_paper_id": "259370889",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "1) Strictly following (Gao et al., 2020; Zhuge et al., 2021; Ma et al., 2022; Yu et al., 2022), the model is required to pick the matched image in 101 samples, including 1 ground-truth image with 100 randomly selected images within the same product sub-category (denoted as ‚ÄúSam-ple‚Äù).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a methodology involving 101 samples, but does not name a dataset.",
      "processing_time": 59.41689133644104,
      "citing_paper_id": "259370889",
      "cited_paper_id": 232417264
    },
    {
      "context_text": "1) Strictly following (Gao et al., 2020; Zhuge et al., 2021; Ma et al., 2022; Yu et al., 2022), the model is required to pick the matched image in 101 samples, including 1 ground-truth image with 100 randomly selected images within the same product sub-category (denoted as ‚ÄúSam-ple‚Äù).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a methodology involving 101 samples, but does not name a dataset.",
      "processing_time": 59.41689133644104,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, several domain-specific VLP models including FashionBERT (Gao et al., 2020), Kalei-149 doBERT (Zhuge et al., 2021), CommerceMM (Yu et al., 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al., 2022) are proposed based on e-commence image-text pairs, which greatly improve the‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several domain-specific VLP models but does not refer to any specific datasets. The focus is on the models themselves rather than the datasets they were trained on.",
      "processing_time": 60.16354417800903,
      "citing_paper_id": "259370889",
      "cited_paper_id": 232417264
    },
    {
      "context_text": "Recall@1/5/10 is regarded as evaluation metrics as previous works (Gao et al., 2020; Zhuge et al., 2021; Yu et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics used in previous works.",
      "processing_time": 57.170992612838745,
      "citing_paper_id": "259370889",
      "cited_paper_id": 232417264
    },
    {
      "context_text": "KaleidoBERT (Zhuge et al., 2021) extracts a series of multi-grained image patches for augmentation to guide masking strategy for fine-grained matching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KaleidoBERT) and its functionality. The context is focused on describing the method rather than using a dataset.",
      "processing_time": 60.71683859825134,
      "citing_paper_id": "259370889",
      "cited_paper_id": 232417264
    },
    {
      "context_text": "Some other work (Ma et al., 2022; Zhu et al., 2021) focuses on entities in text modalities, but rarely considers cross-modal interactions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to entities in text modalities and cross-modal interactions.",
      "processing_time": 58.518465518951416,
      "citing_paper_id": "259370889",
      "cited_paper_id": 237386166
    },
    {
      "context_text": "Some other work (Ma et al., 2022; Zhu et al., 2021) focuses on entities in text modalities, but rarely considers cross-modal interactions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to entities in text modalities and cross-modal interactions.",
      "processing_time": 58.518465518951416,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "Fashion-ViL (Han et al., 2022) designs a flexible architecture for various downstream tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Fashion-ViL as a method or model, not a dataset. It describes the architecture's design for downstream tasks, which does not indicate the use of a specific dataset.",
      "processing_time": 60.9608998298645,
      "citing_paper_id": "259370889",
      "cited_paper_id": 250627337
    },
    {
      "context_text": "Although some e-commerce VLP models use fine-grained information from either image perspectives (Han et al., 2022) or patch-based image classification (Gao et al., 2020; Yu et al., 2022), they are short of semantic-level alignments across modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the limitations of existing e-commerce VLP models in achieving semantic-level alignments across modalities.",
      "processing_time": 60.422126054763794,
      "citing_paper_id": "259370889",
      "cited_paper_id": 250627337
    },
    {
      "context_text": "‚Ä¶including FashionBERT (Gao et al., 2020), Kalei-149 doBERT (Zhuge et al., 2021), CommerceMM (Yu et al., 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al., 2022) are proposed based on e-commence image-text pairs, which greatly improve the performance of e-commerce image-text retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models and methods but does not explicitly refer to any specific datasets. The focus is on the models and their performance improvements in e-commerce image-text retrieval.",
      "processing_time": 60.01436972618103,
      "citing_paper_id": "259370889",
      "cited_paper_id": 250627337
    },
    {
      "context_text": "Instead of building an ontology-based knowledge graph (Deng et al., 2022), we automatically construct FashionMMKG to alleviate the gap with real-world user queries.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'FashionMMKG' as a constructed knowledge graph, but it does not specify a dataset. The reference to 'ontology-based knowledge graph' is not a dataset but a method.",
      "processing_time": 61.1639358997345,
      "citing_paper_id": "259370889",
      "cited_paper_id": 252668458
    },
    {
      "context_text": "2) As some recently published works (Ma et al., 2022) also consider large-scale candidates on the entire set, each query is compared with every item in the full dataset (denoted as ‚ÄúFull‚Äù).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide a specific, verifiable dataset name. The term 'full dataset' is too generic and lacks a clear identifier.",
      "processing_time": 58.27271842956543,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "EI-CLIP (Ma et al., 2022) defines the entityaware retrieval task from the linguistic perspective by introducing a causal model to concatenate different meta-data as e-commerce entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (EI-CLIP) and a task (entity-aware retrieval).",
      "processing_time": 58.11994385719299,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": ", 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets.",
      "processing_time": 58.115936279296875,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "‚Ä¶domain-specific VLP models including FashionBERT (Gao et al., 2020), Kalei-149 doBERT (Zhuge et al., 2021), CommerceMM (Yu et al., 2022), EI-CLIP (Ma et al., 2022) and Fashion-ViL (Han et al., 2022) are proposed based on e-commence image-text pairs, which greatly improve the performance of‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several domain-specific VLP models but does not refer to any specific datasets. The context focuses on models and their improvements in performance.",
      "processing_time": 59.189099073410034,
      "citing_paper_id": "259370889",
      "cited_paper_id": null
    },
    {
      "context_text": "Confidence is calculated by the K -nearest neighbour (KNN) algorithm (Bo, Lu, and Wang 2018), which measures the distance between different features for fitting.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KNN algorithm) for calculating confidence. The context is about the methodology used for feature distance measurement.",
      "processing_time": 59.520310163497925,
      "citing_paper_id": "260654749",
      "cited_paper_id": 13716346
    },
    {
      "context_text": "Traditional machine learning methods are mainly divided into three stages: machine selection (Li and Ding 2012), feature extraction (Cao et al. 2019), and classifier classification (Taskin 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general stages of machine learning processes. No verifiable resources are identified.",
      "processing_time": 58.44120526313782,
      "citing_paper_id": "260654749",
      "cited_paper_id": 18553623
    },
    {
      "context_text": "Traditional machine learning methods are mainly divided into three stages: machine selection (Li and Ding 2012), feature extraction (Cao et al. 2019), and classifier classification (Taskin 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general stages of machine learning processes. No verifiable resources are identified.",
      "processing_time": 58.44120526313782,
      "citing_paper_id": "260654749",
      "cited_paper_id": 28641153
    },
    {
      "context_text": "Traditional machine learning methods are mainly divided into three stages: machine selection (Li and Ding 2012), feature extraction (Cao et al. 2019), and classifier classification (Taskin 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general stages of machine learning processes. No verifiable resources are identified.",
      "processing_time": 58.44120526313782,
      "citing_paper_id": "260654749",
      "cited_paper_id": 214466795
    },
    {
      "context_text": "Chen et al. (2017) used CNN to extract hyperspectral and multispectral features and then classified the extracted heterogeneous features at the pixel level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the use of CNN for feature extraction and classification in remote sensing data.",
      "processing_time": 58.98412036895752,
      "citing_paper_id": "260654749",
      "cited_paper_id": 36279912
    },
    {
      "context_text": "Chen et al. (2017) used CNN to extract hyperspectral and multispectral features and then classified the extracted heterogeneous features at the pixel level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the use of CNN for feature extraction and classification in remote sensing data.",
      "processing_time": 58.98412036895752,
      "citing_paper_id": "260654749",
      "cited_paper_id": 229653947
    },
    {
      "context_text": "Fernandez-Beltran et al. (2018) proposed an unsupervised land cover classification framework for fusing synthetic aperture radar and multi-spectral imaging data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for fusing SAR and multi-spectral imaging data but does not mention any specific datasets.",
      "processing_time": 57.346118688583374,
      "citing_paper_id": "260654749",
      "cited_paper_id": 57755729
    },
    {
      "context_text": "In recent years, object classification algorithms in the field of RS have been constantly evolving, classification efficiency and accuracy have been continuously improved, and significant progress has been made (Duan, Duan, and Ding 2021; Li et al. 2022; Teng et al. 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in object classification algorithms in remote sensing. No verifiable resources are identified.",
      "processing_time": 58.790732622146606,
      "citing_paper_id": "260654749",
      "cited_paper_id": 202144421
    },
    {
      "context_text": "In recent years, object classification algorithms in the field of RS have been constantly evolving, classification efficiency and accuracy have been continuously improved, and significant progress has been made (Duan, Duan, and Ding 2021; Li et al. 2022; Teng et al. 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in object classification algorithms in remote sensing. No verifiable resources are identified.",
      "processing_time": 58.790732622146606,
      "citing_paper_id": "260654749",
      "cited_paper_id": 247362647
    },
    {
      "context_text": "In recent years, object classification algorithms in the field of RS have been constantly evolving, classification efficiency and accuracy have been continuously improved, and significant progress has been made (Duan, Duan, and Ding 2021; Li et al. 2022; Teng et al. 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in object classification algorithms in remote sensing. No verifiable resources are identified.",
      "processing_time": 58.790732622146606,
      "citing_paper_id": "260654749",
      "cited_paper_id": 248505867
    },
    {
      "context_text": "In particular, the distance between node pairs in each graph is calculated and compared with the distance in the corresponding graph, which is called GWD distance (Chen et al. 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating distances between node pairs in graphs.",
      "processing_time": 57.567516803741455,
      "citing_paper_id": "260654749",
      "cited_paper_id": 220128101
    },
    {
      "context_text": "In particular, the distance between node pairs in each graph is calculated and compared with the distance in the corresponding graph, which is called GWD distance (Chen et al. 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating distances between node pairs in graphs.",
      "processing_time": 57.567516803741455,
      "citing_paper_id": "260654749",
      "cited_paper_id": null
    },
    {
      "context_text": "Nock and Gilmour (2020) proposed a feature fusion method based on deep neural networks and fuzzy aggregation for multi-modal RS image classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method for multi-modal RS image classification but does not specify any datasets used. The focus is on the method itself rather than a specific dataset.",
      "processing_time": 59.58937311172485,
      "citing_paper_id": "260654749",
      "cited_paper_id": 221385483
    },
    {
      "context_text": "Shi et al. (2022) proposed a multi-featured cooperative adversarial network for generating multi-modal countermeasure samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating multi-modal countermeasure samples.",
      "processing_time": 57.58844995498657,
      "citing_paper_id": "260654749",
      "cited_paper_id": 226415469
    },
    {
      "context_text": "Shi et al. (2022) proposed a multi-featured cooperative adversarial network for generating multi-modal countermeasure samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating multi-modal countermeasure samples.",
      "processing_time": 57.58844995498657,
      "citing_paper_id": "260654749",
      "cited_paper_id": 252450639
    },
    {
      "context_text": "Currently, commonly deep learning methods include convolutional neural networks (CNN) (Wang et al. 2021), graph convolutional networks (GCN) (Li et al. 2023), vision transformer (Sha and Li 2022), and generative adversarial networks (Shi et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only deep learning methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.211318492889404,
      "citing_paper_id": "260654749",
      "cited_paper_id": 226415469
    },
    {
      "context_text": "Currently, commonly deep learning methods include convolutional neural networks (CNN) (Wang et al. 2021), graph convolutional networks (GCN) (Li et al. 2023), vision transformer (Sha and Li 2022), and generative adversarial networks (Shi et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only deep learning methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.211318492889404,
      "citing_paper_id": "260654749",
      "cited_paper_id": 229653947
    },
    {
      "context_text": "Currently, commonly deep learning methods include convolutional neural networks (CNN) (Wang et al. 2021), graph convolutional networks (GCN) (Li et al. 2023), vision transformer (Sha and Li 2022), and generative adversarial networks (Shi et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only deep learning methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.211318492889404,
      "citing_paper_id": "260654749",
      "cited_paper_id": 252450639
    },
    {
      "context_text": "Gao et al. (2023) incorporated an adversarial complementary learning strategy into a CNN model to optimize common and specific encoders for extracting complementary information between different data sources and then facilitated weighted fusion to achieve complementary advantages.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context focuses on the technique used rather than a particular dataset.",
      "processing_time": 58.97047543525696,
      "citing_paper_id": "260654749",
      "cited_paper_id": 229653947
    },
    {
      "context_text": "Compared with several common algorithms: common subspace (CoSpace) (Hong et al. 2019), shared and specific feature learning (S2FL) (Hong et al. 2021), depthwise feature interaction network (DFINet) (Gao et al. 2022), the experimental results and corresponding analysis are as follows.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and models. The context focuses on comparing different methods, not on the use of datasets.",
      "processing_time": 59.05237102508545,
      "citing_paper_id": "260654749",
      "cited_paper_id": 235125956
    },
    {
      "context_text": "The Augsburg dataset (Hong et al. 2021) comprises two different images ‚Äì a satellite HSI and a dual polarization-polarization synthetic aperture radar (SAR) image.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Augsburg dataset"
      ],
      "dataset_descriptions": {
        "Augsburg dataset": "Used for multimodal remote sensing, specifically combining satellite HSI and dual polarization SAR images for land cover classification with shared and specific feature learning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Augsburg dataset', which is used for multimodal remote sensing, including both HSI and SAR images. This aligns well with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 68.3136157989502,
      "citing_paper_id": "260654749",
      "cited_paper_id": 235125956
    },
    {
      "context_text": "The Houston dataset (Hong et al. 2021) is a commonly used homogeneous dataset that comprises both HSI and MSI data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Houston dataset"
      ],
      "dataset_descriptions": {
        "Houston dataset": "Used for multimodal remote sensing, specifically combining HSI and MSI data for land cover classification, employing shared and specific feature learning models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Houston dataset', which is used for multimodal remote sensing and includes both HSI and MSI data. This aligns well with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 67.19708108901978,
      "citing_paper_id": "260654749",
      "cited_paper_id": 235125956
    },
    {
      "context_text": "Zhang, Wang, and Sun (2021) proposed a KG based on a reasoning network for image classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a knowledge graph (KG) but does not specify a dataset. The focus is on the method rather than a reusable dataset.",
      "processing_time": 58.66618037223816,
      "citing_paper_id": "260654749",
      "cited_paper_id": 238687946
    },
    {
      "context_text": "Different from traditional KGC methods, we mainly construct KG by taking object class as entity and class sample relationship as edge (Zhang, Wang, and Sun 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing a knowledge graph. No verifiable resources are identified.",
      "processing_time": 58.02681803703308,
      "citing_paper_id": "260654749",
      "cited_paper_id": 238687946
    },
    {
      "context_text": "Mainstream image classification methods rely on classifying objects separately, but fail to consider the spatial relationship between objects, which will inevitably lead to biased classification results (Zhang, Wang, and Sun 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the limitations of mainstream image classification methods.",
      "processing_time": 58.527889013290405,
      "citing_paper_id": "260654749",
      "cited_paper_id": 238687946
    },
    {
      "context_text": "However, image data containing multi-modal content have a large amount of information, and there are multiple sources of information imbalance and redundant interference (Li et al. 2022), which will lead to a linear increase in the complexity of the classification calculation process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with multi-modal image data. No clear, verifiable resource names are provided.",
      "processing_time": 58.76623725891113,
      "citing_paper_id": "260654749",
      "cited_paper_id": 247362647
    },
    {
      "context_text": "However, image data containing multi-modal content have a large amount of information, and there are multiple sources of information imbalance and redundant interference (Li et al. 2022), which will lead to a linear increase in the complexity of the classification calculation process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with multi-modal image data. No clear, verifiable resource names are provided.",
      "processing_time": 58.76623725891113,
      "citing_paper_id": "260654749",
      "cited_paper_id": 248505867
    },
    {
      "context_text": "Knowledge graph (KG) (Opdahl et al. 2023), as an efficient data organization and management model, can fully utilize the associations between entity nodes to optimize computation (Li et al. 2022, 2023; Song et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about knowledge graphs and their applications. No verifiable resources are identified.",
      "processing_time": 58.52275252342224,
      "citing_paper_id": "260654749",
      "cited_paper_id": 247362647
    },
    {
      "context_text": "Knowledge graph (KG) (Opdahl et al. 2023), as an efficient data organization and management model, can fully utilize the associations between entity nodes to optimize computation (Li et al. 2022, 2023; Song et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about knowledge graphs and their applications. No verifiable resources are identified.",
      "processing_time": 58.52275252342224,
      "citing_paper_id": "260654749",
      "cited_paper_id": 248505867
    },
    {
      "context_text": "Knowledge graph (KG) (Opdahl et al. 2023), as an efficient data organization and management model, can fully utilize the associations between entity nodes to optimize computation (Li et al. 2022, 2023; Song et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about knowledge graphs and their applications. No verifiable resources are identified.",
      "processing_time": 58.52275252342224,
      "citing_paper_id": "260654749",
      "cited_paper_id": 254154806
    },
    {
      "context_text": "This has led many researchers to combine multiple modes to classify RS images (Li et al. 2022; Liu et al. 2020; Wu et al. 2022; Yao et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the combination of multiple modes to classify RS images. No verifiable resources are named.",
      "processing_time": 58.576438903808594,
      "citing_paper_id": "260654749",
      "cited_paper_id": 247362647
    },
    {
      "context_text": "This has led many researchers to combine multiple modes to classify RS images (Li et al. 2022; Liu et al. 2020; Wu et al. 2022; Yao et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the combination of multiple modes to classify RS images. No verifiable resources are named.",
      "processing_time": 58.576438903808594,
      "citing_paper_id": "260654749",
      "cited_paper_id": 248505867
    },
    {
      "context_text": "Li et al. (2022) proposed a multi-modal RS image classification method based on asymmetric feature fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multi-modal RS image classification. No verifiable resources are identified.",
      "processing_time": 58.04440975189209,
      "citing_paper_id": "260654749",
      "cited_paper_id": 247362647
    },
    {
      "context_text": "Li et al. (2022) proposed a multi-modal RS image classification method based on asymmetric feature fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multi-modal RS image classification. No verifiable resources are identified.",
      "processing_time": 58.04440975189209,
      "citing_paper_id": "260654749",
      "cited_paper_id": 248505867
    },
    {
      "context_text": "In order to reduce the complexity of the calculation process, it is necessary to consider the semantic association between classes to reduce the calculation of information content (Niu et al. 2023; Wu et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general discussion about reducing computational complexity through semantic associations.",
      "processing_time": 57.533676862716675,
      "citing_paper_id": "260654749",
      "cited_paper_id": 252839708
    },
    {
      "context_text": "In order to reduce the complexity of the calculation process, it is necessary to consider the semantic association between classes to reduce the calculation of information content (Niu et al. 2023; Wu et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general discussion about reducing computational complexity through semantic associations.",
      "processing_time": 57.533676862716675,
      "citing_paper_id": "260654749",
      "cited_paper_id": 256780457
    },
    {
      "context_text": "Ma et al. (2022) proposed a transfer representation learning fusion network for collaborative distribution of multi-source RS images, which migrates the dominant attributes of panchromatic and MSI to each other in order to enhance feature sparse fusion and coordinate the use of global and local‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses a method for fusing multi-source remote sensing images but does not mention any specific datasets.",
      "processing_time": 57.280935525894165,
      "citing_paper_id": "260654749",
      "cited_paper_id": 253251385
    },
    {
      "context_text": "Literature (Xue et al. 2022) used hyperspectral or different forms of RS data to construct multiple complementary perspectives for the same observation scene.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'hyperspectral or different forms of RS data' which suggests the use of remote sensing datasets, but no specific dataset names are provided.",
      "processing_time": 59.572317600250244,
      "citing_paper_id": "260654749",
      "cited_paper_id": 253354759
    },
    {
      "context_text": "There are generally methods such as k-nearest neighbour algorithm (Chao and Li 2022), naive Bayesian algorithm (Yang et al. 2017), support vector machine (Singh, Gayathri, and Chaudhuri 2022), and back propagation neural network (Wei, Wei, and Zhang 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various machine learning algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.84544897079468,
      "citing_paper_id": "260654749",
      "cited_paper_id": 253555095
    },
    {
      "context_text": "The multi-modal object classification process is shown in Figure 1 and the algorithm steps are as follows: Step I: The significance features of HSI data are retained by principal component analysis (PCA), noise and interference features are removed, at the same time, information redundancy is reduced, as well as the amount of computation is reduced; Step II: Spatial graphs with labelled and unlabelled samples are constructed for HSI and MSI data, as well as spectral-spatial features are obtained by graph convolution operations; Step III: GTA is used to align HSI and MSI node features and constrain feature extractors, in order to enhance the spatial information representation ability of multi-modal features; Step IV: The features of HSI and MSI are fused by MFM, which uses self-attention and cross-attention mechanism to obtain fusion features rich in two modal information; Step V: An object KG is constructed to use KG to assist in RS image classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a multi-modal object classification process but does not mention any specific, verifiable datasets. It focuses on the methodology and steps involved in the process.",
      "processing_time": 59.62357568740845,
      "citing_paper_id": "260654749",
      "cited_paper_id": 253901615
    },
    {
      "context_text": "For HSI data, PCA (Fang et al. 2022) mainly used linear transformation to map the HSI data to a new low-dimensional space, so that the projection variance is maximized and the reconstruction error is minimized.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PCA but does not refer to a specific dataset. It describes a method for transforming HSI data, which is not a dataset itself.",
      "processing_time": 58.76624822616577,
      "citing_paper_id": "260654749",
      "cited_paper_id": 253901615
    },
    {
      "context_text": "Due to the stronger professional correlation and closer connection between knowledge, it is convenient to use the knowledge reasoning in the KG (Bai et al. 2023) for image classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of knowledge reasoning in knowledge graphs.",
      "processing_time": 56.93387699127197,
      "citing_paper_id": "260654749",
      "cited_paper_id": 254034416
    },
    {
      "context_text": "Due to the stronger professional correlation and closer connection between knowledge, it is convenient to use the knowledge reasoning in the KG (Bai et al. 2023) for image",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to knowledge reasoning in knowledge graphs.",
      "processing_time": 57.05333995819092,
      "citing_paper_id": "260654749",
      "cited_paper_id": 254034416
    },
    {
      "context_text": "Compared to traditional machine learning methods, deep learning methods have better performance in large data scale training than traditional machine learning methods (Bai et al. 2023) and generally do not rely heavily on effective feature engineering.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between deep learning and traditional machine learning methods.",
      "processing_time": 57.194437742233276,
      "citing_paper_id": "260654749",
      "cited_paper_id": 254034416
    },
    {
      "context_text": "Zhang et al. (2023) proposed a multi-level domain adaptation network for information cooperation and cross-scene classification based on HSI and LiDAR data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HSI and LiDAR data but does not specify a named dataset. The context focuses on the method and its application rather than a specific dataset.",
      "processing_time": 59.767317056655884,
      "citing_paper_id": "260654749",
      "cited_paper_id": 258007950
    },
    {
      "context_text": "We employ the Bert-based pre-trained model S-Bert [16] to extract textual features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions S-Bert, which is a pre-trained model, not a dataset. No datasets are explicitly mentioned or used in the given context.",
      "processing_time": 58.92329668998718,
      "citing_paper_id": "261126377",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "(2) RNN-based models are better than FPMC, which indicates that FPMC ignores to consider historical information.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between RNN-based models and FPMC. No verifiable resources are identified.",
      "processing_time": 58.68411612510681,
      "citing_paper_id": "261126377",
      "cited_paper_id": 207178809
    },
    {
      "context_text": "Earlier studies use Markov Chain to compute and Matrix Factorization to recommend Next-POI, such as FPMC [1], PRME-G [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (Markov Chain, Matrix Factorization) and models (FPMC, PRME-G) but does not specify any datasets. The context is about computational methods for next-POI recommendation.",
      "processing_time": 61.180023431777954,
      "citing_paper_id": "261126377",
      "cited_paper_id": 207178809
    },
    {
      "context_text": "Although the above researches have mined check-in data [1] and spatio-temporal information [3], [4], [6], [7], they generally ignore the influence of multi-modal data on user selection results and lack the supplement to the location semantic information.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'check-in data' and 'spatio-temporal information' but does not provide specific dataset names. The cited papers do not clarify the use of specific datasets either.",
      "processing_time": 60.053739070892334,
      "citing_paper_id": "261126377",
      "cited_paper_id": 207178809
    },
    {
      "context_text": "Although the above researches have mined check-in data [1] and spatio-temporal information [3], [4], [6], [7], they generally ignore the influence of multi-modal data on user selection results and lack the supplement to the location semantic information.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'check-in data' and 'spatio-temporal information' but does not provide specific dataset names. The cited papers do not clarify the use of specific datasets either.",
      "processing_time": 60.053739070892334,
      "citing_paper_id": "261126377",
      "cited_paper_id": 231846408
    },
    {
      "context_text": "Traditional approaches use Matrix Factorization (MF) and Markov Chain (MC) to capture the impact of historical behavior on the next locations [1], [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Matrix Factorization and Markov Chain).",
      "processing_time": 57.09576106071472,
      "citing_paper_id": "261126377",
      "cited_paper_id": 207178809
    },
    {
      "context_text": "The baseline methods are described below:\n‚Ä¢ FPMC [1]: Combines Markov Chain and Matrix Factorization to model position transitions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FPMC) that combines Markov Chain and Matrix Factorization. No datasets are referenced for training or evaluation.",
      "processing_time": 59.70728373527527,
      "citing_paper_id": "261126377",
      "cited_paper_id": 207178809
    },
    {
      "context_text": "‚Ä¢ FPMC [1]: Combines Markov Chain and Matrix Factorization to model position transitions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method combining Markov Chain and Matrix Factorization. The title confirms this is about a method, not a dataset.",
      "processing_time": 59.70549511909485,
      "citing_paper_id": "261126377",
      "cited_paper_id": 207178809
    },
    {
      "context_text": "MMRFAN [12] applies MMKG to the medical domain and uses a multirelational feature aggregation network to capture multi-modal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of a method (MMKG) and a network (multirelational feature aggregation network) but does not specify any dataset names. The context is focused on the methodology rather than a specific dataset.",
      "processing_time": 61.45833945274353,
      "citing_paper_id": "261126377",
      "cited_paper_id": 222278475
    },
    {
      "context_text": "Inspired by CyGNet [18], we design a Copy-Generation Network to better process the temporal layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Copy-Generation Network) inspired by another work (CyGNet).",
      "processing_time": 57.68887114524841,
      "citing_paper_id": "261126377",
      "cited_paper_id": 229180723
    },
    {
      "context_text": "And STAN is significantly better than RNN-based models since the former can capture the key information and calculate the exact impact of each visit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between STAN and RNN-based models. No verifiable resources are identified.",
      "processing_time": 57.684439182281494,
      "citing_paper_id": "261126377",
      "cited_paper_id": 231846408
    },
    {
      "context_text": "STAN [7] designs a spatio-temporal biattention model to learn the patterns of non-adjacent check-ins and non-adjacent POIs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (STAN) for next location recommendation.",
      "processing_time": 57.02907919883728,
      "citing_paper_id": "261126377",
      "cited_paper_id": 231846408
    },
    {
      "context_text": "‚Ä¢ STAN [7]: An attention-based method, which constructs a spatio-temporal bi-attention network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an attention-based method called STAN. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.63650345802307,
      "citing_paper_id": "261126377",
      "cited_paper_id": 231846408
    },
    {
      "context_text": "Previous methods are often limited to exploring the association between check-in sequence and uni-modal information [6], [7], which means they hardly model the implicit association between location multi-modal information and the next POI.",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of previous methods in handling multi-modal information.",
      "processing_time": 56.515599966049194,
      "citing_paper_id": "261126377",
      "cited_paper_id": 231846408
    },
    {
      "context_text": "Recently, attention-based works [6]‚Äì[9] also achieve good results by learning hidden weights from the perspective of location categories or spatiotemporal information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to attention-based methods and their performance improvements.",
      "processing_time": 56.82532525062561,
      "citing_paper_id": "261126377",
      "cited_paper_id": 250340301
    },
    {
      "context_text": "GETNext [9] proposes a transformer-based framework to integrate global transition patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework. The context is focused on the proposed transformer-based framework.",
      "processing_time": 58.43588423728943,
      "citing_paper_id": "261126377",
      "cited_paper_id": 250340301
    },
    {
      "context_text": "Recently, there has been a growing focus on the development of spatio-temporal knowledge graphs stored in graph databases, driven by the continuous improvement in computing efficiency for the attribute analysis of large interconnected datasets [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses the general development of spatio-temporal knowledge graphs and improvements in computing efficiency.",
      "processing_time": 58.90127515792847,
      "citing_paper_id": "261019622",
      "cited_paper_id": 2495502
    },
    {
      "context_text": "[20] to annotate the semantic types for source attributes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a general process of annotating semantic types for source attributes.",
      "processing_time": 58.490864276885986,
      "citing_paper_id": "261019622",
      "cited_paper_id": 7040223
    },
    {
      "context_text": "Based on the annotated semantic types, we obtain a candidate semantic model by modeling the relations between them using the Steiner tree algorithm [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Steiner tree algorithm) used for modeling relations between semantic types.",
      "processing_time": 58.184529066085815,
      "citing_paper_id": "261019622",
      "cited_paper_id": 7409058
    },
    {
      "context_text": "Gr√ºninger, M. Verification of the OWL-time ontology.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an ontology verification task.",
      "processing_time": 55.34248399734497,
      "citing_paper_id": "261019622",
      "cited_paper_id": 12476041
    },
    {
      "context_text": "There is an existing Time Ontology in OWL [15] that provides a clear, formal, and standardized description of time concepts and the relations between them.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a Time Ontology in OWL, which is a formal description of time concepts. However, it does not specify a dataset, but rather a structured knowledge representation.",
      "processing_time": 59.20177435874939,
      "citing_paper_id": "261019622",
      "cited_paper_id": 12476041
    },
    {
      "context_text": "Therefore, we incorporate concepts Instant and DateTimeDescription from the Time Ontology in OWL to define the time-related description in our ontology.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only concepts from an ontology. No verifiable resources are identified.",
      "processing_time": 57.3262038230896,
      "citing_paper_id": "261019622",
      "cited_paper_id": 12476041
    },
    {
      "context_text": "Neo4j exhibits remarkable scalability, enabling the efÔ¨Åcient processing of billions of nodes, relationships, and attributes on a single machine.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of Neo4j. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.32436990737915,
      "citing_paper_id": "261019622",
      "cited_paper_id": 13919896
    },
    {
      "context_text": "Cypher is a property graph data query language implemented in the graph database Neo4j [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Neo4j, which is a graph database, but does not refer to a specific dataset. The context is about the query language Cypher, not a dataset.",
      "processing_time": 59.32530903816223,
      "citing_paper_id": "261019622",
      "cited_paper_id": 13919896
    },
    {
      "context_text": "The spatio-temporal knowledge graph constructed in this study is stored using Neo4j, a high-performance NoSQL graph database designed for storing structured data on a network rather than in traditional tables.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to Neo4j, which is a database system, not a dataset.",
      "processing_time": 58.69904804229736,
      "citing_paper_id": "261019622",
      "cited_paper_id": 13919896
    },
    {
      "context_text": "The Neo4j graph database incorporates the Neo4j Spatial extension plugin, which facilitates the representation of spatial data using nodes and relationships.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It focuses on the Neo4j graph database and its Spatial extension plugin, which are tools rather than datasets.",
      "processing_time": 59.03150415420532,
      "citing_paper_id": "261019622",
      "cited_paper_id": 13919896
    },
    {
      "context_text": "The underlying methodology of the Neo4j Spatial plugin involves the construction of an R-tree, a powerful library that enables Neo4j to perform comprehensive spatial operations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (Neo4j Spatial plugin).",
      "processing_time": 56.70503878593445,
      "citing_paper_id": "261019622",
      "cited_paper_id": 13919896
    },
    {
      "context_text": "After removing incorrect relations and substructures, with the help of the existing relevant knowledge graph, we can obtain the most plausible semantic model by adding potentially missed substructures using the modified frequent subgraph mining algorithm [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for frequent subgraph mining. No verifiable resources are identified.",
      "processing_time": 57.85275983810425,
      "citing_paper_id": "261019622",
      "cited_paper_id": 18117752
    },
    {
      "context_text": "However, the world contains a vast amount of dynamic and procedural knowledge that conventional static knowledge graphs cannot adequately represent [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of static knowledge graphs.",
      "processing_time": 57.14752435684204,
      "citing_paper_id": "261019622",
      "cited_paper_id": 245634781
    },
    {
      "context_text": "[11] predicted missing nodes in a maritime knowledge graph by link prediction in the knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a knowledge graph but does not specify a named, verifiable dataset. The focus is on the method of link prediction.",
      "processing_time": 58.69857311248779,
      "citing_paper_id": "261019622",
      "cited_paper_id": 247857460
    },
    {
      "context_text": "proposed a forest fire prediction method that combines spatio-temporal knowledge graphs with machine learning models to efficiently extract the required features [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method for forest fire prediction using spatio-temporal knowledge graphs and machine learning models, but does not specify any particular dataset.",
      "processing_time": 59.23969483375549,
      "citing_paper_id": "261019622",
      "cited_paper_id": 251007195
    },
    {
      "context_text": "[2] proposed the analysis method of Support Vector Machines (SVM).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SVM).",
      "processing_time": 56.952903270721436,
      "citing_paper_id": "261019622",
      "cited_paper_id": 265094990
    },
    {
      "context_text": "In response to these issues, Bernhard Sch√∂lkopf et al. [2] proposed the analysis method of Support Vector Machines (SVM).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SVM).",
      "processing_time": 56.74771571159363,
      "citing_paper_id": "261019622",
      "cited_paper_id": null
    },
    {
      "context_text": "However, artificial neural networks suffer from slow convergence and the potential to converge to local optima [3,4].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of artificial neural networks.",
      "processing_time": 56.2940034866333,
      "citing_paper_id": "261019622",
      "cited_paper_id": null
    },
    {
      "context_text": "A knowledge graph is a data modeling method that represents knowledge as concepts, entities, and semantic relationships between them in the form of a graph [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of knowledge graphs.",
      "processing_time": 57.060478925704956,
      "citing_paper_id": "261019622",
      "cited_paper_id": null
    },
    {
      "context_text": "To address this limitation, researchers have made use of supplementary information, such as item attributes [1], user proÔ¨Åles [2], and contexts [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of supplementary information.",
      "processing_time": 57.293792963027954,
      "citing_paper_id": "269486285",
      "cited_paper_id": 5625629
    },
    {
      "context_text": "To address this limitation, researchers have made use of supplementary information, such as item attributes [1], user proÔ¨Åles [2], and contexts [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of supplementary information.",
      "processing_time": 57.293792963027954,
      "citing_paper_id": "269486285",
      "cited_paper_id": 218571035
    },
    {
      "context_text": "‚Ä¢ CKE [22] uses items as side information aids based on collaborative Ô¨Åltering recommendation algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'items as side information' which could imply a dataset, but there is no specific dataset name provided. The cited paper title does not help in identifying a specific dataset.",
      "processing_time": 60.347843170166016,
      "citing_paper_id": "269486285",
      "cited_paper_id": 7062707
    },
    {
      "context_text": "We use MovieLens dataset [17] for the evaluation test in this paper, which is the public dataset adopted by most recommendation algorithms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MovieLens"
      ],
      "dataset_descriptions": {
        "MovieLens": "Used for evaluating recommendation algorithms, focusing on user-item interactions and rating predictions in the context of movie recommendations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the MovieLens dataset, which is a well-known public dataset used for evaluating recommendation algorithms.",
      "processing_time": 62.70396065711975,
      "citing_paper_id": "269486285",
      "cited_paper_id": 16619709
    },
    {
      "context_text": "So they propose to jointly model user preferences with user behavior ID characteristics and behavior images; Cai et al. [8] of Youku proposed a multi-view active learning framework for video recommendation, which extracts missing text information from visual information to obtain more training‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method and a research approach but does not reference any named datasets.",
      "processing_time": 58.704713106155396,
      "citing_paper_id": "269486285",
      "cited_paper_id": 199466173
    },
    {
      "context_text": "1 Here we use the InceptionV3 model [11] pre-trained on ImageNet Dataset (ILSVRC2012) , T 0 u = { v | y uv = 1 }} , (4) which collects the tail nodes that may be associated with the user u on the graph network after k propagation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet Dataset (ILSVRC2012)"
      ],
      "dataset_descriptions": {
        "ImageNet Dataset (ILSVRC2012)": "Used to pre-train the InceptionV3 model, focusing on image classification tasks and leveraging a large-scale dataset of labeled images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ImageNet Dataset (ILSVRC2012) as a pre-training dataset for the InceptionV3 model. This is a specific, verifiable dataset used in the research.",
      "processing_time": 68.81932044029236,
      "citing_paper_id": "269486285",
      "cited_paper_id": 206593880
    },
    {
      "context_text": "Studies shows that the multi-modal approach is effective in numerous Ô¨Åelds, including medicine [4], trafÔ¨Åc event detection [5], and information retrieval [6].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only fields where multi-modal approaches are effective. No verifiable resources are named.",
      "processing_time": 58.29030179977417,
      "citing_paper_id": "269486285",
      "cited_paper_id": 210971227
    },
    {
      "context_text": "In addition to the features of user and item themselves, some studies [12], [13] have shown that adding supplementary information from the knowledge graph or the multiple modalities to the user and item representations will help improve the recommendation performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about using knowledge graphs and multiple modalities in recommender systems.",
      "processing_time": 58.11438012123108,
      "citing_paper_id": "269486285",
      "cited_paper_id": 226246289
    },
    {
      "context_text": "We also used Silhouette Coefficient [37] to confirm the results of the Elbow method.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Silhouette Coefficient, which is a metric, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 58.117642641067505,
      "citing_paper_id": "260735757",
      "cited_paper_id": 189900
    },
    {
      "context_text": "For image-based recommendation, a user may upload an image with the intention of item recognition and retrieval of similar items, potentially along with their (descriptive) information [5, 26, 27, 31, 47, 56].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general concept of image-based recommendation. The cited papers do not provide additional specific dataset names within the given context.",
      "processing_time": 58.52632212638855,
      "citing_paper_id": "260735757",
      "cited_paper_id": 1012652
    },
    {
      "context_text": "For image-based recommendation, a user may upload an image with the intention of item recognition and retrieval of similar items, potentially along with their (descriptive) information [5, 26, 27, 31, 47, 56].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general concept of image-based recommendation. The cited papers do not provide additional specific dataset names within the given context.",
      "processing_time": 58.52632212638855,
      "citing_paper_id": "260735757",
      "cited_paper_id": 30698343
    },
    {
      "context_text": "For image-based recommendation, a user may upload an image with the intention of item recognition and retrieval of similar items, potentially along with their (descriptive) information [5, 26, 27, 31, 47, 56].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general concept of image-based recommendation. The cited papers do not provide additional specific dataset names within the given context.",
      "processing_time": 58.52632212638855,
      "citing_paper_id": "260735757",
      "cited_paper_id": 195892710
    },
    {
      "context_text": "CDAE [51] uses denoising auto-encoders (DAE) to learn the embeddings of users and items for top-K recommendation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CDAE, which is a method, not a dataset. No specific dataset is referenced in the citation.",
      "processing_time": 57.95833945274353,
      "citing_paper_id": "260735757",
      "cited_paper_id": 6392154
    },
    {
      "context_text": "M 3 TDBN [29] used a multi-modal multi-task beep belief network to learn joint image-ingredient representation for recipe classification/retrieval and ingredient inference from food images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (M 3 TDBN) and its application but does not reference a specific dataset. The context is about a method for learning joint image-ingredient representations.",
      "processing_time": 59.56188702583313,
      "citing_paper_id": "260735757",
      "cited_paper_id": 6655663
    },
    {
      "context_text": "Review-based solutions, as a subdiscipline of text-based RS, benefit from valuable information in (textual) reviews for personalization and recommendation [1, 2, 10, 32, 38, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to textual reviews. No clear, verifiable datasets are identified.",
      "processing_time": 58.67092275619507,
      "citing_paper_id": "260735757",
      "cited_paper_id": 7847519
    },
    {
      "context_text": "Review-based solutions, as a subdiscipline of text-based RS, benefit from valuable information in (textual) reviews for personalization and recommendation [1, 2, 10, 32, 38, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to textual reviews. No clear, verifiable datasets are identified.",
      "processing_time": 58.67092275619507,
      "citing_paper_id": "260735757",
      "cited_paper_id": 153313270
    },
    {
      "context_text": "Review-based solutions, as a subdiscipline of text-based RS, benefit from valuable information in (textual) reviews for personalization and recommendation [1, 2, 10, 32, 38, 39, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to textual reviews. No clear, verifiable datasets are identified.",
      "processing_time": 58.67092275619507,
      "citing_paper_id": "260735757",
      "cited_paper_id": 218870205
    },
    {
      "context_text": "Bossard et al. [5] introduced the Food-101 dataset for visual recipe classification, where every recipe belongs to one of 101 categories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Food-101"
      ],
      "dataset_descriptions": {
        "Food-101": "Used for visual recipe classification, categorizing recipes into 101 distinct classes using image-based methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the Food-101 dataset, which is a specific, verifiable resource used for visual recipe classification.",
      "processing_time": 63.79804563522339,
      "citing_paper_id": "260735757",
      "cited_paper_id": 12726540
    },
    {
      "context_text": "We also evaluated other KGE models, such as TransE [4], TransH [50], and DistMult [52], in our preliminary experiments and none of them showed comparable results to RotatE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several KGE models but does not refer to any specific datasets. The context is focused on comparing model performance.",
      "processing_time": 57.39029288291931,
      "citing_paper_id": "260735757",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "We used PyKeen 15 for the KGE models, and NeuRec 16 from NExT++ 17 for the (neural) baselines for RQ1.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions tools and methods (PyKeen, NeuRec) but does not specify any datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.32900404930115,
      "citing_paper_id": "260735757",
      "cited_paper_id": 15152621
    },
    {
      "context_text": "We used PyKeen 15 for the KGE models, and NeuRec 16 from NExT++ 17 for the (neural) baselines for RQ1.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions tools and methods (PyKeen, NeuRec) but does not specify any datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.32900404930115,
      "citing_paper_id": "260735757",
      "cited_paper_id": 207190913
    },
    {
      "context_text": "The focus of most works is news, book or movie recommendations [35, 43, 45, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of recommendations. No verifiable resources are identified.",
      "processing_time": 57.71958327293396,
      "citing_paper_id": "260735757",
      "cited_paper_id": 52901171
    },
    {
      "context_text": "The focus of most works is news, book or movie recommendations [35, 43, 45, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of recommendations. No verifiable resources are identified.",
      "processing_time": 57.71958327293396,
      "citing_paper_id": "260735757",
      "cited_paper_id": 59291937
    },
    {
      "context_text": "The focus of most works is news, book or movie recommendations [35, 43, 45, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of recommendations. No verifiable resources are identified.",
      "processing_time": 57.71958327293396,
      "citing_paper_id": "260735757",
      "cited_paper_id": 159042183
    },
    {
      "context_text": "The focus of most works is news, book or movie recommendations [35, 43, 45, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of recommendations. No verifiable resources are identified.",
      "processing_time": 57.71958327293396,
      "citing_paper_id": "260735757",
      "cited_paper_id": 237494860
    },
    {
      "context_text": "In that regard, we consider the food.com and allrecipes.com datasets, publicly available on Kaggle.com, for our research work 12,13,14 : ‚Ä¢ Food.com is a digital brand featuring recipes from home cooks and celebrity chefs, food news, and pop culture.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "food.com",
        "allrecipes.com"
      ],
      "dataset_descriptions": {
        "food.com": "Used to study recipes from home cooks and celebrity chefs, focusing on digital brand content and food-related news and pop culture.",
        "allrecipes.com": "Used to analyze recipes and food-related content, contributing to the understanding of culinary trends and user-generated recipe data."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two specific datasets, 'food.com' and 'allrecipes.com', which are publicly available on Kaggle.com. These datasets are used for research work involving recipes and food-related content.",
      "processing_time": 69.0646722316742,
      "citing_paper_id": "260735757",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The NLP embeddings could be obtained from the Statistical-based Embedding methods [23, 34], the word representation models such as Word2Vec [28] and GloVe [33], and language models (LMs) such as BERT [12], RoBERTa [24] and MPNet [36, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various models and methods for obtaining NLP embeddings but does not refer to any specific datasets. The cited papers are also about models and methods, not datasets.",
      "processing_time": 59.67014241218567,
      "citing_paper_id": "260735757",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The NLP embeddings could be obtained from the Statistical-based Embedding methods [23, 34], the word representation models such as Word2Vec [28] and GloVe [33], and language models (LMs) such as BERT [12], RoBERTa [24] and MPNet [36, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various models and methods for obtaining NLP embeddings but does not refer to any specific datasets. The cited papers are also about models and methods, not datasets.",
      "processing_time": 59.67014241218567,
      "citing_paper_id": "260735757",
      "cited_paper_id": 73489935
    },
    {
      "context_text": "The NLP embeddings could be obtained from the Statistical-based Embedding methods [23, 34], the word representation models such as Word2Vec [28] and GloVe [33], and language models (LMs) such as BERT [12], RoBERTa [24] and MPNet [36, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various models and methods for obtaining NLP embeddings but does not refer to any specific datasets. The cited papers are also about models and methods, not datasets.",
      "processing_time": 59.67014241218567,
      "citing_paper_id": "260735757",
      "cited_paper_id": 215827489
    },
    {
      "context_text": "Although the NLP methods were used in settings such as ingredient networks [44], Recipe1M+ [26], and pFoodREQ [11], we found no fully text-based query-recipe and profile-recipe matching solutions in the literature.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Recipe1M+"
      ],
      "dataset_descriptions": {
        "Recipe1M+": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual information in recipe recommendation systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three resources: 'ingredient networks', 'Recipe1M+', and 'pFoodREQ'. Based on the cited paper titles, 'Recipe1M+' is a dataset, while 'ingredient networks' and 'pFoodREQ' are likely methods or tools.",
      "processing_time": 67.7159857749939,
      "citing_paper_id": "260735757",
      "cited_paper_id": 195892710
    },
    {
      "context_text": "Although the NLP methods were used in settings such as ingredient networks [44], Recipe1M+ [26], and pFoodREQ [11], we found no fully text-based query-recipe and profile-recipe matching solutions in the literature.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Recipe1M+"
      ],
      "dataset_descriptions": {
        "Recipe1M+": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual information in recipe recommendation systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three resources: 'ingredient networks', 'Recipe1M+', and 'pFoodREQ'. Based on the cited paper titles, 'Recipe1M+' is a dataset, while 'ingredient networks' and 'pFoodREQ' are likely methods or tools.",
      "processing_time": 67.7159857749939,
      "citing_paper_id": "260735757",
      "cited_paper_id": 207198269
    },
    {
      "context_text": "Although the NLP methods were used in settings such as ingredient networks [44], Recipe1M+ [26], and pFoodREQ [11], we found no fully text-based query-recipe and profile-recipe matching solutions in the literature.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Recipe1M+"
      ],
      "dataset_descriptions": {
        "Recipe1M+": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual information in recipe recommendation systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three resources: 'ingredient networks', 'Recipe1M+', and 'pFoodREQ'. Based on the cited paper titles, 'Recipe1M+' is a dataset, while 'ingredient networks' and 'pFoodREQ' are likely methods or tools.",
      "processing_time": 67.7159857749939,
      "citing_paper_id": "260735757",
      "cited_paper_id": 230770066
    },
    {
      "context_text": "Recipe1M+ [26] is a large-scale and structured dataset with over 1M cooking recipes and 13M food images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Recipe1M+"
      ],
      "dataset_descriptions": {
        "Recipe1M+": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual data in a large-scale structured dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions Recipe1M+ as a large-scale and structured dataset with cooking recipes and food images, which is relevant to multi-modal learning and knowledge graph reasoning.",
      "processing_time": 65.99325013160706,
      "citing_paper_id": "260735757",
      "cited_paper_id": 195892710
    },
    {
      "context_text": "Teng et al. [44] claimed that they predict recipe ratings with features derived from the ingredient networks and nutrition facts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions predicting recipe ratings using features derived from ingredient networks and nutrition facts, but does not specify a dataset name.",
      "processing_time": 58.07396912574768,
      "citing_paper_id": "260735757",
      "cited_paper_id": 207198269
    },
    {
      "context_text": "By relying on FoodKG, pFoodREQ [11] formulates food recommendation as a constrained question answering task over a large-scale food knowledge base (KBQA).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FoodKG"
      ],
      "dataset_descriptions": {
        "FoodKG": "Used as a large-scale food knowledge base for formulating food recommendations through constrained question answering, enhancing personalized recommendations."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'FoodKG', which is a specific knowledge graph used for food recommendation. It is used as a knowledge base for a constrained question answering task.",
      "processing_time": 64.95045709609985,
      "citing_paper_id": "260735757",
      "cited_paper_id": 230770066
    },
    {
      "context_text": "For example, a recent work by Polignano et al. [35] used knowledge graph embeddings and (deep) contextual embeddings for book and movie recommendations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of knowledge graph embeddings and contextual embeddings for recommendations, but does not specify a dataset name. The context is about methodological approaches rather than a specific dataset.",
      "processing_time": 60.2037136554718,
      "citing_paper_id": "260735757",
      "cited_paper_id": 237494860
    },
    {
      "context_text": "Those measures are widely used in the RS and KGE literature [18, 35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general measures used in the literature. No dataset names are present in the citation span.",
      "processing_time": 58.32912492752075,
      "citing_paper_id": "260735757",
      "cited_paper_id": 237494860
    },
    {
      "context_text": "Some existing works (Xu et al. 2017; Yang et al. 2018; Zellers et al. 2018) applied RNNs and GCNs to propagate image contexts in order to achieve better utilizing the contexts for object and relationship prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models (RNNs, GCNs). The context focuses on the application of these methods for object and relationship prediction.",
      "processing_time": 60.189537048339844,
      "citing_paper_id": "261100976",
      "cited_paper_id": 1780254
    },
    {
      "context_text": "Some existing works (Xu et al. 2017; Yang et al. 2018; Zellers et al. 2018) applied RNNs and GCNs to propagate image contexts in order to achieve better utilizing the contexts for object and relationship prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models (RNNs, GCNs). The context focuses on the application of these methods for object and relationship prediction.",
      "processing_time": 60.189537048339844,
      "citing_paper_id": "261100976",
      "cited_paper_id": 4379400
    },
    {
      "context_text": "‚Ä¶(Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining (Yu et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.93537950515747,
      "citing_paper_id": "261100976",
      "cited_paper_id": 4593810
    },
    {
      "context_text": "‚Ä¶(Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining (Yu et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.93537950515747,
      "citing_paper_id": "261100976",
      "cited_paper_id": 220265934
    },
    {
      "context_text": "‚Ä¶(Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining (Yu et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.93537950515747,
      "citing_paper_id": "261100976",
      "cited_paper_id": 251710138
    },
    {
      "context_text": "2022), we utilize the Karpathy (Karpathy and Fei-Fei 2017) split for training and evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Karpathy"
      ],
      "dataset_descriptions": {
        "Karpathy": "Used for training and evaluating image captioning models, focusing on aligning visual and semantic information in images and text."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used for training and evaluation in multi-modal tasks, particularly in image captioning.",
      "processing_time": 65.80480313301086,
      "citing_paper_id": "261100976",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "Consistent with prior work (Li et al. 2022), we utilize the Karpathy (Karpathy and Fei-Fei 2017) split for training and evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Karpathy"
      ],
      "dataset_descriptions": {
        "Karpathy": "Used for training and evaluating multi-modal models, specifically focusing on aligning visual and semantic information in image captioning tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used for training and evaluation in multi-modal tasks involving images and text.",
      "processing_time": 65.54082345962524,
      "citing_paper_id": "261100976",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "Consistent with prior work (Li et al. 2022), we utilize the Karpathy (Karpathy and Fei-Fei 2017) split for training and evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Karpathy"
      ],
      "dataset_descriptions": {
        "Karpathy": "Used for training and evaluating multi-modal models, specifically focusing on aligning visual and semantic information in image captioning tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used for training and evaluation in multi-modal tasks involving images and text.",
      "processing_time": 65.54082345962524,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Following prior work(Li et al. 2022), we use the Karpathy(Karpathy and Fei-Fei 2017) split for this dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Karpathy"
      ],
      "dataset_descriptions": {
        "Karpathy": "Used to train and evaluate image captioning models, focusing on aligning visual and textual information in the context of multi-modal reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used in multi-modal research, particularly for image captioning tasks.",
      "processing_time": 65.6350769996643,
      "citing_paper_id": "261100976",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "Following prior work(Li et al. 2022), we use the Karpathy(Karpathy and Fei-Fei 2017) split for this dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Karpathy"
      ],
      "dataset_descriptions": {
        "Karpathy": "Used to train and evaluate image captioning models, focusing on aligning visual and textual information in the context of multi-modal reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used in multi-modal research, particularly for image captioning tasks.",
      "processing_time": 65.6350769996643,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Given the text sentence w , we parse it into a scene graph (Johnson et al. 2015), which denotes as G ( w ) = < O ( w ) , E ( w ) , K ( w ) > , where O ( w ) is the set of objects mentioned in w , R ( w ) is the set of relationship nodes, and",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for parsing text into a scene graph. No verifiable datasets are referenced.",
      "processing_time": 57.86853647232056,
      "citing_paper_id": "261100976",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "Given the text sentence w, we parse it into a scene graph (Johnson et al. 2015), which denotes as G(w) =< O(w), E(w),K(w) >, where O(w) is the set of objects mentioned in w, R(w) is the set of relationship nodes, and E(w) ‚äÜ O(w)√óR(w)√ó O(w) is the set of hyper-edges representing actual relationships between objects.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method for parsing text into a scene graph, which is not a dataset.",
      "processing_time": 59.141627073287964,
      "citing_paper_id": "261100976",
      "cited_paper_id": 16414666
    },
    {
      "context_text": "The structured Knowledge-Enhanced Encoder is implemented using a 6-layer Transformer architecture initialized with BERT-base (Devlin et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not indicate it is used as a dataset. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 59.686630725860596,
      "citing_paper_id": "261100976",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For each triple ( h i , r i , t i ) in T in , we use Tokenizer and Word Vocabulary Embeddings from BERT (Devlin et al. 2019) to obtain each entity embedding w h , w r , w t : In order to get the triple embedding with each entity embedding, we use the following encoding function: where ENC triple ( .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not indicate it is used as a dataset. It is used for tokenization and word vocabulary embeddings, which are methods, not datasets.",
      "processing_time": 59.83939838409424,
      "citing_paper_id": "261100976",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "They evaluated various well-performing VLMs (e.g., VinVL (Zhang et al. 2021), UNITER (Chen et al. 2020), ViLBERT (Lu et al. 2019), and CLIP (Radford et al. 2021)), with the aim of assessing the structured representations pertaining to objects, attributes, and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating VLMs but does not specify any datasets. The cited papers are models, not datasets.",
      "processing_time": 57.97488045692444,
      "citing_paper_id": "261100976",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "They evaluated various well-performing VLMs (e.g., VinVL (Zhang et al. 2021), UNITER (Chen et al. 2020), ViLBERT (Lu et al. 2019), and CLIP (Radford et al. 2021)), with the aim of assessing the structured representations pertaining to objects, attributes, and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating VLMs but does not specify any datasets. The cited papers are models, not datasets.",
      "processing_time": 57.97488045692444,
      "citing_paper_id": "261100976",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "They evaluated various well-performing VLMs (e.g., VinVL (Zhang et al. 2021), UNITER (Chen et al. 2020), ViLBERT (Lu et al. 2019), and CLIP (Radford et al. 2021)), with the aim of assessing the structured representations pertaining to objects, attributes, and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating VLMs but does not specify any datasets. The cited papers are models, not datasets.",
      "processing_time": 57.97488045692444,
      "citing_paper_id": "261100976",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "They evaluated various well-performing VLMs (e.g., VinVL (Zhang et al. 2021), UNITER (Chen et al. 2020), ViLBERT (Lu et al. 2019), and CLIP (Radford et al. 2021)), with the aim of assessing the structured representations pertaining to objects, attributes, and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating VLMs but does not specify any datasets. The cited papers are models, not datasets.",
      "processing_time": 57.97488045692444,
      "citing_paper_id": "261100976",
      "cited_paper_id": 235692795
    },
    {
      "context_text": "We utilize a pre-trained Scene Graph Generator (Wu et al. 2019b) to extract the Scene Graph Knowledge.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a pre-trained Scene Graph Generator but does not refer to a specific dataset. The generator is a method or tool, not a dataset.",
      "processing_time": 59.12700080871582,
      "citing_paper_id": "261100976",
      "cited_paper_id": 119284150
    },
    {
      "context_text": "Such as image captioning (Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and applications in multi-modal learning. No clear, verifiable datasets are identified.",
      "processing_time": 59.12685012817383,
      "citing_paper_id": "261100976",
      "cited_paper_id": 119284150
    },
    {
      "context_text": "Such as image captioning (Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and applications in multi-modal learning. No clear, verifiable datasets are identified.",
      "processing_time": 59.12685012817383,
      "citing_paper_id": "261100976",
      "cited_paper_id": 237291550
    },
    {
      "context_text": "Such as image captioning (Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and applications in multi-modal learning. No clear, verifiable datasets are identified.",
      "processing_time": 59.12685012817383,
      "citing_paper_id": "261100976",
      "cited_paper_id": 251719655
    },
    {
      "context_text": "Such as image captioning (Yang et al. 2019), image retrieval (Wu et al. 2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multi-modal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and vision-language pretraining‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and applications in multi-modal learning. No clear, verifiable datasets are identified.",
      "processing_time": 59.12685012817383,
      "citing_paper_id": "261100976",
      "cited_paper_id": 263895473
    },
    {
      "context_text": "In our framework, the Scene Graph Parser provided by (Wu et al. 2019b) is adopted to parse texts to scene graphs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a tool (Scene Graph Parser) rather than a dataset. No specific dataset is referenced in the context.",
      "processing_time": 57.93180298805237,
      "citing_paper_id": "261100976",
      "cited_paper_id": 119284150
    },
    {
      "context_text": "‚Ä¶Language Modeling (MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al.‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.911216020584106,
      "citing_paper_id": "261100976",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "‚Ä¶Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv) Cross-Modal Con-trastive Learning (CMCL) (Radford et al. 2021; Jia et al. 2021; Li et al.‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 57.74839949607849,
      "citing_paper_id": "261100976",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "There are mainly 4 types of tasks: (i) Cross-Modal Masked Language Modeling (MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv) Cross-Modal Con-trastive Learning (CMCL) (Radford et al. 2021; Jia et al. 2021; Li et al. 2021a; Huo et al. 2021; Li et al. 2021b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of tasks and methods. No verifiable resources are identified.",
      "processing_time": 58.179248571395874,
      "citing_paper_id": "261100976",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "There are mainly 4 types of tasks: (i) Cross-Modal Masked Language Modeling (MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv) Cross-Modal Con-trastive Learning (CMCL) (Radford et al. 2021; Jia et al. 2021; Li et al. 2021a; Huo et al. 2021; Li et al. 2021b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of tasks and methods. No verifiable resources are identified.",
      "processing_time": 58.179248571395874,
      "citing_paper_id": "261100976",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "There are mainly 4 types of tasks: (i) Cross-Modal Masked Language Modeling (MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv) Cross-Modal Con-trastive Learning (CMCL) (Radford et al. 2021; Jia et al. 2021; Li et al. 2021a; Huo et al. 2021; Li et al. 2021b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of tasks and methods. No verifiable resources are identified.",
      "processing_time": 58.179248571395874,
      "citing_paper_id": "261100976",
      "cited_paper_id": 248512473
    },
    {
      "context_text": "Vision-language models (VLMs) have demonstrated significant performance in various existing multi-modal understanding and generation tasks (Radford et al. 2021; Li et al. 2022; Singh et al. 2022; Li et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to vision-language models and their performance in multi-modal tasks.",
      "processing_time": 58.17640137672424,
      "citing_paper_id": "261100976",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "Vision-language models (VLMs) have demonstrated significant performance in various existing multi-modal understanding and generation tasks (Radford et al. 2021; Li et al. 2022; Singh et al. 2022; Li et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to vision-language models and their performance in multi-modal tasks.",
      "processing_time": 58.17640137672424,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "‚Ä¶(MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv)‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.9589409828186,
      "citing_paper_id": "261100976",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "‚Ä¶al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv) Cross-Modal Con-trastive Learning (CMCL) (Radford et al. 2021; Jia et al.‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 57.866710901260376,
      "citing_paper_id": "261100976",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "‚Ä¶Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et al. 2020; Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iv) Cross-Modal Con-trastive Learning (CMCL) (Radford et al. 2021; Jia et al. 2021; Li et al. 2021a; Huo et‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tasks. The titles of the cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.426536321640015,
      "citing_paper_id": "261100976",
      "cited_paper_id": 216080982
    },
    {
      "context_text": "2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and visionlanguage pretraining (Yu et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as image generation and vision-language pretraining. No verifiable resources are identified.",
      "processing_time": 59.723026275634766,
      "citing_paper_id": "261100976",
      "cited_paper_id": 220265934
    },
    {
      "context_text": "‚Ä¶(Radford et al. 2021; Jia et al. 2021), the fusion-encoder architecture (Tan and Bansal 2019; Li et al. 2021a), the encoder-decoder architecture (Cho et al. 2021; Wang et al. 2022c; Chen et al. 2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures and models. No verifiable resources are identified.",
      "processing_time": 58.1696674823761,
      "citing_paper_id": "261100976",
      "cited_paper_id": 231802355
    },
    {
      "context_text": "‚Ä¶(Radford et al. 2021; Jia et al. 2021), the fusion-encoder architecture (Tan and Bansal 2019; Li et al. 2021a), the encoder-decoder architecture (Cho et al. 2021; Wang et al. 2022c; Chen et al. 2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures and models. No verifiable resources are identified.",
      "processing_time": 58.1696674823761,
      "citing_paper_id": "261100976",
      "cited_paper_id": 237291550
    },
    {
      "context_text": "‚Ä¶(Radford et al. 2021; Jia et al. 2021), the fusion-encoder architecture (Tan and Bansal 2019; Li et al. 2021a), the encoder-decoder architecture (Cho et al. 2021; Wang et al. 2022c; Chen et al. 2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures and models. No verifiable resources are identified.",
      "processing_time": 58.1696674823761,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "‚Ä¶(Radford et al. 2021; Jia et al. 2021), the fusion-encoder architecture (Tan and Bansal 2019; Li et al. 2021a), the encoder-decoder architecture (Cho et al. 2021; Wang et al. 2022c; Chen et al. 2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures and models. No verifiable resources are identified.",
      "processing_time": 58.1696674823761,
      "citing_paper_id": "261100976",
      "cited_paper_id": 251719655
    },
    {
      "context_text": "‚Ä¶(Radford et al. 2021; Jia et al. 2021), the fusion-encoder architecture (Tan and Bansal 2019; Li et al. 2021a), the encoder-decoder architecture (Cho et al. 2021; Wang et al. 2022c; Chen et al. 2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various architectures and models. No verifiable resources are identified.",
      "processing_time": 58.1696674823761,
      "citing_paper_id": "261100976",
      "cited_paper_id": 263895473
    },
    {
      "context_text": "‚Ä¢ BART (Yuan, Neubig, and Liu 2021) is a baseline under a generation-based paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (BART) which is excluded according to the rules.",
      "processing_time": 58.248894929885864,
      "citing_paper_id": "261100976",
      "cited_paper_id": 235593404
    },
    {
      "context_text": "2021a), the encoder-decoder architecture (Cho et al. 2021; Wang et al. 2022c; Chen et al. 2022a), and more recently, the unified transformer architecture (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model architectures. There are no verifiable resources that meet the criteria.",
      "processing_time": 58.7521116733551,
      "citing_paper_id": "261100976",
      "cited_paper_id": 237291550
    },
    {
      "context_text": "Specifically, the original BLIP model performs poorly on two structured representation downstream tasks, achieving only slightly better results than random chance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the performance of the BLIP model on structured representation downstream tasks.",
      "processing_time": 58.243186712265015,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 57.791364908218384,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "2022a), and more recently, the unified transformer architecture (Li et al. 2022; Wang et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 57.791364908218384,
      "citing_paper_id": "261100976",
      "cited_paper_id": 251719655
    },
    {
      "context_text": "Moreover, by applying our proposed method to the superior base model BLIP, we achieve superior structured representations with the performance of 90.3% and 88.2% on VG-Attribution and VG-Relation, respectively, surpassing that of SOTA model NegCLIP of 19.3% and 7.2%.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VG-Attribution",
        "VG-Relation"
      ],
      "dataset_descriptions": {
        "VG-Attribution": "Used to evaluate structured representations in vision-language tasks, focusing on attribute prediction and reasoning.",
        "VG-Relation": "Used to evaluate structured representations in vision-language tasks, focusing on relationship prediction and reasoning."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions performance on VG-Attribution and VG-Relation, which are likely specific datasets used for evaluation. However, the names do not follow the typical naming conventions for datasets, and there is no additional context to confirm their nature.",
      "processing_time": 68.57156467437744,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Firstly, we observe that the BLIP-based models exhibit similar performance trends as the CLIP-based models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance comparisons. No verifiable resources are identified.",
      "processing_time": 57.78602385520935,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "‚Ä¢ BLIP (Li et al. 2022) pre-trains a multi-modal mixture of the encoder-decoder model using a dataset bootstrapped from large-scale noisy image-text pairs by injecting diverse synthetic captions and removing noisy captions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "large-scale noisy image-text pairs"
      ],
      "dataset_descriptions": {
        "large-scale noisy image-text pairs": "Used to pre-train a multi-modal mixture of the encoder-decoder model, focusing on injecting diverse synthetic captions and removing noisy captions to improve model robustness."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions a dataset used for pre-training a multi-modal model, which is relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 65.99598288536072,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "We report results for the ‚ÄúBase‚Äù variants of BLIP.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model variant. The cited paper title confirms BLIP is a model, not a dataset.",
      "processing_time": 59.28010106086731,
      "citing_paper_id": "261100976",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "There are mainly 4 types of tasks: (i) Cross-Modal Masked Language Modeling (MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and methods. No verifiable resources are identified.",
      "processing_time": 57.84511709213257,
      "citing_paper_id": "261100976",
      "cited_paper_id": 248512473
    },
    {
      "context_text": "There are mainly 4 types of tasks: (i) Cross-Modal Masked Language Modeling (MLM) (Kim, Son, and Kim 2021; Lin et al. 2020; Li et al. 2021a; Yu et al. 2022); (ii) Cross-Modal Masked Region Prediction (MRP) (Lu et al. 2019; Chen et al. 2020; Huang et al. 2021); (iii) Image-Text Matching (ITM) (Li et‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only task types and methods. No verifiable resources are identified.",
      "processing_time": 58.505507946014404,
      "citing_paper_id": "261100976",
      "cited_paper_id": 248512473
    },
    {
      "context_text": "They serve as significant sources of knowledge supplementation and data augmentation for diverse tasks such as Question Answering (Chen et al. 2021b, 2022c), and Zero-shot Learning (Chen et al. 2023a, 2021a, 2023c; Geng et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and methods. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 59.43289065361023,
      "citing_paper_id": "261100976",
      "cited_paper_id": 250264976
    },
    {
      "context_text": "We also note that there exist various in-KG tasks, such as Entity Alignment (Chen et al. 2022b, 2023b) and Link Prediction (Zhang, Chen, and Zhang 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks within knowledge graphs. No verifiable resources are identified.",
      "processing_time": 58.25240612030029,
      "citing_paper_id": "261100976",
      "cited_paper_id": 250264976
    },
    {
      "context_text": "We also note that there exist various in-KG tasks, such as Entity Alignment (Chen et al. 2022b, 2023b) and Link Prediction (Zhang, Chen, and Zhang 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks within knowledge graphs. No verifiable resources are identified.",
      "processing_time": 58.25240612030029,
      "citing_paper_id": "261100976",
      "cited_paper_id": 260887576
    },
    {
      "context_text": "2022b), multimodal sentiment classifications (Huang et al. 2022), image generation (Johnson, Gupta, and Fei-Fei 2018) and visionlanguage pretraining (Yu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. No clear identifiers for datasets are present.",
      "processing_time": 58.74084687232971,
      "citing_paper_id": "261100976",
      "cited_paper_id": 251710138
    },
    {
      "context_text": "2019a), visual question answering (Zhang, Chao, and Xuan 2019; Wang et al. 2022b), multimodal sentiment classifications (Huang et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'visual question answering' and cites papers that could be related to datasets, but no specific dataset names are provided.",
      "processing_time": 59.17557978630066,
      "citing_paper_id": "261100976",
      "cited_paper_id": 263895473
    },
    {
      "context_text": "For the Ô¨Årst type, Ô¨Åve traditional data-driven models are compared, including Lasso [21], XGBoost [20], Geo-Spotting [14], D2S3 [17] and NeuMF-RS [15], the state-of-the-art model developed based on deep neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing traditional data-driven models, which are methods, not datasets.",
      "processing_time": 60.12582015991211,
      "citing_paper_id": "275054439",
      "cited_paper_id": 661123
    },
    {
      "context_text": "For the Ô¨Årst type, Ô¨Åve traditional data-driven models are compared, including Lasso [21], XGBoost [20], Geo-Spotting [14], D2S3 [17] and NeuMF-RS [15], the state-of-the-art model developed based on deep neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing traditional data-driven models, which are methods, not datasets.",
      "processing_time": 60.12582015991211,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "For the Ô¨Årst type, Ô¨Åve traditional data-driven models are compared, including Lasso [21], XGBoost [20], Geo-Spotting [14], D2S3 [17] and NeuMF-RS [15], the state-of-the-art model developed based on deep neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on comparing traditional data-driven models, which are methods, not datasets.",
      "processing_time": 60.12582015991211,
      "citing_paper_id": "275054439",
      "cited_paper_id": 15091093
    },
    {
      "context_text": "They usually extract the features, which are fed into different models (XGBoost [20], SVR [21], NeuMF-RS [15], etc.) for the Ô¨Ånal decision.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.83941674232483,
      "citing_paper_id": "275054439",
      "cited_paper_id": 661123
    },
    {
      "context_text": "Data-driven site selection models, which rely on the extracted feature with different machine learning models [20], [21], usually map the problem as the regression problem [14], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to machine learning models and regression problems. No verifiable resources are identified.",
      "processing_time": 59.32873225212097,
      "citing_paper_id": "275054439",
      "cited_paper_id": 661123
    },
    {
      "context_text": "Data-driven site selection models, which rely on the extracted feature with different machine learning models [20], [21], usually map the problem as the regression problem [14], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to machine learning models and regression problems. No verifiable resources are identified.",
      "processing_time": 59.32873225212097,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "Data-driven site selection models, which rely on the extracted feature with different machine learning models [20], [21], usually map the problem as the regression problem [14], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to machine learning models and regression problems. No verifiable resources are identified.",
      "processing_time": 59.32873225212097,
      "citing_paper_id": "275054439",
      "cited_paper_id": 15091093
    },
    {
      "context_text": "Restrictions apply. example, as early attempts to predict optimal locations for stores, Geo-Spotting [14] and DD3S [17] adopt traditional SVR [21] and RankNet [25] to make use of the commonly seen features, e.g., density, competitiveness and area popularity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models (SVR, RankNet) and systems (Geo-Spotting, DD3S) but does not provide names of datasets used.",
      "processing_time": 61.922306537628174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 661123
    },
    {
      "context_text": "Restrictions apply. example, as early attempts to predict optimal locations for stores, Geo-Spotting [14] and DD3S [17] adopt traditional SVR [21] and RankNet [25] to make use of the commonly seen features, e.g., density, competitiveness and area popularity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models (SVR, RankNet) and systems (Geo-Spotting, DD3S) but does not provide names of datasets used.",
      "processing_time": 61.922306537628174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "Restrictions apply. example, as early attempts to predict optimal locations for stores, Geo-Spotting [14] and DD3S [17] adopt traditional SVR [21] and RankNet [25] to make use of the commonly seen features, e.g., density, competitiveness and area popularity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models (SVR, RankNet) and systems (Geo-Spotting, DD3S) but does not provide names of datasets used.",
      "processing_time": 61.922306537628174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 15091093
    },
    {
      "context_text": "Three types of information [14], [16], [81] constitute current urban data, i.e., spatial data, store data, and user behaviour data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions types of urban data but does not specify any named datasets. The cited paper titles do not provide additional clarity on specific datasets.",
      "processing_time": 59.02083468437195,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "Three types of information [14], [16], [81] constitute current urban data, i.e., spatial data, store data, and user behaviour data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions types of urban data but does not specify any named datasets. The cited paper titles do not provide additional clarity on specific datasets.",
      "processing_time": 59.02083468437195,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "(cid:2) MAP@x: (Mean Average Precision), which measures the relative ranking quality of the top-k regions in ÀÜ Y i . where the relevance score rel(ÀÜ y j ) is calculated by [14].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (MAP@x) which is not a dataset. The context is too limited to infer any dataset usage.",
      "processing_time": 59.947916746139526,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "Besides, the relevance score rel(ÀÜ y j ) is calculated by [14].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a relevance score calculation, which is not a verifiable resource.",
      "processing_time": 59.60610222816467,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "Early attempts on site selection via AI techniques [14], [15], [16], [17], [18] mainly rely on feature engineering as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI techniques and feature engineering. No verifiable resources are identified.",
      "processing_time": 59.17426514625549,
      "citing_paper_id": "275054439",
      "cited_paper_id": 1128753
    },
    {
      "context_text": "Early attempts on site selection via AI techniques [14], [15], [16], [17], [18] mainly rely on feature engineering as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI techniques and feature engineering. No verifiable resources are identified.",
      "processing_time": 59.17426514625549,
      "citing_paper_id": "275054439",
      "cited_paper_id": 15091093
    },
    {
      "context_text": "Early attempts on site selection via AI techniques [14], [15], [16], [17], [18] mainly rely on feature engineering as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI techniques and feature engineering. No verifiable resources are identified.",
      "processing_time": 59.17426514625549,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "Early attempts on site selection via AI techniques [14], [15], [16], [17], [18] mainly rely on feature engineering as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI techniques and feature engineering. No verifiable resources are identified.",
      "processing_time": 59.17426514625549,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "Inspired by them, O 2 -SiteRec [26] uses a multi-graph attention network [29] for online-to-ofÔ¨Çine store site recommendation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (multi-graph attention network) and a system (O 2 -SiteRec).",
      "processing_time": 59.921586990356445,
      "citing_paper_id": "275054439",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "‚Ä¶[24], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43] are naturally suitable for it, such as translational models (e.g., TransE [44]), etc.), tensor decomposition models (e.g., DistMult [45], ComplEX [46], and TuckER [47]) and GNN models (e.g., RGCN [48] and COMPGCN [49]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.42528223991394,
      "citing_paper_id": "275054439",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "‚Ä¶[24], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43] are naturally suitable for it, such as translational models (e.g., TransE [44]), etc.), tensor decomposition models (e.g., DistMult [45], ComplEX [46], and TuckER [47]) and GNN models (e.g., RGCN [48] and COMPGCN [49]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.42528223991394,
      "citing_paper_id": "275054439",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "As for the second type, we compare Ô¨Åve typical task-agnostic models, including TransE [44], DistMult [45], ComplEx [46], TuckER [47] and CompGCN [49], and the only task-speciÔ¨Åc model, KnowSite [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models but does not refer to any specific datasets. The context is focused on comparing models, not using datasets.",
      "processing_time": 58.98928666114807,
      "citing_paper_id": "275054439",
      "cited_paper_id": 59316623
    },
    {
      "context_text": "As for the second type, we compare Ô¨Åve typical task-agnostic models, including TransE [44], DistMult [45], ComplEx [46], TuckER [47] and CompGCN [49], and the only task-speciÔ¨Åc model, KnowSite [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models but does not refer to any specific datasets. The context is focused on comparing models, not using datasets.",
      "processing_time": 58.98928666114807,
      "citing_paper_id": "275054439",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "Moreover, these models merely provide an importance score for each feature without logical reasoning, which is insufÔ¨Åcient to persuade corporations [10], [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AHP) and a limitation of current models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 60.350423097610474,
      "citing_paper_id": "275054439",
      "cited_paper_id": 70289226
    },
    {
      "context_text": "The early attempts integrate the features from various modalities with simple operations like summation [66], concatenation [67], and alignment learning [68], [69].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for integrating multi-modal features. No verifiable resources are identified.",
      "processing_time": 58.99820423126221,
      "citing_paper_id": "275054439",
      "cited_paper_id": 102352623
    },
    {
      "context_text": "The early attempts integrate the features from various modalities with simple operations like summation [66], concatenation [67], and alignment learning [68], [69].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for integrating multi-modal features. No verifiable resources are identified.",
      "processing_time": 58.99820423126221,
      "citing_paper_id": "275054439",
      "cited_paper_id": 249191733
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 203163196
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 232335647
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 232380138
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 235829068
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 257729173
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 259075619
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 259129314
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 268063320
    },
    {
      "context_text": "As described in [50], [51], [52], the fusion strategy plays an important role in multi-modal learning to generate high-quality fused features, which are investigated in different Ô¨Åelds [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of fusion strategies in multi-modal learning. No verifiable resources are identified.",
      "processing_time": 59.4360454082489,
      "citing_paper_id": "275054439",
      "cited_paper_id": 268357641
    },
    {
      "context_text": "Asforthestructuralinforma-tion,theCOMPGCN[49]isadoptedassameasKnowSite[19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (COMPGCN and KnowSite). There are no clear identifiers for datasets in the given context.",
      "processing_time": 60.17704200744629,
      "citing_paper_id": "275054439",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "4 as Ô¨Åve typical task-agnostic models, i.e., TransE[44], CompGCN [49], DistMult [45], ComplEx[46], and NeuMF-RS [15], where the former four models belong to KGE backbones, and the NeuMF-RS is a typical feature engineering method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the models used in the research.",
      "processing_time": 59.242923974990845,
      "citing_paper_id": "275054439",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "The core basis in our encoder is COMPGCN [49].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method (COMPGCN) which is a type of graph convolutional network.",
      "processing_time": 59.84297442436218,
      "citing_paper_id": "275054439",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "Concretely, COMPGCN [49] is leveraged as the encoder, as it can Ô¨Çexibly control the information sharing among diverse factors [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (COMPGCN) which is used as an encoder. The cited papers do not provide additional context about datasets.",
      "processing_time": 61.10038375854492,
      "citing_paper_id": "275054439",
      "cited_paper_id": 207847719
    },
    {
      "context_text": "Concretely, COMPGCN [49] is leveraged as the encoder, as it can Ô¨Çexibly control the information sharing among diverse factors [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (COMPGCN) which is used as an encoder. The cited papers do not provide additional context about datasets.",
      "processing_time": 61.10038375854492,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "Previous works store the urban knowledge in the constructed knowledge graphs (KGs), which is an intuitive representation and can be easily encoded via KG learning models to avoid complex and inefÔ¨Åcient feature engineering [23], [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about knowledge graphs and their applications.",
      "processing_time": 58.63190960884094,
      "citing_paper_id": "275054439",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "Thus, most of the knowledge graph learning models [23], [24], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43] are naturally suitable for it, such as translational models (e.g., TransE [44]), etc.), tensor decomposition models (e.g., DistMult [45], ComplEX [46], and TuckER‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.5546498298645,
      "citing_paper_id": "275054439",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "Thus, most of the knowledge graph learning models [23], [24], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43] are naturally suitable for it, such as translational models (e.g., TransE [44]), etc.), tensor decomposition models (e.g., DistMult [45], ComplEX [46], and TuckER‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.5546498298645,
      "citing_paper_id": "275054439",
      "cited_paper_id": 267547866
    },
    {
      "context_text": "Thus, most of the knowledge graph learning models [23], [24], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43] are naturally suitable for it, such as translational models (e.g., TransE [44]), etc.), tensor decomposition models (e.g., DistMult [45], ComplEX [46], and TuckER‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.5546498298645,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "Nowadays, with the development of location services [11] and the wide availability of multi-source urban data [12], [13], artiÔ¨Åcial intelligence (AI) techniques are adopted to make site selection more efÔ¨Åcient and stable.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general availability of multi-source urban data. No clear, verifiable resource names are provided.",
      "processing_time": 59.25927472114563,
      "citing_paper_id": "275054439",
      "cited_paper_id": 234951345
    },
    {
      "context_text": "Graph Fusion Network: Motivated by [53], [87], we design a residual graph neural network (GNN) based on the constructed undirected KNN graphs to further encode and fuse the information. where A ‚àà R V √ó V is the adjacency matrix, and A [ i, j ] = a n i ,n j calculatedin(3).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (residual graph neural network) and a general concept (KNN graphs).",
      "processing_time": 59.65934872627258,
      "citing_paper_id": "275054439",
      "cited_paper_id": 235829068
    },
    {
      "context_text": "‚Ä¶this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79], [80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.52686905860901,
      "citing_paper_id": "275054439",
      "cited_paper_id": 236361803
    },
    {
      "context_text": "‚Ä¶this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79], [80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.52686905860901,
      "citing_paper_id": "275054439",
      "cited_paper_id": 252782215
    },
    {
      "context_text": "‚Ä¶this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79], [80].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.52686905860901,
      "citing_paper_id": "275054439",
      "cited_paper_id": 257168851
    },
    {
      "context_text": "To alleviate this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79],‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.652287006378174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 248834330
    },
    {
      "context_text": "To alleviate this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79],‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.652287006378174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 253801768
    },
    {
      "context_text": "To alleviate this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79],‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.652287006378174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 256868818
    },
    {
      "context_text": "To alleviate this issue and inspired by the great achievements of GNNs [70], [71], [72], [73], [74], some methods tend to model the correlations with graphs, such as K-nearest neighbor (KNN) graphs [75], and leverage graph learning techniques to generate the fused features [76], [77], [78], [79],‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.652287006378174,
      "citing_paper_id": "275054439",
      "cited_paper_id": 258967414
    },
    {
      "context_text": "Knowledge-driven models aim to integrate speciÔ¨Åc expert knowledge with previous data-driven paradigms to make the inference procedure more explainable and achieve better performance, which has achieved great success in other real-world applications [30], [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'real-world applications'. No verifiable resources are identified.",
      "processing_time": 59.242727279663086,
      "citing_paper_id": "275054439",
      "cited_paper_id": 258756463
    },
    {
      "context_text": "Inourwork,the pretrained ViT [84], BERT [85], and Onepeace [86] are adopted forimage,textandaudioencoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (ViT, BERT, Onepeace) but does not refer to any specific datasets. The context is about using these models for encoding different modalities.",
      "processing_time": 60.78616523742676,
      "citing_paper_id": "275054439",
      "cited_paper_id": 258762390
    },
    {
      "context_text": "The fused embed-dings are further injected into the GNN-based knowledge-driven backbone, i.e., KnowSite [19] for learning and inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'KnowSite' but does not refer to it as a dataset. It is described as a GNN-based knowledge-driven backbone, which suggests it is a method or tool rather than a dataset.",
      "processing_time": 61.878178119659424,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "However, KnowSite [19] ignores the multi-modal attributes corresponding to the brands, thus severely limiting their expressive ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'KnowSite' but does not refer to it as a dataset. It is described as a method or tool that leverages an urban knowledge graph, which is not a specific, verifiable dataset.",
      "processing_time": 61.919869899749756,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "Among them, the most representative model, i.e., KnowSite [19], adopts graph neural network (GNN) models as the encoder and the attention relational path scoring function as the decoder, which achieves state-of-the-art performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model called KnowSite. The context focuses on the model's architecture and performance.",
      "processing_time": 58.59324669837952,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "To alleviate the above problems in site selection, the knowledge-driven paradigm is proposed [19], which aims to integrate speciÔ¨Åc expert knowledge with data-driven paradigms to make the inference procedure more explainable and achieve better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach combining expert knowledge with data-driven methods.",
      "processing_time": 58.74271774291992,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "The Ô¨Årst two issues are alleviated by existing knowledge-driven models [19], which fail to fully address the third.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to knowledge-driven models. There are no clear identifiers for datasets in the given context.",
      "processing_time": 59.79712700843811,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "Besides, the relation path-based decoder is proposed to model site selection criteria for brands and stores so that the KnowSite [19] can not only achieve state-of-the-art (SOTA) performance but also provide explainable site decisions based on the relation path logic.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KnowSite"
      ],
      "dataset_descriptions": {
        "KnowSite": "Used to model site selection criteria for brands and stores, leveraging an urban knowledge graph to provide explainable site decisions based on relation path logic."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'KnowSite' in the context of a knowledge graph for site selection, which appears to be a specific dataset or knowledge base. However, the term 'KnowSite' could also refer to a method or tool. Given the context, it is more likely a dataset or knowledge base used for site selection.",
      "processing_time": 66.71285486221313,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "1(a), which are classiÔ¨Åed as data-driven paradigms [19].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to data-driven paradigms. No verifiable resources are identified.",
      "processing_time": 59.37828493118286,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "Overall, the knowledge graphs are constructed based on the existing urban KGs [19] for site selection, which is named urban KG base in this work.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'urban KGs' but does not specify a particular dataset. The term 'urban KG base' is introduced but lacks a clear identifier or provenance.",
      "processing_time": 60.88724684715271,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "Inspired by them, KnowSite [19] is proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called 'KnowSite'. There is no indication of a reusable dataset.",
      "processing_time": 59.711910009384155,
      "citing_paper_id": "275054439",
      "cited_paper_id": 266469822
    },
    {
      "context_text": "With the development of deep learning networks, researchers also try to develop models [15], [16], [18], [26] based on them for better representation ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 59.502490758895874,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "With the development of deep learning networks, researchers also try to develop models [15], [16], [18], [26] based on them for better representation ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 59.502490758895874,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, visual data, such as satellite images of urban blocks, are also considered in some data-driven models, e.g., DeepStore [16] and AR2Net [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to models (DeepStore, AR2Net) and a general type of data (satellite images of urban blocks).",
      "processing_time": 61.648961782455444,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, visual data, such as satellite images of urban blocks, are also considered in some data-driven models, e.g., DeepStore [16] and AR2Net [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to models (DeepStore, AR2Net) and a general type of data (satellite images of urban blocks).",
      "processing_time": 61.648961782455444,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "We evaluate the performance with Ô¨Åve standard metrics of NDCG@x (Normalized Discounted Cumulative Gain), Hits@x, Precision@x, Recall@x, and MAP@x (Mean Average Precision) [15], [18], [89], where the x is set as 5 , 10 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.327980041503906,
      "citing_paper_id": "275054439",
      "cited_paper_id": null
    },
    {
      "context_text": "Mask R-CNN was also introduced in the same year, which added Fully Convolutional Networks ( FCNs ) [53] to Faster R-CNN to generate the corresponding mask branch [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.50177550315857,
      "citing_paper_id": "269075665",
      "cited_paper_id": 1629541
    },
    {
      "context_text": "Knowledge Graphs ( KGs ), or simply KGs, are a critical component of many knowledge-based applications, including semantic search [105], recommender systems [113], and question answering [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 59.16291570663452,
      "citing_paper_id": "269075665",
      "cited_paper_id": 1644335
    },
    {
      "context_text": "Knowledge Graphs ( KGs ), or simply KGs, are a critical component of many knowledge-based applications, including semantic search [105], recommender systems [113], and question answering [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of Knowledge Graphs. No verifiable resources are identified.",
      "processing_time": 59.16291570663452,
      "citing_paper_id": "269075665",
      "cited_paper_id": 7062707
    },
    {
      "context_text": "PTransE [46] mainly models multi-hop relational paths, which can be used for knowledge reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PTransE) for modeling multi-hop relational paths in knowledge bases.",
      "processing_time": 59.496570348739624,
      "citing_paper_id": "269075665",
      "cited_paper_id": 1969092
    },
    {
      "context_text": "TransA [99], proposed by Xiao et al., incorporates a symmetric non-negative matrix M r for each relation r and utilizes the adaptive Mahalanobis distance as the scoring function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called TransA. The context focuses on the method's technical details rather than its application to a particular dataset.",
      "processing_time": 60.74314999580383,
      "citing_paper_id": "269075665",
      "cited_paper_id": 3225424
    },
    {
      "context_text": "CKRL. CKRL [102] is an advanced framework for knowledge representation learning that integrates the notion of confidence into conventional translation-based methodologies.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called CKRL. The context focuses on the method's capabilities and does not reference any data sources.",
      "processing_time": 59.97717094421387,
      "citing_paper_id": "269075665",
      "cited_paper_id": 3333648
    },
    {
      "context_text": "The usage of CNNs for vector mapping ensures good performances and, in particular, convolutional models like ConvE [17], ConvKB [65], and ConvR [36] have been successfully proposed to capture complex relationships between entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to reference research work on convolutional models for knowledge graph embeddings.",
      "processing_time": 59.82332897186279,
      "citing_paper_id": "269075665",
      "cited_paper_id": 4328400
    },
    {
      "context_text": "The usage of CNNs for vector mapping ensures good performances and, in particular, convolutional models like ConvE [17], ConvKB [65], and ConvR [36] have been successfully proposed to capture complex relationships between entities and relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to reference research work on convolutional models for knowledge graph embeddings.",
      "processing_time": 59.82332897186279,
      "citing_paper_id": "269075665",
      "cited_paper_id": null
    },
    {
      "context_text": "Word2Vec [61], a word embedding method proposed by Google in 2013, learns semantic knowledge with an unsupervised method from a large text corpus.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Word2Vec as a method, not a dataset. No specific dataset is referenced.",
      "processing_time": 58.06454586982727,
      "citing_paper_id": "269075665",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "Several KGs are available for commercial and academic use, including Free-base [6], DBpedia [2], and YAGO [88].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase",
        "DBpedia",
        "YAGO"
      ],
      "dataset_descriptions": {
        "Freebase": "Mentioned as a reusable knowledge graph, but specific usage in the research context is not detailed.",
        "DBpedia": "Mentioned as a reusable knowledge graph, but specific usage in the research context is not detailed.",
        "YAGO": "Mentioned as a reusable resource, but specific usage details are not specified."
      },
      "confidence_score": 0.5,
      "reasoning": "The citation mentions several knowledge graphs, but does not specify their usage in the research context. The names are plausible and specific, but the actual use is not detailed.",
      "processing_time": 66.79552841186523,
      "citing_paper_id": "269075665",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "Secondly, while significant research has been done in the field of machine learning algorithms applied to the analysis of multi-modal data (as demonstrated in the excellent article by Baltrusaitis et al. [4]), it is important to note that MMKGs are distinct from multi-modal machine learning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to multi-modal data which is too generic.",
      "processing_time": 57.756760120391846,
      "citing_paper_id": "269075665",
      "cited_paper_id": 10137425
    },
    {
      "context_text": "In the following, we will consider alignment and embedding as key components of the KGR process and, in line with previous work in multi-modal machine learning (see [4] for a detailed review), we will refer to these techniques as challenges .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and techniques. The cited paper is used for a review and taxonomy, not for providing a dataset.",
      "processing_time": 60.036535024642944,
      "citing_paper_id": "269075665",
      "cited_paper_id": 10137425
    },
    {
      "context_text": "In 2014, Goodfellow et al. proposed the GAN, which contain a generative and a discriminative model [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the introduction of GANs, which are a method, not a dataset.",
      "processing_time": 58.9377555847168,
      "citing_paper_id": "269075665",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "TransD [33], proposed by Ji et al., takes into account both the variety of relations and the variety of entities, distinguishing it from the TransR/CTransR model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing TransD with TransR/CTransR models.",
      "processing_time": 59.82395052909851,
      "citing_paper_id": "269075665",
      "cited_paper_id": 11202498
    },
    {
      "context_text": "Never Ending Image Learner ( NEIL ) [13] collects visual information from Internet data runs 24/7 by Google Image Search; it applies a semi-supervised learning algorithm that finds out common sense relationships (for instance wheel is part of a car ) and it labels instances of the given visual‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "NEIL is described as a system that collects visual information from the internet, but it is not a dataset itself. It is a method or tool for extracting visual knowledge.",
      "processing_time": 60.36211395263672,
      "citing_paper_id": "269075665",
      "cited_paper_id": 12350611
    },
    {
      "context_text": "For example, the VGG network was introduced in 2014 and it stacked multiple layers of 3 √ó 3 small convolution kernels and 2 √ó 2 maximum pooling layers to achieve impressive results (see [83]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the VGG network but does not refer to any specific dataset. It is focused on describing the architecture and performance of the VGG network.",
      "processing_time": 59.98815846443176,
      "citing_paper_id": "269075665",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "VGG [83] and ResNet [30] are popular choices to embed visual information because they achieve high performance without requiring extensive training.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (VGG and ResNet). These models are used for embedding visual information, but no dataset is explicitly referenced.",
      "processing_time": 60.3564670085907,
      "citing_paper_id": "269075665",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "We recall that energy functions were used by TransE [7] for the first time: more specifically, given a triple ( h , r , t ) , let h s , r s and t s be the structural embedding of h , r and s respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TransE but does not refer to a specific dataset. It is discussing a method or model.",
      "processing_time": 57.59602189064026,
      "citing_paper_id": "269075665",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Bordes et al. [7] proposed the TransE model, a translation-based model for knowledge graph embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the TransE model but does not refer to any specific dataset. The context is about the method, not a dataset.",
      "processing_time": 59.06506824493408,
      "citing_paper_id": "269075665",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "The TransE approach [7] and its variant model, TransX series, are the most popular methods for constructing a representation space, which we will discuss in detail in the following.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on describing the TransE approach and its variants.",
      "processing_time": 58.34773111343384,
      "citing_paper_id": "269075665",
      "cited_paper_id": 14941970
    },
    {
      "context_text": "Factorization models [78] are encoding models that use matrix factorization techniques to decompose entity-relation matrices into low-dimensional embeddings.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Factorization Machines).",
      "processing_time": 57.23139023780823,
      "citing_paper_id": "269075665",
      "cited_paper_id": 17265929
    },
    {
      "context_text": "Over the years, researchers have developed different types of encoding models, including linear/bilinear models [73], factorization models [78], and neural networks [111].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of encoding models. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 58.91640615463257,
      "citing_paper_id": "269075665",
      "cited_paper_id": 17265929
    },
    {
      "context_text": "Over the years, researchers have developed different types of encoding models, including linear/bilinear models [73], factorization models [78], and neural networks [111].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of encoding models. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 58.91640615463257,
      "citing_paper_id": "269075665",
      "cited_paper_id": 17719760
    },
    {
      "context_text": "‚Ä¶types of neural networks commonly used in encoding models include Multilayer Percep-trons ( MLP ) [31], Convolutional Neural Networks ( CNNs ) [8], Recurrent Neural Networks ( RNNs ) [111], Transformers [92], Graph Neural Networks ( GNNs ) [79], and Restricted Boltz-mann Machines ( RBMs ) [64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of neural networks. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.75136160850525,
      "citing_paper_id": "269075665",
      "cited_paper_id": 17719760
    },
    {
      "context_text": "‚Ä¶types of neural networks commonly used in encoding models include Multilayer Percep-trons ( MLP ) [31], Convolutional Neural Networks ( CNNs ) [8], Recurrent Neural Networks ( RNNs ) [111], Transformers [92], Graph Neural Networks ( GNNs ) [79], and Restricted Boltz-mann Machines ( RBMs )‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of neural networks. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.793113231658936,
      "citing_paper_id": "269075665",
      "cited_paper_id": 33753227
    },
    {
      "context_text": "This mapping is typically achieved through the use of deep CNNs, which excel at preserving visual features of images [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of deep CNNs for preserving visual features of images.",
      "processing_time": 58.0588858127594,
      "citing_paper_id": "269075665",
      "cited_paper_id": 33753227
    },
    {
      "context_text": "To address this issue, DFMKE utilizes a late fusion module called the LMF, which is efficient to leverage knowledge from multiple modalities [52].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LMF) for multimodal fusion. There are no clear identifiers for datasets in the context.",
      "processing_time": 59.36515164375305,
      "citing_paper_id": "269075665",
      "cited_paper_id": 44131945
    },
    {
      "context_text": "Specifically, pre-training models such as BERT [18], GPT [72], RoBERTa [51], and REALM [26] have demonstrated their effectiveness in various NLP-related tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several pre-trained models but does not refer to any specific datasets. The citation is focused on the effectiveness of these models in NLP tasks, not on the datasets used.",
      "processing_time": 60.03691792488098,
      "citing_paper_id": "269075665",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Specifically, pre-training models such as BERT [18], GPT [72], RoBERTa [51], and REALM [26] have demonstrated their effectiveness in various NLP-related tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several pre-trained models but does not refer to any specific datasets. The citation is focused on the effectiveness of these models in NLP tasks, not on the datasets used.",
      "processing_time": 60.03691792488098,
      "citing_paper_id": "269075665",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "Relevant approaches to producing dynamic representations include CoVe [60], ELMo [70], GPT [72], and BERT [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models (CoVe, ELMo, GPT, BERT) but does not refer to any specific datasets. The context is focused on describing models and their contributions to dynamic representations.",
      "processing_time": 60.587934732437134,
      "citing_paper_id": "269075665",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Relevant approaches to producing dynamic representations include CoVe [60], ELMo [70], GPT [72], and BERT [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models (CoVe, ELMo, GPT, BERT) but does not refer to any specific datasets. The context is focused on describing models and their contributions to dynamic representations.",
      "processing_time": 60.587934732437134,
      "citing_paper_id": "269075665",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, MMKG models generally use sequential networks (e.g., biGRU in MKBE [71] and LSTM in MKGAT [89]) to handle variable-length text data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of sequential networks in handling variable-length text data.",
      "processing_time": 59.35440468788147,
      "citing_paper_id": "269075665",
      "cited_paper_id": 52160797
    },
    {
      "context_text": "As shown in Table 7, most models (including GAIA [42], CLIP-Event [41], and MuMuQA [1]) use Faster R-CNN [76, 77] for object detection purposes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and their use of Faster R-CNN for object detection, but does not specify any datasets. The context is focused on methods and models rather than datasets.",
      "processing_time": 59.83958840370178,
      "citing_paper_id": "269075665",
      "cited_paper_id": 53845347
    },
    {
      "context_text": "As shown in Table 7, most models (including GAIA [42], CLIP-Event [41], and MuMuQA [1]) use Faster R-CNN [76, 77] for object detection purposes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and their use of Faster R-CNN for object detection, but does not specify any datasets. The context is focused on methods and models rather than datasets.",
      "processing_time": 59.83958840370178,
      "citing_paper_id": "269075665",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "ImageGraph [67] is an MMKG that extends the FB15K dataset we cited before; it contains 1,330 relation types, 14,870 entities, and 829,931 images crawled from the Web.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15K",
        "ImageGraph"
      ],
      "dataset_descriptions": {
        "FB15K": "FB15K is extended by ImageGraph to include images and additional relation types, enhancing multi-modal reasoning capabilities.",
        "ImageGraph": "ImageGraph extends FB15K with 1,330 relation types, 14,870 entities, and 829 images, used to answer visual-relational queries in web-extracted knowledge graphs."
      },
      "confidence_score": 0.95,
      "reasoning": "The context mentions 'FB15K' and 'ImageGraph', both of which are datasets. FB15K is extended by ImageGraph, which adds images and relations.",
      "processing_time": 66.09204626083374,
      "citing_paper_id": "269075665",
      "cited_paper_id": 53957733
    },
    {
      "context_text": "To address this issue, [67] queried search engines like GoogleImage to retrieve images associated with entities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to using search engines like GoogleImage to retrieve images, which is a method rather than a dataset.",
      "processing_time": 59.549025535583496,
      "citing_paper_id": "269075665",
      "cited_paper_id": 53957733
    },
    {
      "context_text": "As demonstrated by EVA [48] and ImageGraph [67], N-MMKGs offer the advantage of dense information.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide specific details about the datasets used in the research. It mentions N-MMKGs but does not specify any particular dataset.",
      "processing_time": 59.21527862548828,
      "citing_paper_id": "269075665",
      "cited_paper_id": 53957733
    },
    {
      "context_text": "As demonstrated by EVA [48] and ImageGraph [67], N-MMKGs offer the advantage of dense information.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide specific details about the datasets used in the research. It mentions N-MMKGs but does not specify any particular dataset.",
      "processing_time": 59.21527862548828,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "As Li et al. [39] mentioned, there are two main types of translation to other modal information: generative translation and example-based translation .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of translation methods. No verifiable resources are identified.",
      "processing_time": 57.62379550933838,
      "citing_paper_id": "269075665",
      "cited_paper_id": 56895382
    },
    {
      "context_text": "For entities, the model uses LSTM-CRF [45] for coarse-grained extraction and attentive models for fine-grained extraction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of LSTM-CRF and attentive models for entity extraction.",
      "processing_time": 58.8795223236084,
      "citing_paper_id": "269075665",
      "cited_paper_id": 196202217
    },
    {
      "context_text": "Pre-trained models are widely utilized for completing downstream tasks, with MuMuQA [74] using fine-tuned BART [38] as an encoder-decoder to train models for natural language question generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models and their usage. The context focuses on the method (fine-tuned BART) rather than a dataset.",
      "processing_time": 59.28119158744812,
      "citing_paper_id": "269075665",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "In 2016, the simple and fast YOLO model was proposed [75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the YOLO model but does not refer to any specific dataset. The context is about the model's introduction and capabilities.",
      "processing_time": 58.61569666862488,
      "citing_paper_id": "269075665",
      "cited_paper_id": 206594738
    },
    {
      "context_text": "Unlike Faster R-CNN, YOLO can use the global image in detection to learn the general information of the target, which has a certain universality [75].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between Faster R-CNN and YOLO. No verifiable resources are identified.",
      "processing_time": 58.71265649795532,
      "citing_paper_id": "269075665",
      "cited_paper_id": 206594738
    },
    {
      "context_text": "For instance, OpenIE [21], an open extraction tool, can aid in collecting entities and relationships from a data set.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'a data set' but does not specify a named, verifiable dataset. The term 'data set' is too generic and lacks a specific identifier.",
      "processing_time": 59.08430528640747,
      "citing_paper_id": "269075665",
      "cited_paper_id": 207169186
    },
    {
      "context_text": "And for embedding the knowledge graph structure, we can refer to the KEPLER model [94], which provides concrete benefits in various tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the KEPLER model but does not refer to any specific dataset. The context is about embedding knowledge graph structures, which is a methodological aspect.",
      "processing_time": 58.97812485694885,
      "citing_paper_id": "269075665",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "The multi-modal technology was originally combined with the knowledge base, a system for representing knowledge, called Multi-modal Knowledge [3, 120].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'knowledge base' but does not specify a named, verifiable dataset. The term 'Multi-modal Knowledge' appears to be a method or system rather than a dataset.",
      "processing_time": 59.43653059005737,
      "citing_paper_id": "269075665",
      "cited_paper_id": 209097551
    },
    {
      "context_text": "GAIA [42] is a comprehensive open-source multimedia knowledge extraction system that performs information extraction from text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GAIA as a system, not a dataset. It is described as a multimedia knowledge extraction system, which does not fit the criteria for a dataset.",
      "processing_time": 58.97005081176758,
      "citing_paper_id": "269075665",
      "cited_paper_id": 218551030
    },
    {
      "context_text": "A large number of EA algorithms use a structural approach [59, 90].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach used by algorithms. No verifiable resources are identified.",
      "processing_time": 57.86322259902954,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221150486
    },
    {
      "context_text": "A further interesting approach to fusing features is due to the Multi-Modal Entity Alignment (MMEA, in short) framework [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the MMEA framework but does not refer to it as a dataset. It is described as a method or framework for entity alignment in multi-modal knowledge graphs.",
      "processing_time": 58.889103174209595,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "According to this line of reasoning, the MMEA model [12], after representing and fusing the relevant N-Gram ‚Äì ‚àö ‚Äì MMEA [12] Euclidean Distance NCA ‚Äì SGD EVA [48] Cosine Similarity NCA ‚àö SGD DFMKE [118] Cosine Similarity NCA ‚àö SGD CLIP-Event [41] Cosine Similarity Kullback-Leibler Divergence ‚àö Adam‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing various models and their components, such as MMEA, EVA, DFMKE, and CLIP-Event, along with their similarity measures and optimization algorithms.",
      "processing_time": 60.52083420753479,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221193809
    },
    {
      "context_text": "As observed in [48], the visual representation of the real world is universal (think of an image depicting a cat or apple) and such a representation can be understood by every human; as a further advantage, images are often available for free on the Internet and they are of good quality.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses the universality and availability of visual representations but does not reference a particular dataset.",
      "processing_time": 58.45526432991028,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "EVA [48] incorporates an alignment learning component for aligning multi-modal embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (EVA) that incorporates alignment learning for multi-modal embeddings.",
      "processing_time": 58.16588354110718,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "For example, the EVA [48] model uses an attention-weighted concatenation in modality fusion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (EVA) and its method of modality fusion. No verifiable resources are identified.",
      "processing_time": 58.400492668151855,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "‚Ä¶to this line of reasoning, the MMEA model [12], after representing and fusing the relevant N-Gram ‚Äì ‚àö ‚Äì MMEA [12] Euclidean Distance NCA ‚Äì SGD EVA [48] Cosine Similarity NCA ‚àö SGD DFMKE [118] Cosine Similarity NCA ‚àö SGD CLIP-Event [41] Cosine Similarity Kullback-Leibler Divergence ‚àö Adam All‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on model performance and evaluation metrics.",
      "processing_time": 57.50172257423401,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "‚Ä¶act as a bridge entity is particularly appreciable in case of rare entities without adequate contextual information (see [10, 27]): as observed in [48], in fact, it is very hard to match rare entities if we would only rely upon textual descriptions or the neighbors of each entity in each KG while‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of matching rare entities in knowledge graphs.",
      "processing_time": 57.12575364112854,
      "citing_paper_id": "269075665",
      "cited_paper_id": 221995513
    },
    {
      "context_text": "The MCTKG model [101] employed an entity alignment algorithm to fuse information from multiple data sources with the purpose of handling entities with missing attributes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of multiple data sources for entity alignment in a knowledge graph, which aligns with the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 57.832441329956055,
      "citing_paper_id": "269075665",
      "cited_paper_id": 225419066
    },
    {
      "context_text": "The MCTKG system [101] applies regular expressions to obtain semi-structured data from major tourism websites including the Beijing tourism website, Baidu Encyclopedia, and so on.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions the use of specific websites to obtain semi-structured data for constructing a knowledge graph, but does not name a specific, verifiable dataset.",
      "processing_time": 58.29760408401489,
      "citing_paper_id": "269075665",
      "cited_paper_id": 225419066
    },
    {
      "context_text": "To clarify this point, let us consider a key problem in KGs known as Entity Alignment ( EA ) [22, 90, 112].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem in knowledge graphs called Entity Alignment.",
      "processing_time": 55.8954439163208,
      "citing_paper_id": "269075665",
      "cited_paper_id": 234950071
    },
    {
      "context_text": "Additionally, pre-training models can facilitate knowledge graph completion [54, 55, 109], as well as generate text solely based on the information contained within the knowledge graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of pre-training models for knowledge graph completion.",
      "processing_time": 56.985934257507324,
      "citing_paper_id": "269075665",
      "cited_paper_id": 235421836
    },
    {
      "context_text": "As a further example, MKRLN [91] proposes a framework for explaining recommendations over the MKG, effectively reasoning the logical paths of visuals and knowledge for each user leading to the recommended items using reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a framework (MKRLN) for explainable recommendation using multi-modal knowledge graphs, but does not explicitly mention a dataset.",
      "processing_time": 57.3552131652832,
      "citing_paper_id": "269075665",
      "cited_paper_id": 236293464
    },
    {
      "context_text": "The Knowledge perceived multi-modal pre-training Method in E-commerce model ( K3M ) [121] also uses a self-setting module to accomplish the fusion challenge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called K3M. The context focuses on the method's approach to multi-modal pre-training and fusion challenges.",
      "processing_time": 58.28305649757385,
      "citing_paper_id": "269075665",
      "cited_paper_id": 237386166
    },
    {
      "context_text": "For instance, K3M [121] uses WordPieces based on BERT to convert text into token sequences, which are then Assistance of challenges.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BERT) and a model (K3M). There are no clear identifiers for datasets.",
      "processing_time": 57.768935680389404,
      "citing_paper_id": "269075665",
      "cited_paper_id": 237386166
    },
    {
      "context_text": "Specifically, pre-training models are utilized for information extraction [106], information representation [93, 106, 121], and downstream task training [74, 121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pre-training models for various tasks. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 58.09574103355408,
      "citing_paper_id": "269075665",
      "cited_paper_id": 237386166
    },
    {
      "context_text": "Specifically, pre-training models are utilized for information extraction [106], information representation [93, 106, 121], and downstream task training [74, 121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pre-training models for various tasks. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 58.09574103355408,
      "citing_paper_id": "269075665",
      "cited_paper_id": 237513894
    },
    {
      "context_text": "Similarly, the K3M model [121], introduced earlier, is a pre-trained multi-modal model that leverages knowledge from different modalities to correct noise and supplement missing information, relying on fine-tuning to complete several downstream tasks such as multi-modal sentiment analysis and‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (K3M) and its capabilities. The context focuses on the model's functionality rather than a dataset.",
      "processing_time": 57.879493713378906,
      "citing_paper_id": "269075665",
      "cited_paper_id": 237386166
    },
    {
      "context_text": "Alime MKG [106] uses a two-stream Transformer structure from LightDOT to fuse texts and images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Alime MKG, which is a multi-modal knowledge graph, but does not specify a dataset. It describes a method or system rather than a reusable dataset.",
      "processing_time": 58.05176401138306,
      "citing_paper_id": "269075665",
      "cited_paper_id": 237513894
    },
    {
      "context_text": "The DCC model consists of three modules, namely Text2Graph, Image2Graph, and Code2Graph, each responsible for constructing different parts of the knowledge graph [110].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of a model. The context is about the structure of the DCC model and its modules.",
      "processing_time": 57.475932598114014,
      "citing_paper_id": "269075665",
      "cited_paper_id": 239771829
    },
    {
      "context_text": "(2) Multi-modal Knowledge Graphs support Visual Query Answering [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application of multi-modal knowledge graphs.",
      "processing_time": 56.42473912239075,
      "citing_paper_id": "269075665",
      "cited_paper_id": null
    },
    {
      "context_text": "During training, we compare the individual sub-entities in the two generated sub-graphs above, using GloVe embeddings [34].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions GloVe embeddings but does not refer to a specific dataset. GloVe is a method for generating word embeddings, not a dataset.",
      "processing_time": 57.37885808944702,
      "citing_paper_id": "271915694",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "The original non-multi-modal dataset was proposed to counter the problem of inverse relation test leakage [35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide a specific dataset name, only a reference to a 'non-multi-modal dataset'. This is too generic and lacks a clear identifier.",
      "processing_time": 57.18953585624695,
      "citing_paper_id": "271915694",
      "cited_paper_id": 2127100
    },
    {
      "context_text": "The sub-module is based on the transformer [29] (Figure 6).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer).",
      "processing_time": 56.04249310493469,
      "citing_paper_id": "271915694",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "GQE [15] proposed the initial query engine.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called GQE. The context does not provide information about the usage of datasets.",
      "processing_time": 57.43626260757446,
      "citing_paper_id": "271915694",
      "cited_paper_id": 52171640
    },
    {
      "context_text": "‚ñ† Freebase-15k Multi-Modal (FB15k) [31] is a multi-modal extension of the original FB15k (NMM) dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase-15k Multi-Modal (FB15k)"
      ],
      "dataset_descriptions": {
        "Freebase-15k Multi-Modal (FB15k)": "Used to extend the original FB15-III dataset for multi-modal reasoning, focusing on multi-modal reasoning and evaluation in multi-modal knowledge graphs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Freebase-15k Multi-Modal (FB15k)' as a multi-modal extension of the original FB15k dataset, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 62.56420660018921,
      "citing_paper_id": "271915694",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "For evaluation, we use multi-modal datasets FB15k, FB15k-(237), YAGO15k, and DB15k [31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k",
        "FB15k-(237)",
        "YAGO15k",
        "DB15k"
      ],
      "dataset_descriptions": {
        "FB15k": "Used for evaluating multi-modal knowledge graph reasoning, focusing on entity and relation prediction tasks.",
        "FB15k-(237)": "Used to evaluate multi-modal reasoning methods, focusing on entity linking and relation prediction tasks.",
        "YAGO15k": "Used for evaluating multi-modal knowledge graph reasoning, emphasizing entity and relation prediction in a large-scale setting.",
        "DB15k": "Used for evaluating multi-modal knowledge graph reasoning, particularly for cross-lingual entity alignment and relation prediction."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific multi-modal datasets used for evaluation, which are relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 64.82573461532593,
      "citing_paper_id": "271915694",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "‚ñ† YAGO Multi-Modal (YAGO15k) [31] Similar to FB15k, this is a multi-modal extension of Yet Another Great Ontology (YAGO) dataset [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YAGO Multi-Modal (YAGO15k)"
      ],
      "dataset_descriptions": {
        "YAGO Multi-Modal (YAGO15k)": "Used to extend the YAGO dataset with multi-modal information, focusing on integrating visual and textual data for multi-modal knowledge graph reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions YAGO Multi-Modal (YAGO15k) as a multi-modal extension of the YAGO dataset, which is relevant to the topic of multi-modal knowledge graph reasoning.",
      "processing_time": 61.861653327941895,
      "citing_paper_id": "271915694",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "‚ñ† DBPedia Multi-Modal (DB15k) [31] DB15k is an extension of the original DBpedia dataset [37], with images as entities included in it.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DB15k"
      ],
      "dataset_descriptions": {
        "DB15k": "Used to extend the original DBpedia dataset with images, enhancing multi-modal knowledge graph reasoning by incorporating visual information into entity representations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions DB15k as an extension of DBpedia with images included, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 60.251858711242676,
      "citing_paper_id": "271915694",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "It consists, on average, 55.8 scaled images for each entity present in the KG. ‚ñ† Freebase-15k (237) Multi-Modal (FB15k-(237)) [31] is the FB15k dataset with 237 relations in it.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase-15k (237) Multi-Modal (FB15k-(237))"
      ],
      "dataset_descriptions": {
        "Freebase-15k (237) Multi-Modal (20)": "Used to train and evaluate multi-modal reasoning models, specifically focusing on 237 relations in the knowledge graph."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Freebase-15k (237) Multi-Modal (FB15k-(237))' which is a specific dataset used in multi-modal knowledge graph research.",
      "processing_time": 62.53254413604736,
      "citing_paper_id": "271915694",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "To make them independent, we use HRNet-W48 [33] for query generation and HRNet-W32 for training.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HRNet-W48 and HRNet-W32, which are models, not datasets. There are no specific datasets mentioned in the context.",
      "processing_time": 57.327802896499634,
      "citing_paper_id": "271915694",
      "cited_paper_id": 201124533
    },
    {
      "context_text": "TransAE [21] adds multi-modal features and extends the conventional single-hop embedding model TransE [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on extending a model with multi-modal features.",
      "processing_time": 56.727946519851685,
      "citing_paper_id": "271915694",
      "cited_paper_id": 203605587
    },
    {
      "context_text": "They are used in applications such as question-answering and recommendation systems in diverse fields like biomedicine, physics, and geoscience [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of knowledge graphs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 56.8190279006958,
      "citing_paper_id": "271915694",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "Since RConE is the first multi-modal logical query-answering algorithm, we choose non-multi-modal models Be-taE [7] and ConE [6] 3in ) for both answer types (Figure 7).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Be-taE and ConE) used for comparison. The context focuses on the algorithmic aspects rather than data.",
      "processing_time": 57.47572660446167,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "We are not including universal quantification ( ‚àÄ ), similar to [6], [7], as its applications in real-world KGs are rare.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological choice regarding logical reasoning in knowledge graphs.",
      "processing_time": 56.22674012184143,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "We extend a logical query generation algorithm [7] to incorporate the multi-modal entities/sub-entities while generating the training queries.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating training queries. The context is focused on extending an algorithm, not using a particular dataset.",
      "processing_time": 57.12115740776062,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "Geometric [4], [6], and probabilistic [7] methods embed the queries as geometrical shapes and probabilistic distributions, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for embedding queries in knowledge graphs.",
      "processing_time": 55.86041879653931,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "We adopt the query generation method described in [7] and incorporate Type I answers-based queries for all the 14 query structures (Figure 7) as well.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for query generation in the context of multi-hop logical reasoning in knowledge graphs.",
      "processing_time": 56.5953586101532,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "Handling the union operator at the end makes the model scalable [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is too vague to infer any dataset usage.",
      "processing_time": 56.59183955192566,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "BetaE [7] and ConE [6] extended Query2Box to integrate the negation ( ¬¨ ) operator, making them fully compatible with FOL operators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on extending Query2Box with logical operators.",
      "processing_time": 56.67998123168945,
      "citing_paper_id": "271915694",
      "cited_paper_id": 225040315
    },
    {
      "context_text": "We choose FCSGG [27] as a scene graph generation algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions FCSGG as a scene graph generation algorithm, which is a method, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 56.93737006187439,
      "citing_paper_id": "271915694",
      "cited_paper_id": 232417534
    },
    {
      "context_text": "The module generates a scene graph for each image using FCSGG [27].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions FCSGG, which is a method for generating scene graphs, not a dataset. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 56.67521119117737,
      "citing_paper_id": "271915694",
      "cited_paper_id": 232417534
    },
    {
      "context_text": "The outside distance between RConE and entity would be The loss functions for the scene graph generation and embedding are used as is [27], [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only loss functions for scene graph generation and embedding. No verifiable resources are identified.",
      "processing_time": 56.12341928482056,
      "citing_paper_id": "271915694",
      "cited_paper_id": 232417534
    },
    {
      "context_text": "We use FCSGG [27] as our scene graph generation algorithm.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions FCSGG as a scene graph generation algorithm, which is a method, not a dataset. No datasets are mentioned.",
      "processing_time": 56.482093811035156,
      "citing_paper_id": "271915694",
      "cited_paper_id": 232417534
    },
    {
      "context_text": "For the actual projection operator, the rigid region slightly drops in the score if we compare it with the baseline, and the score of the fuzzy part lies in the same range (better in this case).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses performance metrics of different methods.",
      "processing_time": 55.649895906448364,
      "citing_paper_id": "271915694",
      "cited_paper_id": 235422273
    },
    {
      "context_text": "A detailed survey on different query-answering methods is provided in [19].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a survey of methods. The context is too generic to infer any specific dataset usage.",
      "processing_time": 56.20696687698364,
      "citing_paper_id": "271915694",
      "cited_paper_id": 235422273
    },
    {
      "context_text": "A detailed survey on different query-answering methods is provided in [19].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a survey of methods. The context is too generic to infer any specific dataset usage.",
      "processing_time": 56.20696687698364,
      "citing_paper_id": "271915694",
      "cited_paper_id": 248965170
    },
    {
      "context_text": "A detailed survey on different query-answering methods is provided in [19].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a survey of methods. The context is too generic to infer any specific dataset usage.",
      "processing_time": 56.20696687698364,
      "citing_paper_id": "271915694",
      "cited_paper_id": 257220329
    },
    {
      "context_text": "Multi-hop logical query answering over KGs has gained attention recently [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general area of research.",
      "processing_time": 55.389002323150635,
      "citing_paper_id": "271915694",
      "cited_paper_id": 235845598
    },
    {
      "context_text": "[RQ1] Multi-Modal Knowledge Graphs (MMKGs) are KGs with multiple modalities, such as images, texts, and videos, as entities [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a definition of MMKGs. No verifiable resources are identified.",
      "processing_time": 55.97525095939636,
      "citing_paper_id": "271915694",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "A review of the application of MMKGs in different fields is presented in [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of applications. No verifiable resources are identified.",
      "processing_time": 55.87629413604736,
      "citing_paper_id": "271915694",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "We select an arbitrary relation, r i , from the relation set in the FB15k dataset for the projection.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FB15k"
      ],
      "dataset_descriptions": {
        "FB15k": "Used to train and evaluate models on relational reasoning tasks, specifically focusing on the projection of an arbitrary relation from the relation set."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the FB15k dataset, which is a well-known knowledge graph dataset used for training and evaluating models on relational reasoning tasks.",
      "processing_time": 59.244362354278564,
      "citing_paper_id": "271915694",
      "cited_paper_id": 248965170
    },
    {
      "context_text": "MMKGR [23] proposed the initial multi-hop path-based query answering method using reinforcement learning for a unified gate attention network to filter noise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multi-hop path-based query answering in multi-modal knowledge graphs.",
      "processing_time": 56.13918161392212,
      "citing_paper_id": "271915694",
      "cited_paper_id": 252089825
    },
    {
      "context_text": "Several methods have been proposed to embed the KG and queries in the same M. Kharbanda, R. Shah, and R. Mutharaju are with IIIT-Delhi, India. space to tackle these issues [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for embedding knowledge graphs and queries. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.5685019493103,
      "citing_paper_id": "271915694",
      "cited_paper_id": 257767382
    },
    {
      "context_text": "Information about the current state of the art and the prospective future of logical query answering over KGs is contemplated in [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about the state of the art and future prospects.",
      "processing_time": 55.353991985321045,
      "citing_paper_id": "271915694",
      "cited_paper_id": 257767382
    },
    {
      "context_text": "Logical query answering [5] embeds the KG and the query in the same space to evaluate the answer set.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for embedding knowledge graphs and queries in the same space.",
      "processing_time": 55.72045969963074,
      "citing_paper_id": "271915694",
      "cited_paper_id": 257767382
    },
    {
      "context_text": "Compared with the models that use CNN [10] and Transformer [11] to process entity description information, the proposed entity knowledge representation learning model in this paper has achieved improved results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.9094672203064,
      "citing_paper_id": "273225323",
      "cited_paper_id": 258733774
    },
    {
      "context_text": "Div400 2 is constructed by MediaEval Workshop (Ionescu et al. 2014), whose queries about tourist locations have an average length of 3 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Div400"
      ],
      "dataset_descriptions": {
        "Div400": "Used to evaluate social image retrieval result diversification, focusing on tourist location queries with an average length of 3."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Div400' as a dataset constructed by the MediaEval Workshop, which is a specific, verifiable resource.",
      "processing_time": 58.89697194099426,
      "citing_paper_id": "259695422",
      "cited_paper_id": 7668308
    },
    {
      "context_text": "Multi-semantic representation methods learn multiple features for short keywords, i.e. one-to-many sub-semantics (Zhao et al. 2017; Song and Soleymani 2019; Zeng et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.162872314453125,
      "citing_paper_id": "259695422",
      "cited_paper_id": 10832728
    },
    {
      "context_text": "Multi-semantic representation methods learn multiple features for short keywords, i.e. one-to-many sub-semantics (Zhao et al. 2017; Song and Soleymani 2019; Zeng et al. 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.162872314453125,
      "citing_paper_id": "259695422",
      "cited_paper_id": 246926460
    },
    {
      "context_text": "DMIH and PVSE are the multiple instance learning methods that capture multi-semantics, while FCA-Net and VMIG are graph-based retrieval networks to match sub-semantics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.07614469528198,
      "citing_paper_id": "259695422",
      "cited_paper_id": 10832728
    },
    {
      "context_text": "‚Ä¶of our proposed MKHG, we compare it with the following methods: 1) multi-stage retrieval algorithms: DTF (Bo and Gao 2019), DESA (Qin, Dou, and Wen 2020) multi-semantic approaches: DMIH (Zhao et al. 2017), PVSE (Song and Soleymani 2019), FCA-Net (Han et al. 2021) and VMIG (Zeng et al. 2022b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.87739419937134,
      "citing_paper_id": "259695422",
      "cited_paper_id": 10832728
    },
    {
      "context_text": "To verify the effectiveness of our proposed MKHG, we compare it with the following methods: 1) multi-stage retrieval algorithms: DTF (Bo and Gao 2019), DESA (Qin, Dou, and Wen 2020) multi-semantic approaches: DMIH (Zhao et al. 2017), PVSE (Song and Soleymani 2019), FCA-Net (Han et al. 2021) and VMIG (Zeng et al. 2022b).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models used for comparison. There are no clear identifiers for datasets within the text.",
      "processing_time": 56.13650870323181,
      "citing_paper_id": "259695422",
      "cited_paper_id": 10832728
    },
    {
      "context_text": "To verify the effectiveness of our proposed MKHG, we compare it with the following methods: 1) multi-stage retrieval algorithms: DTF (Bo and Gao 2019), DESA (Qin, Dou, and Wen 2020) multi-semantic approaches: DMIH (Zhao et al. 2017), PVSE (Song and Soleymani 2019), FCA-Net (Han et al. 2021) and VMIG (Zeng et al. 2022b).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models used for comparison. There are no clear identifiers for datasets within the text.",
      "processing_time": 56.13650870323181,
      "citing_paper_id": "259695422",
      "cited_paper_id": 221492015
    },
    {
      "context_text": "In fact, keyword-based queries are usually short and semantically incoherent (e.g. 2.7 words on average in the Div150AdHoc dataset), which imposes higher diversity requirements than general retrieval (queries with more than 20 words) (Wang et al. 2017; Cao et al. 2020; Zeng 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Div150AdHoc"
      ],
      "dataset_descriptions": {
        "Div150AdHoc": "Used to demonstrate the short and semantically incoherent nature of topic-specific datasets, focusing on the impact of stereotypical language in engineering and computer science positions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Div150AdHoc dataset' as a specific dataset used to illustrate the characteristics of keyword-based queries. The dataset is used to highlight the short and semantically incoherent nature of such queries.",
      "processing_time": 60.4314911365509,
      "citing_paper_id": "259695422",
      "cited_paper_id": 23672393
    },
    {
      "context_text": "In fact, keyword-based queries are usually short and semantically incoherent (e.g. 2.7 words on average in the Div150AdHoc dataset), which imposes higher diversity requirements than general retrieval (queries with more than 20 words) (Wang et al. 2017; Cao et al. 2020; Zeng 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Div150AdHoc"
      ],
      "dataset_descriptions": {
        "Div150AdHoc": "Used to demonstrate the short and semantically incoherent nature of topic-specific datasets, focusing on the impact of stereotypical language in engineering and computer science positions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Div150AdHoc dataset' as a specific dataset used to illustrate the characteristics of keyword-based queries. The dataset is used to highlight the short and semantically incoherent nature of such queries.",
      "processing_time": 60.4314911365509,
      "citing_paper_id": "259695422",
      "cited_paper_id": 222278304
    },
    {
      "context_text": "Meanwhile, the average diverse rank (ADR) and cluster recall (CR) are used for diversity measurement, while S-qt is the inference time per query (Bo and Gao 2019; Renders and Csurka 2017; Peng et al. 2017).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and methods. The context is focused on evaluation metrics and inference time.",
      "processing_time": 55.60546040534973,
      "citing_paper_id": "259695422",
      "cited_paper_id": 30459525
    },
    {
      "context_text": "Multi-stage ranking based methods follow ‚Äùfirst filter then rank‚Äù, where manual strategies are designed in the ranking stage to adjust the order of retrieved images (Seddati et al. 2017; Peng et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on multi-stage ranking methods and manual strategies in the ranking stage.",
      "processing_time": 55.95334458351135,
      "citing_paper_id": "259695422",
      "cited_paper_id": 30459525
    },
    {
      "context_text": "Keyword-based image retrieval has been a classic task in the multimedia field, because in real-world applications (Ionescu et al. 2021), using keywords as queries is the most convenient and effective retrieval manner for users.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application of keyword-based image retrieval. No verifiable resources are identified.",
      "processing_time": 55.688742876052856,
      "citing_paper_id": "259695422",
      "cited_paper_id": 218806820
    },
    {
      "context_text": "Compared with DESA that uses a self-attention mechanism to capture implicit sub-semantics, both DTF with extending title and GRAPH4DIV for modeling intent achieve better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing performance of different approaches.",
      "processing_time": 55.42294645309448,
      "citing_paper_id": "259695422",
      "cited_paper_id": 221492015
    },
    {
      "context_text": "Among them, the DTF, DESA and GRAPH4DIV combine auxiliary information (e.g. title or subtopic) to perform filtering and ranking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (DTF, DESA, GRAPH4DIV) but does not specify any datasets. The context focuses on methodologies rather than datasets.",
      "processing_time": 55.94480490684509,
      "citing_paper_id": "259695422",
      "cited_paper_id": 221492015
    },
    {
      "context_text": "1) The multi-stage diverse retrieval methods DTF, DESA and GRAPH4DIV rely on auxiliary information to perform well in accuracy and diversity, but they are slow.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance characteristics.",
      "processing_time": 54.62423610687256,
      "citing_paper_id": "259695422",
      "cited_paper_id": 221492015
    },
    {
      "context_text": "‚Ä¶the effectiveness of our proposed MKHG, we compare it with the following methods: 1) multi-stage retrieval algorithms: DTF (Bo and Gao 2019), DESA (Qin, Dou, and Wen 2020) multi-semantic approaches: DMIH (Zhao et al. 2017), PVSE (Song and Soleymani 2019), FCA-Net (Han et al. 2021) and VMIG (Zeng‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 55.35967040061951,
      "citing_paper_id": "259695422",
      "cited_paper_id": 221492015
    },
    {
      "context_text": "Wu and Ngo (2020) design an inactive words loss to expand the semantic concepts, and Su et al. (2021) provide a dynamic intent graph to balance content and intent.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing contributions from other works, which are not directly related to datasets.",
      "processing_time": 55.94466781616211,
      "citing_paper_id": "259695422",
      "cited_paper_id": 222278652
    },
    {
      "context_text": "Wu and Ngo (2020) design an inactive words loss to expand the semantic concepts, and Su et al. (2021) provide a dynamic intent graph to balance content and intent.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing contributions from other works, which are not directly related to datasets.",
      "processing_time": 55.94466781616211,
      "citing_paper_id": "259695422",
      "cited_paper_id": 235792531
    },
    {
      "context_text": "Multi-modal knowledge graph (MMKG) is an extension of plain text knowledge graph, which connects texts and images to build a more general knowledge system (Zhu et al. 2022; Zeng et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general description of multi-modal knowledge graphs. No clear, verifiable resources are identified.",
      "processing_time": 55.61271262168884,
      "citing_paper_id": "259695422",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "We argue that multi-modal knowledge graph (MMKG), which consists of rich multi-modal entities and relations, and is capable of connecting visual and textual (Zhu et al. 2022), is a better knowledge source for the diverse image retrieval task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of multi-modal knowledge graphs. No clear, verifiable datasets are referenced.",
      "processing_time": 55.52177357673645,
      "citing_paper_id": "259695422",
      "cited_paper_id": 246823061
    },
    {
      "context_text": "Three challenging benchmark datasets [6, 11, 12] are used for model evaluation: UAV-Human.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UAV-Human"
      ],
      "dataset_descriptions": {
        "UAV-Human": "Used for evaluating models in human behavior understanding with unmanned aerial vehicles, focusing on large-scale data collection and diverse scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'UAV-Human' as a benchmark dataset used for model evaluation. The cited paper title confirms it is a dataset for human behavior understanding using UAVs.",
      "processing_time": 58.74005746841431,
      "citing_paper_id": "251846584",
      "cited_paper_id": 15928602
    },
    {
      "context_text": "Three challenging benchmark datasets [6, 11, 12] are used for model evaluation: UAV-Human.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UAV-Human"
      ],
      "dataset_descriptions": {
        "UAV-Human": "Used for evaluating models in human behavior understanding with unmanned aerial vehicles, focusing on large-scale data collection and diverse scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'UAV-Human' as a benchmark dataset used for model evaluation. The cited paper title confirms it is a dataset for human behavior understanding using UAVs.",
      "processing_time": 58.74005746841431,
      "citing_paper_id": "251846584",
      "cited_paper_id": 233004700
    },
    {
      "context_text": "NTU-RGB+D 60 [11] is the most widely used dataset for skeleton-based action recognition.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NTU-RGB+D 60"
      ],
      "dataset_descriptions": {
        "NTU-RGB+D 60": "Used for skeleton-based action recognition, providing a large-scale dataset for 3D human activity data, supporting multi-modal reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'NTU-RGB+D 60' as a dataset used for skeleton-based action recognition, which is relevant to multi-modal knowledge graph reasoning involving human activity data.",
      "processing_time": 59.165122270584106,
      "citing_paper_id": "251846584",
      "cited_paper_id": 15928602
    },
    {
      "context_text": "2s-AGCN [8] 82.9 84.9 CVPR‚Äô19 SGN [1] 79.2 81.5 CVPR‚Äô20 Shift-GCN [4] 85.9 87.6 CVPR‚Äô20 DC-GCN+ADG [15] 86.5 88.1 ECCV‚Äô20 MS-G3D [16] 86.9 88.4 CVPR‚Äô20 Dynamic GCN [17] 87.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. The cited papers also do not provide additional information about datasets.",
      "processing_time": 55.40261387825012,
      "citing_paper_id": "251846584",
      "cited_paper_id": 195440283
    },
    {
      "context_text": "2s-AGCN [8] 82.9 84.9 CVPR‚Äô19 SGN [1] 79.2 81.5 CVPR‚Äô20 Shift-GCN [4] 85.9 87.6 CVPR‚Äô20 DC-GCN+ADG [15] 86.5 88.1 ECCV‚Äô20 MS-G3D [16] 86.9 88.4 CVPR‚Äô20 Dynamic GCN [17] 87.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. The cited papers also do not provide additional information about datasets.",
      "processing_time": 55.40261387825012,
      "citing_paper_id": "251846584",
      "cited_paper_id": 214728271
    },
    {
      "context_text": "2s-AGCN [8] 82.9 84.9 CVPR‚Äô19 SGN [1] 79.2 81.5 CVPR‚Äô20 Shift-GCN [4] 85.9 87.6 CVPR‚Äô20 DC-GCN+ADG [15] 86.5 88.1 ECCV‚Äô20 MS-G3D [16] 86.9 88.4 CVPR‚Äô20 Dynamic GCN [17] 87.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance metrics. The cited papers also do not provide additional information about datasets.",
      "processing_time": 55.40261387825012,
      "citing_paper_id": "251846584",
      "cited_paper_id": 219964813
    },
    {
      "context_text": "Following this, the 2s-AGCN [8] is proposed to model the multi-modal relationship between body joint and body bone by a two-stream ensemble, which leads to a large computational burden.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (2s-AGCN) and its application context. The focus is on the computational burden of the method rather than the use of a dataset.",
      "processing_time": 56.30638313293457,
      "citing_paper_id": "251846584",
      "cited_paper_id": 195440283
    },
    {
      "context_text": "Venue ST-GCN [7] 30.25 AAAI‚Äô18 DGNN [13] 29.90 CVPR‚Äô19 2s-AGCN [8] 34.84 CVPR‚Äô19 HARD-Net [14] 36.97 ECCV‚Äô20 Shift-GCN [4] 37.98 CVPR‚Äô20 CTR-GCN [3] 43.42 ICCV‚Äô21 MKE-GCN (Ours) 44.60 - multi-modal GCN.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and their performance metrics. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.644553899765015,
      "citing_paper_id": "251846584",
      "cited_paper_id": 195440283
    },
    {
      "context_text": "Venue ST-GCN [7] 30.25 AAAI‚Äô18 DGNN [13] 29.90 CVPR‚Äô19 2s-AGCN [8] 34.84 CVPR‚Äô19 HARD-Net [14] 36.97 ECCV‚Äô20 Shift-GCN [4] 37.98 CVPR‚Äô20 CTR-GCN [3] 43.42 ICCV‚Äô21 MKE-GCN (Ours) 44.60 - multi-modal GCN.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various methods and their performance metrics. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.644553899765015,
      "citing_paper_id": "251846584",
      "cited_paper_id": 219964813
    },
    {
      "context_text": "ST-GCN [7] 81.5 88.3 AAAI‚Äô18 DGNN [13] 89.9 96.1 CVPR‚Äô19 DC-GCN+ADG [15] 90.8 96.6 ECCV‚Äô20 Shift-GCN [4] 90.7 96.5 CVPR‚Äô20 MS-G3D [16] 91.5 96.2 CVPR‚Äô20 Dynamic GCN [17] 91.5 96.0 ACMMM‚Äô20 SRCA [5] 89.4 94.7 ICME‚Äô21 GCHN [2] 90.6 96.4 ICME‚Äô21 CTR-GCN [3] 92.4 96.8 ICCV‚Äô21 MKE-GCN (Ours) 92.5 96.9 -‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various graph convolutional network models and their performance metrics. No verifiable datasets are referenced.",
      "processing_time": 55.56010293960571,
      "citing_paper_id": "251846584",
      "cited_paper_id": 214728271
    },
    {
      "context_text": "ST-GCN [7] 81.5 88.3 AAAI‚Äô18 DGNN [13] 89.9 96.1 CVPR‚Äô19 DC-GCN+ADG [15] 90.8 96.6 ECCV‚Äô20 Shift-GCN [4] 90.7 96.5 CVPR‚Äô20 MS-G3D [16] 91.5 96.2 CVPR‚Äô20 Dynamic GCN [17] 91.5 96.0 ACMMM‚Äô20 SRCA [5] 89.4 94.7 ICME‚Äô21 GCHN [2] 90.6 96.4 ICME‚Äô21 CTR-GCN [3] 92.4 96.8 ICCV‚Äô21 MKE-GCN (Ours) 92.5 96.9 - benchmarks, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various models and their performance metrics. No verifiable resources are identified.",
      "processing_time": 55.001697063446045,
      "citing_paper_id": "251846584",
      "cited_paper_id": 219964813
    },
    {
      "context_text": "For example, Shift-GCN [4] is proposed to achieve a comparable performance but remain light-weight.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Shift-GCN). The context is about the performance and lightweight nature of the method.",
      "processing_time": 55.26044297218323,
      "citing_paper_id": "251846584",
      "cited_paper_id": 219964813
    },
    {
      "context_text": "Based on this, several GCN-based variants [1, 2, 3, 4, 5] have been developed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to GCN-based variants. No verifiable resources are identified.",
      "processing_time": 54.83970642089844,
      "citing_paper_id": "251846584",
      "cited_paper_id": 219964813
    },
    {
      "context_text": "Based on this, several GCN-based variants [1, 2, 3, 4, 5] have been developed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to GCN-based variants. No verifiable resources are identified.",
      "processing_time": 54.83970642089844,
      "citing_paper_id": "251846584",
      "cited_paper_id": 236234758
    },
    {
      "context_text": "(3) The extensive experiments on three challenging benchmarks, i.e., UAV-Human, NTU-RGB+D 60 and NTU-RGB+D 120, demonstrate that our approach sets a new record for skeleton-based action recognition.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NTU-RGB+D 60",
        "NTU-RGB+D 120"
      ],
      "dataset_descriptions": {
        "NTU RGB+ Depth": "The dataset is used to evaluate the performance of the proposed method in skeleton-based action recognition, focusing on the accuracy and robustness of the model across various actions and environments."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions three benchmarks, two of which are clearly named datasets (NTU-RGB+D 60 and NTU-RGB+D 120). 'UAV-Human' is also mentioned but is excluded as it is a benchmark, not a dataset.",
      "processing_time": 60.26615905761719,
      "citing_paper_id": "251846584",
      "cited_paper_id": 233004700
    },
    {
      "context_text": "UAV-Human [6] is a large benchmark for human behavior understanding with unmanned aerial vehicles, including 22,476√ó3 multi-modal video sequences captured from three sensors.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UAV-Human"
      ],
      "dataset_descriptions": {
        "UAV-Human": "Used for human behavior understanding with unmanned aerial vehicles, specifically analyzing multi-modal video sequences captured from three sensors."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific benchmark dataset, UAV-Human, which is used for human behavior understanding with unmanned aerial vehicles. The dataset is described as large and multi-modal, containing video sequences from three sensors.",
      "processing_time": 58.034000635147095,
      "citing_paper_id": "251846584",
      "cited_paper_id": 233004700
    },
    {
      "context_text": "For example, when directly applying typical GCN-based approaches to unmanned aerial vehicles (UAVs) based action recognition, recent work [6] shows that the performance drops significantly due to severe body changes or highly similar motions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark which is excluded as per instructions. The context focuses on the limitations of GCN-based approaches in UAV action recognition.",
      "processing_time": 55.487192153930664,
      "citing_paper_id": "251846584",
      "cited_paper_id": 233004700
    },
    {
      "context_text": "For data pre-processing, we follow official settings defined in UAV-Human [6] and NTU-RGB+D 120 [12], respectively.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 12 column 1 (char 651), response: ```json\n{\n    \"reasoning\": \"The citation mentions two datasets, UAV-Human and NTU-RGB+D 120, which a",
      "processing_time": 67.19315028190613,
      "citing_paper_id": "251846584",
      "cited_paper_id": 233004700
    },
    {
      "context_text": "Comparison results on UAV-Human, NTU-RGB+D 60 and NTU-RGB+D 120 are reported in Tab.2, Tab.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UAV-Human",
        "NTU-RGB+D 60",
        "NTU-RGB+D 120"
      ],
      "dataset_descriptions": {
        "UAV-Human": "Used to evaluate human behavior understanding with unmanned aerial vehicles, focusing on multi-modal reasoning and multi-modal learning.",
        "CIFAR-10": "Not mentioned in the context, excluded.",
        "NTU-RGB+D 60": "Used to assess multi-modal action recognition, focusing on RGB and depth data for 60 actions.",
        "NTU-RGB+D 120": "Used to assess multi-modal action recognition, focusing on RGB and depth data for 120 actions."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions specific datasets used for comparison results, which are relevant to multi-modal learning and human behavior understanding.",
      "processing_time": 61.750629901885986,
      "citing_paper_id": "251846584",
      "cited_paper_id": 233004700
    },
    {
      "context_text": "Based on this, recent methods [1, 2, 3] first train multiple unimodal GCNs for recognizing human actions from the different modalities of skeleton data, e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to skeleton data, which is too generic.",
      "processing_time": 54.66435718536377,
      "citing_paper_id": "251846584",
      "cited_paper_id": 236234758
    },
    {
      "context_text": "For a comprehensive performance evaluation, we compare our model with the following baselines on the MEmoR dataset: ‚Ä¢ BiLSTM+TFN .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MEmoR"
      ],
      "dataset_descriptions": {
        "MEmoR": "Used to evaluate model performance in multimodal sentiment analysis, comparing against baselines using the MEmoR dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MEmoR dataset, which is used for evaluating the performance of the model against baselines in multimodal sentiment analysis.",
      "processing_time": 57.65022897720337,
      "citing_paper_id": "256698501",
      "cited_paper_id": 950292
    },
    {
      "context_text": "A Bi-LSTM-based model, which uses Tensor Fusion Network (TFN) [67] to perform multimodal fusion on unimodal, bimodal and trimodal components of the input data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of Tensor Fusion Network (TFN) for multimodal fusion but does not specify a dataset. The context focuses on the method rather than a specific dataset.",
      "processing_time": 55.72782373428345,
      "citing_paper_id": "256698501",
      "cited_paper_id": 950292
    },
    {
      "context_text": "Compared with LSTM-based models [29, 67] and GCN-based models [16, 30], Transformer-based reasoning pipeline can better understand and exploit the semantic information and useful cues from multi-level context, and thus results in better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model types (LSTM, GCN, Transformer).",
      "processing_time": 54.57840418815613,
      "citing_paper_id": "256698501",
      "cited_paper_id": 950292
    },
    {
      "context_text": "A Bi-LSTM-based model, which uses Tensor Fusion Network (TFN) [67] to perform multi-modal fusion on unimodal, bimodal and trimodal components of the input data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (Tensor Fusion Network) for multimodal fusion but does not specify a dataset. No dataset names are present in the citation context.",
      "processing_time": 55.19895815849304,
      "citing_paper_id": "256698501",
      "cited_paper_id": 950292
    },
    {
      "context_text": "[67] further introduce tensor fusion to fuse features from variable number of modalities.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing features from multiple modalities.",
      "processing_time": 54.37935400009155,
      "citing_paper_id": "256698501",
      "cited_paper_id": 950292
    },
    {
      "context_text": "For facial component of visual features, we use FaceNet pretrained on VGGFace2 [4] to extract facial identity features and one pretrained on FER2013 [17] to extract facial emotion features, respectively, from the detected faces in each frame.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2",
        "FER2013"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to train a model for extracting facial identity features from detected faces in the context of multi-modal reasoning.",
        "ImageNet-210": "Used to train a model for extracting facial emotion features from detected faces in the context of multi-modal reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, VGGFace2 and FER2013, which are used to train models for extracting facial identity and emotion features, respectively.",
      "processing_time": 59.42979574203491,
      "citing_paper_id": "256698501",
      "cited_paper_id": 1103216
    },
    {
      "context_text": "For facial component of visual features, we use FaceNet pretrained on VGGFace2 [4] to extract facial identity features and one pretrained on FER2013 [17] to extract facial emotion features, respectively, from the detected faces in each frame.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2",
        "FER2013"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to train a model for extracting facial identity features from detected faces in the context of multi-modal reasoning.",
        "ImageNet-210": "Used to train a model for extracting facial emotion features from detected faces in the context of multi-modal reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, VGGFace2 and FER2013, which are used to train models for extracting facial identity and emotion features, respectively.",
      "processing_time": 59.42979574203491,
      "citing_paper_id": "256698501",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "In visual question answering (VQA), [23, 57, 59] introduce external commonsense knowledge to better understand question and encode implicit knowledge in context to enhance answer generation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions the use of external commonsense knowledge in VQA, which is relevant to multi-modal knowledge graph reasoning. However, no specific dataset names are provided.",
      "processing_time": 55.18983006477356,
      "citing_paper_id": "256698501",
      "cited_paper_id": 3896491
    },
    {
      "context_text": "These methods have improved the performance of many computer vision and multimedia tasks, including image captioning [38], visual question answering [42] and video recognition [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks and methods. No verifiable resources are identified.",
      "processing_time": 54.53136444091797,
      "citing_paper_id": "256698501",
      "cited_paper_id": 5083989
    },
    {
      "context_text": "For person P i in the segment S j , speciically, we use openSMILE [14] to extract audio features a i , j with Z-standardization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions openSMILE, which is a tool for extracting audio features, but it does not refer to a specific dataset. The context does not indicate the use of a dataset.",
      "processing_time": 55.57163071632385,
      "citing_paper_id": "256698501",
      "cited_paper_id": 5205529
    },
    {
      "context_text": "For training, our model is optimized using Adam [34] optimizer with a learning rate of 0.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Adam optimizer, which is a method.",
      "processing_time": 54.184640407562256,
      "citing_paper_id": "256698501",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "For training, our model is optimized using Adam [34] optimizer with a learning rate of 0.00005, beta1 0.9, and beta2 0.999.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the optimization method used for training the model.",
      "processing_time": 54.183494567871094,
      "citing_paper_id": "256698501",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "For action concepts inC (act ) j , we adopt a PyTorch implementation of ResNeXt pretrained on UCF101 dataset [55].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UCF101"
      ],
      "dataset_descriptions": {
        "UCF101": "Used to train and evaluate action recognition models, specifically focusing on human actions in videos from diverse sources."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the UCF101 dataset, which is a specific, verifiable dataset used for training and evaluating action recognition models.",
      "processing_time": 57.24357318878174,
      "citing_paper_id": "256698501",
      "cited_paper_id": 7197134
    },
    {
      "context_text": "In supervised learning, leveraging annotated data in CL [33] has drawn increasing attention from several tasks like human-object interaction detection [35], visual grounding [71], and image captioning [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and methods. No verifiable resources are identified.",
      "processing_time": 54.583972454071045,
      "citing_paper_id": "256698501",
      "cited_paper_id": 7900381
    },
    {
      "context_text": "In supervised learning, leveraging annotated data in CL [33] has drawn increasing attention from several tasks like human-object interaction detection [35], visual grounding [71], and image captioning [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and methods. No verifiable resources are identified.",
      "processing_time": 54.583972454071045,
      "citing_paper_id": "256698501",
      "cited_paper_id": 216080787
    },
    {
      "context_text": "In supervised learning, leveraging annotated data in CL [33] has drawn increasing attention from several tasks like human-object interaction detection [35], visual grounding [71], and image captioning [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and methods. No verifiable resources are identified.",
      "processing_time": 54.583972454071045,
      "citing_paper_id": "256698501",
      "cited_paper_id": 238419264
    },
    {
      "context_text": "3, existing counterfactual contrastive learning methods [11, 33, 79] focus on improving the model generalization by exposing the model to counterfactual hard samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.07623910903931,
      "citing_paper_id": "256698501",
      "cited_paper_id": 7900381
    },
    {
      "context_text": "3, existing counterfactual contrastive learning methods [11, 33, 79] focus on improving the model generalization by exposing the model to counterfactual hard samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.07623910903931,
      "citing_paper_id": "256698501",
      "cited_paper_id": 216080787
    },
    {
      "context_text": "[21] in self-supervised learning (SSL), contrastive learning (CL) has shown promising results in producing discriminative representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.744657039642334,
      "citing_paper_id": "256698501",
      "cited_paper_id": 8281592
    },
    {
      "context_text": "The visual features vi, j are obtained from three folds in frame-level: the concatenation of facial identity features and facial emotion features obtained using FaceNet [51] from faces extracted by MTCNN [69] as facial features, the occurrence number vector of objects detected by Detectron2 [64] as object features, and frame appearance features obtained by ResNet152 as environment features.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the extraction of visual features using various tools and models, but does not mention any specific datasets. The cited papers are methods or tools, not datasets.",
      "processing_time": 55.262582302093506,
      "citing_paper_id": "256698501",
      "cited_paper_id": 10585115
    },
    {
      "context_text": "The visual features vi, j are obtained from three folds in frame-level: the concatenation of facial identity features and facial emotion features obtained using FaceNet [51] from faces extracted by MTCNN [69] as facial features, the occurrence number vector of objects detected by Detectron2 [64] as object features, and frame appearance features obtained by ResNet152 as environment features.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the extraction of visual features using various tools and models, but does not mention any specific datasets. The cited papers are methods or tools, not datasets.",
      "processing_time": 55.262582302093506,
      "citing_paper_id": "256698501",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "The visual features vi, j are obtained from three folds in frame-level: the concatenation of facial identity features and facial emotion features obtained using FaceNet [51] from faces extracted by MTCNN [69] as facial features, the occurrence number vector of objects detected by Detectron2 [64] as object features, and frame appearance features obtained by ResNet152 as environment features.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the extraction of visual features using various tools and models, but does not mention any specific datasets. The cited papers are methods or tools, not datasets.",
      "processing_time": 55.262582302093506,
      "citing_paper_id": "256698501",
      "cited_paper_id": null
    },
    {
      "context_text": "The visual features v i , j are obtained from three folds in frame-level: the concatenation of facial identity features and facial emotion features obtained using FaceNet [51] from faces extracted by MTCNN [69] as facial features, the occurrence number vector of objects detected by Detectron2 [64] as object features, and frame appearance features obtained by ResNet152 as environment features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the extraction of visual features using various tools and models, but does not mention any specific datasets. The cited papers are models or methods, not datasets.",
      "processing_time": 55.10248780250549,
      "citing_paper_id": "256698501",
      "cited_paper_id": 10585115
    },
    {
      "context_text": "The visual features v i , j are obtained from three folds in frame-level: the concatenation of facial identity features and facial emotion features obtained using FaceNet [51] from faces extracted by MTCNN [69] as facial features, the occurrence number vector of objects detected by Detectron2 [64] as object features, and frame appearance features obtained by ResNet152 as environment features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the extraction of visual features using various tools and models, but does not mention any specific datasets. The cited papers are models or methods, not datasets.",
      "processing_time": 55.10248780250549,
      "citing_paper_id": "256698501",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "The visual features v i , j are obtained from three folds in frame-level: the concatenation of facial identity features and facial emotion features obtained using FaceNet [51] from faces extracted by MTCNN [69] as facial features, the occurrence number vector of objects detected by Detectron2 [64] as object features, and frame appearance features obtained by ResNet152 as environment features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes the extraction of visual features using various tools and models, but does not mention any specific datasets. The cited papers are models or methods, not datasets.",
      "processing_time": 55.10248780250549,
      "citing_paper_id": "256698501",
      "cited_paper_id": null
    },
    {
      "context_text": "These frameworks are further extended with contextual attention [75] and graph attention [77] to dynamically incorporate knowledge into",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or frameworks. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 54.60663676261902,
      "citing_paper_id": "256698501",
      "cited_paper_id": 51608183
    },
    {
      "context_text": "And we use pretrained BERT [12] model to encode the utterance of the segment and obtain the text features t i, j by averaging the sequence of hidden-states in the last layer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using BERT for encoding utterances, but BERT is a model, not a dataset. No datasets are explicitly mentioned or used in the described methodology.",
      "processing_time": 55.33229351043701,
      "citing_paper_id": "256698501",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Then Z-standardization is performed on these features to get the inal audio features a i , j ‚àà R 6373 . bert-large-uncased For text features, we adopt the PyTorch implementation of BERT model initialized with a pretrained bert-large-uncased variant to obtain the text features t i , j ‚àà R 1024 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for text feature extraction. BERT is a model, not a dataset, and thus is excluded.",
      "processing_time": 55.208779096603394,
      "citing_paper_id": "256698501",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "And we use pretrained BERT [12] model to encode the utterance of the segment and obtain the text features t i , j by averaging the sequence of hidden-states in the last layer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using BERT for encoding utterances, but BERT is a model, not a dataset. No datasets are explicitly mentioned.",
      "processing_time": 54.7294282913208,
      "citing_paper_id": "256698501",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "[41] propose DialogueRNN model that uses three GRUs to model the state of speaker, the context of the preceding utterances, and the emotions of the preceding utterances, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (DialogueRNN).",
      "processing_time": 53.65175700187683,
      "citing_paper_id": "256698501",
      "cited_paper_id": 53172956
    },
    {
      "context_text": "1 INTRODUCTION Recent deep learning methods have achieved great progress in perceiving human emotions in videos from visual [2, 43], audio [31, 36] or multi-modal signals [16, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to visual, audio, or multi-modal signals. No verifiable resources are identified.",
      "processing_time": 55.04344701766968,
      "citing_paper_id": "256698501",
      "cited_paper_id": 53172956
    },
    {
      "context_text": "1 INTRODUCTION Recent deep learning methods have achieved great progress in perceiving human emotions in videos from visual [2, 43], audio [31, 36] or multi-modal signals [16, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to visual, audio, or multi-modal signals. No verifiable resources are identified.",
      "processing_time": 55.04344701766968,
      "citing_paper_id": "256698501",
      "cited_paper_id": 226202653
    },
    {
      "context_text": "1 INTRODUCTION Recent deep learning methods have achieved great progress in perceiving human emotions in videos from visual [2, 43], audio [31, 36] or multi-modal signals [16, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to visual, audio, or multi-modal signals. No verifiable resources are identified.",
      "processing_time": 55.04344701766968,
      "citing_paper_id": "256698501",
      "cited_paper_id": 232153047
    },
    {
      "context_text": "As an eicient causal strategy, counterfactual thinking has been applied in many tasks to enhance training by synthesizing counterfactual input samples [8, 19, 45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for enhancing training through counterfactual thinking.",
      "processing_time": 54.29591989517212,
      "citing_paper_id": "256698501",
      "cited_paper_id": 54447939
    },
    {
      "context_text": "Also, for environment component, the ResNet152 model is pretrained on LVIS dataset [20] to obtain the frame features.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LVIS"
      ],
      "dataset_descriptions": {
        "LVIS": "Used to pretrain the ResNet152 model for obtaining frame features, focusing on the multi-modal reasoning aspect, employing the dataset to address the research question of integrating visual and environmental components."
      },
      "confidence_score": 1.0,
      "reasoning": "The LVIS dataset is mentioned as the source for pretraining the ResNet152 model to obtain frame features, which is directly relevant to the multi-modal knowledge graph reasoning context.",
      "processing_time": 57.89978647232056,
      "citing_paper_id": "256698501",
      "cited_paper_id": 195441339
    },
    {
      "context_text": "[39] propose a residual LSTM framework to fuse features by sharing module weights.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (residual LSTM framework).",
      "processing_time": 54.04339075088501,
      "citing_paper_id": "256698501",
      "cited_paper_id": 204837049
    },
    {
      "context_text": "A typical contrastive learning scheme [10, 27, 49] encourages model to learn feature representations by contrasting positive sample pairs against negative sample pairs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method for contrastive learning. No dataset names are provided in the context.",
      "processing_time": 54.77313208580017,
      "citing_paper_id": "256698501",
      "cited_paper_id": 207930212
    },
    {
      "context_text": "A typical contrastive learning scheme [10, 27, 49] encourages model to learn feature representations by contrasting positive sample pairs against negative sample pairs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general method for contrastive learning. No dataset names are provided in the context.",
      "processing_time": 54.77313208580017,
      "citing_paper_id": "256698501",
      "cited_paper_id": 211096730
    },
    {
      "context_text": "An assertion is deined as a ‚ü® concept1, relation, concept2 ‚ü© triplet, which is usually associated with a conidence score in the interval of [1, 10].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general structure for assertions. No verifiable resources are identified.",
      "processing_time": 54.282039403915405,
      "citing_paper_id": "256698501",
      "cited_paper_id": 211096730
    },
    {
      "context_text": "[7] propose a gradient-based counterfactual sample synthesis (CSS) strategy to produce robust positive samples and hard negative samples for contrastive training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating samples. The context is focused on the technique rather than a particular dataset.",
      "processing_time": 54.441081523895264,
      "citing_paper_id": "256698501",
      "cited_paper_id": 212725353
    },
    {
      "context_text": "In recent years, there is an emerging research trend on incorporating external knowledge into reasoning task for improving performance [32, 40].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in research. No verifiable resources are identified.",
      "processing_time": 54.43687295913696,
      "citing_paper_id": "256698501",
      "cited_paper_id": 254097121
    },
    {
      "context_text": "The fusion of multi-modal information is also crucial for perceiving emotions in videos [62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of multi-modal information for emotion perception in videos.",
      "processing_time": 54.353687047958374,
      "citing_paper_id": "256698501",
      "cited_paper_id": 254097189
    },
    {
      "context_text": "In dialogue system, Han et al. [22], Young et al. [66] and Liu et al. [37] have proposed generative models, which can incorporate external knowledge to capture and understand key semantic entities in conversation history, and provide contextual information for subsequent generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models and the incorporation of external knowledge. No clear, verifiable datasets are identified.",
      "processing_time": 54.97921442985535,
      "citing_paper_id": "256698501",
      "cited_paper_id": 261514205
    },
    {
      "context_text": "For object component, we use the Detectron2 [64] detector pretrained on LVIS dataset to detect objects and obtain features.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LVIS"
      ],
      "dataset_descriptions": {
        "LVIS": "Used to detect objects and extract features, contributing to the object component in multi-modal knowledge graph reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the LVIS dataset for object detection and feature extraction, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 56.35308218002319,
      "citing_paper_id": "256698501",
      "cited_paper_id": null
    },
    {
      "context_text": "For every segment S j , we keep all non-stopword tokens in the utterance and collect all unique tokens as entity concept set C ( ent ) detected by Detectron2 as object concept set C ( obj ) j .",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a process using Detectron2, which is a method or tool, not a dataset.",
      "processing_time": 54.92624807357788,
      "citing_paper_id": "256698501",
      "cited_paper_id": null
    },
    {
      "context_text": "For object concepts in C ( obj ) j , we use the same Detectron2 detector used for visual feature extraction to obtain object labels.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (Detectron2 detector) used for obtaining object labels.",
      "processing_time": 54.624834060668945,
      "citing_paper_id": "256698501",
      "cited_paper_id": null
    },
    {
      "context_text": "These progressions can be meaningfully encoded into patient representations using deep learning as shown by numerous prior works (Choi et al., 2016, 2018, 2017; Bai et al., 2018; Lu et al., 2021a, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works that have used deep learning to encode patient representations. No verifiable datasets are named.",
      "processing_time": 54.8930025100708,
      "citing_paper_id": "259501302",
      "cited_paper_id": 948039
    },
    {
      "context_text": "These progressions can be meaningfully encoded into patient representations using deep learning as shown by numerous prior works (Choi et al., 2016, 2018, 2017; Bai et al., 2018; Lu et al., 2021a, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works that have used deep learning to encode patient representations. No verifiable datasets are named.",
      "processing_time": 54.8930025100708,
      "citing_paper_id": "259501302",
      "cited_paper_id": 50766964
    },
    {
      "context_text": "These progressions can be meaningfully encoded into patient representations using deep learning as shown by numerous prior works (Choi et al., 2016, 2018, 2017; Bai et al., 2018; Lu et al., 2021a, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works that have used deep learning to encode patient representations. No verifiable datasets are named.",
      "processing_time": 54.8930025100708,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "‚Ä¶depth of 0 means we have used a simple embedding layer (i.e. lookup table with trainable embeddings) instead of running a GNN over the node embeddings. curve ( AuROC ) for untresholded performance evaluation; this is in line with work by Lu et al. (2021a,b, 2022); Choi et al. (2016) and others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and metrics. The context focuses on the use of embeddings and performance evaluation metrics.",
      "processing_time": 54.53944778442383,
      "citing_paper_id": "259501302",
      "cited_paper_id": 948039
    },
    {
      "context_text": "Choi et al. (2016); Bai et al. (2018) propose EHR-specific visit sequence models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on EHR-specific visit sequence models, which are methods rather than datasets.",
      "processing_time": 54.88434362411499,
      "citing_paper_id": "259501302",
      "cited_paper_id": 948039
    },
    {
      "context_text": "Choi et al. (2016); Bai et al. (2018) propose EHR-specific visit sequence models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on EHR-specific visit sequence models, which are methods rather than datasets.",
      "processing_time": 54.88434362411499,
      "citing_paper_id": "259501302",
      "cited_paper_id": 50766964
    },
    {
      "context_text": "We aim to leverage the success of GNNs to learn node (and edge) representations (Kipf and Welling, 2017; Hamilton et al., 2017; VeliÀáckovi¬¥c et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about leveraging GNNs for learning node and edge representations.",
      "processing_time": 54.6856427192688,
      "citing_paper_id": "259501302",
      "cited_paper_id": 3144218
    },
    {
      "context_text": "We aim to leverage the success of GNNs to learn node (and edge) representations (Kipf and Welling, 2017; Hamilton et al., 2017; VeliÀáckovi¬¥c et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about leveraging GNNs for learning node and edge representations.",
      "processing_time": 54.6856427192688,
      "citing_paper_id": "259501302",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "We then compute a heterogeneous (nodes of different types) multi-layer GNN (see Schlichtkrull et al. (2018); Mao et al. (2022)) over these node and edge sets, where each edge set is associated with its own parametrized graph convolution operator.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the computation of a multi-layer GNN over node and edge sets, which is a methodological detail.",
      "processing_time": 55.09430456161499,
      "citing_paper_id": "259501302",
      "cited_paper_id": 5458500
    },
    {
      "context_text": "The hidden states at the output of the GRU are aggregated using a temporal attention mechanism where the query is a trainable embedding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological detail about the use of a GRU and attention mechanism.",
      "processing_time": 54.235323667526245,
      "citing_paper_id": "259501302",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "4.3), we feed them through a GRU (Cho et al., 2014).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GRU). The context is about the use of a GRU in the research, which is a model/architecture, not a dataset.",
      "processing_time": 55.20957899093628,
      "citing_paper_id": "259501302",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "Choi et al. (2018) propose to focus on the inherent structure of EHRs w.r.t. treatments, diagnosis, visits, and patients.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach applied to EHRs. The context does not provide enough information to identify a specific, verifiable dataset.",
      "processing_time": 54.712743282318115,
      "citing_paper_id": "259501302",
      "cited_paper_id": 53037206
    },
    {
      "context_text": "Multi-Modality Prior work has considered learning representations from either structured components of EHR data (Choi et al., 2018, 2017; Rasmy et al., 2021) or from unstructured clinical text reports (Alsentzer et al., 2019; Peng et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of data (EHR data, clinical text reports). No specific dataset names are provided.",
      "processing_time": 54.64970564842224,
      "citing_paper_id": "259501302",
      "cited_paper_id": 53037206
    },
    {
      "context_text": "Multi-Modality Prior work has considered learning representations from either structured components of EHR data (Choi et al., 2018, 2017; Rasmy et al., 2021) or from unstructured clinical text reports (Alsentzer et al., 2019; Peng et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of data (EHR data, clinical text reports). No specific dataset names are provided.",
      "processing_time": 54.64970564842224,
      "citing_paper_id": "259501302",
      "cited_paper_id": 102352093
    },
    {
      "context_text": "‚Ä¶2016), HiRID (Faltys et al., 2021), and eICU (Pollard et al., 2018) enable modeling of disease progressions within a single hospital visit, for example in Intensive Care Units (ICU) (Harutyunyan et al., 2019; Y`eche et al., 2021), or progressions across multiple patient visits (Choi et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MIMIC-III",
        "HiRID",
        "eICU"
      ],
      "dataset_descriptions": {
        "MIMIC-III": "Used to train models for multi-modal reasoning in critical care settings, specifically for training and evaluation in ICU environments.",
        "HiRID": "Utilized to model disease progressions within a single hospital visit, focusing on intensive care unit scenarios.",
        "eICU": "Applied to study disease progressions across multiple patient visits, enabling longitudinal analysis in predictive healthcare."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions three specific datasets used for modeling disease progressions in healthcare settings, which are relevant to multi-modal knowledge graph reasoning in medical contexts.",
      "processing_time": 59.11384320259094,
      "citing_paper_id": "259501302",
      "cited_paper_id": 53037206
    },
    {
      "context_text": "Meng et al. (2021); Gong and Guttag (2018); Suresh et al. (2017) have proposed multi-modal architectures and Park et al. (2022) go a step further and introduce even stronger structural priors, while considering the two modalities of structured EHR data, as well as unstructured clinical reports.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions structured EHR data and unstructured clinical reports, which are likely to be datasets used in the cited papers. However, no specific dataset names are provided in the citation span.",
      "processing_time": 54.95739984512329,
      "citing_paper_id": "259501302",
      "cited_paper_id": 54527549
    },
    {
      "context_text": "Meng et al. (2021); Gong and Guttag (2018); Suresh et al. (2017) have proposed multi-modal architectures and Park et al. (2022) go a step further and introduce even stronger structural priors, while considering the two modalities of structured EHR data, as well as unstructured clinical reports.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions structured EHR data and unstructured clinical reports, which are likely to be datasets used in the cited papers. However, no specific dataset names are provided in the citation span.",
      "processing_time": 54.95739984512329,
      "citing_paper_id": "259501302",
      "cited_paper_id": 220977071
    },
    {
      "context_text": "Meng et al. (2021); Gong and Guttag (2018); Suresh et al. (2017) have proposed multi-modal architectures and Park et al. (2022) go a step further and introduce even stronger structural priors, while considering the two modalities of structured EHR data, as well as unstructured clinical reports.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions structured EHR data and unstructured clinical reports, which are likely to be datasets used in the cited papers. However, no specific dataset names are provided in the citation span.",
      "processing_time": 54.95739984512329,
      "citing_paper_id": "259501302",
      "cited_paper_id": 227239576
    },
    {
      "context_text": "Meng et al. (2021); Gong and Guttag (2018); Suresh et al. (2017) have proposed multi-modal architectures and Park et al. (2022) go a step further and introduce even stronger structural priors, while considering the two modalities of structured EHR data, as well as unstructured clinical reports.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions structured EHR data and unstructured clinical reports, which are likely to be datasets used in the cited papers. However, no specific dataset names are provided in the citation span.",
      "processing_time": 54.95739984512329,
      "citing_paper_id": "259501302",
      "cited_paper_id": 247595191
    },
    {
      "context_text": "Secondly, we compare to Cui2Vec (Beam et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Cui2Vec' but does not indicate it is a dataset. It appears to be a method or model, and there is no specific dataset mentioned.",
      "processing_time": 54.328593015670776,
      "citing_paper_id": "259501302",
      "cited_paper_id": 59379420
    },
    {
      "context_text": "Concept Embeddings using Cui2Vec Cui2Vec as introduced by Beam et al. (2020) is a collection of pretrained medical concept embeddings mapped to the space of UMLS.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Cui2Vec, which is described as a collection of pretrained medical concept embeddings. However, it does not specify a dataset but rather a method or tool.",
      "processing_time": 54.69585299491882,
      "citing_paper_id": "259501302",
      "cited_paper_id": 59379420
    },
    {
      "context_text": "We use the Cui2Vec embeddings to initialize a lookup matrix from which concept embeddings are retrieved by index and replace our GNN-based concept embeddings.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Cui2Vec embeddings but does not indicate their use as a dataset. Instead, they are used as pre-trained embeddings to initialize a lookup matrix.",
      "processing_time": 54.3216609954834,
      "citing_paper_id": "259501302",
      "cited_paper_id": 59379420
    },
    {
      "context_text": "We identify two drawbacks of current UMLS-based approaches (Beam et al., 2020; Skreta et al., 2021; Mao and Fung, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only drawbacks of UMLS-based approaches. No verifiable resources are identified.",
      "processing_time": 54.04846787452698,
      "citing_paper_id": "259501302",
      "cited_paper_id": 59379420
    },
    {
      "context_text": "We identify two drawbacks of current UMLS-based approaches (Beam et al., 2020; Skreta et al., 2021; Mao and Fung, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only drawbacks of UMLS-based approaches. No verifiable resources are identified.",
      "processing_time": 54.04846787452698,
      "citing_paper_id": "259501302",
      "cited_paper_id": 237441141
    },
    {
      "context_text": "Cui2Vec consists of medical concept embeddings pretrained on a large-scale corpus using a Word2Vec (Mikolov et al., 2013) style objective function.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Cui2Vec, which is a method for generating medical concept embeddings, but does not refer to a specific dataset. The citation is about the method and its training process.",
      "processing_time": 54.8095338344574,
      "citing_paper_id": "259501302",
      "cited_paper_id": 59379420
    },
    {
      "context_text": "Existing work in natural language processing has established the benefits of knowledge graph representations to various downstream applications (Zhang et al., 2019; Sun et al., 2020; He et al., 2020; Wang et al., 2021); where the most recent approaches include GNNs (Yasunaga et al., 2021, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to knowledge graph representations and GNNs. No verifiable resources are identified.",
      "processing_time": 54.18196415901184,
      "citing_paper_id": "259501302",
      "cited_paper_id": 158046772
    },
    {
      "context_text": "Existing work in natural language processing has established the benefits of knowledge graph representations to various downstream applications (Zhang et al., 2019; Sun et al., 2020; He et al., 2020; Wang et al., 2021); where the most recent approaches include GNNs (Yasunaga et al., 2021, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to knowledge graph representations and GNNs. No verifiable resources are identified.",
      "processing_time": 54.18196415901184,
      "citing_paper_id": "259501302",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "Existing work in natural language processing has established the benefits of knowledge graph representations to various downstream applications (Zhang et al., 2019; Sun et al., 2020; He et al., 2020; Wang et al., 2021); where the most recent approaches include GNNs (Yasunaga et al., 2021, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to knowledge graph representations and GNNs. No verifiable resources are identified.",
      "processing_time": 54.18196415901184,
      "citing_paper_id": "259501302",
      "cited_paper_id": 227239576
    },
    {
      "context_text": "Existing work in natural language processing has established the benefits of knowledge graph representations to various downstream applications (Zhang et al., 2019; Sun et al., 2020; He et al., 2020; Wang et al., 2021); where the most recent approaches include GNNs (Yasunaga et al., 2021, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to knowledge graph representations and GNNs. No verifiable resources are identified.",
      "processing_time": 54.18196415901184,
      "citing_paper_id": "259501302",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "Other modalities of medical data exist outside of in-hospital datasets, where a vast amount of prior medical knowledge is stored in static form in databases such as the Unified Medical Language System (UMLS) (Bodenreider, 2004).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Unified Medical Language System (UMLS)"
      ],
      "dataset_descriptions": {
        "Unified Medical Language System (UMLS)": "Used to integrate biomedical terminology, providing a comprehensive knowledge base for multi-modal knowledge graph reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The UMLS is mentioned as a static database storing prior medical knowledge, which is relevant for multi-modal knowledge graph reasoning.",
      "processing_time": 56.055065870285034,
      "citing_paper_id": "259501302",
      "cited_paper_id": 205228801
    },
    {
      "context_text": "Existing work in natural language processing has established the benefits of knowledge graph representations to various downstream applications (Zhang et al., 2019; Sun et al., 2020; He et al., 2020; Wang et al., 2021); where the most recent approaches include GNNs (Yasunaga et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to knowledge graph representations and GNNs. No verifiable resources are identified.",
      "processing_time": 54.025840520858765,
      "citing_paper_id": "259501302",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "Other works learn over heterogeneous graphs with different types of nodes (Lu et al., 2021a; Mao et al., 2022; Gong et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that deal with heterogeneous graphs. No verifiable resources are identified.",
      "processing_time": 54.08772158622742,
      "citing_paper_id": "259501302",
      "cited_paper_id": 227239576
    },
    {
      "context_text": "Other works learn over heterogeneous graphs with different types of nodes (Lu et al., 2021a; Mao et al., 2022; Gong et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works that deal with heterogeneous graphs. No verifiable resources are identified.",
      "processing_time": 54.08772158622742,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "Knowledge Graphs and GNNs A vast amount of static prior medical knowledge often remains un-touched in current modeling approaches.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about static prior medical knowledge. No verifiable resources are identified.",
      "processing_time": 53.95622801780701,
      "citing_paper_id": "259501302",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "G-BERT: Pre-training of Graph Augmented Transformers for Medication Recommendation Shang et al. (2019) show performance improvements on a medication recommendation task by pretraining disease and medication code embeddings using GNNs over two ontologies.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions the use of two ontologies for pretraining disease and medication code embeddings, but does not specify the names of these ontologies. Therefore, no specific datasets can be extracted.",
      "processing_time": 54.64150929450989,
      "citing_paper_id": "259501302",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "To model structural and relational data explicitly, approaches have started to use GNNs. Shang et al. (2019) proposed to use the Graph Attention (VeliÀáckovi¬¥c et al., 2018) operator together with an architecture to pretrain embeddings over two ontologies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures. The context focuses on the use of GNNs and Graph Attention operators, which are not datasets.",
      "processing_time": 54.51367402076721,
      "citing_paper_id": "259501302",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "We compare to two alternative approaches without GNNs for learning concept embeddings by replacing the Concept Embedding 4.1 module and performing the same proposed training procedure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 54.07963228225708,
      "citing_paper_id": "259501302",
      "cited_paper_id": 233219869
    },
    {
      "context_text": "We also considerably outperform MedPath (Ye et al., 2021), which considers personalized graphs to enhance the predictive performance of backbone time-series architectures for EHR.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MedPath but does not refer to it as a dataset. It is described as a method or model for enhancing predictive performance in EHR.",
      "processing_time": 54.38677000999451,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235324796
    },
    {
      "context_text": "‚Ä¶text data (Lu et al., 2021a), hyperbolic embeddings (Lu et al., 2021b), ontology graphs (Choi et al., 2017; Shang et al., 2019; Mao et al., 2022), temporally localized graphs (Lu et al., 2022), personalized patient graphs (Ye et al., 2021), and language model knowledge graphs (Jiang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various types of graphs and knowledge graphs but does not specify any named, verifiable datasets. The context focuses on methodologies and approaches rather than specific datasets.",
      "processing_time": 54.57006001472473,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235324796
    },
    {
      "context_text": "With MedPath Ye et al. (2021) propose to enhance the performance of existing EHR representation learning architectures by incorporating a personalized graph extracted using knowledge from Semantic MEDLINE (Rindflesch et al., 2011).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Semantic MEDLINE"
      ],
      "dataset_descriptions": {
        "Semantic MEDLINE": "Used to extract personalized graphs for enhancing EHR representation learning, focusing on multi-modal reasoning and knowledge integration."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'Semantic MEDLINE' as a knowledge source used to extract a personalized graph, which is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 55.75492072105408,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235324796
    },
    {
      "context_text": "Overall we can conclude improved performance against approaches using pretraining schemes (Lu et al., 2021b), including text data (Lu et al., 2021a), hyperbolic embeddings (Lu et al., 2021b), ontology graphs (Choi et al., 2017; Shang et al., 2019; Mao et al., 2022), temporally localized graphs (Lu‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing performance against various pretraining schemes and graph types.",
      "processing_time": 54.01811361312866,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "In this work, Lu et al. (2021a) propose a collaborative graph learning approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method proposed by the authors.",
      "processing_time": 52.96989941596985,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "We can additionally consider co-occurrence information (e.g., Lu et al. (2021a); Liu et al. (2022)) to connect the two hierarchies.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only co-occurrence information which is not a verifiable resource. No dataset names are present in the context.",
      "processing_time": 54.25848412513733,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "We can additionally consider co-occurrence information (e.g., Lu et al. (2021a); Liu et al. (2022)) to connect the two hierarchies.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only co-occurrence information which is not a verifiable resource. No dataset names are present in the context.",
      "processing_time": 54.25848412513733,
      "citing_paper_id": "259501302",
      "cited_paper_id": 253098669
    },
    {
      "context_text": "Diagnosis Similar to the previous heart failure task we compare to the results of CGL , Chet , and Sherbet .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons to other methods. No verifiable resources are identified.",
      "processing_time": 53.256072998046875,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "First the tuple of the ICD and ATC hierarchy ICD/ATC (Shang et al., 2019) and second an extension thereof including co-occurrence information ICD/ATC ‚àí CO (e.g. Lu et al. (2021a)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ICD/ATC",
        "ICD/ATC ‚àí CO"
      ],
      "dataset_descriptions": {
        "MIMIC-III": "Used to train and evaluate models for temporal health event prediction, focusing on the hierarchical structure of medical codes.",
        "ICD/ATC ‚àí CO": "Extended with co-occurrence information to enhance the predictive power of models, specifically for temporal health event prediction."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions two specific resources, ICD/ATC and ICD/ATC ‚àí CO, which are used as hierarchical and co-occurrence data for health event prediction.",
      "processing_time": 57.76144003868103,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "ICD/ATC with Co-Occurrence Similar to work done by Lu et al. (2021a) or Liu et al. (2022) we can additionally consider co-occurrence information present in our dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'co-occurrence information' but does not specify a named dataset. The term 'our dataset' is too generic and lacks a specific identifier.",
      "processing_time": 54.25126814842224,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "ICD/ATC with Co-Occurrence Similar to work done by Lu et al. (2021a) or Liu et al. (2022) we can additionally consider co-occurrence information present in our dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'co-occurrence information' but does not specify a named dataset. The term 'our dataset' is too generic and lacks a specific identifier.",
      "processing_time": 54.25126814842224,
      "citing_paper_id": "259501302",
      "cited_paper_id": 253098669
    },
    {
      "context_text": "We consider thresholded weighted F1 ( w-F1 ) score, and to be comparable to Lu et al. (2021a) we consider their adapted computation of F1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (F1 score) and a method (adapted computation).",
      "processing_time": 53.885923862457275,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "‚Ä¶improved performance against approaches using pretraining schemes (Lu et al., 2021b), including text data (Lu et al., 2021a), hyperbolic embeddings (Lu et al., 2021b), ontology graphs (Choi et al., 2017; Shang et al., 2019; Mao et al., 2022), temporally localized graphs (Lu et al., 2022),‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing different pretraining schemes and graph types.",
      "processing_time": 53.884212493896484,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "Sherbet: Self-Supervised Graph Learning With Hyperbolic Embedding for Temporal Health Event Prediction With Sherbet (Lu et al., 2021b) propose to encode the structure of a disease ontology in hyperbolic space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method for encoding a disease ontology in hyperbolic space but does not specify a dataset. The focus is on the method rather than a reusable dataset.",
      "processing_time": 53.9623122215271,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "For MMUGL with w ‚Ä¢ ,m = 0 including L sum in pretraining we observe a downstream heart failure prediction performance (on the CGL Lu et al. (2021a) patient split) of 87 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CGL Lu et al. (2021a) patient split"
      ],
      "dataset_descriptions": {
        "CIFAR-10": "Used to evaluate heart failure prediction performance, focusing on downstream tasks after pretraining with hyperbolic embedding."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'CGL Lu et al. (2021a) patient split' which appears to be a specific dataset used for evaluating heart failure prediction performance.",
      "processing_time": 56.46393632888794,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "The learned embed-2 dings can then be incorporated into task-specific architectures (Rasmy et al., 2021; Shang et al., 2019; Lu et al., 2021a; Ma et al., 2018) to improve outcome predictions in different healthcare settings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and models. The context focuses on the use of learned embeddings in healthcare settings.",
      "processing_time": 53.84183096885681,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "Finally, Lu et al. (2021b, 2019) consider hyperbolic embeddings for medical ontologies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of hyperbolic embeddings for medical ontologies. Medical ontologies are not considered datasets according to the extraction rules.",
      "processing_time": 54.01356077194214,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "We perform a minor modification here w.r.t. to the architecture by Lu et al. (2021a) and introduce a hyperparameter n q , which refers to the number of trainable queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a modification to an architecture and the introduction of a hyperparameter.",
      "processing_time": 53.34342312812805,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "For all other tasks, we use an RNN based model proposed by Lu et al. (2021a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 53.652732610702515,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "Heart Failure This task has been benchmarked in CGL (Lu et al., 2021a), Chet (Lu et al., 2022), and Sherbet (Lu et al., 2021b); who have performed extensive benchmarking against prior work.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarking but does not specify any datasets. The names 'CGL', 'Chet', and 'Sherbet' are likely methods or benchmarks, not datasets.",
      "processing_time": 54.2186975479126,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "We compared to CGL by Lu et al. (2021a), Chet by Lu et al. (2022), and Sherbet by Lu et al. (2021b), as well as G-BERT (Shang et al., 2019) using the GitHub repository at https://github.com/jshang123/G-Bert .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models and a GitHub repository but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 53.879897594451904,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "We see advantages of recent multi-modal approaches in the ICU setting (Khadanga et al., 2019; Husmann et al., 2022) and visit sequence modeling (Lu et al., 2021a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to multi-modal approaches and visit sequence modeling in the ICU setting.",
      "processing_time": 53.58363485336304,
      "citing_paper_id": "259501302",
      "cited_paper_id": 235376996
    },
    {
      "context_text": "Second, prior solutions (Skreta et al., 2021; Mao and Fung, 2020) specify the usage of hierarchical relations, which implies the use of an underlying graph in the form of a tree (single vocabulary).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hierarchical relations and graphs. No verifiable resources are identified.",
      "processing_time": 53.267080783843994,
      "citing_paper_id": "259501302",
      "cited_paper_id": 237441141
    },
    {
      "context_text": "In multimodal EHR representation learning (Park et al., 2022), we benefit from two modalities: structured EHR data (e.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'structured EHR data' but does not provide a specific, named dataset. The reference is too generic and lacks a clear identifier.",
      "processing_time": 54.01978778839111,
      "citing_paper_id": "259501302",
      "cited_paper_id": 247595191
    },
    {
      "context_text": "In multi-modal EHR representation learning (Park et al., 2022), we benefit from two modalities: structured EHR data (e.g., billing codes) and unstructured text information stored in rich clinical reports.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'structured EHR data' and 'unstructured text information', which are types of data but do not specify a named dataset. No clear, verifiable dataset names are provided.",
      "processing_time": 54.06966495513916,
      "citing_paper_id": "259501302",
      "cited_paper_id": 247595191
    },
    {
      "context_text": "Using GN-NExplainer (Ying et al., 2019) we can retrieve a soft mask on the adjacency matrix (entries in [0 , 1]), which represents an edge-weighted explanation subgraph.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GN-NExplainer) for explaining graph neural networks.",
      "processing_time": 53.38142776489258,
      "citing_paper_id": "259501302",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent research [6‚Äì8] used knowledge graphs for ontology construction and achieved some success.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research using knowledge graphs for ontology construction.",
      "processing_time": 53.192097425460815,
      "citing_paper_id": "246938066",
      "cited_paper_id": 2158023
    },
    {
      "context_text": "Examples of classic general domain knowledge graphs include YAGO [11], DBpedia [12], Wikidata [13], etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YAGO",
        "DBpedia",
        "Wikidata"
      ],
      "dataset_descriptions": {
        "YAGO": "Used to explore multi-modal reasoning in knowledge graphs, focusing on entity linking and type inference across different modalities.",
        "DBpedia": "Utilized to enhance multi-modal reasoning by integrating structured data from Wikipedia, specifically for entity disambiguation and relation extraction.",
        "Wikidata": "Employed to support multi-modal reasoning tasks, particularly in linking entities and enriching knowledge graph representations with diverse data types."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions YAGO, DBpedia, and Wikidata as examples of classic general domain knowledge graphs. These are specific, verifiable resources that can be traced to their sources.",
      "processing_time": 58.14505696296692,
      "citing_paper_id": "246938066",
      "cited_paper_id": 2858079
    },
    {
      "context_text": "Common machine learning models include Hidden Markov Model (HMM) [26], Maximum Support Vector Machine (SVM) [27], Conditional Random Field (CRF) [28], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only machine learning models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.534804344177246,
      "citing_paper_id": "246938066",
      "cited_paper_id": 3632923
    },
    {
      "context_text": "The rule-based and dictionary-based approach was first applied to NER, which involves rules being manually written in order to identify entities by matching text to rules [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to NER. No verifiable resources are identified.",
      "processing_time": 53.42022705078125,
      "citing_paper_id": "246938066",
      "cited_paper_id": 8310135
    },
    {
      "context_text": "Classical encoders include Convolution Neural Networks (CNN) [29], Bidirectional Long Short-Term Memory (BiLSTM) [30], and their variants [31‚Äì34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 364), response: ```json\n{\n    \"reasoning\": \"The citation context mentions classical encoders such as CNN and BiLSTM ",
      "processing_time": 57.43788433074951,
      "citing_paper_id": "246938066",
      "cited_paper_id": 12740621
    },
    {
      "context_text": "Classical encoders include Convolution Neural Networks (CNN) [29], Bidirectional Long Short-Term Memory (BiLSTM) [30], and their variants [31‚Äì34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 364), response: ```json\n{\n    \"reasoning\": \"The citation context mentions classical encoders such as CNN and BiLSTM ",
      "processing_time": 57.43788433074951,
      "citing_paper_id": "246938066",
      "cited_paper_id": 16119010
    },
    {
      "context_text": "Classical encoders include Convolution Neural Networks (CNN) [29], Bidirectional Long Short-Term Memory (BiLSTM) [30], and their variants [31‚Äì34].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 364), response: ```json\n{\n    \"reasoning\": \"The citation context mentions classical encoders such as CNN and BiLSTM ",
      "processing_time": 57.43788433074951,
      "citing_paper_id": "246938066",
      "cited_paper_id": 102485964
    },
    {
      "context_text": "Liu et al. predicted the potential relationship between the concept and the course by mapping an online course to the general space of the concept [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or process of mapping an online course to a concept space, which is not a dataset.",
      "processing_time": 53.771156787872314,
      "citing_paper_id": "246938066",
      "cited_paper_id": 15795805
    },
    {
      "context_text": "Recently, pre-trained language models (PLMs) have made historic breakthroughs in many natural language processing tasks, such as Embeddings from Language Models (ELMO) [35], Generative Pre-training (GPT) [36], and Bidirectional Encoder Representation from Transformer (BERT) [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several pre-trained language models but does not refer to any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 53.03903365135193,
      "citing_paper_id": "246938066",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Recently, pre-trained language models (PLMs) have made historic breakthroughs in many natural language processing tasks, such as Embeddings from Language Models (ELMO) [35], Generative Pre-training (GPT) [36], and Bidirectional Encoder Representation from Transformer (BERT) [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several pre-trained language models but does not refer to any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 53.03903365135193,
      "citing_paper_id": "246938066",
      "cited_paper_id": 226096901
    },
    {
      "context_text": "Senthilkumar introduced a concept map constructed by software into teaching and learning [17].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 310), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific, verifiable datasets. It refe",
      "processing_time": 56.74851059913635,
      "citing_paper_id": "246938066",
      "cited_paper_id": 206618067
    },
    {
      "context_text": "The above research [8‚Äì10, 16 , 22 ] points out that these relationships where W 0 ‚àà R d ‚àó d , and d is the hidden state size from BERT.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a relationship involving BERT. No verifiable resources are identified.",
      "processing_time": 52.86427330970764,
      "citing_paper_id": "246938066",
      "cited_paper_id": 208176418
    },
    {
      "context_text": "Dang et al. used Wikipedia for entity extraction and constructed an MOOC knowledge graph [21] Yao et al. proposed a novel model for embedding the learning of educational knowledge graphs to promote knowledge graph construction [22].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions the use of Wikipedia for entity extraction and the construction of an MOOC knowledge graph. However, 'Wikipedia' is not a dataset but a general knowledge source, and 'MOOC knowledge graph' is too generic without a specific identifier.",
      "processing_time": 54.33299446105957,
      "citing_paper_id": "246938066",
      "cited_paper_id": 208176418
    },
    {
      "context_text": "Cheng et al. designed a new network architecture with a special loss function as a downstream PLM model [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a new network architecture and loss function. There are no verifiable resources that meet the criteria.",
      "processing_time": 53.019943714141846,
      "citing_paper_id": "246938066",
      "cited_paper_id": 215415914
    },
    {
      "context_text": "Structural Encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a structural encoder which is likely a method or model. No verifiable resources are identified.",
      "processing_time": 52.95962738990784,
      "citing_paper_id": "264492774",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Based on this, a ùêæ -heads ( ùêæ = 3 ) of independent structural embedding ‚Ñé ùë†ùëñ where ùëé ùëòùëñùëó Triplet Encoder.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 350), response: ```json\n{\n    \"reasoning\": \"The citation span does not mention any specific, verifiable datasets. It",
      "processing_time": 56.70029807090759,
      "citing_paper_id": "264492774",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Here, ùë¢ ùëñ ùëñ Visual Encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a visual encoder. No verifiable resources are identified.",
      "processing_time": 52.49731683731079,
      "citing_paper_id": "264492774",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "Second, we use TransReID[7] pretrainedonMarket1501[27]andInceptionResNetv1[21]pretrained on VGGFace2[2] for feature extraction.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 12 column 1 (char 668), response: ```json\n{\n    \"reasoning\": \"The context mentions 'Market1501' and 'VGGFace2', both of which are data",
      "processing_time": 63.67462205886841,
      "citing_paper_id": "252782878",
      "cited_paper_id": 216009
    },
    {
      "context_text": "Second, we use TransReID[7] pretrainedonMarket1501[27]andInceptionResNetv1[21]pretrained on VGGFace2[2] for feature extraction.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 12 column 1 (char 668), response: ```json\n{\n    \"reasoning\": \"The context mentions 'Market1501' and 'VGGFace2', both of which are data",
      "processing_time": 63.67462205886841,
      "citing_paper_id": "252782878",
      "cited_paper_id": 1023605
    },
    {
      "context_text": "For instance, MovieQA [23] attempts to comprehend story both from video and text via question answering paradigm, [24] tries to predict social relation among characters and [22] aligns book chapters with video scenes in order to better understand video contents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MovieQA"
      ],
      "dataset_descriptions": {
        "MovieQA": "Used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, focusing on understanding movie stories."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions MovieQA, which is a specific dataset used for question-answering tasks involving movies. The other two citations refer to methodologies or approaches rather than datasets.",
      "processing_time": 54.91627264022827,
      "citing_paper_id": "252782878",
      "cited_paper_id": 1017389
    },
    {
      "context_text": "For instance, MovieQA [23] attempts to comprehend story both from video and text via question answering paradigm, [24] tries to predict social relation among characters and [22] aligns book chapters with video scenes in order to better understand video contents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MovieQA"
      ],
      "dataset_descriptions": {
        "MovieQA": "Used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, focusing on understanding movie stories."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions MovieQA, which is a specific dataset used for question-answering tasks involving movies. The other two citations refer to methodologies or approaches rather than datasets.",
      "processing_time": 54.91627264022827,
      "citing_paper_id": "252782878",
      "cited_paper_id": 6839244
    },
    {
      "context_text": "For instance, MovieQA [23] attempts to comprehend story both from video and text via question answering paradigm, [24] tries to predict social relation among characters and [22] aligns book chapters with video scenes in order to better understand video contents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MovieQA"
      ],
      "dataset_descriptions": {
        "MovieQA": "Used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, focusing on understanding movie stories."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions MovieQA, which is a specific dataset used for question-answering tasks involving movies. The other two citations refer to methodologies or approaches rather than datasets.",
      "processing_time": 54.91627264022827,
      "citing_paper_id": "252782878",
      "cited_paper_id": 239011786
    },
    {
      "context_text": "Different from dealing with isolated visual concepts retrieval task such as MovieQA [23] or \"The Large Scale Movie Description Challenge\" [18], DVU challenge requires the system to fully understand the entities and the relations among them in the video from the hierarchical perspective, which makes‚Ä¶",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MovieQA' and 'The Large Scale Movie Description Challenge', but does not indicate their use as datasets. They are referenced to contrast the DVU challenge, which requires a deeper understanding of entities and relations in videos.",
      "processing_time": 53.635207653045654,
      "citing_paper_id": "252782878",
      "cited_paper_id": 1017389
    },
    {
      "context_text": "Second, we use TransReID[7] pretrained onMarket1501[27] and InceptionResNetv1[21] pretrained on VGGFace2[2] for feature extraction.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MIMIC-III",
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "Market1501": "Used for pretraining TransReID for feature extraction, focusing on person re-identification tasks.",
        "VGGFace2": "Used for pretraining InceptionResNetv1 for feature extraction, focusing on face recognition tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Market1501' and 'VGGFace2' as datasets used for pretraining models. These are specific datasets with clear identifiers and are relevant to the research context.",
      "processing_time": 59.08846306800842,
      "citing_paper_id": "252782878",
      "cited_paper_id": 1023605
    },
    {
      "context_text": "For text embedding, we tokenize a sub-title sentence into a sequence of WordPieces[25].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for tokenizing text. The reference to WordPieces is a technique, not a dataset.",
      "processing_time": 52.904096603393555,
      "citing_paper_id": "252782878",
      "cited_paper_id": 3603249
    },
    {
      "context_text": "Recentyearshavewitnessedtremendousprogressinvision-language representationlearning[15,17,19],especiallyinvideodomain[1, 11,20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in vision-language representation learning. No verifiable resources are identified.",
      "processing_time": 52.582491636276245,
      "citing_paper_id": "252782878",
      "cited_paper_id": 102483628
    },
    {
      "context_text": "Recentyearshavewitnessedtremendousprogressinvision-language representationlearning[15,17,19],especiallyinvideodomain[1, 11,20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in vision-language representation learning. No verifiable resources are identified.",
      "processing_time": 52.582491636276245,
      "citing_paper_id": "252782878",
      "cited_paper_id": 233296845
    },
    {
      "context_text": "First, we use YOLOv5[9] pretrained on the COCO[16] dataset to detect persons in each frame, then the detection results are passed to StrongSORT[5] based on OSNet[28] to aggregate the bounding box in adjacent video frames to generate track-lets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-5000"
      ],
      "dataset_descriptions": {
        "COCO": "Used to train YOLOv5 for person detection in video frames, focusing on accurate bounding box generation for subsequent tracking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the COCO dataset, which is a well-known multi-modal dataset used for object detection and other computer vision tasks. It is used here for person detection in video frames.",
      "processing_time": 54.96210026741028,
      "citing_paper_id": "252782878",
      "cited_paper_id": 145050804
    },
    {
      "context_text": "First, we use YOLOv5[9] pretrained on the COCO[16] dataset to detect persons in each frame, then the detection results are passed to StrongSORT[5] based on OSNet[28] to aggregate the bounding box in adjacent video frames to generate track-lets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-5000"
      ],
      "dataset_descriptions": {
        "COCO": "Used to train YOLOv5 for person detection in video frames, focusing on accurate bounding box generation for subsequent tracking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the COCO dataset, which is a well-known multi-modal dataset used for object detection and other computer vision tasks. It is used here for person detection in video frames.",
      "processing_time": 54.96210026741028,
      "citing_paper_id": "252782878",
      "cited_paper_id": null
    },
    {
      "context_text": "First, we use YOLOv5[9] pretrained on the COCO[16] dataset to detect persons in each frame, then the detection results are passed to StrongSORT[5] based on OSNet[28] to aggregate the bounding box in adjacent video frames to generate track-lets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-5000"
      ],
      "dataset_descriptions": {
        "COCO": "Used to train YOLOv5 for person detection in video frames, focusing on accurate bounding box generation for subsequent tracking."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the COCO dataset, which is a well-known multi-modal dataset used for object detection and other computer vision tasks. It is used here for person detection in video frames.",
      "processing_time": 54.96210026741028,
      "citing_paper_id": "252782878",
      "cited_paper_id": null
    },
    {
      "context_text": "2 RELATEDWORK Recent years havewitnessed tremendous progress in vision-language representation learning [15, 17, 19], especially in video domain [1, 11, 20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in vision-language representation learning and the video domain.",
      "processing_time": 52.73893451690674,
      "citing_paper_id": "252782878",
      "cited_paper_id": 199528533
    },
    {
      "context_text": "For video embedding, we extract 2D and 3D visual features for each video frame by ResNet pretrained on ImageNet[4] and SlowFast[6] pretrained on Kinetics[10] respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet",
        "ImageNet",
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used for pretraining ResNet to extract 2D visual features from video frames, enhancing the model's ability to recognize objects and scenes.",
        "Kinetics": "Used for pretraining SlowFast to extract 3D visual features from video frames, improving the model's temporal understanding and action recognition."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions ImageNet and Kinetics, which are well-known datasets used for pretraining models. However, they are not used directly in the research described but rather as sources for pretrained models.",
      "processing_time": 56.2934091091156,
      "citing_paper_id": "252782878",
      "cited_paper_id": null
    },
    {
      "context_text": "First, we use YOLOv5[9] pretrained on the COCO[16] dataset to detect persons in each frame, then the detection results are passed to StrongSORT[5] based on OSNet[28] to aggregate the bounding box in adjacent video frames to generate tracklets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO"
      ],
      "dataset_descriptions": {
        "COCO": "The COCO dataset is used for detecting persons in video frames using YOLOv5, providing a rich set of annotations for object detection and segmentation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the COCO dataset, which is a well-known multi-modal dataset used for object detection and segmentation. It is used here for person detection in video frames.",
      "processing_time": 54.55897092819214,
      "citing_paper_id": "252782878",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast, the second approach, which we refer to as P-MKG in this paper, integrates images as attributes of existing nodes in the KG [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating images into knowledge graphs.",
      "processing_time": 52.040119886398315,
      "citing_paper_id": "269987660",
      "cited_paper_id": 53957733
    },
    {
      "context_text": "It involves mapping discrete entities and relationships within the KG to a vector space through training [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general process of mapping entities and relationships within a knowledge graph to a vector space.",
      "processing_time": 52.53305673599243,
      "citing_paper_id": "269987660",
      "cited_paper_id": 211010433
    },
    {
      "context_text": "The first method, known as E-MKG, involves using images as entity nodes within MKGs [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method involving images in multi-modal knowledge graphs.",
      "processing_time": 52.201058864593506,
      "citing_paper_id": "269987660",
      "cited_paper_id": 225115084
    },
    {
      "context_text": "Their applications span diverse fields such as medicine, education, finance, e-commerce, and beyond [2], [3], [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of multi-modal knowledge graphs. No verifiable resources are identified.",
      "processing_time": 52.41923499107361,
      "citing_paper_id": "269987660",
      "cited_paper_id": 236567496
    },
    {
      "context_text": "There are already many datasets for evaluating the multi-modal capabilities of models. and Manning, 2019] are used in visual question answering tasks, while COCO [Lin et al. , 2014], NoCaps [Agrawal et al. , 2019], and Flickr30K [Plummer et al. , 2015] are employed in image captioning tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "NoCaps",
        "Flickr30K"
      ],
      "dataset_descriptions": {
        "COCO": "Used for image captioning tasks, providing a large-scale dataset of images and corresponding captions to train and evaluate models.",
        "NoCaps": "Employed in image captioning tasks, focusing on generating captions for novel objects not seen during training.",
        "Flickr30K": "Utilized for image captioning tasks, containing 30,000 images with five crowd-sourced captions each, to assess model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating multi-modal capabilities, specifically in visual question answering and image captioning tasks.",
      "processing_time": 56.719456911087036,
      "citing_paper_id": "267211941",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "There are already many datasets for evaluating the multi-modal capabilities of models. and Manning, 2019] are used in visual question answering tasks, while COCO [Lin et al. , 2014], NoCaps [Agrawal et al. , 2019], and Flickr30K [Plummer et al. , 2015] are employed in image captioning tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "NoCaps",
        "Flickr30K"
      ],
      "dataset_descriptions": {
        "COCO": "Used for image captioning tasks, providing a large-scale dataset of images and corresponding captions to train and evaluate models.",
        "NoCaps": "Employed in image captioning tasks, focusing on generating captions for novel objects not seen during training.",
        "Flickr30K": "Utilized for image captioning tasks, containing 30,000 images with five crowd-sourced captions each, to assess model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for evaluating multi-modal capabilities, specifically in visual question answering and image captioning tasks.",
      "processing_time": 56.719456911087036,
      "citing_paper_id": "267211941",
      "cited_paper_id": 261065787
    },
    {
      "context_text": "We require more extensive data to evaluate MLLMs, and there",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific dataset names, only a general statement about the need for more extensive data. The cited paper titles do not introduce specific datasets either.",
      "processing_time": 52.643397092819214,
      "citing_paper_id": "267211941",
      "cited_paper_id": 259165040
    },
    {
      "context_text": "We require more extensive data to evaluate MLLMs, and there",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific dataset names, only a general statement about the need for more extensive data. The cited paper titles do not introduce specific datasets either.",
      "processing_time": 52.643397092819214,
      "citing_paper_id": "267211941",
      "cited_paper_id": 259837088
    },
    {
      "context_text": "Firstly, following the Circular Evaluation [ Liu et al. , 2023c ] , we cyclically change the positions of the options and let the model answer questions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating multi-modal models.",
      "processing_time": 51.90917611122131,
      "citing_paper_id": "267211941",
      "cited_paper_id": 259837088
    },
    {
      "context_text": "In addition, to mitigate the position bias [ Zheng et al. , 2023 ] in LLM and ensure genuine correctness rather than guessing, inspired by Circu-larEval [ Liu et al. , 2023c ] , we adopt a ShiftCheck approach for multiple-choice questions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on mitigating position bias in LLMs using a ShiftCheck approach.",
      "processing_time": 52.69281578063965,
      "citing_paper_id": "267211941",
      "cited_paper_id": 259837088
    },
    {
      "context_text": "Currently, multi-modal large language models (MLLM) like GPT-4[OpenAI, 2023], Gemini[Team et al. , 2023], LLaVA[Liu et al. , 2023a] and Qwen-VL [Bai et al. , 2023] have showed powerful abilities in this field of multi-model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 52.42486214637756,
      "citing_paper_id": "267211941",
      "cited_paper_id": 259837088
    },
    {
      "context_text": "Currently, multi-modal large language models (MLLM) like GPT-4[OpenAI, 2023], Gemini[Team et al. , 2023], LLaVA[Liu et al. , 2023a] and Qwen-VL [Bai et al. , 2023] have showed powerful abilities in this field of multi-model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 52.42486214637756,
      "citing_paper_id": "267211941",
      "cited_paper_id": 261101015
    },
    {
      "context_text": "Currently, multi-modal large language models (MLLM) like GPT-4[OpenAI, 2023], Gemini[Team et al. , 2023], LLaVA[Liu et al. , 2023a] and Qwen-VL [Bai et al. , 2023] have showed powerful abilities in this field of multi-model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 52.42486214637756,
      "citing_paper_id": "267211941",
      "cited_paper_id": null
    },
    {
      "context_text": "Datasets like MMBench [Fu et al. , 2023], MME [ Fu et al. , 2023 ] , and SEED-Bench [ Li et al. , 2023b; Li et al. , 2023a ] evaluate models through a diverse range of questions, ranging from perception to reasoning abilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMBench",
        "CIFAR-10",
        "ImageNet"
      ],
      "dataset_descriptions": {
        "MMBench": "Used to evaluate models' multi-modal reasoning abilities through a diverse range of questions, focusing on perception and reasoning tasks.",
        "CIFAR-10": "Used to evaluate models' performance on image classification tasks, specifically focusing on the ability to recognize and categorize visual content.",
        "ImageNet": "Used to assess models' image recognition capabilities, particularly in identifying and classifying a wide variety of objects across multiple categories."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions specific datasets used for evaluating models' multi-modal reasoning abilities, which aligns with the research topic of multi-modal knowledge graph reasoning.",
      "processing_time": 56.84815812110901,
      "citing_paper_id": "267211941",
      "cited_paper_id": 261065787
    },
    {
      "context_text": "Benefiting from the development of large language models(LLM) like GPT [Macfarlane, 2023 3 The CMMU Benchmark CMMU is a novel multi-modal benchmark designed to evaluate domain-specific knowledge across seven foundational subjects: math, biology, physics, chemistry, geography, politics, and history.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MIMIC-III"
      ],
      "dataset_descriptions": {
        "CMMU": "CMMU is used to evaluate domain-specific knowledge across seven foundational subjects using multi-modal reasoning, focusing on the performance of large language models in various domains."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'CMMU' as a benchmark, which is a specific, downloadable dataset used for evaluating domain-specific knowledge across multiple subjects. It is relevant to multi-modal knowledge graph reasoning.",
      "processing_time": 54.79162240028381,
      "citing_paper_id": "267211941",
      "cited_paper_id": 265656168
    },
    {
      "context_text": "Among various ensemble functions, [15] point out that simple concatenation works better than DeViSE [8] and Imagined [4] on multi-modal KGC tasks, and taking limited computational resources and scalability of MULTIFORM, we use simple concatenation to aggregate the visual embedding, textual‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DeViSE' and 'Imagined' but does not refer to them as datasets. They are likely methods or models. No specific datasets are mentioned.",
      "processing_time": 52.871729612350464,
      "citing_paper_id": "252518772",
      "cited_paper_id": 261138
    },
    {
      "context_text": "Among various ensemble functions, [15] point out that simple concatenation works better than DeViSE [8] and Imagined [4] on multi-modal KGC tasks, and taking limited computational resources and scalability of MULTIFORM, we use simple concatenation to aggregate the visual embedding, textual‚Ä¶",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DeViSE' and 'Imagined' but does not refer to them as datasets. They are likely methods or models. No specific datasets are mentioned.",
      "processing_time": 52.871729612350464,
      "citing_paper_id": "252518772",
      "cited_paper_id": 27263492
    },
    {
      "context_text": "Mousselly et al. [15] uses three diÔ¨Äerent ensemble function, i.e., simple concatenation, DeViSE [8], and Imagined [4] to fuse multi-modal context embeddings.",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DeViSE' and 'Imagined' as methods for fusing multi-modal context embeddings, but does not refer to them as datasets. The context is focused on methods rather than datasets.",
      "processing_time": 53.16448712348938,
      "citing_paper_id": "252518772",
      "cited_paper_id": 261138
    },
    {
      "context_text": "Mousselly et al. [15] uses three diÔ¨Äerent ensemble function, i.e., simple concatenation, DeViSE [8], and Imagined [4] to fuse multi-modal context embeddings.",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DeViSE' and 'Imagined' as methods for fusing multi-modal context embeddings, but does not refer to them as datasets. The context is focused on methods rather than datasets.",
      "processing_time": 53.16448712348938,
      "citing_paper_id": "252518772",
      "cited_paper_id": 27263492
    },
    {
      "context_text": "For example, prototypical networks [24] classify each instance by calculating the similarity to prototype representation of each class, whose idea is similar to some nearest neighbor algorithms.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (prototypical networks).",
      "processing_time": 51.683704137802124,
      "citing_paper_id": "252518772",
      "cited_paper_id": 309759
    },
    {
      "context_text": "We consider the four widely used baseline methods as follows: TransE [2], DistMult [39], ComplEx [30] and Rot-Pro [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions methods but does not reference any specific datasets. The context is focused on describing baseline methods used in the research.",
      "processing_time": 52.16061544418335,
      "citing_paper_id": "252518772",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "Following this line of research, more eÔ¨Äective models such as DistMult [39], ComplEx [30], ConvE [5], Rotate [26], and Rot-Pro [25] have been proposed for further improvements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on describing advancements in models for knowledge graph reasoning.",
      "processing_time": 52.356247901916504,
      "citing_paper_id": "252518772",
      "cited_paper_id": 2768038
    },
    {
      "context_text": "For parameter updates, we use Adam [10] with the initial learning rate of 0.001 and we have the learning rate decay 0.2 for each 50k training step.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Adam optimizer) for training models.",
      "processing_time": 51.61300826072693,
      "citing_paper_id": "252518772",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "There are several models [22,36] taking rich text descriptions into consideration to handle unseen entities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models that consider rich text descriptions for handling unseen entities in knowledge graphs.",
      "processing_time": 51.74275231361389,
      "citing_paper_id": "252518772",
      "cited_paper_id": 31606602
    },
    {
      "context_text": "Following [38], we construct few-shot multi-modal KGs by selecting those relations that do not have too many training triples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for constructing few-shot multi-modal KGs. No verifiable resources are identified.",
      "processing_time": 51.746098279953,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "The division ratio is about 15 : 1 : 4 , similar to the data split in [38,40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific dataset names, only a data split ratio. The cited papers' titles suggest a focus on relational learning and few-shot knowledge graph completion, but do not provide specific dataset names.",
      "processing_time": 52.28598165512085,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "The division ratio is about 15 : 1 : 4 , similar to the data split in [38,40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific dataset names, only a data split ratio. The cited papers' titles suggest a focus on relational learning and few-shot knowledge graph completion, but do not provide specific dataset names.",
      "processing_time": 52.28598165512085,
      "citing_paper_id": "252518772",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "For fair comparison, we select three typical neighbor encoder based models, i.e., GMatching [38], FSRL [40], FAAN [21].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 328), response: ```json\n{\n    \"reasoning\": \"The citation context does not mention any specific datasets, only models",
      "processing_time": 51.59175944328308,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "For fair comparison, we select three typical neighbor encoder based models, i.e., GMatching [38], FSRL [40], FAAN [21].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 328), response: ```json\n{\n    \"reasoning\": \"The citation context does not mention any specific datasets, only models",
      "processing_time": 51.59175944328308,
      "citing_paper_id": "252518772",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "For our model‚Äôs scalability, we use the same multi-step matching processor as [38].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or processor. The title of the cited paper also does not indicate a dataset.",
      "processing_time": 51.34622406959534,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "Existing few-shot learning models [38,40,21] on KGC tasks all sorely depend on local graph structures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to few-shot learning models on KGC tasks. No verifiable resources are identified.",
      "processing_time": 51.666242361068726,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "Existing few-shot learning models [38,40,21] on KGC tasks all sorely depend on local graph structures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to few-shot learning models on KGC tasks. No verifiable resources are identified.",
      "processing_time": 51.666242361068726,
      "citing_paper_id": "252518772",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "These embedding-based models heavily rely on extensive collections of training examples, and they are not qualiÔ¨Åed to deal with sparse triples, as presented in [2] and [38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'extensive collections of training examples'. No specific, verifiable datasets are named.",
      "processing_time": 51.3816351890564,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "There are also some few-shot learning methods, such as GMatching [38] and FAAN [21], concentrating on alleviating the challenge of the lack of training examples for the long-tail relations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (GMatching and FAAN) used for few-shot learning in knowledge graphs.",
      "processing_time": 51.41927671432495,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "GMatching [38], FSRL [40], and FAAN [21] can also be considered as a metric-based approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about comparing different methods for relational learning and knowledge graph completion.",
      "processing_time": 51.25804328918457,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "GMatching [38], FSRL [40], and FAAN [21] can also be considered as a metric-based approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about comparing different methods for relational learning and knowledge graph completion.",
      "processing_time": 51.25804328918457,
      "citing_paper_id": "252518772",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "Recently, Xiong et al. [38] and Zhang et al. [40] have demonstrated the eÔ¨Äectiveness of encoding local graph structures as entity representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the effectiveness of encoding local graph structures. No verifiable resources are identified.",
      "processing_time": 51.1163010597229,
      "citing_paper_id": "252518772",
      "cited_paper_id": 52110037
    },
    {
      "context_text": "Recently, Xiong et al. [38] and Zhang et al. [40] have demonstrated the eÔ¨Äectiveness of encoding local graph structures as entity representations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the effectiveness of encoding local graph structures. No verifiable resources are identified.",
      "processing_time": 51.1163010597229,
      "citing_paper_id": "252518772",
      "cited_paper_id": 208291464
    },
    {
      "context_text": "The third group is optimization-based approaches [20,7,13], which aim to learn faster by changing the optimization methods on few-shot reference instances.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization methods and approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 51.11059880256653,
      "citing_paper_id": "252518772",
      "cited_paper_id": 67413369
    },
    {
      "context_text": "Following the standard meta-learning pipelines [20,7], we describe the settings for training and evaluation of our few-shot learning model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to meta-learning pipelines. No verifiable resources are identified.",
      "processing_time": 51.1555860042572,
      "citing_paper_id": "252518772",
      "cited_paper_id": 67413369
    },
    {
      "context_text": "Our constructed multi-modal datasets MM-FB15K and MM-DBpedia are based on FB15K [2,1] and DBpedia [14,11,23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MM-FB15K",
        "MM-DBpedia"
      ],
      "dataset_descriptions": {
        "MM-FB15K": "Used to construct a multi-modal knowledge graph, extending FB15K with visual and textual modalities to enhance reasoning capabilities.",
        "MM-DBpedia": "Used to construct a multi-modal knowledge graph, integrating DBpedia with visual and textual information to support multi-modal reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two constructed multi-modal datasets, MM-FB15K and MM-DBpedia, which are based on existing datasets FB15K and DBpedia. These are specific, verifiable datasets used in the research.",
      "processing_time": 55.53572869300842,
      "citing_paper_id": "252518772",
      "cited_paper_id": 76663467
    },
    {
      "context_text": "We also select RotatE [26], which has been reported very robust under diÔ¨Äer-ent evaluation protocols in the extensive conducted experiments, comparing with a series of state-of-the-art knowledge embedding methods [27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the performance of RotatE compared to other knowledge embedding methods.",
      "processing_time": 51.61475396156311,
      "citing_paper_id": "252518772",
      "cited_paper_id": 207852450
    },
    {
      "context_text": "RSME [34] evaluates diÔ¨Äerent image encoders for multi-modal KGC and verify the eÔ¨Äectiveness of Visual Transformer (ViT), so we adopt ViT as image encoder in this paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of image encoders for multi-modal knowledge graph completion (KGC).",
      "processing_time": 51.052765130996704,
      "citing_paper_id": "252518772",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "[34] shows that incorrect images account for only a small proportion of all images in KGs. Inspired by [34], we utilize a Ô¨Ålter gate based on the empirical analysis that the incorrect images have low similarity with the right images.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to images in knowledge graphs. No clear, verifiable dataset names are provided.",
      "processing_time": 50.960638761520386,
      "citing_paper_id": "252518772",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "As ViT achieves the best performance over the Convolutional Neural Network (CNN) based models according to [34], we adopt ViT to encode the selected right images to obtain the corresponding embeddings of images in V as { z v 1 , z v 2 , . . . , z v n } .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of ViT for encoding images. No verifiable resources are identified.",
      "processing_time": 50.8275420665741,
      "citing_paper_id": "252518772",
      "cited_paper_id": 239011538
    },
    {
      "context_text": "We select two state-of-the-art methods, i.e., TransAE [35] and RSME [34] as our baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (TransAE and RSME). No datasets are referenced or used in the described context.",
      "processing_time": 50.86785817146301,
      "citing_paper_id": "252518772",
      "cited_paper_id": 239011538
    }
  ],
  "filtering_stats": {
    "original_papers_count": 97,
    "filtered_papers_count": 91,
    "filtered_percentage": "93.8%"
  },
  "extraction_stats": {
    "unique_contexts_processed": 2592,
    "total_citation_instances": 3349,
    "successful_extractions": 359,
    "failed_extractions": 2990,
    "total_processing_time": 165.54203724861145
  }
}