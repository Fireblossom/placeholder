{
  "summary": {
    "total_unique_datasets": 184,
    "total_dataset_mentions": 501,
    "unique_dataset_names": 184,
    "extraction_successful": 1155,
    "extraction_failed": 6333,
    "unique_contexts_processed": 5690,
    "total_citation_instances": 7488,
    "total_processing_time": 373.45660042762756
  },
  "datasets_sorted_by_citation_count": [
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 38,
      "unique_datasets": [
        "DBpedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a core dataset for linking and integrating open data on the web, supporting multi-modal knowledge graph reasoning by providing structured information from Wikipedia. | Used to query visual concepts extracted from images, contributing to the construction of a multi-modal knowledge base.",
          "citing_paper_id": "221397171",
          "cited_paper_id": 7278297,
          "context_text": "The knowledge base is constructed by extracting the top visual concepts from all the images in the dataset and querying those concepts from three knowledge bases, including DBPedia [36], ConceptNet [37] and WebChild [38].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of three knowledge bases to construct a knowledge base by extracting visual concepts from images. These knowledge bases are DBPedia, ConceptNet, and WebChild.",
          "citing_paper_doi": "10.1016/j.patcog.2020.107563",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e526624783b3b5687da54b8cd4a7190a26a0b5e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2020,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to perform link prediction by merging embeddings from images, text, and knowledge graph, focusing on multi-modal representation learning.",
          "citing_paper_id": "53957733",
          "cited_paper_id": 33793649,
          "context_text": "[Thoma et al., 2017], they merge in a joint representation the embeddings from images, text, and KG and use the representation to perform link prediction on DBpedia [Lehmann et al.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of DBpedia for performing link prediction, which is a specific, verifiable dataset. The citation indicates that the dataset is used to merge embeddings from multiple modalities.",
          "citing_paper_doi": "10.24432/C56P45",
          "cited_paper_doi": "10.1007/978-3-319-68288-4_41",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ddd6c9d27e522b2237ea67ab92d578273b767f0b",
          "citing_paper_year": 2017,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as an external knowledge source for multimodal tasks, specifically to enhance reasoning capabilities by integrating visual and textual information. | Used to enrich a multi-modal knowledge graph with structured information, focusing on integrating numeric literals and image data. | Used to augment a multi-modal knowledge graph with structured data, including numeric literals and image information. | Used to enhance a multi-modal knowledge graph by adding structured data, particularly numeric literals and image information.",
          "citing_paper_id": "264492337",
          "cited_paper_id": 76663467,
          "context_text": "MMKG [19] follows a more traditional philosophy, enriching DBPEDIA, YAGO and Freebase-15k with numeric literals and image information to form an MMKG, which also provides an early example of typical practice for MMKG construction.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions DBPEDIA, YAGO, and Freebase-15k as datasets used to construct a multi-modal knowledge graph (MMKG). These are specific, verifiable datasets.",
          "citing_paper_doi": "10.1145/3581783.3612266",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac7474000c867ea3b0ad64bf7301e102621a2afb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, providing structured data from Wikipedia. | Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, offering multilingual knowledge from Wikipedias. | Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, serving as a collaboratively created graph database.",
          "citing_paper_id": "264492372",
          "cited_paper_id": 6611164,
          "context_text": "KGs, e.g., DBpedia [12], YAGO [19], and Freebase [1] have been extensively used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning [31, 35, 37, 39].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific knowledge graphs (DBpedia, YAGO, Freebase) used in various applications. These are verifiable resources and fit the criteria for datasets.",
          "citing_paper_doi": "10.1145/3581783.3612151",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d09265120997e305c262b13e7a675f7c5a3ab9d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c5b5adc3830ac45bf1d764603b1b71e5f729616",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, providing structured data from Wikipedia. | Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, offering multilingual knowledge from Wikipedias. | Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, serving as a collaboratively created graph database.",
          "citing_paper_id": "264492372",
          "cited_paper_id": 20667722,
          "context_text": "KGs, e.g., DBpedia [12], YAGO [19], and Freebase [1] have been extensively used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning [31, 35, 37, 39].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific knowledge graphs (DBpedia, YAGO, Freebase) used in various applications. These are verifiable resources and fit the criteria for datasets.",
          "citing_paper_doi": "10.1145/3581783.3612151",
          "cited_paper_doi": "10.18653/v1/D17-1060",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d09265120997e305c262b13e7a675f7c5a3ab9d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/857176d022369e963d3ff1be2cb9e1ca2f674520",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, providing structured data from Wikipedia. | Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, offering multilingual knowledge from Wikipedias. | Used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning, serving as a collaboratively created graph database.",
          "citing_paper_id": "264492372",
          "cited_paper_id": 207167677,
          "context_text": "KGs, e.g., DBpedia [12], YAGO [19], and Freebase [1] have been extensively used in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning [31, 35, 37, 39].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific knowledge graphs (DBpedia, YAGO, Freebase) used in various applications. These are verifiable resources and fit the criteria for datasets.",
          "citing_paper_doi": "10.1145/3581783.3612151",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d09265120997e305c262b13e7a675f7c5a3ab9d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2023,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to highlight the inherent incompleteness in knowledge graphs, specifically focusing on the lack of associated images for entities.",
          "citing_paper_id": "260334664",
          "cited_paper_id": 1181640,
          "context_text": "This incompleteness is inherent to the DBpedia KG [26], as not every entity possesses an associated image.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions DBpedia, which is a knowledge base, but does not specify its use as a dataset in the context provided.",
          "citing_paper_doi": "10.48550/arXiv.2307.16210",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to construct a multi-modal knowledge graph, focusing on integrating structured information from Wikipedia. The research investigates the effectiveness of DBpedia in enhancing knowledge representation and reasoning.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 10318045,
          "context_text": "ucted either by manual-annotation/crowd-sourcing (e.g., DBpedia [2], Freebase [6] and Wikidata [39]), or by automatically extracting from unstructured/semistructured data (e.g., YAGO [22, 28], OpenIE [3, 14, 15], NELL [9], NEIL [10, 11]). The KB we use here is DBpedia, which contains structured information extracted from Wikipedia. Compared to KBs extracted automatically from unstructured data (such as OpenI",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions DBpedia as a knowledge base used in the research, which is relevant to multi-modal knowledge graph reasoning. Other mentioned resources are methods or tools, not datasets.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d4b651d6a904f69f8fa1dcad4ebe972296af3a9a",
          "citing_paper_year": 2015,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to link images and textual information in a multi-modal knowledge graph, supporting multi-modal reasoning by integrating visual and textual data.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 11797257,
          "context_text": "he resulting RDF graph includes all of the relevant information in DBpedia, linked from the query image. This combined image and DBpedia information is then accessed through a local OpenLink Virtuoso [13] RDBMS. 2In DBpedia, each entity has a uniform resource identier (URI). For example, the animal girae corresponds to URI: http://dbpedia.org/resource/Giraffe. 7 3.2 Answering Questions Having gather",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions DBpedia as a source of information linked to images, which is relevant to multi-modal knowledge graph reasoning. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/04129379d4120364c8ff81c3dac5c0f546181e83",
          "citing_paper_year": 2015,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to extract structured information from Wikipedia, enhancing the knowledge graph with a wide range of entities and relations. | Harvested and organized commonsense knowledge from the web, contributing to the multi-modal reasoning capabilities of the system. | Utilized to integrate commonsense knowledge, providing a rich semantic network for reasoning tasks.",
          "citing_paper_id": "269157470",
          "cited_paper_id": 3088903,
          "context_text": "This external knowledge is being extracted from DBpedia(Auer et al., 2007), ConceptNet(Speer et al., 2017), and We-bChild(Tandon et al., 2014).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific knowledge bases that are used to extract external knowledge, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2404.10226",
          "cited_paper_doi": "10.1145/2556195.2556245",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a712f22b419623590b5b121278b7709cd1d08a64",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db95087540c956788a8560495bf03ded1239e062",
          "citing_paper_year": 2024,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "This dataset 'YAGO' was mentioned in the citation context but no detailed description was generated. | This dataset 'DBpedia' was mentioned in the citation context but no detailed description was generated. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, providing a rich source of structured information for multi-modal reasoning.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 14494942,
          "context_text": "Popular large-scale structured KBs are constructed either by manual-annotation/crowd-sourcing ( e.g ., DBpedia [2], Freebase [6] and Wikidata [39]), or by automatically extracting from unstructured/semi-structured data ( e.g ., YAGO [22, 28], OpenIE [3, 14, 15], NELL [9], NEIL [10, 11]).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases, which are relevant to multi-modal knowledge graph reasoning. However, only those with clear identifiers and specific names are included.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": "10.1145/2629489",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dab7e605237ad4f4fe56dcba2861b8f0a57112be",
          "citing_paper_year": 2015,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to extract structured information from Wikipedia, enhancing the knowledge graph with a wide range of entities and relations. | Harvested and organized commonsense knowledge from the web, contributing to the multi-modal reasoning capabilities of the system. | Utilized to integrate commonsense knowledge, providing a rich semantic network for reasoning tasks.",
          "citing_paper_id": "269157470",
          "cited_paper_id": 7278297,
          "context_text": "This external knowledge is being extracted from DBpedia(Auer et al., 2007), ConceptNet(Speer et al., 2017), and We-bChild(Tandon et al., 2014).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific knowledge bases that are used to extract external knowledge, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2404.10226",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a712f22b419623590b5b121278b7709cd1d08a64",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to extend a dataset with textual descriptions and images associated with entities, enhancing multi-modal reasoning capabilities.",
          "citing_paper_id": "52160797",
          "cited_paper_id": 1181640,
          "context_text": "We extend this dataset with the textual description (as an additional relation) and the images associated with each entity (for half of the entities), provided by DBpedia (Lehmann et al., 2015).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions extending a dataset with textual descriptions and images from DBpedia, which is a specific, verifiable resource.",
          "citing_paper_doi": "10.18653/v1/D18-1359",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2018,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to assess the performance of visual reasoning, providing rich annotations for images. | Used to test part-whole relationships, enhancing the reasoning capabilities with hierarchical data. | Used to evaluate the accuracy of multi-modal reasoning, focusing on structured data from Wikipedia. | Used as a knowledge base for multi-modal reasoning, providing structured information extracted from Wikipedia. | Used as a primary source of knowledge triplets for multi-modal reasoning, providing a large-scale, structured representation of information. | Utilized as a semantic network for multi-modal reasoning, offering a large set of common-sense assertions. | Used to construct a knowledge graph for OK-VQA, providing visual knowledge and scene graphs. | Used to construct a knowledge graph for OK-VQA, providing structured information extracted from Wikipedia. | Used as a large-scale knowledge graph derived from Wikipedia, providing structured information for multi-modal reasoning tasks. | Used to enhance semantic understanding, providing a large network of common-sense knowledge. | Used to construct a knowledge graph for OK-VQA, offering a commonsense knowledge base. | Mentioned as a large-scale knowledge graph containing millions of edges, but specific usage in multi-modal reasoning is not detailed. | Used to construct a knowledge graph for OK-VQA, contributing part-of relationships.",
          "citing_paper_id": "229339845",
          "cited_paper_id": 7278297,
          "context_text": "DBPedia is created automatically from data from Wikipedia [1].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "DBPedia is mentioned as a dataset derived from Wikipedia, which is relevant for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01389",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2020,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as an open-source multimodal knowledge graph for reasoning, combining structured knowledge from DBpedia and images from search engines. | Used to crawl textual descriptions and align them with corresponding entities using sameAs links, enhancing the knowledge graph with additional information. | Used to supplement textual information for entities, enhancing the knowledge graph with structured data extracted from Wikipedia.",
          "citing_paper_id": "252783084",
          "cited_paper_id": 1181640,
          "context_text": "Considering that each entity lacks textual description, we supplemented the textual information of entities from the database of DBpedia [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions using DBpedia to supplement textual information for entities, which indicates the use of a specific, verifiable dataset.",
          "citing_paper_doi": "10.1145/3503161.3548388",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4ba9aab31d0a5e2af0147bd18e3381bdcfd15cd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to construct multi-modal knowledge graphs by extracting structured knowledge, focusing on integrating textual and ontological information. | Used to crawl textual descriptions and align them with corresponding entities using sameAs links, enhancing the knowledge graph with additional information.",
          "citing_paper_id": "252783084",
          "cited_paper_id": 212737039,
          "context_text": "For textual descriptions, we crawled them from DBpedia and then aligned them with corresponding entities through extra sameAs link provided by [33].",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions crawling textual descriptions from DBpedia, which is a specific, verifiable dataset. However, it does not provide details on how DBpedia is used in the research context.",
          "citing_paper_doi": "10.1145/3503161.3548388",
          "cited_paper_doi": "10.14778/3407790.3407828",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4ba9aab31d0a5e2af0147bd18e3381bdcfd15cd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65baa67a7cdb3b4b948d126ac5b41ca9c98b1f3b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to provide structural knowledge for various applications, enhancing the representation and reasoning capabilities of multi-modal knowledge graphs. | Used to provide multilingual structural knowledge, integrating data from Wikipedia, WordNet, and GeoNames to support multi-modal reasoning. | Used to construct a wide-coverage multilingual semantic network, enhancing the semantic richness and cross-lingual capabilities of multi-modal knowledge graphs.",
          "citing_paper_id": "202770936",
          "cited_paper_id": 6063065,
          "context_text": "Recently, many Knowledge Graphs (KGs) (e.g., DBpedia (Lehmann et al., 2015), YAGO (Rebele et al., 2016) and BabelNet (Navigli and Ponzetto, 2012)) have emerged to provide structural knowledge for different applications.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific Knowledge Graphs (KGs) that are used to provide structural knowledge for various applications. These KGs are clearly identified and are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/D19-1274",
          "cited_paper_doi": "10.1016/J.ARTINT.2012.07.001",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc9f702abca7d2c164e2a4207a835f27098fa63b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7f90ef42f22d4f9b86d33b0ad7f16261273c8612",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to provide structural knowledge for various applications, enhancing the representation and reasoning capabilities of multi-modal knowledge graphs. | Used to evaluate alignment between DBpedia and YAGO3, focusing on entity linking and cross-referencing in multi-modal knowledge graphs. | Used to construct a wide-coverage multilingual semantic network, enhancing the semantic richness and cross-lingual capabilities of multi-modal knowledge graphs. | Used to evaluate alignment between DBpedia and Wikidata, focusing on entity linking and cross-referencing in multi-modal knowledge graphs. | Used to provide multilingual structural knowledge, integrating data from Wikipedia, WordNet, and GeoNames to support multi-modal reasoning.",
          "citing_paper_id": "202770936",
          "cited_paper_id": 9770446,
          "context_text": "Recently, many Knowledge Graphs (KGs) (e.g., DBpedia (Lehmann et al., 2015), YAGO (Rebele et al., 2016) and BabelNet (Navigli and Ponzetto, 2012)) have emerged to provide structural knowledge for different applications.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific Knowledge Graphs (KGs) that are used to provide structural knowledge for various applications. These KGs are clearly identified and are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/D19-1274",
          "cited_paper_doi": "10.1007/978-3-319-46547-0_19",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc9f702abca7d2c164e2a4207a835f27098fa63b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/772f69b1dd966610edaa4ef2576d36142dc05928",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to link extracted named entities to obtain their entity types, enhancing the knowledge graph with structured information.",
          "citing_paper_id": "5083989",
          "cited_paper_id": 11275066,
          "context_text": "Given the related text, we apply EDL algorithms (Pan et al., 2015) to extract named entities and link them to DBpedia to obtain their entity types.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of DBpedia for entity linking, which is a specific, verifiable resource. However, it does not mention a specific dataset used for training or evaluation.",
          "citing_paper_doi": "10.18653/v1/D18-1435",
          "cited_paper_doi": "10.3115/v1/N15-1119",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e8feffa2280e41ceb864b940869c5408db89285",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e56a36dd9efc573bc4dab3d3d2fcc183448b6a31",
          "citing_paper_year": 2018,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to link named entities extracted from text to obtain their entity types, enhancing the construction of information networks.",
          "citing_paper_id": "5083989",
          "cited_paper_id": 15552794,
          "context_text": "Given the related text, we apply EDL algorithms (Li et al., 2014; Pan et al., 2015) to extract named entities and link them to DBpedia to obtain their entity types.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions using DBpedia for linking named entities to obtain their entity types, which is a specific, verifiable resource.",
          "citing_paper_doi": "10.18653/v1/D18-1435",
          "cited_paper_doi": "10.3115/v1/D14-1198",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e8feffa2280e41ceb864b940869c5408db89285",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e83eda2c3806e285a231d762424bb1efb890f170",
          "citing_paper_year": 2018,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Mentioned as a representative knowledge graph, benefiting downstream tasks like semantic analysis, question-answer systems, and machine comprehension.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 213399202,
          "context_text": "The most representative KGs, e.g., DBpedia [15], Freebase [3], and WordNet [21], benefit various kinds of downstream tasks, such as semantic analysis [29], question-answer systems [14], and machine comprehension [16].",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions DBpedia, Freebase, and WordNet as representative KGs, but does not specify their usage in a particular research context. These are general references to well-known knowledge graphs.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1016/j.neucom.2020.01.056",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2721cd5c2afe83ec3d81bc3a216b0bb9ce16b986",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a structured content source from Wikimedia projects, providing a nucleus for a web of open data in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 7278297,
          "context_text": "• DBpedia [226] consists of structured content from the information created in various Wikimedia projects.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DBpedia is mentioned as a structured content source from Wikimedia projects, which is relevant for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to build multilingual knowledge graphs, representing real-world knowledge as structured data in multiple languages. | Used to construct a multilingual knowledge base from Wikipedia, WordNet, and GeoNames, enhancing cross-lingual knowledge representation. | Used to represent real-world knowledge as a multilingual knowledge base, integrating data from Wikipedia, WordNet, and GeoNames. | Used to represent real-world knowledge as a multilingual knowledge base, integrating data from Wikipedia and WordNet. | Used to integrate lexical and encyclopedic knowledge, creating a multilingual knowledge graph for various applications.",
          "citing_paper_id": "202121966",
          "cited_paper_id": 9770446,
          "context_text": "A growing number of multilingual knowledge graphs (KGs) have been built, such as DBpedia (Bizer et al., 2009), YAGO (Suchanek et al., 2008; Rebele et al., 2016), and BabelNet (Navigli and Ponzetto, 2012), which typically represent realworld knowledge as separately-structured monolingual KGs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, which are relevant to multi-modal knowledge graph reasoning. These are specific, verifiable resources with clear provenance.",
          "citing_paper_doi": "10.18653/v1/D19-1451",
          "cited_paper_doi": "10.1007/978-3-319-46547-0_19",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf4dcc7d67f0776ffe7aa5b4ee3217f9bd757282",
          "cited_paper_url": "https://www.semanticscholar.org/paper/772f69b1dd966610edaa4ef2576d36142dc05928",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Mentioned as a multilingual knowledge base, but the specific usage in the research context is not detailed.",
          "citing_paper_id": "49299019",
          "cited_paper_id": 15206880,
          "context_text": "Multilingual knowledge bases (KBs) such as DBpedia [Lehmann et al. , 2015], ConceptNet [Speer et al. , 2017], and Yago [Mahdisoltani et al. , 2015] constitute crucial sources of knowledge for AI-related applications.",
          "confidence_score": 0.6,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multilingual knowledge bases, which are relevant to multi-modal knowledge graph reasoning. However, the context does not specify how these resources are used in the research.",
          "citing_paper_doi": "10.24963/ijcai.2018/556",
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5902fa7e6f637fea16ff325ab0e25c88bf31f27a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a multilingual knowledge graph to support multi-modal reasoning, providing structured information from Wikipedia. | Used to represent real-world knowledge in a multilingual context, focusing on the structure and integration of mono-lingual knowledge graphs.",
          "citing_paper_id": "202121966",
          "cited_paper_id": 16081721,
          "context_text": "A growing number of multilingual knowledge graphs (KGs) have been built, such as DBpedia (Bizer et al., 2009), YAGO (Suchanek et al., 2008; Rebele et al., 2016), and BabelNet (Navigli and Ponzetto, 2012), which typically represent real-world knowledge as separately-structured mono-lingual KGs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, which are relevant to multi-modal knowledge graph reasoning. These are specific, verifiable resources with clear provenance.",
          "citing_paper_doi": "10.18653/v1/D19-1451",
          "cited_paper_doi": "10.1016/J.WEBSEM.2009.07.002",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf4dcc7d67f0776ffe7aa5b4ee3217f9bd757282",
          "cited_paper_url": "https://www.semanticscholar.org/paper/58f72b53d576c6e4a42b4d8812e5542ffa2c03cc",
          "citing_paper_year": 2019,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a knowledge graph containing relational facts for various tasks including question answering, recommendation systems, and multimedia reasoning.",
          "citing_paper_id": "239011538",
          "cited_paper_id": 16081721,
          "context_text": "pedia [2], contain relational facts in the form of head relation −−−−−−→ tail , which have been widely used in various kinds of tasks, such as question answering [13], recommendation system [11], and multimedia reasoning [17, 27].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'pedia' which likely refers to DBpedia, a well-known knowledge graph. However, there are no specific datasets mentioned in the citation span.",
          "citing_paper_doi": "10.1145/3474085.3475470",
          "cited_paper_doi": "10.1016/J.WEBSEM.2009.07.002",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/58f72b53d576c6e4a42b4d8812e5542ffa2c03cc",
          "citing_paper_year": 2021,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a knowledge graph containing relational facts for various tasks including question answering, recommendation systems, and multimedia reasoning.",
          "citing_paper_id": "239011538",
          "cited_paper_id": 59528287,
          "context_text": "pedia [2], contain relational facts in the form of head relation −−−−−−→ tail , which have been widely used in various kinds of tasks, such as question answering [13], recommendation system [11], and multimedia reasoning [17, 27].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'pedia' which likely refers to DBpedia, a well-known knowledge graph. However, there are no specific datasets mentioned in the citation span.",
          "citing_paper_doi": "10.1145/3474085.3475470",
          "cited_paper_doi": "10.1145/3289600.3290956",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7572aefcd241ec76341addcb2e2e417587cb2e4c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to extract structured data for constructing a knowledge graph, focusing on metallic materials and their properties. | Used to generate a metallic materials knowledge graph, integrating structured information from DBpedia and Wikipedia. | Used to construct a knowledge graph based on WordNet, focusing on lexical and semantic relationships. | Used to build a multilingual knowledge graph, providing lexical and encyclopedic information for various languages. | Used to extract textual information for constructing a knowledge graph, complementing structured data from DBpedia.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 588863,
          "context_text": "MMKG [11] extracts data from DBpedia and Wikipedia to construct knowledge graph.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions DBpedia and Wikipedia as sources for constructing a knowledge graph, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1016/J.CPC.2016.07.005",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34f3dc2393f5bc166c66ceed5b915d816ad78e64",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Mentioned as a large-scale, multilingual knowledge base extracted from Wikipedia, but the specific usage in the research context is not detailed.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 1181640,
          "context_text": "And most of the text resources are associated with DBpedia [10].",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DBpedia' as a knowledge base, which is a specific, verifiable resource. However, it does not provide details on how it is used in the research context.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to construct a multi-modal knowledge graph, focusing on integrating structured and semi-structured data from the web.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 7278297,
          "context_text": ", 2008] and DBpedia [Auer et al., 2007]) or by automatic extraction from unstructured/semi-structured data (e.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions DBpedia, which is a well-known knowledge base derived from Wikipedia. However, it is not clear from the citation context alone whether DBpedia is used as a dataset or referenced as a method or discovery.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2015,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a pre-built large-scale knowledge base to extract information using RDF queries, supporting multi-modal knowledge graph reasoning. | Used as a large-scale Knowledge Base in natural language QA systems, providing structured data for question answering tasks. | Used as an external data source for multi-modal knowledge graph reasoning, providing structured information from Wikipedia to enrich the knowledge graph. | Serves as a nucleus for a web of open data, utilized in QA systems to enhance semantic understanding and answer retrieval.",
          "citing_paper_id": "206594383",
          "cited_paper_id": 7278297,
          "context_text": "The external data source that we use here is DBpedia [2].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "DBpedia is mentioned as an external data source, which is a well-known knowledge base. It fits the criteria for a dataset and is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/CVPR.2016.500",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/20dbdf02497aa84510970d0f5e8b599073bca1bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2015,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Mentioned as a collaboratively created graph database for structuring human knowledge, but the specific usage in the research context is not detailed. | Referenced as a knowledge graph derived from Wikipedia, but the specific application in the research is not specified. | Used for experiments in representation learning of knowledge bases, focusing on embedding instances and ontological concepts from the DBpedia knowledge graph. | Used to evaluate the effectiveness of the model design through an ablation study, focusing on the impact of different components on performance.",
          "citing_paper_id": "259341618",
          "cited_paper_id": 196187271,
          "context_text": "We conducted experiments over two datasets: one public dataset from DBpedia (Hao et al., 2019), which describes general concepts and ﬁne-grained entities crawled from DBpedia.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset from DBpedia, which is a public knowledge graph. The dataset is used for experiments involving representation learning of knowledge bases.",
          "citing_paper_doi": "10.48550/arXiv.2307.01933",
          "cited_paper_doi": "10.1145/3292500.3330838",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0c2052ea02e1916263841db5b9ca3b13e10ccd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64f82cf7dc68fe028f280c61cc9734ab299b031c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a collaboratively created graph database for structuring human knowledge, essential for knowledge-driven applications. | Used to construct a multilingual knowledge base from Wikipedia, WordNet, and GeoNames, focusing on entity linking and knowledge representation. | Used to store multilingual structured knowledge from Wikipedia, WordNet, and GeoNames, supporting applications like question answering and entity linking. | Used to integrate structured knowledge from Wikipedia, WordNet, and GeoNames, enhancing applications such as text generation and information extraction. | Used to store structured knowledge from Wikipedia, crucial for knowledge-driven applications like question answering and entity linking. | Used to store structured knowledge from Wikipedia, supporting applications like question answering and entity linking. | Used as a large-scale structured knowledge base, contributing to the construction and enrichment of knowledge graphs. | Used to store collaboratively created structured knowledge, supporting various knowledge-driven applications including entity linking and information extraction.",
          "citing_paper_id": "221995513",
          "cited_paper_id": 9770446,
          "context_text": "Knowledge graphs (KGs) such as DBpedia (Lehmann et al. 2015), YAGO (Rebele et al. 2016) and Freebase (Bollacker et al. 2008) store structured knowledge that is crucial to numerous knowledge-driven applications including question answering (Bordes, Chopra, and Weston 2014), entity linking…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific knowledge graphs that are used as structured knowledge sources for various applications. These are clearly identifiable and relevant datasets.",
          "citing_paper_doi": "10.1609/aaai.v35i5.16550",
          "cited_paper_doi": "10.1007/978-3-319-46547-0_19",
          "citing_paper_url": "https://www.semanticscholar.org/paper/083b2d08cfce5cf397a965c29168c78eb1ddb1cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/772f69b1dd966610edaa4ef2576d36142dc05928",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a collaboratively created graph database for structuring human knowledge, essential for knowledge-driven applications. | Used to store multilingual structured knowledge from Wikipedia, WordNet, and GeoNames, supporting applications like question answering and entity linking. | Used to integrate structured knowledge from Wikipedia, WordNet, and GeoNames, enhancing applications such as text generation and information extraction. | Used to store structured knowledge from Wikipedia, crucial for knowledge-driven applications like question answering and entity linking. | Used to store collaboratively created structured knowledge, supporting various knowledge-driven applications including entity linking and information extraction. | Used to store structured knowledge from Wikipedia, supporting applications like question answering and entity linking. | Used as a structured knowledge source for knowledge-driven applications, including question answering, leveraging its collaboratively created graph database.",
          "citing_paper_id": "221995513",
          "cited_paper_id": 207167677,
          "context_text": "Knowledge graphs (KGs) such as DBpedia (Lehmann et al. 2015), YAGO (Rebele et al. 2016) and Freebase (Bollacker et al. 2008) store structured knowledge that is crucial to numerous knowledge-driven applications including question answering (Bordes, Chopra, and Weston 2014), entity linking…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific knowledge graphs that are used as structured knowledge sources for various applications. These are clearly identifiable and relevant datasets.",
          "citing_paper_doi": "10.1609/aaai.v35i5.16550",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/083b2d08cfce5cf397a965c29168c78eb1ddb1cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2020,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used for diverse downstream tasks, including link prediction, question answering, and item recommendation, leveraging well-known cross-domain entities.",
          "citing_paper_id": "271961547",
          "cited_paper_id": 3766110,
          "context_text": "Various open KGs such as DBpedia[18], YAGO[21], and Wikidata[35] contain well-known cross-domain entities, which are applicable for diverse downstream tasks, e.g., link prediction[5], question answering[27], and item recommendation[36].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions open KGs (Knowledge Graphs) such as DBpedia, YAGO, and Wikidata, which are used for various downstream tasks including link prediction, question answering, and item recommendation.",
          "citing_paper_doi": "10.1145/3637528.3671941",
          "cited_paper_doi": "10.1145/3269206.3271739",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e546dbcee7e0b88e200cbae08769ffbeacabe837",
          "cited_paper_url": "https://www.semanticscholar.org/paper/76da7eab258081c257ebd87f7a559d44e31d8315",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to conduct experiments on two-view KGs, leveraging YAGO as a base ontology to explore multi-modal reasoning approaches. | Used to study two-view knowledge graphs, leveraging DBpedia as the base ontology for multi-modal reasoning tasks. | Used to study two-view knowledge graphs, leveraging YAGO as the base ontology for multi-modal reasoning tasks. | Employed for experiments on two-view KGs, incorporating Wikidata as a base ontology to assess multi-modal reasoning techniques. | Used for diverse downstream tasks, including link prediction, question answering, and item recommendation, leveraging well-known cross-domain entities. | Utilized for experiments on two-view KGs, using DBpedia as a base ontology to test multi-modal reasoning methodologies. | Used to study two-view knowledge graphs, leveraging Wikidata as the base ontology for multi-modal reasoning tasks.",
          "citing_paper_id": "271961547",
          "cited_paper_id": 6611164,
          "context_text": "Various open KGs such as DBpedia[18], YAGO[21], and Wikidata[35] contain well-known cross-domain entities, which are applicable for diverse downstream tasks, e.g., link prediction[5], question answering[27], and item recommendation[36].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions open KGs (Knowledge Graphs) such as DBpedia, YAGO, and Wikidata, which are used for various downstream tasks including link prediction, question answering, and item recommendation.",
          "citing_paper_doi": "10.1145/3637528.3671941",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e546dbcee7e0b88e200cbae08769ffbeacabe837",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c5b5adc3830ac45bf1d764603b1b71e5f729616",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used for diverse downstream tasks, including link prediction, question answering, and item recommendation, leveraging well-known cross-domain entities.",
          "citing_paper_id": "271961547",
          "cited_paper_id": 14941970,
          "context_text": "Various open KGs such as DBpedia[18], YAGO[21], and Wikidata[35] contain well-known cross-domain entities, which are applicable for diverse downstream tasks, e.g., link prediction[5], question answering[27], and item recommendation[36].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions open KGs (Knowledge Graphs) such as DBpedia, YAGO, and Wikidata, which are used for various downstream tasks including link prediction, question answering, and item recommendation.",
          "citing_paper_doi": "10.1145/3637528.3671941",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e546dbcee7e0b88e200cbae08769ffbeacabe837",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2024,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to conduct experiments on two-view KGs, leveraging YAGO as a base ontology to explore multi-modal reasoning approaches. | Used to study two-view knowledge graphs, leveraging DBpedia as the base ontology for multi-modal reasoning tasks. | Used to study two-view knowledge graphs, leveraging YAGO as the base ontology for multi-modal reasoning tasks. | Employed for experiments on two-view KGs, incorporating Wikidata as a base ontology to assess multi-modal reasoning techniques. | Used for diverse downstream tasks, including link prediction, question answering, and item recommendation, leveraging well-known cross-domain entities. | Utilized for experiments on two-view KGs, using DBpedia as a base ontology to test multi-modal reasoning methodologies. | Used to study two-view knowledge graphs, leveraging Wikidata as the base ontology for multi-modal reasoning tasks.",
          "citing_paper_id": "271961547",
          "cited_paper_id": null,
          "context_text": "Various open KGs such as DBpedia[18], YAGO[21], and Wikidata[35] contain well-known cross-domain entities, which are applicable for diverse downstream tasks, e.g., link prediction[5], question answering[27], and item recommendation[36].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions open KGs (Knowledge Graphs) such as DBpedia, YAGO, and Wikidata, which are used for various downstream tasks including link prediction, question answering, and item recommendation.",
          "citing_paper_doi": "10.1145/3637528.3671941",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e546dbcee7e0b88e200cbae08769ffbeacabe837",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "221246340",
      "citation_count": 0,
      "total_dataset_mentions": 21,
      "unique_datasets": [
        "Visual Genome"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to integrate language and vision modalities for visual-relational reasoning, providing a knowledge graph based on WordNet with annotations of objects, attributes, and relationships.",
          "citing_paper_id": "53957733",
          "cited_paper_id": 685050,
          "context_text": "[8, 26, 29, 33] which are required for more complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37]. The VisualGenome project [19] is a knowledge base that integrates language and vision modalities. The project provides a knowledge graph, based on WORDNET, which provides annotations of objects, att",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the VisualGenome project, which is a knowledge base integrating language and vision modalities. It is used for visual-relational reasoning and provides a knowledge graph based on WordNet.",
          "citing_paper_doi": "10.24432/C56P45",
          "cited_paper_doi": "10.1109/CVPR.2014.37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ee95e13e9379ff58baaada2764c58fbc831b1640",
          "citing_paper_year": 2017,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to integrate language and vision modalities for visual-relational reasoning, providing a knowledge graph based on WordNet with annotations of objects, attributes, and relationships.",
          "citing_paper_id": "53957733",
          "cited_paper_id": 7352553,
          "context_text": "[8, 26, 29, 33] which are required for more complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37]. The VisualGenome project [19] is a knowledge base that integrates language and vision modalities. The project provides a knowledge graph, based on WORDNET, which provides annotations of objects, att",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the VisualGenome project, which is a knowledge base integrating language and vision modalities. It is used for visual-relational reasoning and provides a knowledge graph based on WordNet.",
          "citing_paper_doi": "10.24432/C56P45",
          "cited_paper_doi": "10.1109/CVPR.2010.5540235",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/927432c50d920e647260c67506859d7845c7f729",
          "citing_paper_year": 2017,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to integrate language and vision modalities for visual-relational reasoning, providing a knowledge graph based on WordNet with annotations of objects, attributes, and relationships.",
          "citing_paper_id": "53957733",
          "cited_paper_id": 11928049,
          "context_text": "[8, 26, 29, 33] which are required for more complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about human-object, object-object, and object-attribute relationships [14, 9, 22, 35, 10, 6, 16, 37]. The VisualGenome project [19] is a knowledge base that integrates language and vision modalities. The project provides a knowledge graph, based on WORDNET, which provides annotations of objects, att",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the VisualGenome project, which is a knowledge base integrating language and vision modalities. It is used for visual-relational reasoning and provides a knowledge graph based on WordNet.",
          "citing_paper_doi": "10.24432/C56P45",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f99696049ebb0df7a20dbaa099b41833e6dca17",
          "citing_paper_year": 2017,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to extract object features from images using a pretrained Faster-RCNN model, focusing on object detection and feature extraction in multi-modal reasoning.",
          "citing_paper_id": "67856593",
          "cited_paper_id": 10328909,
          "context_text": "Image We use the pretrained Faster-RCNN [10] by [1] on Visual Genome [9] to extract objects features from each image.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions using a pretrained model on Visual Genome, which is a dataset. However, the primary focus is on the model and its application, not the dataset itself.",
          "citing_paper_doi": "10.1109/CVPR.2019.00209",
          "cited_paper_doi": "10.1109/TPAMI.2016.2577031",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cfc9ef5c7ef8056cff7bf1f1cfdd75e120f28231",
          "cited_paper_url": "https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used for training models to recognize objects, parts, and attributes, providing dense image annotations. | Used to connect language and vision using crowdsourced dense image annotations, specifically employing scene graphs for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "229339845",
          "cited_paper_id": 4492210,
          "context_text": "Following [53], we also use the scene graphs from VisualGenome [37] as another source of knowledge.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of scene graphs from VisualGenome, which is a specific dataset used for connecting language and vision.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01389",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used for training models to recognize objects, parts, and attributes, providing dense image annotations.",
          "citing_paper_id": "229339845",
          "cited_paper_id": 195441339,
          "context_text": "Therefore we run four classiﬁers and detectors trained on images from the following datasets: ImageNet [65] for objects, Places365 [86] for places, LVIS [28] for objects and object parts and Visual Genome [37] for objects, parts and attributes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions four datasets used for training classifiers and detectors for various visual recognition tasks. Each dataset is used for a specific purpose, such as object detection, place recognition, and attribute identification.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01389",
          "cited_paper_doi": "10.1109/CVPR.2019.00550",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f902a64f7d08aaa6bfca7463e8729952ddc6134e",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used as a pre-trained dataset for CNN to extract visual features, enhancing the model's ability to detect visual relationships. | Used to detect and recognize visual relationships in images, focusing on compositional phrases and their occurrences in real-world scenes. | Used to highlight differences in data content and distribution compared to the authors' multi-modal relationship dataset, emphasizing the unique characteristics of each. | Used to understand complex visual scenes, including objects, attributes, and relationships, supporting multi-modal reasoning tasks. | Used to recognize visual relationships, specifically evaluating the model's performance on identifying composite object pairs and their interactions. | Used for fine-tuning models on multi-object images, addressing the limitation of objects not listed in ImageNet, enhancing multi-modal reasoning capabilities. | Used to fine-tune the image feature extractor, enhancing the model's ability to recognize complex visual scenes and relationships. | Used to train models on multi-object images with a large number of classes, focusing on visual relationship detection using language priors. | Used to fine-tune the ResNet-152 parameters of a pretrained CNN model, focusing on visual relationship detection with language priors.",
          "citing_paper_id": "27494872",
          "cited_paper_id": 8701238,
          "context_text": "The reason of introducing Visual Genome for ne-tuning is that images of our dataset are multiobject and some objects are not listed in ImageNet.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for fine-tuning, and it is relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3123266.3123443",
          "cited_paper_doi": "10.1007/978-3-319-46448-0_51",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c54e00aadcdc8c4dcf556dbe4d30ff3952df94f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4d9506257186023b78cf19ed4f9e77a4ae4fa0f0",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to fine-tune the ResNet-152 parameters of a pretrained CNN model, focusing on visual relationship detection with language priors.",
          "citing_paper_id": "27494872",
          "cited_paper_id": 206594692,
          "context_text": "For pretrained CNN model, we ne-tune the ResNet-152 [14] parameters of the released CNN model on ImageNet with the images from Visual Genome [19].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for fine-tuning a pretrained CNN model. 'ImageNet' is also mentioned but in the context of the pretrained model, not as a dataset used in this specific research.",
          "citing_paper_doi": "10.1145/3123266.3123443",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c54e00aadcdc8c4dcf556dbe4d30ff3952df94f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to generate scene graphs from images with annotated objects and relationships, focusing on iterative message passing for improved graph generation. | Used to evaluate the performance of the proposed method on scene graph generation tasks, comparing mean and overall triplet recall with and without graph constraints.",
          "citing_paper_id": "210064217",
          "cited_paper_id": 1780254,
          "context_text": "Visual Genome [18] consists of 108,077 images with annotated objects (entities) and pairwise relationships (predicates), which is then post-processed by [37] to create scene graphs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions Visual Genome as a dataset with annotated images and relationships, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1007/978-3-030-58592-1_36",
          "cited_paper_doi": "10.1109/CVPR.2017.330",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3e5f240d634be018536e6419672daf38dd4d24bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34b73c1aa158b892bbe41705b4ae5bf01ecaea86",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to enhance scene graph generation by providing external commonsense knowledge, specifically to address the complexity of the task. | Used to parse scene graphs with global context, focusing on entity and predicate interactions in images.",
          "citing_paper_id": "210064217",
          "cited_paper_id": 4379400,
          "context_text": "esponds to a localized and categorized object (entity), and every edge encodes a pairwise interaction (predicate). This has inspired two lines of follow-up work, some improving the performance on SGG [24,31,52,47,23,43,11,10,2], and others exploiting 1 https://github.com/alirezazareian/gbnet arXiv:2001.02314v4 [cs.CV] 18 Jul 2020 2 Alireza Zareian et al. Fig.1. Left: An example of a Visual Genome image and its ground truth",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for scene graph parsing, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1007/978-3-030-58592-1_36",
          "cited_paper_doi": "10.1109/CVPR.2018.00611",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3e5f240d634be018536e6419672daf38dd4d24bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0da8af8d81e84381ffe656a0bbf2f3937ffac618",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Not used for training, but contains image-based scene graphs, relevant for multi-modal knowledge graph reasoning. | Not used for training, but contains dense captioning of actions in videos, relevant for multi-modal knowledge graph reasoning. | Used to highlight dataset bias in predicting relationships from object categories, serving as a strong baseline for comparison. | Used to connect language and vision using crowdsourced dense image annotations, providing a scaffold for scene graph representation. | Used to densely represent scenes with objects and visual relationships, contrasting with Action Genome's focus on annotating action segments in videos. | Used to represent scenes as graphs of objects and their relationships, enabling vision models to perform complex inference tasks by breaking down scenes into manageable components. | Used to derive temporally changing scene graph representations, focusing on connecting language and vision using crowdsourced dense image annotations.",
          "citing_paper_id": "209376177",
          "cited_paper_id": 4492210,
          "context_text": "We derive our representation as a temporally changing version of Visual Genome’s scene graphs [43].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual Genome’s scene graphs' which is a specific dataset used for connecting language and vision. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.1109/cvpr42600.2020.01025",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d1242ba8fdb994b82a0575dc92f30f7b26a75707",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to select salient image regions and extract region features, supporting multi-modal reasoning by integrating visual and textual information.",
          "citing_paper_id": "220265934",
          "cited_paper_id": 206594692,
          "context_text": "For the image, we adopt Faster R-CNN [31] (with ResNet-101 [32] backbone) pre-trained on the Visual-Genome dataset to select salient image regions and extract region features.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Visual-Genome dataset, which is a specific, verifiable dataset used for selecting salient image regions and extracting region features.",
          "citing_paper_doi": "10.1609/aaai.v35i4.16431",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to analyze motifs in scene graphs, specifically modeling intra-graph interactions using LSTMs to understand regularly appearing substructures in images.",
          "citing_paper_id": "254097189",
          "cited_paper_id": 4379400,
          "context_text": "Zellers et al. [67] analyze the role of motifs (i.e., regularly appearing substructures in scene graphs) in the Visual Genome dataset and propose to model these intra-graph interactions by LSTMs. Newell et al. [37] propose an end-to-end learning framework based on convolutional networks to predict…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Visual Genome dataset, which is a specific, verifiable dataset used for scene graph parsing and analyzing motifs in images.",
          "citing_paper_doi": "10.1145/3572914",
          "cited_paper_doi": "10.1109/CVPR.2018.00611",
          "citing_paper_url": "https://www.semanticscholar.org/paper/690fe20f24a3f78a241f8493e2f9ed5c74bc2849",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0da8af8d81e84381ffe656a0bbf2f3937ffac618",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to train the Faster-RCNN detector on 1600 object classes, connecting language and vision using crowdsourced dense image annotations.",
          "citing_paper_id": "204402762",
          "cited_paper_id": 3753452,
          "context_text": "Therefore, we take the Faster-RCNN [21] detector, which is trained on the Visual Genome dataset [16] by 1600 object classes in [2].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Visual Genome dataset' which is a specific, verifiable dataset used for training the Faster-RCNN detector. The dataset is relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/WACV45572.2020.9093614",
          "cited_paper_doi": "10.1109/CVPR.2018.00636",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c9f1b9e51c1d16c346e1ef679c243d61702a8a80",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to train the Faster-RCNN detector on 1600 object classes, connecting language and vision using crowdsourced dense image annotations.",
          "citing_paper_id": "204402762",
          "cited_paper_id": 4492210,
          "context_text": "Therefore, we take the Faster-RCNN [21] detector, which is trained on the Visual Genome dataset [16] by 1600 object classes in [2].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Visual Genome dataset' which is a specific, verifiable dataset used for training the Faster-RCNN detector. The dataset is relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/WACV45572.2020.9093614",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c9f1b9e51c1d16c346e1ef679c243d61702a8a80",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Employed for pre-training, offering a large-scale dataset of image-caption pairs to improve the model's ability to reason across modalities. | Used for pretraining, containing images with single-sentence captions, supporting the training of models for image captioning and retrieval. | Utilized for pre-training, providing a rich set of image annotations and descriptions to enhance multi-modal understanding. | Used for pre-training with a large set of image-text pairs, contributing to the development of multi-modal reasoning capabilities. | Used for pretraining, consisting of web images paired with descriptive captions, aiding in the development of image-text alignment models. | Used for pre-training, providing additional image-caption pairs to support the development of robust multi-modal reasoning systems. | Used for pretraining in multi-modal reasoning, providing a large set of images annotated with captions, object segmentations, and keypoint annotations. | Used for pretraining, offering dense image annotations including objects, attributes, and relationships, enhancing multi-modal understanding.",
          "citing_paper_id": "247011309",
          "cited_paper_id": 14579301,
          "context_text": "Following previous experimental protocols [7, 22], we use COCO [25], Visual Genome (VG) [21], Conceptual Captions (CC) [39], and SBU Captions [30] as the pretraining dataset in our study, where a total of 4.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four datasets used for pretraining in the study: COCO, Visual Genome, Conceptual Captions, and SBU Captions. These are all well-known datasets in the field of multi-modal learning.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01522",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0ad84c4bf7499df6945fc51b24ae2ac779f218ec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4",
          "citing_paper_year": 2022,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to connect structured image concepts with semantic relations, enhancing multi-modal knowledge graph reasoning through dense image annotations. | Used to train a model on a multi-modal graph, integrating visual and linguistic information for comprehensive knowledge representation. | Used to train a model on scene graphs, connecting language and vision using crowdsourced dense image annotations. | Used to train a model on a language-based graph, providing a large-scale knowledge base for natural language processing.",
          "citing_paper_id": "252917745",
          "cited_paper_id": 4492210,
          "context_text": "We train our model on three knowledge graph datasets, namely Visual-Genome [24] (scene graph), ConceptNet [46] (language-based graph), and VisualSem [2] (multi-modal graph), and also adopt part of datasets from CLIP to avoid the model forgetting problem.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions three knowledge graph datasets used for training a model, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.08901",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used as a high-quality multi-modal knowledge graph dataset for vision and language concepts, including entities with multilingual glosses, multiple illustrative images, and visually relevant relations. | Used to train a model on a multi-modal graph, integrating visual and linguistic information for comprehensive knowledge representation. | Used to train a model on scene graphs, connecting language and vision using crowdsourced dense image annotations. | Used to train a model on a language-based graph, providing a large-scale knowledge base for natural language processing.",
          "citing_paper_id": "252917745",
          "cited_paper_id": 221246340,
          "context_text": "We train our model on three knowledge graph datasets, namely Visual-Genome [24] (scene graph), ConceptNet [46] (language-based graph), and VisualSem [2] (multi-modal graph), and also adopt part of datasets from CLIP to avoid the model forgetting problem.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions three knowledge graph datasets used for training a model, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.08901",
          "cited_paper_doi": "10.18653/v1/2021.mrl-1.13",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9644fcac32c1f469f0dd9a02188fcbc206771203",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to provide location triples of visual objects, integrating visual and textual information in the multi-modal knowledge graph. | Employed to connect language and vision using dense image annotations, enhancing multi-modal reasoning capabilities. | Used to incorporate commonsense triples, enriching the multi-modal knowledge graph with general world knowledge. | Used to connect language and vision using crowdsourced dense image annotations, enhancing multi-modal knowledge graph reasoning. | Used to extract hasPart/isA triples, enhancing the explicit knowledge in a multi-modal knowledge graph with structured information. | Used to provide has-Part triples, contributing to the construction of explicit knowledge in a multi-modal knowledge graph. | Utilized for training and evaluating image-text alignment models, providing a large-scale dataset with diverse image captions. | Used for evaluating image-text matching, focusing on aligning textual descriptions with corresponding images in a multi-modal context. | Used to label objects with bounding boxes, connecting language and vision using crowdsourced dense image annotations. | Used to generate more fluent stories through logical inference between images, demonstrating superior performance compared to non-KG methods in connecting language and vision using crowdsourced dense image annotations.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 4492210,
          "context_text": "2: Examples of labeling images: (a) labeling components after image segmentation in Visipedia [58]; (b) labeling objects with bounding boxes in Visual Genome [59]; (c) labeling two objects where one is a part of the other in NEIL [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Visual Genome' as an example of labeling objects with bounding boxes, which is a specific dataset used for connecting language and vision.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to pre-train Faster R-CNN + ResNet-101 for object detection, focusing on visual content embedding in multi-modal reasoning tasks. | Used to pre-train a Faster R-CNN detector for generating input RoIs in VL-BERT, focusing on bottom-up attention mechanisms in multi-modal reasoning.",
          "citing_paper_id": "201317624",
          "cited_paper_id": 3753452,
          "context_text": "Visual content embedding is produced by Faster R-CNN + ResNet-101, initialized from parameters pre-trained on Visual Genome (Krishna et al., 2017) for object detection (see BUTD (Anderson et al., 2018)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for pre-training the Faster R-CNN + ResNet-101 model for object detection. No other datasets are explicitly mentioned.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2018.00636",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to pre-train Faster R-CNN + ResNet-101 for object detection, focusing on visual content embedding in multi-modal reasoning tasks. | Used to train VL-BERT on caption-image pairs, focusing on multi-modal reasoning by localizing and categorizing regions of interest in images.",
          "citing_paper_id": "201317624",
          "cited_paper_id": 10328909,
          "context_text": "Visual content embedding is produced by Faster R-CNN + ResNet-101, initialized from parameters pre-trained on Visual Genome (Krishna et al., 2017) for object detection (see BUTD (Anderson et al., 2018)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for pre-training the Faster R-CNN + ResNet-101 model for object detection. No other datasets are explicitly mentioned.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TPAMI.2016.2577031",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "10442573",
      "citation_count": 0,
      "total_dataset_mentions": 20,
      "unique_datasets": [
        "Freebase"
      ],
      "dataset_details": [
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to unify WordNet and Wikipedia, providing a core of semantic knowledge for AI applications, including potential multi-modal reasoning. | Used as a lexical database to support semantic and linguistic AI applications, potentially including multi-modal reasoning. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia to support AI applications, including potential multi-modal reasoning. | Used as a foundational knowledge base to support various AI applications, including potential multi-modal reasoning tasks.",
          "citing_paper_id": "255393926",
          "cited_paper_id": 1181640,
          "context_text": "Many knowledge graph projects such as Freebase (Bollacker et al. 2008), WordNet (Miller 1994), YAGO (Suchanek, Kasneci, and Weikum 2007) and DB-pedia (Lehmann et al. 2015) are signiﬁcant foundations to support artiﬁcial intelligence applications.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases but does not specify their use in multi-modal reasoning. However, they are significant foundations for AI applications, which could include multi-modal reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2301.00982",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/354b651dbc3ba2af4c3785ccbecd3df0585d30b2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to unify WordNet and Wikipedia, providing a core of semantic knowledge for AI applications, including potential multi-modal reasoning. | Used as a lexical database to support semantic and linguistic AI applications, potentially including multi-modal reasoning. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia to support AI applications, including potential multi-modal reasoning. | Used as a foundational knowledge base to support various AI applications, including potential multi-modal reasoning tasks.",
          "citing_paper_id": "255393926",
          "cited_paper_id": 207163173,
          "context_text": "Many knowledge graph projects such as Freebase (Bollacker et al. 2008), WordNet (Miller 1994), YAGO (Suchanek, Kasneci, and Weikum 2007) and DB-pedia (Lehmann et al. 2015) are signiﬁcant foundations to support artiﬁcial intelligence applications.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases but does not specify their use in multi-modal reasoning. However, they are significant foundations for AI applications, which could include multi-modal reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2301.00982",
          "cited_paper_doi": "10.1145/1242572.1242667",
          "citing_paper_url": "https://www.semanticscholar.org/paper/354b651dbc3ba2af4c3785ccbecd3df0585d30b2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on visual knowledge discovery and segmentation. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, focusing on general knowledge and entities. | Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on open information extraction from the web. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, focusing on providing a free, collaborative, multilingual, secondary database. | Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on continuous learning of knowledge from the web. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, focusing on extracting structured information from Wikipedia. | Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on linking Wikipedia and WordNet.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 207169186,
          "context_text": "Popular large-scale structured KBs are constructed either by manual-annotation/crowd-sourcing (e.g ., DBpedia [2], Freebase [6] and Wikidata [39]), or by automatically extracting from unstructured/semistructured data (e.g ., YAGO [22, 28], OpenIE [3, 14, 15], NELL [9], NEIL [10, 11]).",
          "confidence_score": 0.6,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases and extraction systems, but does not specify their usage in a particular research context. The names are plausible and specific, but the actual use case is not clear.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": "10.1145/1409360.1409378",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/498bb0efad6ec15dd09d941fb309aa18d6df9f5f",
          "citing_paper_year": 2015,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to construct multi-modal knowledge graphs, focusing on integrating structured data with textual and visual information.",
          "citing_paper_id": "235358423",
          "cited_paper_id": 76663467,
          "context_text": "Dataset and Evaluation Metric In the experiment, we use the datasets built in [26], which are extracted from FreeBase, DBpedia, and YAGO respectively.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions datasets built from FreeBase, DBpedia, and YAGO, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1016/J.NEUCOM.2021.03.132",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f2599cdebc880d5b499a22c82f96c951b65cbbb3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to introduce entity descriptions into the knowledge graph, extracting feature vectors of text using the CBOW model to learn knowledge from both text and structure simultaneously.",
          "citing_paper_id": "203605587",
          "cited_paper_id": 16447573,
          "context_text": "Proposed in [8], DKRL introduces entity descriptions from Freebase [2] to knowledge graph, it extracts feature vectors of text using CBOW [14] model, and learns knowledge from text and structure simultaneously.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Freebase' as a source of entity descriptions, which is a specific, verifiable dataset. It is used in the DKRL method to introduce entity descriptions into the knowledge graph.",
          "citing_paper_doi": "10.1109/IJCNN.2019.8852079",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/02ea29150d2526641bad18a0bc1932e6235c69a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f",
          "citing_paper_year": 2019,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to highlight the sparsity issue in knowledge bases, specifically the lack of birthplace information for person entries, which is relevant for evaluating the completeness of knowledge graphs.",
          "citing_paper_id": "258219651",
          "cited_paper_id": 4557963,
          "context_text": "For example, in Freebase more than 66% of the person entries are missing a birthplace [4].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions Freebase, which is a well-known knowledge base, but does not specify its use in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICTAI56018.2022.00074",
          "cited_paper_doi": "10.1145/2623330.2623623",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b5c311ac38a03458614bfb411aee114214939e4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to introduce entity descriptions into the knowledge graph, extracting feature vectors of text using the CBOW model to learn knowledge from both text and structure simultaneously.",
          "citing_paper_id": "203605587",
          "cited_paper_id": 31606602,
          "context_text": "Proposed in [8], DKRL introduces entity descriptions from Freebase [2] to knowledge graph, it extracts feature vectors of text using CBOW [14] model, and learns knowledge from text and structure simultaneously.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Freebase' as a source of entity descriptions, which is a specific, verifiable dataset. It is used in the DKRL method to introduce entity descriptions into the knowledge graph.",
          "citing_paper_doi": "10.1109/IJCNN.2019.8852079",
          "cited_paper_doi": "10.1609/aaai.v30i1.10329",
          "citing_paper_url": "https://www.semanticscholar.org/paper/02ea29150d2526641bad18a0bc1932e6235c69a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96acb1c882ad655c6b8459c2cd331803801446ca",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a real-world knowledge graph to support the development and evaluation of question answering systems over knowledge graphs.",
          "citing_paper_id": "258219651",
          "cited_paper_id": 246869684,
          "context_text": "In recent years, with the release of more and more real-world knowledge graphs such as Freebase [1] and DBpedia [2], as an important downstream task [3] of knowledge graphs, question answering over knowledge graph (KGQA) have received increasing attention.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Freebase' and 'DBpedia' as examples of real-world knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICTAI56018.2022.00074",
          "cited_paper_doi": "10.1016/j.ipm.2021.102818",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b5c311ac38a03458614bfb411aee114214939e4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/98e1474e00e5c57e11777449336885420435a873",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to represent explicit knowledge in structured form, where entities are nodes and relations are edges, facilitating multi-modal knowledge graph reasoning.",
          "citing_paper_id": "233219869",
          "cited_paper_id": 2687019,
          "context_text": "Typically, knowledge can be implicitly encoded in large language models (LMs) pre-trained on unstructured text (Petroni et al., 2019; Bosselut et al., 2019), or explicitly represented in structured knowledge graphs (KGs), such as Freebase (Bollacker et al., 2008) and ConceptNet (Speer et al., 2017), where entities are represented as nodes and relations between them as edges.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions Freebase and ConceptNet as structured knowledge graphs, which are relevant to multi-modal knowledge graph reasoning. These are specific, verifiable resources with clear provenance.",
          "citing_paper_doi": "10.18653/V1/2021.NAACL-MAIN.45",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3950df97ea527009a32569cb7016bc3df1383dca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d",
          "citing_paper_year": 2021,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a structured knowledge graph to represent entities and relations, supporting multi-modal reasoning by providing a rich, interconnected dataset of human knowledge. | Used as a structured knowledge graph to represent human knowledge, specifically for integrating and reasoning over multi-modal data in the research. | Utilized as a structured knowledge graph to represent entities and relations, enhancing multi-modal reasoning through a comprehensive and interconnected dataset of common-sense knowledge. | Used to represent explicit knowledge in structured form, where entities are nodes and relations are edges, facilitating multi-modal knowledge graph reasoning.",
          "citing_paper_id": "233219869",
          "cited_paper_id": 207167677,
          "context_text": "Typically, knowledge can be implicitly encoded in large language models (LMs) pre-trained on unstructured text (Petroni et al., 2019; Bosselut et al., 2019), or explicitly represented in structured knowledge graphs (KGs), such as Freebase (Bollacker et al., 2008) and ConceptNet (Speer et al., 2017), where entities are represented as nodes and relations between them as edges.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions Freebase and ConceptNet as structured knowledge graphs, which are relevant to multi-modal knowledge graph reasoning. These are specific, verifiable resources with clear provenance.",
          "citing_paper_doi": "10.18653/V1/2021.NAACL-MAIN.45",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3950df97ea527009a32569cb7016bc3df1383dca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a large, collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia and NNDB.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 207167677,
          "context_text": "• FreeBASE [227] is a large knowledge base generated from multiple sources, such as Wikipedia, NNDB, Fashion Model Directory, etc.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FreeBASE is identified as a large knowledge base used in the context of multi-modal knowledge graph reasoning, which aligns with the research topic.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a general knowledge graph to provide structured information about entities and their relationships. | Used as a domain-specific knowledge graph to provide structured information about music and artists. | Used as a domain-specific knowledge graph to provide structured information about movies and actors. | Used as a large-scale knowledge graph to integrate diverse sources of information. | Used as a general knowledge graph to represent concepts and their relationships. | Used as a general knowledge graph to enhance semantic understanding and reasoning over entities. | Used as a domain-specific knowledge graph to link drugs to their side effects, enhancing medical reasoning.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 10442573,
          "context_text": "…graph can be clas-siﬁed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB [37].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, including Freebase, YAGO, Knowledge Vault, Microsoft Concept Graph, SIDER, MusicBrainz, and IMDB. These are all specific, verifiable datasets or knowledge bases.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1093/nar/gkv1075",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1e62b537f3d30018e7979a89b0e0f15e2b6eecc",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to exemplify a domain-specific knowledge graph in the music industry, providing structured information about artists and recordings. | Used to exemplify a general knowledge graph, focusing on linking entities to Wikipedia and WordNet. | Used to exemplify a general knowledge graph, derived from web-scale data to represent concepts and their relationships. | Used to exemplify a general knowledge graph, emphasizing large-scale extraction and integration of facts. | Used to exemplify a domain-specific knowledge graph in the movie industry, containing detailed information about films and actors. | Used to exemplify a general knowledge graph, structured collaboratively to represent human knowledge. | Used to exemplify a domain-specific knowledge graph in the medical field, focusing on drug side effects.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 14775471,
          "context_text": "The knowledge graph can be clas-siﬁed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, which are specific, verifiable resources. These are used to classify knowledge graphs into general and domain-specific categories.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1145/2213836.2213891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/760103b363b1557372e048c4c31b5f01162bfcfa",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to exemplify a domain-specific knowledge graph in the music industry, providing structured information about artists and recordings. | Used to exemplify a general knowledge graph, focusing on linking entities to Wikipedia and WordNet. | Used to exemplify a general knowledge graph, derived from web-scale data to represent concepts and their relationships. | Used to exemplify a general knowledge graph, emphasizing large-scale extraction and integration of facts. | Used to exemplify a domain-specific knowledge graph in the movie industry, containing detailed information about films and actors. | Used to exemplify a general knowledge graph, structured collaboratively to represent human knowledge. | Used to exemplify a domain-specific knowledge graph in the medical field, focusing on drug side effects.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 207167677,
          "context_text": "The knowledge graph can be clas-siﬁed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, which are specific, verifiable resources. These are used to classify knowledge graphs into general and domain-specific categories.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2019,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to illustrate the challenges of constructing large-scale knowledge graphs, highlighting issues of time consumption and labor intensity.",
          "citing_paper_id": "250340389",
          "cited_paper_id": 207163173,
          "context_text": "Currently, constructing KGs of large scale (such as Freebase [5], YAGO [42], DBpedia [2] and Wikidata [50]) is exceptionally time-consuming and labor-intensive [23, 29], which leads to the incompleteness of KGs and further weakens the performance of downstream applications.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (KGs) but does not specify their use in multi-modal reasoning. However, they are verifiable resources and are relevant to the broader topic of knowledge graph construction and usage.",
          "citing_paper_doi": "10.1145/3477495.3531996",
          "cited_paper_doi": "10.1145/1242572.1242667",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8c720eb939259971bcb87f46c92ebb0d2d5497c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to illustrate the challenges of constructing large-scale knowledge graphs, highlighting issues of time consumption and labor intensity.",
          "citing_paper_id": "250340389",
          "cited_paper_id": 229377193,
          "context_text": "Currently, constructing KGs of large scale (such as Freebase [5], YAGO [42], DBpedia [2] and Wikidata [50]) is exceptionally time-consuming and labor-intensive [23, 29], which leads to the incompleteness of KGs and further weakens the performance of downstream applications.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (KGs) but does not specify their use in multi-modal reasoning. However, they are verifiable resources and are relevant to the broader topic of knowledge graph construction and usage.",
          "citing_paper_doi": "10.1145/3477495.3531996",
          "cited_paper_doi": "10.1145/2187980.2188242",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8c720eb939259971bcb87f46c92ebb0d2d5497c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1858073b92df5a796123d2e592670219d840c625",
          "citing_paper_year": 2022,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Mentioned as a web-scale knowledge graph, but no specific usage or research context is provided.",
          "citing_paper_id": "263830580",
          "cited_paper_id": 257220329,
          "context_text": "Web-scale knowledge graphs (KGs) [22] such as FreeBase [3], YAGO [39] and WikiData [47] are the wisdom essence and the infrastructure of the modern world wide web.",
          "confidence_score": 0.6,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge graphs but does not specify their use in the research context. These are general references to well-known KGs.",
          "citing_paper_doi": "10.1145/3664647.3681327",
          "cited_paper_doi": "10.1109/TPAMI.2024.3417451",
          "citing_paper_url": "https://www.semanticscholar.org/paper/582c2f270a6c0ce89679eebaa78797711fa20293",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Mentioned as a collaboratively created graph database for structuring human knowledge, but the specific usage in the research context is not detailed. | Referenced as a knowledge graph derived from Wikipedia, but the specific application in the research is not specified.",
          "citing_paper_id": "259341618",
          "cited_paper_id": 207167677,
          "context_text": "Knowledge graphs (KGs) like Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015) have motivated various knowledge-driven applications (Yasunaga et al., 2021; Lin et al., 2021).",
          "confidence_score": 0.6,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Freebase' and 'DBpedia' as examples of knowledge graphs, which are relevant to multi-modal knowledge graph reasoning. However, the context does not specify how these resources are used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2307.01933",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0c2052ea02e1916263841db5b9ca3b13e10ccd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2023,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a lexical database for English, offering a structured vocabulary and semantic relations that enhance the representation and reasoning capabilities in knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, providing a rich source of multi-relational data for knowledge graph reasoning.",
          "citing_paper_id": "219573823",
          "cited_paper_id": 1671874,
          "context_text": "Among multi-relational graphs, Knowledge Graphs (KGs) , which represent knowledge bases (KBs) such as Freebase [3] and WordNet [27], get the most attention.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Freebase' and 'WordNet' as examples of knowledge bases. These are specific, verifiable resources that are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/225d571f97f9097be436626f225cff572007aaf4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2020,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a lexical database for English, offering a structured vocabulary and semantic relations that enhance the representation and reasoning capabilities in knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, providing a rich source of multi-relational data for knowledge graph reasoning. | Used to validate OOG link prediction performance of GENs, focusing on the ability to predict links outside the training distribution in a knowledge graph completion task.",
          "citing_paper_id": "219573823",
          "cited_paper_id": 207167677,
          "context_text": "Among multi-relational graphs, Knowledge Graphs (KGs) , which represent knowledge bases (KBs) such as Freebase [3] and WordNet [27], get the most attention.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Freebase' and 'WordNet' as examples of knowledge bases. These are specific, verifiable resources that are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/225d571f97f9097be436626f225cff572007aaf4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2020,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "3509328",
      "citation_count": 0,
      "total_dataset_mentions": 16,
      "unique_datasets": [
        "ImageNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pretraining ResNet-152 and Faster-RCNN models for convolutional feature extraction, focusing on large-scale hierarchical image classification. | Used to connect language and vision using crowdsourced dense image annotations, enhancing multi-modal reasoning capabilities. | Used for pretraining Faster-RCNN for regional-level feature extraction, providing dense image annotations and connecting language and vision. | Utilized to provide more diverse descriptions, enriching the semantic understanding in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 4492210,
          "context_text": "Following the previous works [1], [17], [27], the proposed KAGS model adopts the ResNet-152 [15] pretrained on the ImageNet [28] dataset for convolutional feature extraction and utilizes the Faster-RCNN [16] pretrained on the ImageNet [28] dataset and the Visual Genome [23] dataset for regional-level feature extraction, where the original convolutional feature and the regional feature are a 7 7 2048 tensor and a 1 2048 tensor, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ImageNet and Visual Genome datasets, which are used for pretraining models for feature extraction. These datasets are directly relevant to the multi-modal knowledge graph reasoning topic.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pretraining ResNet-152 and Faster-RCNN models for convolutional feature extraction, focusing on large-scale hierarchical image classification. | Used for pretraining Faster-RCNN for regional-level feature extraction, providing dense image annotations and connecting language and vision.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 57246310,
          "context_text": "Following the previous works [1], [17], [27], the proposed KAGS model adopts the ResNet-152 [15] pretrained on the ImageNet [28] dataset for convolutional feature extraction and utilizes the Faster-RCNN [16] pretrained on the ImageNet [28] dataset and the Visual Genome [23] dataset for regional-level feature extraction, where the original convolutional feature and the regional feature are a 7 7 2048 tensor and a 1 2048 tensor, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ImageNet and Visual Genome datasets, which are used for pretraining models for feature extraction. These datasets are directly relevant to the multi-modal knowledge graph reasoning topic.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to implement GraphAdapter by decoupling the sub-graph with 1000 nodes for each modality into four graphs with 256 nodes to reduce computational cost. | Used for few-shot classification tasks, providing action recognition data to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large-scale hierarchical image database to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on pet species recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on fine-grained aircraft recognition to validate the GraphAdapter model. | Used to sample 20 classes for visualizing the distribution of nodes corresponding to textual fractures using t-SNE for classification. | Used for few-shot classification tasks, providing a large set of food categories to validate the GraphAdapter model. | Used for few-shot classification tasks, offering a diverse set of object categories to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on fine-grained car recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large set of scene categories to validate the GraphAdapter model.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 57246310,
          "context_text": "Following previous adapter-style studies [17, 71, 75], we validate our GraphAdapter on 11 few-shot classification tasks, including ImageNet [12], StandfordCars [28], UCF101 [57], Caltech101 [16], Flowers102 [45], SUN397 [68], DTD [11], EuroSAT [20], FGVCAircraft [42], OxfordPets [47], and Food101…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for few-shot classification tasks, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for few-shot classification tasks, providing action recognition data to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large-scale hierarchical image database to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on pet species recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on fine-grained aircraft recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large set of food categories to validate the GraphAdapter model. | Used for few-shot classification tasks, offering a diverse set of object categories to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on fine-grained car recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large set of scene categories to validate the GraphAdapter model.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 250698940,
          "context_text": "Following previous adapter-style studies [17, 71, 75], we validate our GraphAdapter on 11 few-shot classification tasks, including ImageNet [12], StandfordCars [28], UCF101 [57], Caltech101 [16], Flowers102 [45], SUN397 [68], DTD [11], EuroSAT [20], FGVCAircraft [42], OxfordPets [47], and Food101…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for few-shot classification tasks, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": "10.48550/arXiv.2207.09519",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a32887af7eb1fcfd3b3d892ad41f3516a37f11c1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Mentioned as a source for pre-trained CNN features used in image-text matching tasks, but specific usage details are not provided.",
          "citing_paper_id": "49867191",
          "cited_paper_id": 8039072,
          "context_text": "In the field of image-text matching, most recent methods directly use fixed CNN features [21, 28, 33, 36, 42, 48, 51, 52, 56, 66, 67, 70] as input, which are extracted from the models pre-trained on ImageNet.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet, which is a well-known dataset used for pre-training models. However, it does not specify how ImageNet is used in the current research context.",
          "citing_paper_doi": "10.1145/3383184",
          "cited_paper_doi": "10.1109/CVPR.2017.767",
          "citing_paper_url": "https://www.semanticscholar.org/paper/58555c7d168d1f50422ed9435d31ecd28d66eaa8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e1b6735f6ecb09e1d83b0aa9d2cde42993ee2eb0",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Mentioned as a source for pre-trained CNN features used in image-text matching tasks, but specific usage details are not provided.",
          "citing_paper_id": "49867191",
          "cited_paper_id": 897596,
          "context_text": "In the field of image-text matching, most recent methods directly use fixed CNN features [21, 28, 33, 36, 42, 48, 51, 52, 56, 66, 67, 70] as input, which are extracted from the models pre-trained on ImageNet.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet, which is a well-known dataset used for pre-training models. However, it does not specify how ImageNet is used in the current research context.",
          "citing_paper_doi": "10.1145/3383184",
          "cited_paper_doi": "10.1109/TPAMI.2018.2797921",
          "citing_paper_url": "https://www.semanticscholar.org/paper/58555c7d168d1f50422ed9435d31ecd28d66eaa8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/134f4f4d4a4830b96971952d9a8b06ba2da5cc02",
          "citing_paper_year": 2017,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Pre-trained models on ImageNet are used to extract fixed CNN features for image-text matching, focusing on leveraging existing visual recognition capabilities.",
          "citing_paper_id": "49867191",
          "cited_paper_id": 2930547,
          "context_text": "In the field of image-text matching, most recent methods directly use fixed CNN features [8]–[10], [16]–[22] as input which are extracted from the models pre-trained on ImageNet.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a source of pre-trained models, but does not indicate direct use of a specific dataset for the research. ImageNet is a dataset, but the context focuses on pre-trained models rather than the dataset itself.",
          "citing_paper_doi": "10.1145/3383184",
          "cited_paper_doi": "10.1007/s11263-015-0816-y",
          "citing_paper_url": "https://www.semanticscholar.org/paper/58555c7d168d1f50422ed9435d31ecd28d66eaa8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
          "citing_paper_year": 2017,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Pre-trained models on ImageNet are used to extract fixed CNN features for image-text matching, focusing on leveraging existing visual recognition capabilities.",
          "citing_paper_id": "49867191",
          "cited_paper_id": 3509328,
          "context_text": "In the field of image-text matching, most recent methods directly use fixed CNN features [8]–[10], [16]–[22] as input which are extracted from the models pre-trained on ImageNet.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a source of pre-trained models, but does not indicate direct use of a specific dataset for the research. ImageNet is a dataset, but the context focuses on pre-trained models rather than the dataset itself.",
          "citing_paper_doi": "10.1145/3383184",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/58555c7d168d1f50422ed9435d31ecd28d66eaa8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745",
          "citing_paper_year": 2017,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pre-train the ResNet model for encoding images into representations, focusing on capturing diverse visual features.",
          "citing_paper_id": "5083989",
          "cited_paper_id": 8457705,
          "context_text": "Encoder: Similar to previous work (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015; Venugopalan et al., 2017), we encode images into representations using a ResNet (He et al., 2016) model pre-trained on the ImageNet dataset (Deng et al., 2009) and use the outputs before the last…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ImageNet dataset, which is a well-known image dataset used for pre-training the ResNet model. The dataset is used for encoding images into representations.",
          "citing_paper_doi": "10.18653/v1/D18-1435",
          "cited_paper_doi": "10.1109/CVPR.2017.130",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e8feffa2280e41ceb864b940869c5408db89285",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b9aa3bafa9e8e21bb92908ae23b468fa248239b3",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pre-training a ResNet model to encode images into representations, focusing on the outputs before the last fully-connected layer.",
          "citing_paper_id": "5083989",
          "cited_paper_id": 206594692,
          "context_text": "Encoder: Similar to previous work (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015; Venugopalan et al., 2017), we encode images into representations using a ResNet (He et al., 2016) model pre-trained on the ImageNet dataset (Deng et al., 2009) and use the outputs before the last fully-connected layer.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ImageNet dataset, which is a well-known image classification dataset. It is used here for pre-training a ResNet model.",
          "citing_paper_doi": "10.18653/v1/D18-1435",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e8feffa2280e41ceb864b940869c5408db89285",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2018,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Pretrained ResNet-50 on ImageNet to extract feature vectors from image regions, focusing on deep residual learning for image recognition.",
          "citing_paper_id": "259165073",
          "cited_paper_id": 206594692,
          "context_text": "Here we use a ResNet-50 [17] pretrained on ImageNet [9] to catch the feature vector f𝑟 ∈ R𝑑𝑓 = 𝑓 (𝐼𝑟 ) of region 𝑟 , where 𝑓 (·) denotes the ResNet50 network.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet but does not indicate it is used as a dataset for the current research. It is only referenced as the dataset on which ResNet-50 was pretrained.",
          "citing_paper_doi": "10.1145/3580305.3599486",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/074c07282aedfd9a027bdea509df1d0490a9bbd3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used as an auxiliary dataset to pre-train a convolutional neural network (CNN) model, enhancing cross-modal representation learning.",
          "citing_paper_id": "27837890",
          "cited_paper_id": 2560991,
          "context_text": "which limits the training eectiveness and easily leads to overﬁtting. Some recent works take auxiliary singlemodal datasets to directly pre-train the network component for only one modality as [26], [28]. For example, ImageNet is adopted as an auxiliary dataset in the work of [26] to pre-train a convolutional neural networks (CNN) model, and then the images in cross-modal dataset are further used to",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as an auxiliary dataset used for pre-training a CNN model in a cited work. It is a specific, verifiable dataset.",
          "citing_paper_doi": "10.1109/TCYB.2018.2879846",
          "cited_paper_doi": "10.1109/CVPR.2016.321",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e5567742e53fd795cba76da8c23ca3123f9f8cee",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7e64992091458256f438fbe1bd44fffcc197b76c",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pre-train AlexNet, providing visual features for cross-modal retrieval tasks in the target domain. | Used to report performance metrics of cross-modal retrieval methods, specifically comparing the accuracy improvements of MHTN over other methods. | Used as an auxiliary dataset to pretrain a CNN model, enhancing the model's ability to learn visual features before fine-tuning on cross-modal data.",
          "citing_paper_id": "27837890",
          "cited_paper_id": 13618178,
          "context_text": "For example, ImageNet is adopted as an auxiliary dataset in the work of [23] to pretrain a convolutional neural network (CNN) model, and then the images in the cross-modal dataset are further used to fine-tune it.",
          "confidence_score": 1.0,
          "citation_intent": [
            "n",
            "b",
            "o",
            "r",
            "a",
            "i",
            " ",
            "f",
            "s",
            "d",
            "g",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet as an auxiliary dataset for pretraining a CNN model, which is relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TCYB.2018.2879846",
          "cited_paper_doi": "10.1109/TCYB.2016.2519449",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e5567742e53fd795cba76da8c23ca3123f9f8cee",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ce3c5b0691d9e05061adc0a6fed8acb2607b1423",
          "citing_paper_year": 2017,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to provide a collection of up to ten images per synset, supporting multi-modal reasoning by linking textual concepts with visual data.",
          "citing_paper_id": "44145776",
          "cited_paper_id": 57246310,
          "context_text": "Furthermore, for each synset a collection of up to ten images obtained from ImageNet (Deng et al., 2009) is provided.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet' as a source of images for each synset, which is a well-known dataset. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.18653/v1/S18-2027",
          "cited_paper_doi": "10.1109/CVPR.2009.5206848",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
          "citing_paper_year": 2018,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for few-shot classification tasks, providing action recognition data to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large-scale hierarchical image database to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on pet species recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on fine-grained aircraft recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large set of food categories to validate the GraphAdapter model. | Used for few-shot classification tasks, offering a diverse set of object categories to validate the GraphAdapter model. | Used for few-shot classification tasks, focusing on fine-grained car recognition to validate the GraphAdapter model. | Used for few-shot classification tasks, providing a large set of scene categories to validate the GraphAdapter model.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 253708136,
          "context_text": "Following previous adapter-style studies [17, 71, 75], we validate our GraphAdapter on 11 few-shot classification tasks, including ImageNet [12], StandfordCars [28], UCF101 [57], Caltech101 [16], Flowers102 [45], SUN397 [68], DTD [11], EuroSAT [20], FGVCAircraft [42], OxfordPets [47], and Food101…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for few-shot classification tasks, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": "10.1109/CVPR52729.2023.01049",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/04acada438826233ad10730a8b4cb4c2acd4e42d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used as a manually built visual knowledge base to support multi-modal reasoning in knowledge graphs, focusing on image classification and annotation. | Used as a manually built visual knowledge base to support multi-modal reasoning in knowledge graphs, focusing on fine-grained visual categorization and crowdsourced annotations.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 1569292,
          "context_text": "There are a few manually built visual knowledge bases like ImageNet [18] and Visipedia [30].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ImageNet' and 'Visipedia' as manually built visual knowledge bases. Both are specific, verifiable datasets.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1016/j.patrec.2015.11.023",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/80cdeed2db4a90f2706577fbce3a3abf9d77c995",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "8429835",
      "citation_count": 0,
      "total_dataset_mentions": 16,
      "unique_datasets": [
        "FB15K-237"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to associate crawled images with knowledge graphs, focusing on multi-modal reasoning and entity linking in the context of knowledge base inference.",
          "citing_paper_id": "53957733",
          "cited_paper_id": 5378837,
          "context_text": "Any KG derived from FB15k such as FB15k-237[Toutanova and Chen, 2015] can also be associated with the crawled images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FB15k-237 is mentioned as a specific dataset derived from FB15k, which is used in the context of associating crawled images with knowledge graphs.",
          "citing_paper_doi": "10.24432/C56P45",
          "cited_paper_doi": "10.18653/v1/W15-4007",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate zero-shot learning performance of various models, focusing on knowledge base completion tasks and comparing their effectiveness.",
          "citing_paper_id": "211003696",
          "cited_paper_id": 7557552,
          "context_text": "Model MRR Hits@10\n0% 5% 10% 20% 100% 0% 5% 10% 20% 100%\nMLN - - - - 0.10 - - - - 16.0 NTN 0.09 0.10 0.10 0.11 0.13 17.9 19.3 19.1 19.6 23.9 Neural LP 0.01 0.13 0.15 0.16 0.24 1.5 23.2 24.7 26.4 36.2 DistMult 0.23 0.24 0.24 0.24 0.31 40.0 40.4 40.7 41.4 48.5 ComplEx 0.24 0.24 0.24 0.25 0.32 41.1 41.3 41.9 42.5 51.1 TransE 0.24 0.25 0.25 0.25 0.33 42.7 43.1 43.4 43.9 52.7 RotatE 0.25 0.25 0.25 0.26 0.34 42.6 43.0 43.5 44.1 53.1 pLogicNet - - - - 0.33 - - - - 52.8\nExpressGNN-E 0.42 0.42 0.42 0.44 0.45 53.1 53.1 53.3 55.2 57.3 ExpressGNN-EM 0.42 0.42 0.43 0.45 0.49 53.8 54.6 55.3 55.6 60.8\nTable 4: Zero-shot learning performance on FB15K-237.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FB15K-237' as a dataset used for zero-shot learning performance evaluation. This is a well-known dataset in the field of knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/W17-2609",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c77faa8577817b26c1a2e81914f19eecbe13ee1f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c2e4d8f97d85783895db2c7066b6f3199c08c68c",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate zero-shot learning performance of various models, focusing on knowledge base completion tasks and comparing their effectiveness.",
          "citing_paper_id": "211003696",
          "cited_paper_id": 8429835,
          "context_text": "Model MRR Hits@10\n0% 5% 10% 20% 100% 0% 5% 10% 20% 100%\nMLN - - - - 0.10 - - - - 16.0 NTN 0.09 0.10 0.10 0.11 0.13 17.9 19.3 19.1 19.6 23.9 Neural LP 0.01 0.13 0.15 0.16 0.24 1.5 23.2 24.7 26.4 36.2 DistMult 0.23 0.24 0.24 0.24 0.31 40.0 40.4 40.7 41.4 48.5 ComplEx 0.24 0.24 0.24 0.25 0.32 41.1 41.3 41.9 42.5 51.1 TransE 0.24 0.25 0.25 0.25 0.33 42.7 43.1 43.4 43.9 52.7 RotatE 0.25 0.25 0.25 0.26 0.34 42.6 43.0 43.5 44.1 53.1 pLogicNet - - - - 0.33 - - - - 52.8\nExpressGNN-E 0.42 0.42 0.42 0.44 0.45 53.1 53.1 53.3 55.2 57.3 ExpressGNN-EM 0.42 0.42 0.43 0.45 0.49 53.8 54.6 55.3 55.6 60.8\nTable 4: Zero-shot learning performance on FB15K-237.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FB15K-237' as a dataset used for zero-shot learning performance evaluation. This is a well-known dataset in the field of knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c77faa8577817b26c1a2e81914f19eecbe13ee1f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/50d53cc562225549457cbc782546bfbe1ac6f0cf",
          "citing_paper_year": 2020,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to collect multi-modal information, enhancing the dataset with visual and textual data for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "222278475",
          "cited_paper_id": 76663467,
          "context_text": "We collect the multi-modal information of FB15k-237 according to [22].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FB15k-237 is a well-known dataset in the field of knowledge graphs, and the context indicates it is used for collecting multi-modal information.",
          "citing_paper_doi": "10.1145/3394171.3413736",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b5da714dc0f27029fb724131121cc57c56fbc0de",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate knowledge graph embedding models, comparing performance metrics such as MRR and hits at k=10 across various models including LowFER-1, R-GCN, RotatE, DistMult, and ComplEx.",
          "citing_paper_id": "221082536",
          "cited_paper_id": 5458500,
          "context_text": "For FB15k-237, LowFER-1 ( ∼ 3M parameters) outperforms R-GCN, RotatE, DistMult and ComplEx by an average of 5.9% on MRR, and it additionally outperforms convolutional models (ConvE, HypER) at k = 10 with only + 0.8M parameters.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FB15k-237, which is a well-known dataset used for evaluating knowledge graph embedding models. The dataset is used to compare the performance of different models.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-319-93417-4_38",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0126fce30b412d583f8e33714908dd09b86293d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd8a9914d50b0ac63315872530274d158d6aff09",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to train and evaluate the performance of multi-modal reasoning models, focusing on entity linking and relation prediction tasks in knowledge graphs. | This dataset 'FB15k-237' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 52967399,
          "context_text": "We collect the textual information for entities of FB15k-237, WN18RR, DB15K, and YAGO15K from KG-BERT [46].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used to collect textual information for entities. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.18653/v1/N19-1423",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate multi-modal reasoning methods, focusing on the task of entity prediction and relation prediction.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 262690390,
          "context_text": "We also find out that our method gets a faster convergence speed compared with the CNN-based model ConvE [13] and the GNN-based model WGCN [33] on the FB15k-237 dataset.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the FB15k-237 dataset for comparing convergence speeds of different models. The dataset is a well-known benchmark for knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.1609/AAAI.V33I01.33013060",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8f3812a4f2093b488fa4bd3adda3cea0e770dc3d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to test the model's capability in handling image and text modalities, focusing on cross-modal relation prediction. | Used to evaluate multi-relational data modeling, focusing on link prediction tasks in knowledge graphs. | Used to assess the performance of the model on wordnet relations, specifically evaluating the ability to predict semantic relationships. | Used to evaluate multi-modal reasoning, combining textual and visual information in knowledge graph tasks. | Used to evaluate the proposed knowledge graph completion model, focusing on entity and relation prediction in a large-scale knowledge graph.",
          "citing_paper_id": "252918783",
          "cited_paper_id": 14941970,
          "context_text": "To evaluate the proposed model, we conduct experiments on three widely used KGC datasets: FB15K-237 (Toutanova et al., 2015), WN18 (Bordes et al., 2013), and WN9-IMG (Xie et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for evaluating a knowledge graph completion model. These datasets are clearly named and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.08821",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f399dce9bbb2754d33c5aefb60bfe54f260a4b21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2022,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate the performance of MoSE against KG-BERT and MKGformer in knowledge graph completion tasks, focusing on relation prediction accuracy.",
          "citing_paper_id": "252918783",
          "cited_paper_id": 248524814,
          "context_text": "It is worth noting that even compared to the pre-trained language model methods, MoSE outperforms KG-BERT and MKGformer in all metrics on FB15K-237 and WN18 datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, FB15K-237 and WN18, which are used to evaluate the performance of different models in knowledge graph completion tasks.",
          "citing_paper_doi": "10.48550/arXiv.2210.08821",
          "cited_paper_doi": "10.1145/3477495.3531992",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f399dce9bbb2754d33c5aefb60bfe54f260a4b21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bedcfb163368f2d802de3e892acb34cc5a75a22d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate the performance of COMP GCN in knowledge graph reasoning, focusing on link prediction tasks and comparing against existing methods. | Used for evaluating multi-modal knowledge graph reasoning models, focusing on link prediction tasks with a reduced version of Freebase. | Used to assess the effectiveness of knowledge graph embeddings, particularly in handling relational patterns and avoiding inverse relations. | Used to evaluate knowledge graph embedding models, focusing on link prediction performance and robustness against false positives. | Used to evaluate the performance of COMP GCN on knowledge graph reasoning tasks, focusing on link prediction accuracy across multiple metrics. | Utilized for assessing the performance of knowledge graph embedding models, specifically targeting relation prediction in WordNet. | Used to assess the effectiveness of COMP GCN in knowledge graph reasoning, particularly in link prediction tasks, using various evaluation metrics. | Used to assess the effectiveness of COMP GCN in knowledge graph reasoning, particularly in link prediction, and benchmarking against other models.",
          "citing_paper_id": "207847719",
          "cited_paper_id": 4328400,
          "context_text": "In our experiments, we utilize FB15k-237 (Toutanova & Chen, 2015) and WN18RR (Dettmers et al., 2018) datasets for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation clearly mentions the use of FB15k-237 and WN18RR datasets for evaluation in the experiments.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1609/aaai.v32i1.11573",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4b244a6778c95b1df8e9e02332ff8d22e675f628",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9697d32ed0a16da167f2bdba05ef96d0da066eb5",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate model performance on link prediction tasks, focusing on mean reciprocal rank (MRR) and hit rates at various thresholds. | Used to compare model performance on link prediction tasks, specifically evaluating hit rates at various thresholds and comparing results to baselines.",
          "citing_paper_id": "59516071",
          "cited_paper_id": 2768038,
          "context_text": "FB15k-237 MRR MRR Hit@1 Hit@3 Hit@10 (raw) (filter) (filter) (filter) (filter) DistMult[ For WN18 (Table 3), CrossE achieves Hit@10 results that are comparable to the best baselines.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions FB15k-237 and WN18, which are known datasets in the field of knowledge graphs. However, the context does not provide specific details on how these datasets are used beyond reporting performance metrics.",
          "citing_paper_doi": "10.1145/3289600.3291014",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/74de6f71cd60d8f1b057aa5bd99e8ee350822af8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86412306b777ee35aba71d4795b02915cb8a04c3",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate knowledge graph embedding models, specifically comparing performance of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE on link prediction tasks.",
          "citing_paper_id": "257631615",
          "cited_paper_id": 3882054,
          "context_text": "For FB15K-237, we directly obtain the results of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT and TuckER from the re-evaluation work [30] and run the models of QuatE, HAKE, IKRL, MKGC and MKBE with their released codes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FB15K-237, which is a well-known dataset used for knowledge graph completion tasks. The dataset is used to evaluate various models, including TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE.",
          "citing_paper_doi": "10.1145/3543507.3583554",
          "cited_paper_doi": "10.18653/v1/N18-2053",
          "citing_paper_url": "https://www.semanticscholar.org/paper/607d9eb8ceab0533bcfc65e8aced69aee4e40976",
          "cited_paper_url": "https://www.semanticscholar.org/paper/41cbe3cdfee7539f8ff20144e6bd028ef762e00c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate knowledge graph embedding models, specifically comparing performance of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE on link prediction tasks.",
          "citing_paper_id": "257631615",
          "cited_paper_id": 52160797,
          "context_text": "For FB15K-237, we directly obtain the results of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT and TuckER from the re-evaluation work [30] and run the models of QuatE, HAKE, IKRL, MKGC and MKBE with their released codes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FB15K-237, which is a well-known dataset used for knowledge graph completion tasks. The dataset is used to evaluate various models, including TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE.",
          "citing_paper_doi": "10.1145/3543507.3583554",
          "cited_paper_doi": "10.18653/v1/D18-1359",
          "citing_paper_url": "https://www.semanticscholar.org/paper/607d9eb8ceab0533bcfc65e8aced69aee4e40976",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate knowledge graph embedding models, specifically comparing performance of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE on link prediction tasks.",
          "citing_paper_id": "257631615",
          "cited_paper_id": 208201975,
          "context_text": "For FB15K-237, we directly obtain the results of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT and TuckER from the re-evaluation work [30] and run the models of QuatE, HAKE, IKRL, MKGC and MKBE with their released codes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FB15K-237, which is a well-known dataset used for knowledge graph completion tasks. The dataset is used to evaluate various models, including TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE.",
          "citing_paper_doi": "10.1145/3543507.3583554",
          "cited_paper_doi": "10.1609/AAAI.V34I03.5701",
          "citing_paper_url": "https://www.semanticscholar.org/paper/607d9eb8ceab0533bcfc65e8aced69aee4e40976",
          "cited_paper_url": "https://www.semanticscholar.org/paper/81e82158510edb72680a9a832df970d2743d9a63",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to evaluate knowledge graph embedding models, specifically comparing performance of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE on link prediction tasks.",
          "citing_paper_id": "257631615",
          "cited_paper_id": 207852450,
          "context_text": "For FB15K-237, we directly obtain the results of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT and TuckER from the re-evaluation work [30] and run the models of QuatE, HAKE, IKRL, MKGC and MKBE with their released codes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FB15K-237, which is a well-known dataset used for knowledge graph completion tasks. The dataset is used to evaluate various models, including TransE, ConvE, ConvKB, CapsE, RotatE, KBAT, TuckER, QuatE, HAKE, IKRL, MKGC, and MKBE.",
          "citing_paper_doi": "10.1145/3543507.3583554",
          "cited_paper_doi": "10.18653/v1/2020.acl-main.489",
          "citing_paper_url": "https://www.semanticscholar.org/paper/607d9eb8ceab0533bcfc65e8aced69aee4e40976",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fb3fa2a6a8c0d7a58b1d5dee8b676104d1a5da6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FB15K-237",
          "dataset_description": "Used to reuse visual and textual features for knowledge base and text inference, focusing on the subset of FB15K.",
          "citing_paper_id": "257631615",
          "cited_paper_id": 5378837,
          "context_text": "FB15K-2376 [31] is a subset of FB15K, the visual and textual features in FB15K can be directly reused.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FB15K-237 is identified as a dataset, and the context mentions its relationship to FB15K, indicating it is used for reusing visual and textual features.",
          "citing_paper_doi": "10.1145/3543507.3583554",
          "cited_paper_doi": "10.18653/v1/W15-4007",
          "citing_paper_url": "https://www.semanticscholar.org/paper/607d9eb8ceab0533bcfc65e8aced69aee4e40976",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "229339845",
      "citation_count": 0,
      "total_dataset_mentions": 15,
      "unique_datasets": [
        "ConceptNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to retrieve relevant linguistic subgraphs for multi-modal reasoning in VQA-GNN, enhancing the model's ability to understand and answer questions about images. | Used to connect language and vision using dense image annotations, providing a rich source of multimodal data for VQA systems. | Used to retrieve relevant linguistic subgraphs for multi-modal reasoning, enhancing the connection between language and visual context. | Used to retrieve relevant visual subgraphs for multi-modal reasoning, providing dense image annotations and structured visual context. | Used to provide general knowledge and reasoning scaffolds for integrating knowledge graphs into VQA systems, enhancing the ability to answer complex questions. | Used to extract and represent visual context through scene graphs, providing a structured representation of the visual elements in images for VQA-GNN.",
          "citing_paper_id": "263895473",
          "cited_paper_id": 4492210,
          "context_text": "Knowledge graphs (KGs) such as ConceptNet (Speer et al., 2017) and VisualGenome (Krishna et al., 2017) are readily-available resources to provide explicit background knowledge and reasoning scaffolds (Yasunaga et al., 2021), motivating research in integrating KGs into VQA systems.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two knowledge graphs, ConceptNet and VisualGenome, which are used to provide background knowledge and reasoning scaffolds for integrating KGs into VQA systems.",
          "citing_paper_doi": "10.48550/arXiv.2205.11501",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9d846ed3e6cd0e60f50ebc547318815ebb2bb9ca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to retrieve relevant linguistic subgraphs for multi-modal reasoning in VQA-GNN, enhancing the model's ability to understand and answer questions about images. | Used to connect language and vision using dense image annotations, providing a rich source of multimodal data for VQA systems. | Used to retrieve relevant linguistic subgraphs for multi-modal reasoning, enhancing the connection between language and visual context. | Used to retrieve relevant visual subgraphs for multi-modal reasoning, providing dense image annotations and structured visual context. | Used to retrieve concept-level knowledge in conjunction with image-level knowledge, enhancing multi-modal reasoning by integrating general-domain knowledge (38 words) | Used to provide general knowledge and reasoning scaffolds for integrating knowledge graphs into VQA systems, enhancing the ability to answer complex questions. | Used to retrieve 1-hop neighbor nodes and prune irrelevant nodes in a multi-modal knowledge graph reasoning system, enhancing semantic relevance. | Used to extract and represent visual context through scene graphs, providing a structured representation of the visual elements in images for VQA-GNN.",
          "citing_paper_id": "263895473",
          "cited_paper_id": 15206880,
          "context_text": "Knowledge graphs (KGs) such as ConceptNet (Speer et al., 2017) and VisualGenome (Krishna et al., 2017) are readily-available resources to provide explicit background knowledge and reasoning scaffolds (Yasunaga et al., 2021), motivating research in integrating KGs into VQA systems.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two knowledge graphs, ConceptNet and VisualGenome, which are used to provide background knowledge and reasoning scaffolds for integrating KGs into VQA systems.",
          "citing_paper_doi": "10.48550/arXiv.2205.11501",
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9d846ed3e6cd0e60f50ebc547318815ebb2bb9ca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Contributes part-whole relationships to enhance the knowledge base for multi-modal reasoning tasks. | Used to connect visual and textual data, providing dense image annotations and scene graphs for multi-modal reasoning. | Used to train and evaluate knowledge-based visual question answering models, containing 32,910 images and 157,201 QA pairs. | Provides structured data extracted from Wikipedia, used to enrich multi-modal reasoning with semantic information. | Serves as a comprehensive textual resource for extracting factual information and context for multi-modal reasoning tasks. | Used to train and evaluate knowledge-routed visual question reasoning models, focusing on integrating external knowledge with visual and textual information. | Used to connect visual and textual data through dense image annotations, supporting multi-modal reasoning and knowledge extraction. | Used to extract structured knowledge for multi-modal reasoning, enhancing the understanding of relationships between entities. | Serves as the image source for the KRVQA dataset, providing 32,910 images with dense annotations connecting language and vision. | Provided the images used in the KRVQA dataset, serving as a rich source of visual content for multi-modal reasoning tasks.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 4492210,
          "context_text": "Several models such as KG-AUG [16], KRISP [24], RVL [34], and MAVEx [43] extract relevant knowledge from multiple knowledge bases, including ConceptNet [19], Wikipedia [42], DBpedia [2], VisualGenome [15] and haspartKB [11].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases used by various models for multi-modal reasoning. These knowledge bases are specific, verifiable resources that are publicly accessible.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Contributes part-whole relationships to enhance the knowledge base for multi-modal reasoning tasks. | Used to connect visual and textual data, providing dense image annotations and scene graphs for multi-modal reasoning. | Used to acquire comparative commonsense knowledge, enhancing the reasoning capabilities of the models with web-derived information. | Provides practical commonsense reasoning, integrating a wide range of knowledge to support the models' reasoning tasks. | Used to provide a large, multilingual knowledge graph that enhances VQA models with semantic and relational information, improving reasoning accuracy. | Serves as a nucleus for a web of open data, providing structured information from Wikipedia for knowledge graph reasoning. | Used to enhance VQA models with comparative commonsense knowledge, specifically integrating web-derived information to improve accuracy. | Provides structured data extracted from Wikipedia, used to enrich multi-modal reasoning with semantic information. | Serves as a comprehensive textual resource for extracting factual information and context for multi-modal reasoning tasks. | Serves as a nucleus for a web of open data, providing structured information to enrich the knowledge base of the models. | Used to acquire comparative commonsense knowledge, focusing on relational and conceptual information for multi-modal reasoning. | Used to connect visual and textual data through dense image annotations, supporting multi-modal reasoning and knowledge extraction. | Used to augment VQA models with structured knowledge from Wikipedia, enhancing the models' ability to reason about entities and their relationships. | Used to extract structured knowledge for multi-modal reasoning, enhancing the understanding of relationships between entities. | Provides a large, multilingual knowledge graph of common-sense assertions, enhancing the models' ability to reason about everyday concepts.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 7278297,
          "context_text": "Several models such as KG-AUG [16], KRISP [24], RVL [34], and MAVEx [43] extract relevant knowledge from multiple knowledge bases, including ConceptNet [19], Wikipedia [42], DBpedia [2], VisualGenome [15] and haspartKB [11].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases used by various models for multi-modal reasoning. These knowledge bases are specific, verifiable resources that are publicly accessible.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Contributes part-whole relationships to enhance the knowledge base for multi-modal reasoning tasks. | Used to connect visual and textual data, providing dense image annotations and scene graphs for multi-modal reasoning. | Utilized to augment visual question answering with structured common-sense knowledge, enhancing the model's reasoning capabilities. | Used as a knowledge base to enhance visual question answering by providing contextual information and background knowledge. | Provides structured data extracted from Wikipedia, used to enrich multi-modal reasoning with semantic information. | Serves as a comprehensive textual resource for extracting factual information and context for multi-modal reasoning tasks. | Used to connect visual and textual data through dense image annotations, supporting multi-modal reasoning and knowledge extraction. | Used to extract structured knowledge for multi-modal reasoning, enhancing the understanding of relationships between entities.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 222278428,
          "context_text": "Several models such as KG-AUG [16], KRISP [24], RVL [34], and MAVEx [43] extract relevant knowledge from multiple knowledge bases, including ConceptNet [19], Wikipedia [42], DBpedia [2], VisualGenome [15] and haspartKB [11].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases used by various models for multi-modal reasoning. These knowledge bases are specific, verifiable resources that are publicly accessible.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1145/3394171.3413943",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Contributes part-whole relationships to enhance the knowledge base for multi-modal reasoning tasks. | Used to connect visual and textual data, providing dense image annotations and scene graphs for multi-modal reasoning. | Provides structured data extracted from Wikipedia, used to enrich multi-modal reasoning with semantic information. | Serves as a comprehensive textual resource for extracting factual information and context for multi-modal reasoning tasks. | Used to connect visual and textual data through dense image annotations, supporting multi-modal reasoning and knowledge extraction. | Used to extract structured knowledge for multi-modal reasoning, enhancing the understanding of relationships between entities.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 231627467,
          "context_text": "Several models such as KG-AUG [16], KRISP [24], RVL [34], and MAVEx [43] extract relevant knowledge from multiple knowledge bases, including ConceptNet [19], Wikipedia [42], DBpedia [2], VisualGenome [15] and haspartKB [11].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases used by various models for multi-modal reasoning. These knowledge bases are specific, verifiable resources that are publicly accessible.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/48847adb786cb8a193818aca8519a887680c2d83",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Serves as a visual knowledge source to support multi-modal reasoning in the MAVEx system. | Serves as a source of visual information, complementing textual data to support multi-modal reasoning and answer validation. | Used as a structured knowledge graph to enhance the reasoning capabilities of the VQA system, integrating semantic relationships. | Used to connect visual and textual data, providing dense image annotations and scene graphs for multi-modal reasoning. | Used as an external knowledge source to enhance multi-modal reasoning in VQA models, specifically to improve answer accuracy. | Used as a textual knowledge source for multi-modal answer validation in knowledge-based VQA, providing contextual information. | Used as a visual data source to complement textual information, aiding in the validation of answers in VQA tasks. | Contributes part-whole relationships to enhance the knowledge base for multi-modal reasoning tasks. | Utilized as a commonsense reasoning tool-kit to enhance the knowledge retrieval capabilities of MAVEx. | Utilized to provide structured common-sense knowledge, supporting the system's reasoning capabilities in validating answers. | Used to extract structured knowledge for multi-modal reasoning, enhancing the understanding of relationships between entities. | Serves as a primary VQA dataset, combined with OKVQA to train and evaluate multi-modal knowledge-based VQA models. | Used as a knowledge source to retrieve diverse information for multi-modal reasoning in the MAVEx system. | Supplements Wikipedia and ConceptNet to provide visual context, improving the accuracy of multi-modal VQA models. | Provides structured data extracted from Wikipedia, used to enrich multi-modal reasoning with semantic information. | Serves as a comprehensive textual resource for extracting factual information and context for multi-modal reasoning tasks. | Integrated with Wikipedia to provide structured common-sense knowledge, enhancing the performance of VQA models in multi-modal reasoning tasks. | Used alongside VQAv2 to provide additional training data and knowledge, specifically for multi-modal reasoning tasks in VQA. | Used to connect visual and textual data through dense image annotations, supporting multi-modal reasoning and knowledge extraction. | Used as a knowledge base to retrieve textual information for multi-modal reasoning, enhancing the system's ability to understand and validate answers.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 232320648,
          "context_text": "Several models such as KG-AUG [16], KRISP [24], RVL [34], and MAVEx [43] extract relevant knowledge from multiple knowledge bases, including ConceptNet [19], Wikipedia [42], DBpedia [2], VisualGenome [15] and haspartKB [11].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases used by various models for multi-modal reasoning. These knowledge bases are specific, verifiable resources that are publicly accessible.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1609/aaai.v36i3.20174",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8dce342a435034fa0521b24b61393397df95c095",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Contributes part-whole relationships to enhance the knowledge base for multi-modal reasoning tasks. | Used to connect visual and textual data, providing dense image annotations and scene graphs for multi-modal reasoning. | Used to acquire comparative commonsense knowledge, enhancing the reasoning capabilities of the models with web-derived information. | Provides practical commonsense reasoning, integrating a wide range of knowledge to support the models' reasoning tasks. | Serves as a visual knowledge source to support multi-modal reasoning in the MAVEx system. | Used to introduce structured commonsense knowledge into models, enhancing reasoning capabilities through a practical toolkit. | Utilized as a commonsense reasoning tool-kit to enhance the knowledge retrieval capabilities of MAVEx. | Used to retrieve fact-based knowledge and construct a fact graph, enhancing multi-modal reasoning by integrating commonsense knowledge. | Used as a knowledge source to retrieve diverse information for multi-modal reasoning in the MAVEx system. | Provides structured data extracted from Wikipedia, used to enrich multi-modal reasoning with semantic information. | Serves as a comprehensive textual resource for extracting factual information and context for multi-modal reasoning tasks. | Serves as a nucleus for a web of open data, providing structured information to enrich the knowledge base of the models. | Used to extract structured knowledge for multi-modal reasoning, enhancing the understanding of relationships between entities. | Used as an unstructured knowledge base to provide additional context and information, enriching the model's understanding and reasoning.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 266028051,
          "context_text": "Several models such as KG-AUG [16], KRISP [24], RVL [34], and MAVEx [43] extract relevant knowledge from multiple knowledge bases, including ConceptNet [19], Wikipedia [42], DBpedia [2], VisualGenome [15] and haspartKB [11].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases used by various models for multi-modal reasoning. These knowledge bases are specific, verifiable resources that are publicly accessible.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1023/B:BTTJ.0000047600.45421.6D",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b3fea597033a46d5ae282464a8f16d6715187e70",
          "citing_paper_year": 2023,
          "cited_paper_year": 2004
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to extract a subgraph focusing on general knowledge, serving as the initial skeleton for constructing TIVA-KG. | Used to extract the basic topology of TIVA-KG, focusing on relational knowledge gathered from multiple sources, enhancing the multi-modal reasoning capabilities.",
          "citing_paper_id": "264492337",
          "cited_paper_id": 46745196,
          "context_text": "Thebasictopologyof TIVA-KGis extractedfrom ConceptNet[32], a publicly available single modality knowledge graph carrying general knowledge via texts, which is gathered from multiple sources.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ConceptNet is identified as a publicly available single modality knowledge graph used to extract the basic topology of TIVA-KG. It is a specific, verifiable resource.",
          "citing_paper_doi": "10.1145/3581783.3612266",
          "cited_paper_doi": "10.1007/978-3-642-35085-6_6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac7474000c867ea3b0ad64bf7301e102621a2afb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1382e951677ae167638eb3c97054bb704e3bc359",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to provide location triples of visual objects, integrating spatial information into the knowledge graph. | Used to provide commonsense triples, enriching the knowledge graph with everyday knowledge. | Used to provide hasPart triples, contributing to the construction of a knowledge graph for open-domain VQA. | Used to provide hasPart and isA triples, enhancing the knowledge graph with structured information.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 229339845,
          "context_text": "For example, the explicit knowledge in [51] has four sources: hasPart triples from hasPart KB [163], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used as sources of explicit knowledge for a knowledge graph. Each dataset is clearly identified and used for a specific purpose.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01389",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to assess QA-GNN's performance on open book question answering, emphasizing the integration of scientific facts and reasoning. | Utilized as a knowledge graph to provide common sense knowledge for reasoning tasks, enhancing the ability to answer complex questions. | Serves as the knowledge graph for QA-GNN evaluations, providing structured common sense and scientific knowledge for reasoning tasks. | Used to evaluate QA-GNN on commonsense reasoning questions, focusing on the model's ability to integrate external knowledge. | Used to assess QA-GNN's ability to answer questions requiring scientific knowledge, emphasizing reasoning over a knowledge graph. | Used to evaluate QA-GNN on commonsense reasoning tasks, focusing on multi-hop inference over knowledge graphs. | Used to provide an additional corpus of scientific facts in textual form for open book question answering, enhancing the knowledge base for reasoning tasks.",
          "citing_paper_id": "233219869",
          "cited_paper_id": 52183757,
          "context_text": ", 2019) and OpenBookQA (Mihaylov et al., 2018), using the ConceptNet KG (Speer et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions OpenBookQA and ConceptNet KG, both of which are relevant to multi-modal knowledge graph reasoning. OpenBookQA is a dataset, and ConceptNet is a knowledge graph.",
          "citing_paper_doi": "10.18653/V1/2021.NAACL-MAIN.45",
          "cited_paper_doi": "10.18653/v1/D18-1260",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3950df97ea527009a32569cb7016bc3df1383dca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1536e8958697c5364f68b2e2448905dbbeb3a0ca",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Leveraged as an external knowledge corpus to enhance relation learning, providing abundant object and relation knowledge for the multi-modal knowledge graph. | Used to extract semantic relations for multi-modal knowledge graph reasoning, enhancing the understanding of object interactions and related concepts. | Used to represent commonsense knowledge in a graph format, focusing on nodes as concepts and edges as relations, enhancing multi-modal reasoning capabilities. | Leveraged as an external knowledge corpus to understand and enhance object relations in a graph neural network framework, focusing on general knowledge across multiple languages. | Used to build the object graph by exploring intra-event object relations, enhancing fine-grained event information through external knowledge integration.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 15206880,
          "context_text": "To better understand the relations, we leverage an external knowledge corpus ConceptNet [41] which has abundant object and relation knowledge to conduct the relation learning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ConceptNet is mentioned as an external knowledge corpus used for relation learning, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to prepare initial KG node embeddings for multi-hop relational reasoning, focusing on the general commonsense domain.",
          "citing_paper_id": "252968266",
          "cited_paper_id": 218486837,
          "context_text": "For the ConceptNet knowledge graph used in the general commonsense domain (§3), we follow the method of MHGRN [33] to prepare the initial KG node embeddings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of the ConceptNet knowledge graph for preparing initial KG node embeddings, which is directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": "10.18653/v1/2020.emnlp-main.99",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e979667aa81c294062c02ab3a48e87e47c54987",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to analyze the presence of stereotypes in the knowledge graph, specifically focusing on representational harms in commonsense knowledge resources.",
          "citing_paper_id": "252968266",
          "cited_paper_id": 232307375,
          "context_text": "Second, the ConceptNet knowledge graph [7] used in this work has been shown to encode stereotypes [83], rather than completely clean commonsense knowledge.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the ConceptNet knowledge graph, which is a specific, verifiable resource. It is used to highlight the presence of stereotypes in the knowledge graph, which is relevant to the research topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": "10.18653/v1/2021.emnlp-main.410",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26b7eacd6aaff6c2bd1beac40b96597fb1d29a1e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Mentioned as a commonly used knowledge base built using textual information, but no specific usage or research context is provided. | Mentioned as a commonly used knowledge base built using textual information, but no specific usage in the current research context is described.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 14775471,
          "context_text": "Most of the commonly used knowledge bases such as Word-Net, ConceptNet, WebChild [16], Probase [17] and NELL are built using only textual information.",
          "confidence_score": 0.5,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases, but does not specify their usage in the current research context. The mention is more about the nature of these resources rather than their application.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1145/2213836.2213891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/760103b363b1557372e048c4c31b5f01162bfcfa",
          "citing_paper_year": 2020,
          "cited_paper_year": 2012
        }
      ]
    },
    {
      "cited_paper_id": "221193809",
      "citation_count": 0,
      "total_dataset_mentions": 14,
      "unique_datasets": [
        "DBP15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used for cross-lingual entity alignment, specifically to align entities across different languages while preserving their attributes. The dataset is utilized to train and evaluate models for joint attribute-preserving embedding. | Used to evaluate cross-lingual entity alignment methods, focusing on entities with attached images. The dataset is utilized to assess the performance of attribute-preserving embedding techniques. | Used to evaluate entity alignment in multi-modal knowledge graphs, focusing on the challenges of aligning entities across different modalities and ensuring attribute preservation.",
          "citing_paper_id": "260334664",
          "cited_paper_id": 19197215,
          "context_text": "DBP15K [28] contains three datasets ( R sa = 0 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions DBP15K, which is a known dataset for cross-lingual entity alignment. The context indicates that it contains multiple datasets, but only DBP15K is explicitly named.",
          "citing_paper_doi": "10.48550/arXiv.2307.16210",
          "cited_paper_doi": "10.1007/978-3-319-68288-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c186baf0ece4167bcb2a5fd0a7272e4a51583dd8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to demonstrate model stability with CLIP vision encoder, focusing on vision feature dimension of 512 in multi-modal knowledge graph entity alignment. | Used to demonstrate model stability with ResNet-152 vision encoder, focusing on vision feature dimension of 2048 in multi-modal knowledge graph entity alignment. | Used to evaluate entity alignment in multi-modal knowledge graphs, focusing on the challenges of aligning entities across different modalities and ensuring attribute preservation.",
          "citing_paper_id": "260334664",
          "cited_paper_id": 221193809,
          "context_text": "(ii) To demonstrate model stability, following [4,20], the vision encoders Enc v are set to ResNet-152 [11] on DBP15K where the vision feature dimension d v is 2048 , and set to CLIP [25] on Multi-OpenEA with d v = 512 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'DBP15K' and 'Multi-OpenEA' as datasets used for demonstrating model stability. These are specific datasets used in multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.16210",
          "cited_paper_doi": "10.1007/978-3-030-55130-8_12",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ca3072dad2ee809c8fc3639e6fc1728b46f9ef66",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to demonstrate model stability with CLIP vision encoder, focusing on vision feature dimension of 512 in multi-modal knowledge graph entity alignment. | Used to evaluate model performance across bilingual, monolingual, and high-degree categories using ResNet-152 and CLIP visual encoders, focusing on standard and iterative settings. | Used to evaluate model stability with ResNet-152 vision encoder, focusing on vision feature dimension of 2048. | Used to evaluate the performance of four models under two settings (standard and iterative) using pre-trained visual encoders (ResNet-152 and CLIP). | Used to demonstrate model stability with ResNet-152 vision encoder, focusing on vision feature dimension of 2048 in multi-modal knowledge graph entity alignment. | Used to evaluate model stability with CLIP vision encoder, focusing on vision feature dimension of 512.",
          "citing_paper_id": "260334664",
          "cited_paper_id": 231591445,
          "context_text": "(ii) To demonstrate model stability, following [4,20], the vision encoders Enc v are set to ResNet-152 [11] on DBP15K where the vision feature dimension d v is 2048 , and set to CLIP [25] on Multi-OpenEA with d v = 512 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'DBP15K' and 'Multi-OpenEA' as datasets used for demonstrating model stability. These are specific datasets used in multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.16210",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to demonstrate model stability with CLIP vision encoder, focusing on vision feature dimension of 512 in multi-modal knowledge graph entity alignment. | Used to demonstrate model stability with ResNet-152 vision encoder, focusing on vision feature dimension of 2048 in multi-modal knowledge graph entity alignment.",
          "citing_paper_id": "260334664",
          "cited_paper_id": 252070586,
          "context_text": "(ii) To demonstrate model stability, following [4,20], the vision encoders Enc v are set to ResNet-152 [11] on DBP15K where the vision feature dimension d v is 2048 , and set to CLIP [25] on Multi-OpenEA with d v = 512 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'DBP15K' and 'Multi-OpenEA' as datasets used for demonstrating model stability. These are specific datasets used in multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.16210",
          "cited_paper_doi": "10.48550/arXiv.2209.00891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0658b1f4478021c6aa932f7685a614192a9fcf4e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to evaluate the model's stability with VGG-16 vision encoder, focusing on vision feature dimension 4096 for entity alignment. | Used to evaluate the model's stability with ResNet-152 vision encoder, focusing on vision feature dimension 2048 for entity alignment.",
          "citing_paper_id": "255340818",
          "cited_paper_id": 206594692,
          "context_text": "(ii) To demonstrate the model’s stability, following [4, 20], the vision encoders 𝐸𝑛𝑐 𝑣 are set to ResNet-152 [16] on DBP15K following EVA/MCLEA, where the vision feature dimension 𝑑 𝑣 is 2048, and set to VGG-16 [26] on FBDB15K/FBYG15K with 𝑑 𝑣 = 4096.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (DBP15K, FBDB15K, FBYG15K) used for evaluating the model's stability with different vision encoders. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3611786",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2978e7e2159549b9b6be5a6d0fa456bb902aeed1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to demonstrate model stability with ResNet-152 vision encoder, focusing on entity alignment with a vision feature dimension of 2048. | Used to evaluate the model's stability with VGG-16 vision encoder, focusing on vision feature dimension 4096 for entity alignment. | Used to evaluate the model's stability with ResNet-152 vision encoder, focusing on vision feature dimension 2048 for entity alignment. | Used to demonstrate model stability with VGG-16 vision encoder, focusing on entity alignment with a vision feature dimension of 4096.",
          "citing_paper_id": "255340818",
          "cited_paper_id": 252070586,
          "context_text": "(ii) To demonstrate the model’s stability, following [4, 20], the vision encoders 𝐸𝑛𝑐 𝑣 are set to ResNet-152 [16] on DBP15K following EVA/MCLEA, where the vision feature dimension 𝑑 𝑣 is 2048, and set to VGG-16 [26] on FBDB15K/FBYG15K with 𝑑 𝑣 = 4096.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (DBP15K, FBDB15K, FBYG15K) used for evaluating the model's stability with different vision encoders. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3611786",
          "cited_paper_doi": "10.48550/arXiv.2209.00891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2978e7e2159549b9b6be5a6d0fa456bb902aeed1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0658b1f4478021c6aa932f7685a614192a9fcf4e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to evaluate the model's stability with ResNet-152 vision encoder, focusing on vision feature dimensions and alignment in multi-modal knowledge graphs. | Used to evaluate the model's stability with ResNet-152 vision encoder, focusing on vision feature dimension 2048 for entity alignment. | Used to evaluate the model's stability with VGG-16 vision encoder, focusing on vision feature dimensions and alignment in multi-modal knowledge graphs. | Used to evaluate the model's stability with VGG-16 vision encoder, focusing on vision feature dimension 4096 for entity alignment. | Used for monolingual experiments in multi-modal knowledge graph reasoning, focusing on entity alignment and link prediction tasks. | Used for training multi-modal knowledge graph alignments with varying percentages of reference entity alignment pairs, specifically 20%, 50%, and 80%.",
          "citing_paper_id": "255340818",
          "cited_paper_id": null,
          "context_text": "(ii) To demonstrate the model’s stability, following [4, 20], the vision encoders 𝐸𝑛𝑐 𝑣 are set to ResNet-152 [16] on DBP15K following EVA/MCLEA, where the vision feature dimension 𝑑 𝑣 is 2048, and set to VGG-16 [26] on FBDB15K/FBYG15K with 𝑑 𝑣 = 4096.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (DBP15K, FBDB15K, FBYG15K) used for evaluating the model's stability with different vision encoders. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3611786",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2978e7e2159549b9b6be5a6d0fa456bb902aeed1",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used for evaluating cross-lingual entity alignment methods, focusing on aligning entities across different languages using joint attribute-preserving embedding techniques. | Utilized for assessing the performance of entity alignment algorithms, particularly in the context of cross-lingual knowledge graph alignment using graph convolutional networks.",
          "citing_paper_id": "202770936",
          "cited_paper_id": 19197215,
          "context_text": "Following (Sun et al., 2017, 2018; Wang et al., 2018), we use the DBP15K and the DWY100K datasets for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, DBP15K and DWY100K, which are used for evaluation in the context of cross-lingual entity alignment and knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/D19-1274",
          "cited_paper_doi": "10.1007/978-3-319-68288-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc9f702abca7d2c164e2a4207a835f27098fa63b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c186baf0ece4167bcb2a5fd0a7272e4a51583dd8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used for evaluating cross-lingual entity alignment methods, focusing on aligning entities across different languages using joint attribute-preserving embedding techniques. | Utilized for assessing the performance of entity alignment algorithms, particularly in the context of cross-lingual knowledge graph alignment using graph convolutional networks.",
          "citing_paper_id": "202770936",
          "cited_paper_id": 51605357,
          "context_text": "Following (Sun et al., 2017, 2018; Wang et al., 2018), we use the DBP15K and the DWY100K datasets for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, DBP15K and DWY100K, which are used for evaluation in the context of cross-lingual entity alignment and knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/D19-1274",
          "cited_paper_doi": "10.24963/ijcai.2018/611",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc9f702abca7d2c164e2a4207a835f27098fa63b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d899e434a7f2eecf33a90053df84cf32842fbca9",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used for evaluating cross-lingual entity alignment methods, focusing on aligning entities across different languages using joint attribute-preserving embedding techniques. | Used to evaluate cross-lingual knowledge graph alignment methods, focusing on Hits@N performance between YAGO and GeoNames entities. | Used to evaluate cross-lingual knowledge graph alignment methods, focusing on Hits@N performance between Chinese and English entities. | Utilized for assessing the performance of entity alignment algorithms, particularly in the context of cross-lingual knowledge graph alignment using graph convolutional networks.",
          "citing_paper_id": "202770936",
          "cited_paper_id": 53082628,
          "context_text": "Following (Sun et al., 2017, 2018; Wang et al., 2018), we use the DBP15K and the DWY100K datasets for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, DBP15K and DWY100K, which are used for evaluation in the context of cross-lingual entity alignment and knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/D19-1274",
          "cited_paper_doi": "10.18653/v1/D18-1032",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cc9f702abca7d2c164e2a4207a835f27098fa63b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/89c650ccb646bdcce886e96e837db3767855c6d2",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used for entity alignment, selecting top-20 candidate target entities for alignment tasks in larger-scale knowledge graphs. | Used to evaluate entity alignment methods, focusing on cross-lingual knowledge base linking and alignment accuracy. | Used to evaluate entity alignment methods, focusing on cross-lingual knowledge graph alignment with graph information. | Used to evaluate entity alignment methods across multilingual knowledge graphs, focusing on performance metrics such as precision, recall, and F1 scores. | Used to evaluate the joint embedding method for entity alignment, combining graph and textual information to improve alignment accuracy. | Used to set the dimensionality of topological, relation, and attribute embeddings for entity alignment in knowledge bases, focusing on optimizing embedding dimensions for better alignment performance. | Used for entity alignment, selecting top-200 candidate target entities for alignment tasks in smaller-scale knowledge graphs.",
          "citing_paper_id": "202121966",
          "cited_paper_id": 43100819,
          "context_text": "@10 @50 DBP15K Hao et al. (2016) 21.2 42.7 56.7 19.5 39.3 53.2 18.9 39.9 54.2 17.8 38.4 52.4 15.3 38.8 56.5 14.6 37.2 54.0 Chen et al. (2017a) 30.8 61.4 79.1 24.7 52.4 70.4 27.8 57.4 75.9 23.7 49.9 67.9 24.4 55.5 74.4 25.2 46.0 57.9 25.2 45.9 57.9 28.6 52.6 62.2 28.5 53.0 63.0 32.8 60.9 70.0 32.9…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'DBP15K' which is a known dataset used for entity alignment in multi-lingual knowledge graphs. The context also includes performance metrics, suggesting the dataset is used for evaluation.",
          "citing_paper_doi": "10.18653/v1/D19-1451",
          "cited_paper_doi": "10.1007/978-981-10-3168-7_1",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf4dcc7d67f0776ffe7aa5b4ee3217f9bd757282",
          "cited_paper_url": "https://www.semanticscholar.org/paper/15e3ac44e82541ecbfd67f9fda2b680aed8a8ac0",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to evaluate entity alignment methods across multilingual knowledge graphs, focusing on performance metrics such as precision, recall, and F1 scores.",
          "citing_paper_id": "202121966",
          "cited_paper_id": 51987187,
          "context_text": "@10 @50 DBP15K Hao et al. (2016) 21.2 42.7 56.7 19.5 39.3 53.2 18.9 39.9 54.2 17.8 38.4 52.4 15.3 38.8 56.5 14.6 37.2 54.0 Chen et al. (2017a) 30.8 61.4 79.1 24.7 52.4 70.4 27.8 57.4 75.9 23.7 49.9 67.9 24.4 55.5 74.4 25.2 46.0 57.9 25.2 45.9 57.9 28.6 52.6 62.2 28.5 53.0 63.0 32.8 60.9 70.0 32.9…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DBP15K' which is a known dataset used for entity alignment in multi-lingual knowledge graphs. The context also includes performance metrics, suggesting the dataset is used for evaluation.",
          "citing_paper_doi": "10.18653/v1/D19-1451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf4dcc7d67f0776ffe7aa5b4ee3217f9bd757282",
          "cited_paper_url": "https://www.semanticscholar.org/paper/625c98e41774da0afe714dd4f6fb6900e7a1b37e",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to conduct cross-lingual entity alignment experiments, focusing on joint attribute-preserving embedding methods.",
          "citing_paper_id": "221995513",
          "cited_paper_id": 19197215,
          "context_text": "The experiments are conducted on DBP15k (Sun, Hu, and Li 2017) and DWY15k (Guo, Sun, and Hu 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, DBP15k and DWY15k, which are used for conducting experiments. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1609/aaai.v35i5.16550",
          "cited_paper_doi": "10.1007/978-3-319-68288-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/083b2d08cfce5cf397a965c29168c78eb1ddb1cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c186baf0ece4167bcb2a5fd0a7272e4a51583dd8",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Specifically used for Chinese-English entity alignment, evaluating the performance of cross-lingual alignment methods. | Used for cross-lingual entity alignment, providing multi-lingual datasets from DBpedia to train and evaluate alignment models. | Used to analyze entity alignment, focusing on the neighboring entities of aligned entity pairs to understand their distribution and relationships. | Specifically used for French-English entity alignment, testing the robustness of cross-lingual alignment algorithms. | Specifically used for Japanese-English entity alignment, assessing the effectiveness of alignment techniques across languages.",
          "citing_paper_id": "208176414",
          "cited_paper_id": 19197215,
          "context_text": "• DBP15K (Sun, Hu, and Li 2017) has three datasets built from multi-lingual DBpedia, namely DBP ZH-EN (Chinese-English), DBP JA-EN (Japanese-English) and DBP FR-EN (French-English).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets derived from multi-lingual DBpedia, which are used for cross-lingual entity alignment.",
          "citing_paper_doi": "10.1609/AAAI.V34I01.5354",
          "cited_paper_doi": "10.1007/978-3-319-68288-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8f5bac59311dc32ed75d76587ff60df00e1be502",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c186baf0ece4167bcb2a5fd0a7272e4a51583dd8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "3441497",
      "citation_count": 0,
      "total_dataset_mentions": 13,
      "unique_datasets": [
        "MSCOCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used for pre-training vision-and-language models, providing a large set of annotated images and captions to improve multimodal reasoning. | Used to provide image data for vision-and-language tasks, focusing on common objects in context. The dataset supports multi-modal reasoning by linking textual descriptions to visual content. | Used for pre-training vision-and-language models, offering a richly annotated dataset with detailed scene graphs and relationships to enhance multimodal understanding. | Used to provide richly annotated image data for vision-and-language tasks, enhancing multi-modal reasoning through detailed scene graphs and object relationships.",
          "citing_paper_id": "201103729",
          "cited_paper_id": 14113767,
          "context_text": "1, we aggregate pre-training data from ﬁve vision-and-language datasets whose images come from MS COCO (Lin et al., 2014) or Visual Genome (Krishna et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, MS COCO and Visual Genome, which are used for pre-training in vision-and-language tasks.",
          "citing_paper_doi": "10.18653/v1/D19-1514",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to categorize common objects in images, specifically focusing on 80 categories for object detection and image captioning tasks.",
          "citing_paper_id": "204972196",
          "cited_paper_id": 14113767,
          "context_text": ", 80 categories in MS COCO [18]).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "MS COCO is a well-known dataset used for object detection and image captioning. The citation indicates it is used for categorizing common objects in images.",
          "citing_paper_doi": "10.1109/ICCV.2019.00438",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0a7bb3c251498a7c700c5f0563a53aea54345653",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to provide visual concepts for answering 'Visual' questions, focusing on object recognition and classification tasks. | Used to train attributes for visual recognition, prioritizing higher importance in the research context. | Used to train the visual concept detector, focusing on common objects in context. | Used to train the visual concept detector, focusing on object recognition in images. | Used to train a Fast-RCNN detector for object detection in images, specifically focusing on 80-class objects for multi-modal reasoning tasks. | Used for multi-modal reasoning, specifically to provide image captions, enhancing the understanding of visual and textual data in a unified framework. | Used for multi-modal reasoning, specifically to provide class labels and image captions, enhancing the understanding of visual and textual data in a unified framework. | The COCO-VQA data is used to train and evaluate the LSTM model for visual question answering, focusing on generating accurate answers based on image features and questions. | Used to provide visual concepts for answering 'Visual' questions, focusing on object detection and captioning tasks. | Used to train a Fast-RCNN detector for object detection in images, specifically focusing on 200-class objects for multi-modal reasoning tasks. | Used to evaluate multi-modal reasoning models, focusing on rich contextual information and diverse object classes in images. | This dataset 'COCO-VQA' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 14113767,
          "context_text": "We select 700 of the validation images from the MS COCO [Lin et al., 2014] dataset due to the rich contextual information and diverse object classes therein.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MS COCO dataset, which is a well-known dataset used for object detection and image captioning. It is clearly identified and used for its rich contextual information and diverse object classes.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2015,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Employed for person re-identification using textual descriptions, aiding in matching images of individuals with their textual descriptions. | Utilized for object detection and image captioning, offering a large set of images annotated with bounding boxes and captions. | Used to classify image/text groups into 113,287 classes, demonstrating the robustness and effectiveness of instance loss in multi-modal classification tasks. | Used for image captioning, providing images paired with descriptive sentences to train and evaluate multi-modal models. | Used to train and evaluate multi-modal models, focusing on image-caption alignment and generation, with a large set of images and corresponding descriptions. | Used to train and evaluate models on image-text pairs, focusing on multi-modal reasoning and convergence without hyper-parameter tuning.",
          "citing_paper_id": "49867191",
          "cited_paper_id": 14113767,
          "context_text": "The robustness and effectiveness of instance loss are demonstrated by classifying each image/text group into one of the 113,287 classes on MSCOCO [41].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MSCOCO' which is a well-known dataset used for multi-modal tasks involving images and text. The citation demonstrates the use of this dataset for classification tasks.",
          "citing_paper_doi": "10.1145/3383184",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/58555c7d168d1f50422ed9435d31ecd28d66eaa8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2017,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used for multi-modal image and caption analysis, providing a large-scale dataset with diverse images and annotations. | Utilized for image captioning and multi-modal reasoning, providing a dataset with images and corresponding descriptive captions.",
          "citing_paper_id": "218551030",
          "cited_paper_id": 88166,
          "context_text": "0), MSCOCO (Lin et al., 2014), FDDB (Jain and Learned-Miller, 2010), LFW (Huang et al., 2008), Oxf105k (Philbin et al., 2007), YoutubeBB (Real et al., 2017), and Flickr30k (Plum-mer et al., 2015).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple datasets by name, all of which are specific and verifiable. Each dataset is used in the context of multi-modal learning, particularly for image and video recognition tasks.",
          "citing_paper_doi": "10.18653/v1/2020.acl-demos.11",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/51c8975d88aa66781300e8ca88272ab3112445c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3",
          "citing_paper_year": 2020,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to evaluate the accuracy of visual grounding methods in the GAIA system, focusing on fine-grained multimedia knowledge extraction. | Used to train detectors with supervised data, focusing on image-text pairs with entity annotations for multi-modal learning tasks. | Used to evaluate the precision of the visual object extraction model in GAIA, focusing on fine-grained multimedia knowledge extraction. | Used to train detectors with supervised data, focusing on image-text pairs for multi-modal learning tasks.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 218551030,
          "context_text": "These detectors are trained with supervised data from public images-text datasets [21] (such as MSCOCO [58], Flickr30k [59], Flick30k Entities [60] and Open Images [61]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image-text datasets used for training detectors, which are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.18653/v1/2020.acl-demos.11",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51c8975d88aa66781300e8ca88272ab3112445c0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to compare the performance of the proposed method against several recent methods, focusing on multi-modal reasoning tasks involving common objects in context. | Used to study image-caption relationships, providing a large set of images with multiple captions each, suitable for multi-modal reasoning tasks. | Used for image captioning experiments, focusing on generating textual descriptions from images.",
          "citing_paper_id": "213104909",
          "cited_paper_id": 14113767,
          "context_text": "Table 2 shows the comparison results between our method and several recent methods that are closely related to our method on the MSCOCO dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the MSCOCO dataset for comparing the performance of different methods. The dataset is clearly identified and used for evaluation.",
          "citing_paper_doi": "10.1609/AAAI.V34I07.6731",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4df184d6a74f1ffd84b644735c9afb5060552770",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2020,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used for zero-shot evaluation of the model, assessing cross-modal alignment between images and captions without direct training on this dataset. | Used to evaluate the FILIP model for image-text retrieval, providing a large-scale dataset with caption annotations. | Used to evaluate the FILIP model for image-text retrieval, focusing on region-to-phrase correspondences in images. | Used to evaluate FILIP's performance in image-to-text and text-to-image retrieval tasks, achieving state-of-the-art results except in zero-shot text-to-image retrieval. | Used to evaluate fine-tuned image-text retrieval models, focusing on zero-shot performance and cross-modal alignment. | Used to evaluate image-to-text and text-to-image models, focusing on rich image-to-sentence mappings and cross-modal retrieval performance. | Used for zero-shot image-text retrieval experiments, evaluating the performance of models on a diverse set of images and captions. | Used to evaluate FILIP's performance in image-to-text and text-to-image retrieval tasks, achieving state-of-the-art results across all metrics. | Used to evaluate the FILIP model in image-text retrieval tasks, focusing on region-to-phrase correspondences under zero-shot and fine-tuned settings. | Used to evaluate the FILIP model in image-text retrieval tasks, focusing on common objects in context under zero-shot and fine-tuned settings. | Used for fine-tuning the model, providing a large set of image-caption pairs to improve multi-modal reasoning capabilities.",
          "citing_paper_id": "244117525",
          "cited_paper_id": 6941275,
          "context_text": "We measure models’ performance on MSCOCO zero-7 Table 3: Results of ﬁne-tuned image-text retrieval on Flickr30K and MSCOCO datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the use of Flickr30K and MSCOCO datasets for evaluating image-text retrieval models. These are specific, verifiable datasets used in the context of multi-modal learning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/s11263-016-0965-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11c9c31dff70de92ada9160c78ff8bb46b2912d6",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to evaluate image-to-text and text-to-image models, focusing on rich image-to-sentence mappings and cross-modal retrieval performance.",
          "citing_paper_id": "244117525",
          "cited_paper_id": 201058752,
          "context_text": "Flickr30K MSCOCO image-to-text text-to-image image-to-text text-to-image R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Unicoder-VL 64.3",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions Flickr30K and MSCOCO, which are well-known datasets in multi-modal learning. These datasets are used for evaluating image-to-text and text-to-image tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1609/AAAI.V34I07.6795",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2bc1c8bd00bbf7401afcb5460277840fd8bab029",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to extract attributes from image captions, focusing on large-scale data for multi-modal reasoning and knowledge graph construction.",
          "citing_paper_id": "206594383",
          "cited_paper_id": 2210455,
          "context_text": "Each attribute in our vocabulary is extracted from captions from MS COCO [5], a large-scale image-captioning dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "MS COCO is a well-known dataset used for image captioning, which aligns with the context of extracting attributes from captions.",
          "citing_paper_doi": "10.1109/CVPR.2016.500",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/20dbdf02497aa84510970d0f5e8b599073bca1bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628",
          "citing_paper_year": 2015,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used for pretraining ERNIE-ViL, providing dense image annotations to improve understanding of complex visual scenes. | Used for pretraining ERNIE-ViL, focusing on common objects in context to enhance multi-modal reasoning capabilities.",
          "citing_paper_id": "220265934",
          "cited_paper_id": 4492210,
          "context_text": "And for fair comparison with the models pretrained on both out-of-domain and in-domain datasets, we further pretrain ERNIE-ViL on MS-COCO [18] and Visual-Genome [19] (in-domain datasets for downstream tasks).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, MS-COCO and Visual-Genome, which are used for pretraining ERNIE-ViL. These datasets are relevant to multi-modal learning and are clearly identified.",
          "citing_paper_doi": "10.1609/aaai.v35i4.16431",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to provide bounding box proposals for object detection, enhancing the accuracy of referring expression comprehension in images.",
          "citing_paper_id": "220265934",
          "cited_paper_id": 3441497,
          "context_text": "In this paper, we use the bounding box proposals provided by [35] pre-trained on the MS-COCO dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MS-COCO dataset, which is a well-known dataset used for object detection and image captioning tasks. It is clearly identified and used for providing bounding box proposals.",
          "citing_paper_doi": "10.1609/aaai.v35i4.16431",
          "cited_paper_doi": "10.1109/CVPR.2018.00142",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fdce9cbe5c726201575b3c8a8c1af0752f1af53f",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used for pretraining ERNIE-ViL, providing dense image annotations to improve understanding of complex visual scenes. | Used for pretraining ERNIE-ViL, focusing on common objects in context to enhance multi-modal reasoning capabilities.",
          "citing_paper_id": "220265934",
          "cited_paper_id": 14113767,
          "context_text": "And for fair comparison with the models pretrained on both out-of-domain and in-domain datasets, we further pretrain ERNIE-ViL on MS-COCO [18] and Visual-Genome [19] (in-domain datasets for downstream tasks).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, MS-COCO and Visual-Genome, which are used for pretraining ERNIE-ViL. These datasets are relevant to multi-modal learning and are clearly identified.",
          "citing_paper_doi": "10.1609/aaai.v35i4.16431",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2020,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "14124313",
      "citation_count": 0,
      "total_dataset_mentions": 12,
      "unique_datasets": [
        "WN9-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used for image-embodied knowledge representation learning, integrating visual and textual information to enhance multimodal reasoning. | Utilized for multimodal translation-based knowledge graph representation learning, combining image and text data to improve model performance.",
          "citing_paper_id": "258509157",
          "cited_paper_id": 9909815,
          "context_text": "There includes multi-modal datasets: WN9-IMG [41] and FB-IMG [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, WN9-IMG and FB-IMG, which are relevant to multi-modal knowledge graph reasoning. These datasets are used for knowledge representation learning involving images.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to learn visual embeddings using the VGG19 model, focusing on multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "258509157",
          "cited_paper_id": 14124313,
          "context_text": "As for the WN9-IMG dataset, we take the VGG19 [45] model to learn visual embeddings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for multi-modal learning tasks. The citation intent is to describe the use of a reusable resource.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to construct an image-embodied knowledge representation, focusing on integrating visual and textual information for multi-modal reasoning.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 9909815,
          "context_text": "WN9-IMG This dataset constructed by Xie et al. [37] is the subset of WN18 [4], which comes from the large lexical knowledge base WordNet [21].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions WN9-IMG as a dataset constructed by Xie et al., which is a subset of WN18 derived from WordNet. It is clearly identified and used in the research.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on Raw Hits@10.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 31606602,
          "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1609/aaai.v30i1.10329",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96acb1c882ad655c6b8459c2cd331803801446ca",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on Raw Hits@10. | Used as a subset for training and evaluating knowledge graph representation models, focusing on multimodal translation-based approaches to enhance reasoning capabilities.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 44145776,
          "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on Raw Hits@10.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 211137418,
          "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1109/TCDS.2019.2906685",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3f3bba81ab55d7ca7d3064241d7595592bc9dc86",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to establish baseline performance for multi-modal knowledge graph reasoning, comparing TransE, TransR, and IKRL models in the context of image and word embeddings.",
          "citing_paper_id": "203605587",
          "cited_paper_id": 14941970,
          "context_text": "with WN9-IMG, which is also used in [9], we directly adopt the experimental results of model TransE [4], TransR [6] and IKRL [9] as our baseline results from paper [9].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WN9-IMG' as a dataset used for experiments, and it is specific enough to be included. The citation intent is to use this dataset as a reusable resource for baseline comparisons.",
          "citing_paper_doi": "10.1109/IJCNN.2019.8852079",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/02ea29150d2526641bad18a0bc1932e6235c69a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2019,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to compare statistical properties with MarKG, focusing on entity, relation, triple, image, and data source counts.",
          "citing_paper_id": "252683295",
          "cited_paper_id": 9909815,
          "context_text": "B.3 DATASET DETAILS\nThe statistical comparison of MarKG with two multimodal knowledge graph datasets WN9IMG (Xie et al., 2017) and FB15k-IMG (Liu et al., 2019) as shown in Table 5, we report the number of entity, relation, triple, image and the data source.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific multimodal knowledge graph datasets used for statistical comparison with MarKG. These datasets are clearly identified and used to report various statistics.",
          "citing_paper_doi": "10.48550/arXiv.2210.00312",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8c43cbe3dff3f556bf09462a7bdbbb8a292af7f9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to generate negative instances for multi-modal knowledge graph reasoning by randomly replacing head or tail entities, following a specific protocol.",
          "citing_paper_id": "9909815",
          "cited_paper_id": 67340212,
          "context_text": "Since WN9-IMG has no explicit negative instances, we generate the negative instances by randomly replacing head or tail entities with another entity following the same protocol utilized in [ Socher et al. , 2013 ] .",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'WN9-IMG' which appears to be a dataset used for generating negative instances in multi-modal knowledge graph reasoning. No other datasets are mentioned.",
          "citing_paper_doi": "10.24963/ijcai.2017/438",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bcfd3aec538b21b13496329314671774e5a41ff7",
          "citing_paper_year": 2016,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used without regularization in multi-modal knowledge graph reasoning experiments. | Used with dropout regularization to prevent overfitting in multi-modal knowledge graph reasoning experiments.",
          "citing_paper_id": "44145776",
          "cited_paper_id": 6844431,
          "context_text": "In the case of WN9-IMG, we used dropout regularization (Srivastava et al., 2014) with a dropout ratio of 10%; we applied no regularization on the FB-IMG dataset.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, WN9-IMG and FB-IMG, but does not provide specific details about their content or origin. The usage is clear in terms of applying dropout regularization to WN9-IMG and no regularization to FB-IMG.",
          "citing_paper_doi": "10.18653/v1/S18-2027",
          "cited_paper_doi": "10.5555/2627435.2670313",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698",
          "citing_paper_year": 2018,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to produce embeddings with the TransE method, focusing on the integration of image and textual information for knowledge graph completion tasks. | Used to create a new large-scale dataset for multi-modal knowledge graph completion, focusing on link prediction and triple classification tasks.",
          "citing_paper_id": "44145776",
          "cited_paper_id": 9909815,
          "context_text": "To gain initial insights into the potential beneﬁts of external information for the KGC task, let us consider the embeddings produced by the translation-based TransE method (Bordes et al., 2013) on the WN9-IMG dataset (Xie et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the WN9-IMG dataset, which is a specific multi-modal dataset used for knowledge graph completion tasks. The dataset is used to produce embeddings with the TransE method.",
          "citing_paper_doi": "10.18653/v1/S18-2027",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Serves as the source dataset for WN-Increment, providing a basis for extracting and analyzing incremental changes in multimodal knowledge graphs. | Serves as the source dataset from which FB-Increment is derived, providing a foundation for incremental learning experiments in knowledge graph representation. | Used to construct the incremental dataset WN-Increment, focusing on multi-modal knowledge graph representation learning with structure, text, and image modalities. | Served as the source dataset for FB-Increment, providing a foundation for analyzing multimodal data in knowledge graph representation learning. | Used to study incremental learning in knowledge graph representation, focusing on three snapshots (FB-1SP, FB-2SP, FB-3SP) for evaluating model performance over time. | Used to study incremental changes in multimodal knowledge graphs, focusing on the evolution of entities and relationships across three snapshots. | Used to study incremental updates in knowledge graphs, focusing on multimodal translation-based representation learning across three snapshots.",
          "citing_paper_id": "258264587",
          "cited_paper_id": 44145776,
          "context_text": "The ﬁrst incremental dataset WN-Increment consists of three snapshots denoted as WN-{ 1SP, 2SP, 3SP } constructed from a real-world multi-modal dataset WN9-IMG [21] which includes three modalities, i.e., structure, text, and image.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for constructing an incremental dataset, including WN9-IMG, which is a real-world multi-modal dataset.",
          "citing_paper_doi": "10.1007/s10115-023-01868-9",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bad478ff7fe91cd84f2f09ef1b22a5e31fd98ac0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "235248430",
      "citation_count": 0,
      "total_dataset_mentions": 11,
      "unique_datasets": [
        "Wikidata"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Mentioned as an example of a Knowledge Graph, but the specific usage in the research context is not detailed. It is relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_id": "248779998",
          "cited_paper_id": 235248430,
          "context_text": "Recently, many Knowledge Graphs (KGs) have been curated ( e.g., Wikidata (Vrandeˇci´c and Krötzsch, 2014)) and successfully applied to various applications, ranging from information extraction (Lai et al., 2021) to information retrieval (Dong et al., 2014).",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions Wikidata as an example of a Knowledge Graph, which is relevant to the topic of multi-modal knowledge graph reasoning. However, it does not specify how Wikidata is used in the research context.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": "10.18653/v1/2021.acl-long.488",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/983fdd94067ff52972a931595bffb8933d6df968",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Mentioned as an example of a Knowledge Graph, but the specific usage in the research context is not detailed. It is relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_id": "248779998",
          "cited_paper_id": null,
          "context_text": "Recently, many Knowledge Graphs (KGs) have been curated ( e.g., Wikidata (Vrandeˇci´c and Krötzsch, 2014)) and successfully applied to various applications, ranging from information extraction (Lai et al., 2021) to information retrieval (Dong et al., 2014).",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions Wikidata as an example of a Knowledge Graph, which is relevant to the topic of multi-modal knowledge graph reasoning. However, it does not specify how Wikidata is used in the research context.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Mentioned as a collaboratively created graph database for structuring human knowledge, but not used in a specific research context. | Mentioned as an encyclopedia knowledge base, but not used in a specific research context.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 207167677,
          "context_text": "…common sense knowledge (e.g., Cyc [1], ConceptNet [2]), lexical knowledge (e.g., WordNet [3], BabelNet [4]), encyclopedia knowledge (e.g., Freebase [5], DBpedia [6], YAGO [7], WikiData [8], CN-DBpedia [9]), taxonomic knowledge (e.g., Probase [10]) and geographic knowledge (e.g., GeoNames [11]).",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge bases, which are relevant to multi-modal knowledge graph reasoning. However, they are not explicitly used in a specific research context within the given citation.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to provide structured world knowledge to multi-modal data, enhancing entity descriptions and relationships in vision/text data.",
          "citing_paper_id": "258352810",
          "cited_paper_id": 15206880,
          "context_text": "Meanwhile, large knowledge graphs (KGs), such as Wikidata [31] and ConceptNet [28], can provide structured world knowledge to multi-modal data by representing entity descriptions and their relationships, which is implicit in vision/text but comprises complementary information [33, 38].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two large knowledge graphs, Wikidata and ConceptNet, which are used to provide structured world knowledge to multi-modal data. These resources are clearly identified and are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3613848",
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0fe1b1bfd634ee42846afbd64cef1c682e02e5e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to provide structured world knowledge to multi-modal data, enhancing entity descriptions and relationships in vision/text data. | Used to capture background world knowledge for the KG data, specifically for training and evaluating multi-modal reasoning models.",
          "citing_paper_id": "258352810",
          "cited_paper_id": 208006241,
          "context_text": "Meanwhile, large knowledge graphs (KGs), such as Wikidata [31] and ConceptNet [28], can provide structured world knowledge to multi-modal data by representing entity descriptions and their relationships, which is implicit in vision/text but comprises complementary information [33, 38].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two large knowledge graphs, Wikidata and ConceptNet, which are used to provide structured world knowledge to multi-modal data. These resources are clearly identified and are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3613848",
          "cited_paper_doi": "10.1162/tacl_a_00360",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0fe1b1bfd634ee42846afbd64cef1c682e02e5e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to provide structured world knowledge to multi-modal data, enhancing entity descriptions and relationships in vision/text data.",
          "citing_paper_id": "258352810",
          "cited_paper_id": 252968266,
          "context_text": "Meanwhile, large knowledge graphs (KGs), such as Wikidata [31] and ConceptNet [28], can provide structured world knowledge to multi-modal data by representing entity descriptions and their relationships, which is implicit in vision/text but comprises complementary information [33, 38].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two large knowledge graphs, Wikidata and ConceptNet, which are used to provide structured world knowledge to multi-modal data. These resources are clearly identified and are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3613848",
          "cited_paper_doi": "10.48550/arXiv.2210.09338",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0fe1b1bfd634ee42846afbd64cef1c682e02e5e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used as a structured knowledge base to enhance multi-modal reasoning, providing rich entity and relation information. | Used to store and query relational facts for various tasks, including question answering, recommendation systems, and multimedia reasoning. | Used as a collaboratively created graph database to support multi-modal knowledge graph reasoning, integrating diverse human knowledge.",
          "citing_paper_id": "239011538",
          "cited_paper_id": 207167677,
          "context_text": "Knowledge graphs (KGs), e.g., Wikidata [30], Freebase [3], and DB-pedia[2],containrelationalfactsinthe formof head relation −−−−−−→ tail , which have been widely used in various kinds of tasks, such as question answering [13], recommendation system [11], and multimedia reasoning [17, 27].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (Wikidata, Freebase, DBpedia) and their applications in various tasks. These are specific, verifiable resources used in the research context.",
          "citing_paper_doi": "10.1145/3474085.3475470",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to store and query relational facts for various tasks, including question answering, recommendation systems, and multimedia reasoning.",
          "citing_paper_id": "239011538",
          "cited_paper_id": 211677665,
          "context_text": "Knowledge graphs (KGs), e.g., Wikidata [30], Freebase [3], and DB-pedia[2],containrelationalfactsinthe formof head relation −−−−−−→ tail , which have been widely used in various kinds of tasks, such as question answering [13], recommendation system [11], and multimedia reasoning [17, 27].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (Wikidata, Freebase, DBpedia) and their applications in various tasks. These are specific, verifiable resources used in the research context.",
          "citing_paper_doi": "10.1145/3474085.3475470",
          "cited_paper_doi": "10.1360/ssi-2019-0274",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4cc53835ecee68bc37db1fbb1631573a3900906",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to store and query relational facts for various tasks, including question answering, recommendation systems, and multimedia reasoning.",
          "citing_paper_id": "239011538",
          "cited_paper_id": 222278428,
          "context_text": "Knowledge graphs (KGs), e.g., Wikidata [30], Freebase [3], and DB-pedia[2],containrelationalfactsinthe formof head relation −−−−−−→ tail , which have been widely used in various kinds of tasks, such as question answering [13], recommendation system [11], and multimedia reasoning [17, 27].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (Wikidata, Freebase, DBpedia) and their applications in various tasks. These are specific, verifiable resources used in the research context.",
          "citing_paper_doi": "10.1145/3474085.3475470",
          "cited_paper_doi": "10.1145/3394171.3413943",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used as the primary Knowledge Graph for multi-modal reasoning, focusing on integrating and querying diverse data types and sources. | Used as a large-scale collaboratively edited Knowledge Graph containing 50 million items, including world knowledge about persons, taxons, administrative territorial entities, and more. | Used as a large-scale collaboratively edited Knowledge Graph containing 50 million items, including world knowledge about various entities, to support multi-modal knowledge graph reasoning. | Used to compile an image collection of persons, focusing on exhaustively listing individuals for multi-modal reasoning tasks. | Used to retrieve structured information via SPARQL queries, specifically to answer templated questions about the order of people in images, enhancing multi-modal reasoning capabilities.",
          "citing_paper_id": "85558018",
          "cited_paper_id": null,
          "context_text": "We use Wikidata (Vrandecic and Krötzsch 2014) as our Knowledge Graph.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "Wikidata is mentioned as a Knowledge Graph used in the research. It is a specific, verifiable resource with clear provenance.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": null,
          "citing_paper_year": 2019,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to extract entity descriptions as textual features, integrating them with relational and visual features for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "257631615",
          "cited_paper_id": 14494942,
          "context_text": "Specifically, we utilize the relational triples as structural features, entity images as visual features and we extract the entity descriptions fromWikidata [34] as textual features.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions using Wikidata for extracting entity descriptions as textual features, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3543507.3583554",
          "cited_paper_doi": "10.1145/2629489",
          "citing_paper_url": "https://www.semanticscholar.org/paper/607d9eb8ceab0533bcfc65e8aced69aee4e40976",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dab7e605237ad4f4fe56dcba2861b8f0a57112be",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "67700681",
      "citation_count": 0,
      "total_dataset_mentions": 10,
      "unique_datasets": [
        "WordNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used to initialize the ontology by providing a lexical database for semantic relationships, enhancing the multi-modal reasoning capabilities. | Merged into the ontology to incorporate visual semantic role labeling, improving the understanding of image content and context. | Integrated into the ontology to contribute frame-based semantic structures, supporting the representation of complex events and roles. | Used to train models for visual semantic role labeling, focusing on image understanding and situation recognition tasks.",
          "citing_paper_id": "248779998",
          "cited_paper_id": 2424223,
          "context_text": "Based on schema, we initialize the ontology by merging WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and imSitu (Yatskar et al., 2016) Ontology.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions merging WordNet, FrameNet, and imSitu Ontology to initialize an ontology. These are specific resources used in the research.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": "10.1109/CVPR.2016.597",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b65faba7088864e134e7eb3b68c8e2f18cc5b4f6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Merged into the ontology to incorporate visual semantic role labeling, improving the understanding of image content and context. | Used as a lexical database to provide semantic relationships between words, enhancing multi-modal reasoning by integrating linguistic knowledge. | Extended the ontology by aligning WordNet synsets to annotated frames, enhancing the visual ontology for multi-modal reasoning. | Used to initialize the ontology by providing a lexical database for semantic relationships, enhancing the multi-modal reasoning capabilities. | Integrated into the ontology to contribute frame-based semantic structures, supporting the representation of complex events and roles.",
          "citing_paper_id": "248779998",
          "cited_paper_id": 57814228,
          "context_text": "Based on schema, we initialize the ontology by merging WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and imSitu (Yatskar et al., 2016) Ontology.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions merging WordNet, FrameNet, and imSitu Ontology to initialize an ontology. These are specific resources used in the research.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d53bcbac7ea19173e95d3bd855b998fab765737d",
          "citing_paper_year": 2022,
          "cited_paper_year": 1998
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used as a lexical database to enrich the knowledge graph, contributing semantic relationships and lexical information. | Utilized as a structured information source to enhance the knowledge graph, providing links to Wikipedia articles and structured data.",
          "citing_paper_id": "69481030",
          "cited_paper_id": 26517743,
          "context_text": "Its knowledge is collected from many sources including WordNet (Bond and Foster 2013), DBpe-dia (Auer et al. 2007) et al. Following previous work (Fang et al. 2017), we only employ its English subgraph with about 1 .",
          "confidence_score": 0.6,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions WordNet and DBpedia as sources of knowledge for a knowledge graph, but does not specify their usage in the current research context. No clear indication of how these datasets are used in multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1609/AAAI.V33I01.33018303",
          "cited_paper_doi": "10.24963/IJCAI.2017/230",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3cf367c96ea895473a26c580b4f1dfd168bd8c2c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/74793980135737800c9d579f80605669d97cfcdb",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used as a class hierarchy to aid in image recognition, providing a structured vocabulary for categorizing visual concepts.",
          "citing_paper_id": "229339845",
          "cited_paper_id": 786357,
          "context_text": "Class hierarchies such as WordNet [53] have often been used to aid in image recognition [85, 60].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions WordNet as a class hierarchy used in image recognition, but does not specify its use as a dataset. It is more likely a reference to a knowledge base or ontology.",
          "citing_paper_doi": "10.1109/CVPR46437.2021.01389",
          "cited_paper_doi": "10.1109/CVPR.2017.690",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used to compile SimilarTo edges in a commonsense graph, leveraging the hierarchical structure of WordNet to initialize node representations.",
          "citing_paper_id": "210064217",
          "cited_paper_id": 1671874,
          "context_text": "ed by [37], including background. We use the GloVE [29] embedding of category titles to initialize their node representation (via ˚ init). We compile our commonsense edges from three sources, WordNet [27], ConceptNet [24], and Visual Genome. To summarize, there are three groups of edge types in our commonsense graph. We have SimilarTo from WordNet hierarchy, we have PartOf, RelatedTo, IsA, MannerOf, a",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of WordNet, ConceptNet, and Visual Genome as sources for compiling commonsense edges in a graph. However, only WordNet is a verifiable dataset with a clear identifier.",
          "citing_paper_doi": "10.1007/978-3-030-58592-1_36",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3e5f240d634be018536e6419672daf38dd4d24bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2020,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Serves as a large lexical knowledge base for constructing and evaluating knowledge graphs, providing a rich structure of semantic relations. | Used as a subset of WordNet for evaluating link prediction models in knowledge graphs, focusing on relational patterns and entity types.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 67700681,
          "context_text": "[37] is the subset of WN18 [4], which comes from the large lexical knowledge base WordNet [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions WN18, which is derived from WordNet. Both are specific, verifiable datasets used in knowledge graph research.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2fd46c6acedb9fb68c59c9ec435eb192ec0cd914",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used as a lexical knowledge graph to support the baseline approaches, enhancing semantic understanding in the research context.",
          "citing_paper_id": "4703853",
          "cited_paper_id": 261138,
          "context_text": "We use the same settings as the baseline approaches [13, 34, 4] together with the WordNet [32] knowledge graph.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions using WordNet, which is a well-known lexical database and knowledge graph. However, it does not specify how WordNet is used in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/CVPR.2018.00717",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ff65e3bf34e892ef75d91c5e3d7294e0b64d867d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4aa4069693bee00d1b0759ca3df35e59284e9845",
          "citing_paper_year": 2018,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Referenced as a lexical database for English, used to provide structured vocabulary and semantic relationships in the research context.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 1671874,
          "context_text": "WordNet [3], BabelNet [4]), encyclopedia knowledge (e.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions WordNet, which is a lexical database, but does not indicate it is used as a dataset in the research. It is referenced as part of a list of knowledge sources.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2022,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Harnesses human intelligence to create a large-scale knowledge graph, supporting various applications including multi-modal reasoning and semantic understanding. | Used as a lexical database for English, providing a structured vocabulary for text-based knowledge representation and reasoning. | Used as a knowledge base for never-ending learning, continuously extracting and integrating new knowledge.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 1671874,
          "context_text": "Knowledge-base can be used in various application such as imageretrieval[57],visualquestion-answering[59],imageclus-tering[58],etc.Text-basedknowledgebasesarewellstudiedin theliteratureandanumberofthemareavailable,mostnotably, WordNet[13]andConceptNet[14]whichharnesshumanin-telligence.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WordNet and ConceptNet as text-based knowledge bases. These are well-known resources in the field of knowledge graphs and multi-modal learning.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2020,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Harnesses human intelligence to create a large-scale knowledge graph, supporting various applications including multi-modal reasoning and semantic understanding. | Used as a lexical database for English, providing a structured vocabulary for text-based knowledge representation and reasoning.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 43922976,
          "context_text": "Knowledge-base can be used in various application such as imageretrieval[57],visualquestion-answering[59],imageclus-tering[58],etc.Text-basedknowledgebasesarewellstudiedin theliteratureandanumberofthemareavailable,mostnotably, WordNet[13]andConceptNet[14]whichharnesshumanin-telligence.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WordNet and ConceptNet as text-based knowledge bases. These are well-known resources in the field of knowledge graphs and multi-modal learning.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b51dc3abb6cac334d49cfaa8b29e0ebd3b9a52f9",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "6611164",
      "citation_count": 0,
      "total_dataset_mentions": 10,
      "unique_datasets": [
        "ICEWS14"
      ],
      "dataset_details": [
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to test the accuracy of temporal knowledge graph models, particularly in event prediction and temporal relationships. | Used to assess the performance of temporal reasoning models, emphasizing event forecasting and historical context. | Used to enhance knowledge graph reasoning by integrating multilingual Wikipedia data, focusing on entity linking and semantic enrichment. | Used to analyze global events and their impact on knowledge graphs, focusing on large-scale event data and temporal reasoning. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and temporal dynamics.",
          "citing_paper_id": "250630286",
          "cited_paper_id": 6611164,
          "context_text": "There are five typical datasets commonly used in previous studies [Li et al. , 2021a; Li et al. , 2021b], namely, ICEWS14, ICEWS05-15, ICEWS18 [Boschee et al. , 2015], GDELT [Leetaru and Schrodt, 2013] and YAGO [Mahdisoltani et al. , 2015].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used in previous studies, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.24963/ijcai.2022/284",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/68107a3de4d5a9c76fa482111cfa547d2e07d32f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c5b5adc3830ac45bf1d764603b1b71e5f729616",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to test the accuracy of temporal knowledge graph models, particularly in event prediction and temporal relationships. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and temporal dynamics. | Used to enhance knowledge graph reasoning by integrating multilingual Wikipedia data, focusing on entity linking and semantic enrichment. | Used to analyze global events and their impact on knowledge graphs, focusing on large-scale event data and temporal reasoning.",
          "citing_paper_id": "250630286",
          "cited_paper_id": 235266233,
          "context_text": "There are five typical datasets commonly used in previous studies [Li et al. , 2021a; Li et al. , 2021b], namely, ICEWS14, ICEWS05-15, ICEWS18 [Boschee et al. , 2015], GDELT [Leetaru and Schrodt, 2013] and YAGO [Mahdisoltani et al. , 2015].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used in previous studies, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.24963/ijcai.2022/284",
          "cited_paper_doi": "10.18653/v1/2021.acl-long.365",
          "citing_paper_url": "https://www.semanticscholar.org/paper/68107a3de4d5a9c76fa482111cfa547d2e07d32f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a697045644788d9c975bf5ac544ed211f6fc9d5b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to verify rGalT model's ability to discover relevant historical facts over longer time intervals, leveraging the dataset's frequent updates (15-minute intervals) to test the model's temporal reasoning capabilities. | Used to enhance knowledge graph reasoning by integrating multilingual Wikipedia data, focusing on entity linking and semantic enrichment. | Used to analyze global events and their impact on knowledge graphs, focusing on large-scale event data and temporal reasoning. | Used to set the optimal length of history for multi-modal knowledge graph reasoning, focusing on temporal dynamics in event prediction. | Used to test the accuracy of temporal knowledge graph models, particularly in event prediction and temporal relationships. | Used to test the accuracy of temporal knowledge graph models, particularly in predicting future events and interactions. | Used to assess the effectiveness of rGalT in handling large-scale global event data, emphasizing temporal and relational reasoning. | Used to assess the performance of temporal reasoning models, emphasizing event forecasting and historical context. | Used as a knowledge graph, though specific research context and methodology are not detailed in the citation. | Used to evaluate multi-modal knowledge graph reasoning models, focusing on temporal event prediction and entity linking. | Used to assess the performance of temporal reasoning models, emphasizing event forecasting and relationship dynamics. | Used to analyze global event data, focusing on the extraction and reasoning over large-scale, real-world events. | Used to evaluate the accuracy of multi-modal reasoning models on a large-scale knowledge graph, focusing on entity linking and relation prediction. | Used to evaluate temporal knowledge graph completion methods, focusing on event prediction and entity resolution. | Used to configure model parameters, specifically setting the number of transformer encoder/decoder layers and heads. The dataset is part of a multi-dataset evaluation setup. | Used to enhance knowledge graph reasoning by integrating structured information from Wikipedia, focusing on entity linking and type inference. | Used to assess the performance of multi-modal reasoning models on global event data, emphasizing event detection and entity recognition. | Used to evaluate the performance of rGalT on temporal knowledge graph reasoning tasks, focusing on event prediction and entity forecasting. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and temporal dynamics.",
          "citing_paper_id": "250630286",
          "cited_paper_id": null,
          "context_text": "There are five typical datasets commonly used in previous studies [Li et al. , 2021a; Li et al. , 2021b], namely, ICEWS14, ICEWS05-15, ICEWS18 [Boschee et al. , 2015], GDELT [Leetaru and Schrodt, 2013] and YAGO [Mahdisoltani et al. , 2015].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used in previous studies, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.24963/ijcai.2022/284",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/68107a3de4d5a9c76fa482111cfa547d2e07d32f",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate the performance of TeLM, focusing on temporal knowledge graph completion with multi-vector embeddings for entities, relations, and timestamps. | Used to evaluate performance changes by varying hyper-parameters such as the length of time window and the number of concurrent facts in multi-modal knowledge graph reasoning. | Used to evaluate multi-modal reasoning models on a broader range of event-based facts from 2005 to 2015, focusing on long-term trends and event patterns. | Used to evaluate the performance of the method on temporal reasoning tasks in dynamic knowledge graphs, focusing on event prediction and entity resolution. | Used to test the model's capability in handling large-scale global event data, focusing on event prediction and temporal reasoning. | Used to test the method's effectiveness in handling large-scale global event data, evaluating its performance on entity linking and event prediction tasks. | Used to assess the model's ability to predict events over time in dynamic knowledge graphs, emphasizing temporal dynamics. | Used to evaluate multi-modal reasoning models on event-based facts from 2014, focusing on temporal dynamics and event prediction. | Used to evaluate the model's performance on temporal reasoning tasks in dynamic knowledge graphs, focusing on event prediction. | Used to study temporal reasoning in dynamic knowledge graphs, focusing on facts from April 2015 to March 2016, as provided by Trivedi et al. (2017). | Used to assess the method's ability to handle long-term temporal dependencies in dynamic knowledge graphs, specifically for event forecasting and relationship evolution. | Used to evaluate SToKE's ability to learn contextual information related to queries, focusing on event prediction and reasoning in a multi-modal knowledge graph setting.",
          "citing_paper_id": "259858848",
          "cited_paper_id": null,
          "context_text": "ICEWS14 ICEWS05-15 GDELT Method MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 Table 2: Performance comparison on three benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'ICEWS14', 'ICEWS05-15', and 'GDELT' as benchmark datasets, which are commonly used in multi-modal knowledge graph reasoning. These datasets are used to evaluate the performance of the method.",
          "citing_paper_doi": "10.18653/v1/2023.findings-acl.28",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f9f2158801d2163658be943c43afcefb2f8f75b7",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate the performance of TeLM, focusing on temporal knowledge graph completion with multi-vector embeddings for entities, relations, and timestamps.",
          "citing_paper_id": "259858848",
          "cited_paper_id": 235097561,
          "context_text": "Among temporal extensions of semantic matching methods, TeLM achieves the best performance on ICEWS14 and ICEWS05-15 due to its more expressive multi-vector embedding for modeling entities, relations and timestamps of TKGE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets (ICEWS14 and ICEWS05-15) used to evaluate the performance of TeLM, a method for temporal knowledge graph completion.",
          "citing_paper_doi": "10.18653/v1/2023.findings-acl.28",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.202",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f9f2158801d2163658be943c43afcefb2f8f75b7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e467b66fd3505e8ea0d1f0c61df6ea832858fe1f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate static knowledge graph reasoning models, focusing on entity and relation prediction. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and link prediction over time.",
          "citing_paper_id": "246828738",
          "cited_paper_id": 2687749,
          "context_text": "Method ICEWS14 ICEWS18 WIKI YAGO GDELT\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nSt at ic DistMult 9.72 10.09 22.53 13.86 15.22 31.26 27.96 32.45 39.51 44.05 49.70 59.94 8.61 8.27 17.04 R-GCN 15.03 16.12 31.47 15.05 16.49 29.00 13.96 15.75 22.05 27.43 31.24 44.75 12.17 12.37 20.63 ConvE 21.64 23.16 38.37 22.56 25.41 41.67 26.41 30.36 39.41 41.31 47.10 59.67 18.43 19.57 32.25 RotateE 9.79 9.37 22.24 11.63 12.31 28.03 26.08 31.63 38.51 42.08 46.77 59.39 3.62 2.26 8.37\nTe m po\nra l TA-DistMult 11.29 11.60 23.71 15.62 17.09 32.21 26.44 31.36 38.97 44.98 50.64 61.11 10.34 10.44 21.63 HyTE 7.72 7.94 20.16 7.41 7.33 16.01 25.40 29.16 37.54 14.42 39.73 46.98 6.69 7.57 19.06 dyngraph2vecAE 6.95 8.17 12.18 1.36 1.54 1.61 2.67 2.75 3.00 0.81 0.74 0.76 4.53 1.87 1.87 tNodeEmbed 13.36 13.13 24.31 7.21 7.64 15.75 8.86 10.11 16.36 3.82 3.88 8.07 12.97 12.61 21.22 EvolveGCN 8.32 7.64 18.81 10.31 10.52 23.65 27.19 31.35 38.13 40.50 45.78 55.29 6.54 5.64 15.22 Know-Evolve 0.05 0.00 0.10 0.11 0.00 0.47 0.03 0.00 0.04 0.02 0.00 0.01 0.11 0.02 0.10 Know-Evolve+MLP 16.81 18.63 29.20 7.41 7.87 14.76 10.54 13.08 20.21 5.23 5.63 10.23 15.88 15.69 22.28 DyRep+MLP 17.54 19.87 30.34 7.82 7.73 16.33 10.41 12.06 20.93 4.98 5.54 10.19 16.25 16.45 23.86 R-GCRN+MLP 21.39 23.60 38.96 23.46 26.62 41.96 28.68 31.44 38.58 43.71 48.53 56.98 18.63 19.80 32.42 RE-Net 23.91 26.63 42.70 26.81 30.58 45.92 31.55 34.45 42.26 46.37 51.95 61.59 19.44 20.73 33.81\nEvoKG 27.18±0.001 30.84 ±0.001 47.67 ±0.001 29.28 ±0.002 33.94 ±0.004 50.09 ±0.002 68.03 ±0.031 79.60 ±0.036 85.91 ±0.063 68.59 ±0.003 81.13 ±0.005 92.73 ±0.009 19.28 ±0.001 20.55 ±0.001 34.44 ±0.002\ninferences, EvoKG is 177× on average, and up to 291×, faster than RE-Net.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context provides performance metrics for various methods on different datasets. The datasets mentioned are ICEWS14, ICEWS18, WIKI, YAGO, and GDELT. These are all knowledge graphs or datasets used for evaluating multi-modal reasoning models.",
          "citing_paper_doi": "10.1145/3488560.3498451",
          "cited_paper_doi": "10.1007/978-3-030-04167-0_33",
          "citing_paper_url": "https://www.semanticscholar.org/paper/96dd81bda02205264fdd527bd8a150987220dbb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6b1793ece5993523855ce67c646de408318d1b12",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate static knowledge graph reasoning models, focusing on entity and relation prediction. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and link prediction over time.",
          "citing_paper_id": "246828738",
          "cited_paper_id": 4328400,
          "context_text": "Method ICEWS14 ICEWS18 WIKI YAGO GDELT\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nSt at ic DistMult 9.72 10.09 22.53 13.86 15.22 31.26 27.96 32.45 39.51 44.05 49.70 59.94 8.61 8.27 17.04 R-GCN 15.03 16.12 31.47 15.05 16.49 29.00 13.96 15.75 22.05 27.43 31.24 44.75 12.17 12.37 20.63 ConvE 21.64 23.16 38.37 22.56 25.41 41.67 26.41 30.36 39.41 41.31 47.10 59.67 18.43 19.57 32.25 RotateE 9.79 9.37 22.24 11.63 12.31 28.03 26.08 31.63 38.51 42.08 46.77 59.39 3.62 2.26 8.37\nTe m po\nra l TA-DistMult 11.29 11.60 23.71 15.62 17.09 32.21 26.44 31.36 38.97 44.98 50.64 61.11 10.34 10.44 21.63 HyTE 7.72 7.94 20.16 7.41 7.33 16.01 25.40 29.16 37.54 14.42 39.73 46.98 6.69 7.57 19.06 dyngraph2vecAE 6.95 8.17 12.18 1.36 1.54 1.61 2.67 2.75 3.00 0.81 0.74 0.76 4.53 1.87 1.87 tNodeEmbed 13.36 13.13 24.31 7.21 7.64 15.75 8.86 10.11 16.36 3.82 3.88 8.07 12.97 12.61 21.22 EvolveGCN 8.32 7.64 18.81 10.31 10.52 23.65 27.19 31.35 38.13 40.50 45.78 55.29 6.54 5.64 15.22 Know-Evolve 0.05 0.00 0.10 0.11 0.00 0.47 0.03 0.00 0.04 0.02 0.00 0.01 0.11 0.02 0.10 Know-Evolve+MLP 16.81 18.63 29.20 7.41 7.87 14.76 10.54 13.08 20.21 5.23 5.63 10.23 15.88 15.69 22.28 DyRep+MLP 17.54 19.87 30.34 7.82 7.73 16.33 10.41 12.06 20.93 4.98 5.54 10.19 16.25 16.45 23.86 R-GCRN+MLP 21.39 23.60 38.96 23.46 26.62 41.96 28.68 31.44 38.58 43.71 48.53 56.98 18.63 19.80 32.42 RE-Net 23.91 26.63 42.70 26.81 30.58 45.92 31.55 34.45 42.26 46.37 51.95 61.59 19.44 20.73 33.81\nEvoKG 27.18±0.001 30.84 ±0.001 47.67 ±0.001 29.28 ±0.002 33.94 ±0.004 50.09 ±0.002 68.03 ±0.031 79.60 ±0.036 85.91 ±0.063 68.59 ±0.003 81.13 ±0.005 92.73 ±0.009 19.28 ±0.001 20.55 ±0.001 34.44 ±0.002\ninferences, EvoKG is 177× on average, and up to 291×, faster than RE-Net.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context provides performance metrics for various methods on different datasets. The datasets mentioned are ICEWS14, ICEWS18, WIKI, YAGO, and GDELT. These are all knowledge graphs or datasets used for evaluating multi-modal reasoning models.",
          "citing_paper_doi": "10.1145/3488560.3498451",
          "cited_paper_doi": "10.1609/aaai.v32i1.11573",
          "citing_paper_url": "https://www.semanticscholar.org/paper/96dd81bda02205264fdd527bd8a150987220dbb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9697d32ed0a16da167f2bdba05ef96d0da066eb5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate temporal reasoning in dynamic knowledge graphs, focusing on global events and their impact over time. | Used to evaluate temporal reasoning in dynamic knowledge graphs, focusing on event prediction and entity tracking over time. | Used as an event-based TKG to study temporal knowledge graph reasoning, focusing on global events, language, and tone. | Used as a knowledge base with temporally associated facts to study multi-modal knowledge graph reasoning, focusing on structured information from multilingual Wikipedias. | Used as an event-based TKG to study temporal knowledge graph reasoning, focusing on crisis early warning systems. | Used to evaluate static and dynamic knowledge graph reasoning, focusing on entity linking and relation extraction from multilingual Wikipedias. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and link prediction over time. | Used as a knowledge base with temporally associated facts to study multi-modal knowledge graph reasoning, focusing on structured information from Wikipedia. | Used to evaluate static knowledge graph reasoning models, focusing on entity and relation prediction. | Used to evaluate the performance of the model, focusing on events occurring at regular intervals in multilingual Wikipedia articles. | Used to evaluate the performance of the model, focusing on events occurring at regular intervals in the YAGO knowledge base. | Used to evaluate static and dynamic knowledge graph reasoning, focusing on entity linking and relation extraction from Wikipedia.",
          "citing_paper_id": "246828738",
          "cited_paper_id": 6611164,
          "context_text": "Method ICEWS14 ICEWS18 WIKI YAGO GDELT\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nSt at ic DistMult 9.72 10.09 22.53 13.86 15.22 31.26 27.96 32.45 39.51 44.05 49.70 59.94 8.61 8.27 17.04 R-GCN 15.03 16.12 31.47 15.05 16.49 29.00 13.96 15.75 22.05 27.43 31.24 44.75 12.17 12.37 20.63 ConvE 21.64 23.16 38.37 22.56 25.41 41.67 26.41 30.36 39.41 41.31 47.10 59.67 18.43 19.57 32.25 RotateE 9.79 9.37 22.24 11.63 12.31 28.03 26.08 31.63 38.51 42.08 46.77 59.39 3.62 2.26 8.37\nTe m po\nra l TA-DistMult 11.29 11.60 23.71 15.62 17.09 32.21 26.44 31.36 38.97 44.98 50.64 61.11 10.34 10.44 21.63 HyTE 7.72 7.94 20.16 7.41 7.33 16.01 25.40 29.16 37.54 14.42 39.73 46.98 6.69 7.57 19.06 dyngraph2vecAE 6.95 8.17 12.18 1.36 1.54 1.61 2.67 2.75 3.00 0.81 0.74 0.76 4.53 1.87 1.87 tNodeEmbed 13.36 13.13 24.31 7.21 7.64 15.75 8.86 10.11 16.36 3.82 3.88 8.07 12.97 12.61 21.22 EvolveGCN 8.32 7.64 18.81 10.31 10.52 23.65 27.19 31.35 38.13 40.50 45.78 55.29 6.54 5.64 15.22 Know-Evolve 0.05 0.00 0.10 0.11 0.00 0.47 0.03 0.00 0.04 0.02 0.00 0.01 0.11 0.02 0.10 Know-Evolve+MLP 16.81 18.63 29.20 7.41 7.87 14.76 10.54 13.08 20.21 5.23 5.63 10.23 15.88 15.69 22.28 DyRep+MLP 17.54 19.87 30.34 7.82 7.73 16.33 10.41 12.06 20.93 4.98 5.54 10.19 16.25 16.45 23.86 R-GCRN+MLP 21.39 23.60 38.96 23.46 26.62 41.96 28.68 31.44 38.58 43.71 48.53 56.98 18.63 19.80 32.42 RE-Net 23.91 26.63 42.70 26.81 30.58 45.92 31.55 34.45 42.26 46.37 51.95 61.59 19.44 20.73 33.81\nEvoKG 27.18±0.001 30.84 ±0.001 47.67 ±0.001 29.28 ±0.002 33.94 ±0.004 50.09 ±0.002 68.03 ±0.031 79.60 ±0.036 85.91 ±0.063 68.59 ±0.003 81.13 ±0.005 92.73 ±0.009 19.28 ±0.001 20.55 ±0.001 34.44 ±0.002\ninferences, EvoKG is 177× on average, and up to 291×, faster than RE-Net.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context provides performance metrics for various methods on different datasets. The datasets mentioned are ICEWS14, ICEWS18, WIKI, YAGO, and GDELT. These are all knowledge graphs or datasets used for evaluating multi-modal reasoning models.",
          "citing_paper_doi": "10.1145/3488560.3498451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/96dd81bda02205264fdd527bd8a150987220dbb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c5b5adc3830ac45bf1d764603b1b71e5f729616",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate temporal reasoning in dynamic knowledge graphs, focusing on global events and their impact over time. | Used to evaluate temporal reasoning in dynamic knowledge graphs, focusing on event prediction and entity tracking over time. | Used to evaluate static and dynamic knowledge graph reasoning, focusing on entity linking and relation extraction from multilingual Wikipedias. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and link prediction over time. | Used to evaluate static knowledge graph reasoning models, focusing on entity and relation prediction. | Used for experiments on event time prediction in dynamic knowledge graphs, focusing on a smaller subset of nodes compared to ICEWS18. | Used to evaluate static and dynamic knowledge graph reasoning, focusing on entity linking and relation extraction from Wikipedia.",
          "citing_paper_id": "246828738",
          "cited_paper_id": 8040343,
          "context_text": "Method ICEWS14 ICEWS18 WIKI YAGO GDELT\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nSt at ic DistMult 9.72 10.09 22.53 13.86 15.22 31.26 27.96 32.45 39.51 44.05 49.70 59.94 8.61 8.27 17.04 R-GCN 15.03 16.12 31.47 15.05 16.49 29.00 13.96 15.75 22.05 27.43 31.24 44.75 12.17 12.37 20.63 ConvE 21.64 23.16 38.37 22.56 25.41 41.67 26.41 30.36 39.41 41.31 47.10 59.67 18.43 19.57 32.25 RotateE 9.79 9.37 22.24 11.63 12.31 28.03 26.08 31.63 38.51 42.08 46.77 59.39 3.62 2.26 8.37\nTe m po\nra l TA-DistMult 11.29 11.60 23.71 15.62 17.09 32.21 26.44 31.36 38.97 44.98 50.64 61.11 10.34 10.44 21.63 HyTE 7.72 7.94 20.16 7.41 7.33 16.01 25.40 29.16 37.54 14.42 39.73 46.98 6.69 7.57 19.06 dyngraph2vecAE 6.95 8.17 12.18 1.36 1.54 1.61 2.67 2.75 3.00 0.81 0.74 0.76 4.53 1.87 1.87 tNodeEmbed 13.36 13.13 24.31 7.21 7.64 15.75 8.86 10.11 16.36 3.82 3.88 8.07 12.97 12.61 21.22 EvolveGCN 8.32 7.64 18.81 10.31 10.52 23.65 27.19 31.35 38.13 40.50 45.78 55.29 6.54 5.64 15.22 Know-Evolve 0.05 0.00 0.10 0.11 0.00 0.47 0.03 0.00 0.04 0.02 0.00 0.01 0.11 0.02 0.10 Know-Evolve+MLP 16.81 18.63 29.20 7.41 7.87 14.76 10.54 13.08 20.21 5.23 5.63 10.23 15.88 15.69 22.28 DyRep+MLP 17.54 19.87 30.34 7.82 7.73 16.33 10.41 12.06 20.93 4.98 5.54 10.19 16.25 16.45 23.86 R-GCRN+MLP 21.39 23.60 38.96 23.46 26.62 41.96 28.68 31.44 38.58 43.71 48.53 56.98 18.63 19.80 32.42 RE-Net 23.91 26.63 42.70 26.81 30.58 45.92 31.55 34.45 42.26 46.37 51.95 61.59 19.44 20.73 33.81\nEvoKG 27.18±0.001 30.84 ±0.001 47.67 ±0.001 29.28 ±0.002 33.94 ±0.004 50.09 ±0.002 68.03 ±0.031 79.60 ±0.036 85.91 ±0.063 68.59 ±0.003 81.13 ±0.005 92.73 ±0.009 19.28 ±0.001 20.55 ±0.001 34.44 ±0.002\ninferences, EvoKG is 177× on average, and up to 291×, faster than RE-Net.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context provides performance metrics for various methods on different datasets. The datasets mentioned are ICEWS14, ICEWS18, WIKI, YAGO, and GDELT. These are all knowledge graphs or datasets used for evaluating multi-modal reasoning models.",
          "citing_paper_doi": "10.1145/3488560.3498451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/96dd81bda02205264fdd527bd8a150987220dbb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22f95fb555278ebe0ad2e5e18aaaf8be3e4e53fe",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ICEWS14",
          "dataset_description": "Used to evaluate static knowledge graph reasoning models, focusing on entity and relation prediction. | Used to evaluate temporal knowledge graph reasoning models, focusing on event prediction and link prediction over time.",
          "citing_paper_id": "246828738",
          "cited_paper_id": 52183483,
          "context_text": "Method ICEWS14 ICEWS18 WIKI YAGO GDELT\nMRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10 MRR H@3 H@10\nSt at ic DistMult 9.72 10.09 22.53 13.86 15.22 31.26 27.96 32.45 39.51 44.05 49.70 59.94 8.61 8.27 17.04 R-GCN 15.03 16.12 31.47 15.05 16.49 29.00 13.96 15.75 22.05 27.43 31.24 44.75 12.17 12.37 20.63 ConvE 21.64 23.16 38.37 22.56 25.41 41.67 26.41 30.36 39.41 41.31 47.10 59.67 18.43 19.57 32.25 RotateE 9.79 9.37 22.24 11.63 12.31 28.03 26.08 31.63 38.51 42.08 46.77 59.39 3.62 2.26 8.37\nTe m po\nra l TA-DistMult 11.29 11.60 23.71 15.62 17.09 32.21 26.44 31.36 38.97 44.98 50.64 61.11 10.34 10.44 21.63 HyTE 7.72 7.94 20.16 7.41 7.33 16.01 25.40 29.16 37.54 14.42 39.73 46.98 6.69 7.57 19.06 dyngraph2vecAE 6.95 8.17 12.18 1.36 1.54 1.61 2.67 2.75 3.00 0.81 0.74 0.76 4.53 1.87 1.87 tNodeEmbed 13.36 13.13 24.31 7.21 7.64 15.75 8.86 10.11 16.36 3.82 3.88 8.07 12.97 12.61 21.22 EvolveGCN 8.32 7.64 18.81 10.31 10.52 23.65 27.19 31.35 38.13 40.50 45.78 55.29 6.54 5.64 15.22 Know-Evolve 0.05 0.00 0.10 0.11 0.00 0.47 0.03 0.00 0.04 0.02 0.00 0.01 0.11 0.02 0.10 Know-Evolve+MLP 16.81 18.63 29.20 7.41 7.87 14.76 10.54 13.08 20.21 5.23 5.63 10.23 15.88 15.69 22.28 DyRep+MLP 17.54 19.87 30.34 7.82 7.73 16.33 10.41 12.06 20.93 4.98 5.54 10.19 16.25 16.45 23.86 R-GCRN+MLP 21.39 23.60 38.96 23.46 26.62 41.96 28.68 31.44 38.58 43.71 48.53 56.98 18.63 19.80 32.42 RE-Net 23.91 26.63 42.70 26.81 30.58 45.92 31.55 34.45 42.26 46.37 51.95 61.59 19.44 20.73 33.81\nEvoKG 27.18±0.001 30.84 ±0.001 47.67 ±0.001 29.28 ±0.002 33.94 ±0.004 50.09 ±0.002 68.03 ±0.031 79.60 ±0.036 85.91 ±0.063 68.59 ±0.003 81.13 ±0.005 92.73 ±0.009 19.28 ±0.001 20.55 ±0.001 34.44 ±0.002\ninferences, EvoKG is 177× on average, and up to 291×, faster than RE-Net.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context provides performance metrics for various methods on different datasets. The datasets mentioned are ICEWS14, ICEWS18, WIKI, YAGO, and GDELT. These are all knowledge graphs or datasets used for evaluating multi-modal reasoning models.",
          "citing_paper_doi": "10.1145/3488560.3498451",
          "cited_paper_doi": "10.18653/v1/D18-1516",
          "citing_paper_url": "https://www.semanticscholar.org/paper/96dd81bda02205264fdd527bd8a150987220dbb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/508b864a5c46a1adac622cbac877ed3bdac9aec7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "263866442",
      "citation_count": 0,
      "total_dataset_mentions": 8,
      "unique_datasets": [
        "YAGO"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used to emphasize general knowledge, integrating information about people, cities, countries, movies, and organizations from WordNet and Wikipedia.",
          "citing_paper_id": "245904709",
          "cited_paper_id": 207163173,
          "context_text": "Empha-sizing general knowledge, the dataset contains various information about people, cities, countries, movies, and organizations [27].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a dataset containing various information, which aligns with the characteristics of YAGO, a knowledge graph that unifies WordNet and Wikipedia.",
          "citing_paper_doi": "10.48550/arXiv.2309.01169",
          "cited_paper_doi": "10.1145/1242572.1242667",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3034935bf12190465f7dbee1c05d74abbbd7767",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff",
          "citing_paper_year": 2023,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used as a classic example of a general domain knowledge graph, focusing on a collaborative, multilingual knowledge base. | Used as a classic example of a general domain knowledge graph, focusing on structured information extraction and representation. | Used as a classic example of a general domain knowledge graph, focusing on extracting structured information from Wikipedia.",
          "citing_paper_id": "246938066",
          "cited_paper_id": 2858079,
          "context_text": "Examples of classic general domain knowledge graphs include YAGO [11], DBpedia [12], Wikidata [13], etc.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions YAGO, DBpedia, and Wikidata as examples of classic general domain knowledge graphs. These are specific, verifiable resources that can be traced to their sources.",
          "citing_paper_doi": "10.3390/info13020091",
          "cited_paper_doi": "10.1145/2487788.2487935",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf7eb1e29a3d6eed2ec739e9cd5c8c96f8027f1b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec59845da9ee7efd33787b23f2f7c4be532da00e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used to obtain fine-grained types for entity linking, which are then mapped to the DARPA AIDA ontology, enhancing the granularity and specificity of entity types.",
          "citing_paper_id": "218551030",
          "cited_paper_id": 263866442,
          "context_text": "We obtain the YAGO (Suchanek et al., 2008) ﬁne-grained types from the results of Freebase entity linking, and map these types to the DARPA AIDA ontology.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "YAGO is mentioned as a source of fine-grained types, which are mapped to the DARPA AIDA ontology. It is used for entity linking and type mapping.",
          "citing_paper_doi": "10.18653/v1/2020.acl-demos.11",
          "cited_paper_doi": "10.1016/J.WEBSEM.2008.06.001",
          "citing_paper_url": "https://www.semanticscholar.org/paper/51c8975d88aa66781300e8ca88272ab3112445c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/302882085086648b05cf961e91cd759ae6a925a0",
          "citing_paper_year": 2020,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used to evaluate entity matching approaches in the OAEI, focusing on linking entities across knowledge graphs.",
          "citing_paper_id": "76663467",
          "cited_paper_id": 18372770,
          "context_text": "A large number of approaches of the entity matching literature have been evaluated as part of the Ontology Alignment Evaluation Initiative (OAEI) [1] using data sets such as YAGO, FREEBASE, and IMDB[16,28,22].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used in the OAEI for evaluating entity matching approaches. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1007/978-3-030-21348-0_30",
          "cited_paper_doi": "10.1007/978-3-642-13489-0_23",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4e5d4ce582e57eff190a404d82b10875677f006b",
          "citing_paper_year": 2019,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used as a lightweight and extensible ontology built from Wikidata and unified with WordNet, focusing on semantic knowledge integration. | Used to enhance temporal Knowledge Graph Reasoning by incorporating extra time information, specifically focusing on the integration of temporal data into the graph structure.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 207163173,
          "context_text": "• YAGO [233], [234] for temporal KGR contains extra time information.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "YAGO is mentioned as a dataset containing extra time information for temporal Knowledge Graph Reasoning. It is a specific, verifiable resource with clear provenance.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/1242572.1242667",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used as an example of a multilingual knowledge base constructed from Wikipedia, WordNet, and Geonames, providing structural knowledge for various applications and languages.",
          "citing_paper_id": "196205749",
          "cited_paper_id": 9770446,
          "context_text": "Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions YAGO as an example of a knowledge graph, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/P19-1140",
          "cited_paper_doi": "10.1007/978-3-319-46547-0_19",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf0f3b886f4d4ab232a2fdf47819c2a89a5b1625",
          "cited_paper_url": "https://www.semanticscholar.org/paper/772f69b1dd966610edaa4ef2576d36142dc05928",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used to identify and link entity mentions in news content, enhancing multi-modal reasoning by integrating structured knowledge.",
          "citing_paper_id": "235306349",
          "cited_paper_id": 207167677,
          "context_text": "First, we identify entity mentions in the news contents, and then obtain corresponding entities through external knowledge graph such as YAGO (Suchanek, Kasneci, and Weikum 2007), Freebase (Bollacker et al. 2008), Wikidata (Vrande-cic and Kr¨otzsch 2014) and Probase (Wu et al. 2012).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs used for obtaining corresponding entities from news content. These are specific, verifiable resources with clear provenance.",
          "citing_paper_doi": "10.1609/aaai.v35i1.16080",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b357470ddbd30729cb39dc88ea76ed08bc5dabb0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used to identify and link entity mentions in news content, enhancing multi-modal reasoning by integrating structured knowledge. | Used as a structured knowledge base to enhance multi-modal reasoning, providing a rich source of linked data for entity linking and knowledge integration. | Utilized as a large-scale taxonomy and concept hierarchy to enrich multi-modal reasoning, offering a comprehensive set of concepts and categories for semantic enrichment. | Used to extract entity contexts by retrieving neighboring entities, focusing on enhancing multi-modal knowledge graph reasoning through rich entity relationships.",
          "citing_paper_id": "235306349",
          "cited_paper_id": null,
          "context_text": "First, we identify entity mentions in the news contents, and then obtain corresponding entities through external knowledge graph such as YAGO (Suchanek, Kasneci, and Weikum 2007), Freebase (Bollacker et al. 2008), Wikidata (Vrande-cic and Kr¨otzsch 2014) and Probase (Wu et al. 2012).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs used for obtaining corresponding entities from news content. These are specific, verifiable resources with clear provenance.",
          "citing_paper_doi": "10.1609/aaai.v35i1.16080",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b357470ddbd30729cb39dc88ea76ed08bc5dabb0",
          "cited_paper_url": null,
          "citing_paper_year": 2021,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "214775221",
      "citation_count": 0,
      "total_dataset_mentions": 8,
      "unique_datasets": [
        "VQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "VQA",
          "dataset_description": "Used to train and evaluate visual question answering models, containing 50,000 images and 150,000 questions, with clipart images to abstract the visual reasoning task. | Used to train and evaluate models on visual question answering tasks, focusing on the integration of image and text modalities. | Used to develop and evaluate visual question answering systems, focusing on the ability to answer questions about images using multi-modal reasoning.",
          "citing_paper_id": "85558018",
          "cited_paper_id": 3180429,
          "context_text": "Moreover, introduction of largescale Visual Question Answering datasets, such VQA (Antol et al. 2015) and VQA v2.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions VQA and VQA v2 as largescale Visual Question Answering datasets. VQA is confirmed as a dataset by the cited paper title.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "VQA is used to train and evaluate models for visual question answering, focusing on the relationship between images and text. | GQA balanced version is used to evaluate models on real-world visual reasoning and compositional question answering, ensuring balanced distribution of questions. | VG-QA is used to evaluate models on visual question answering tasks using the Visual Genome dataset, focusing on complex scene understanding.",
          "citing_paper_id": "237453242",
          "cited_paper_id": 152282269,
          "context_text": "0 (Antol et al., 2015b), GQA balanced version (Hudson and Manning, 2019) and VG-QA (Zhu et al., 2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three datasets: VQA, GQA balanced version, and VG-QA. These are all visual question answering datasets, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/2021.emnlp-main.517",
          "cited_paper_doi": "10.1109/CVPR.2019.00686",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4e92fec0a61972ae076707d0630d1333affccdfc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a7ac99d7cf3f568ab1a741392144b646b856ae0c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "This dataset 'VQA' was mentioned in the citation context but no detailed description was generated. | Used to train and evaluate a multi-modal model, specifically for reasoning tasks involving visual and textual inputs, focusing on the integration of image and question data.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 3180429,
          "context_text": ", “Commonsense” and “KB-knowledge”), 55% of which are mentioned very rarely (less than 20 times) in the 300K questions of the VQA [Antol et al., 2015] dataset.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the VQA dataset, which is a well-known multi-modal dataset combining images and textual questions. It is used to evaluate visual question answering systems.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2015,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "Used for fact-based visual question answering, integrating factual knowledge with visual information. | Focuses on open-ended questions that require external knowledge beyond the image content. | Utilizes a subset of VQA with wh-questions to evaluate fine-grained reasoning. | Utilizes knowledge bases to enhance visual question answering by incorporating external knowledge. | Used for visual question answering, focusing on image-text pairs to test reasoning capabilities. | Employs knowledge graphs to answer questions that require reasoning over structured data and images. | Employs complex questions requiring multi-step reasoning over images and knowledge graphs.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 7483388,
          "context_text": "VQA GQA [119] Visual7w [120] OK-VQA [56] FVQA [33] KVQA [121] KB-VQA [122] 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for visual question answering and knowledge-based reasoning, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "Used to assess the model's ability to answer questions about images, emphasizing the combination of visual and textual information. | Used to evaluate the model's performance on visual question answering, focusing on the integration of image and text modalities. | Used to evaluate visual question answering systems, focusing on the integration of visual and textual information for complex query resolution. | Used for visual question answering, containing 614,163 questions and 6,141,630 answers based on 204,721 MS COCO images, focusing on multi-modal reasoning between text and images. | Utilized to assess the performance of models in visual question answering, emphasizing the ability to reason over images and text. | Used to develop and evaluate visual question answering systems, integrating image and text data to answer questions about images. | Used to support free-form and open-ended Visual Question Answering, providing a resource for training and evaluating multi-modal reasoning systems.",
          "citing_paper_id": "206594383",
          "cited_paper_id": 3180429,
          "context_text": "in [1] provide the VQA dataset which is intended to support “free-form and open-ended Visual Question Answering”.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the VQA dataset, which is a specific, verifiable resource used for visual question answering tasks.",
          "citing_paper_doi": "10.1109/CVPR.2016.500",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/20dbdf02497aa84510970d0f5e8b599073bca1bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2015,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "Mentioned as an example application area for visual commonsense knowledge, specifically for visual question answering tasks.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 3180429,
          "context_text": "Our VTKG datasets and VISTA model can be utilized in diverse applications and scenarios (Sekuboyina et al., 2019; Kwak et al., 2022; Lee et al., 2023), including those requiring visual commonsense knowledge such as VQA (Marino et al., 2021) or scene graph generation (Chang et al., 2023; Zareian et al., 2020) and commonsense reasoning (Lin et al., 2019).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions VQA, which is a well-known dataset for visual question answering, but does not explicitly state that it is used in the research. The context suggests it is mentioned as an example application area.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "Used to evaluate visual question answering models, focusing on the ability to answer questions about images using multimodal fusion. | Used to assess visual commonsense reasoning, specifically the ability to understand and reason about complex visual scenes and their contexts. | Used to test referring expression comprehension, focusing on the ability to locate objects in images based on textual descriptions.",
          "citing_paper_id": "250627337",
          "cited_paper_id": 3180429,
          "context_text": "For example, operating on concatenated image and text tokens, a single-stream model [32,53,8,29,27] is suitable for multimodal fusion tasks such as VQA [2], VCR [71] and RefCOCO [70].",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions VQA, VCR, and RefCOCO as multimodal fusion tasks, which are datasets used for evaluating multimodal models. However, the context does not specify the actual usage of these datasets in the research.",
          "citing_paper_doi": "10.48550/arXiv.2207.08150",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9802779322febdb9fdd6177d3da41c1269a6e05c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "VQA",
          "dataset_description": "Used to evaluate visual question answering models, focusing on the ability to answer questions about images using multimodal fusion. | Used to assess visual commonsense reasoning, specifically the ability to understand and reason about complex visual scenes and their contexts. | Used to test referring expression comprehension, focusing on the ability to locate objects in images based on textual descriptions.",
          "citing_paper_id": "250627337",
          "cited_paper_id": 214775221,
          "context_text": "For example, operating on concatenated image and text tokens, a single-stream model [32,53,8,29,27] is suitable for multimodal fusion tasks such as VQA [2], VCR [71] and RefCOCO [70].",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions VQA, VCR, and RefCOCO as multimodal fusion tasks, which are datasets used for evaluating multimodal models. However, the context does not specify the actual usage of these datasets in the research.",
          "citing_paper_doi": "10.48550/arXiv.2207.08150",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9802779322febdb9fdd6177d3da41c1269a6e05c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/598a2ee223e2949c3b28389e922c1892b4717d2a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "202539519",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "WN18"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN18",
          "dataset_description": "Used to highlight test leakage issues in knowledge graph reasoning, where test triples appear in the training dataset with inverse relations.",
          "citing_paper_id": "255393926",
          "cited_paper_id": 4328400,
          "context_text": "Some previous work (Dettmers et al. 2018) has indicated the test leakage ﬂaw in WN18 and FB15k, which means test triples appear in train dataset with inverse relations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets (WN18 and FB15k) and discusses a test leakage issue, indicating their use in evaluating knowledge graph embeddings.",
          "citing_paper_doi": "10.48550/arXiv.2301.00982",
          "cited_paper_doi": "10.1609/aaai.v32i1.11573",
          "citing_paper_url": "https://www.semanticscholar.org/paper/354b651dbc3ba2af4c3785ccbecd3df0585d30b2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9697d32ed0a16da167f2bdba05ef96d0da066eb5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "WN18",
          "dataset_description": "Referenced as the source of lexical information, specifically used to extract semantic relationships for the research. | Used as a subset for WN9-IMG to construct a multi-modal knowledge graph, focusing on the integration of textual and visual information for reasoning tasks.",
          "citing_paper_id": "9909815",
          "cited_paper_id": 1671874,
          "context_text": "The triple part of WN9-IMG is the subset of a classical KG dataset WN18 [Bordes et al. , 2014], which is originally extracted from WordNet [Miller, 1995].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WN18 as a classical KG dataset, which is a specific, verifiable dataset. It is used as a subset for WN9-IMG, indicating its relevance to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.24963/ijcai.2017/438",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2016,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "WN18",
          "dataset_description": "Used to include triples from a subset of the Knowledge Graph dataset, focusing on evaluating multi-modal reasoning approaches using lexical data.",
          "citing_paper_id": "211137418",
          "cited_paper_id": 1671874,
          "context_text": "First, we included triples from a subset of the KG data set WN18 [50], which was originally developed based on WordNet [51].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WN18' as a dataset derived from WordNet, which is a lexical database. WN18 is a specific, verifiable dataset used in the research.",
          "citing_paper_doi": "10.1109/TCDS.2019.2906685",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3f3bba81ab55d7ca7d3064241d7595592bc9dc86",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2020,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "WN18",
          "dataset_description": "Used to assess robustness and reliability of knowledge graph embeddings, addressing issues in WN18 by filtering out inverse relations. | Used to evaluate knowledge graph embedding models, focusing on link prediction performance using a subset of WordNet. | Used to test knowledge graph embedding models, focusing on relation prediction in a subset of Freebase. | Used to test the scalability and effectiveness of knowledge graph embeddings, derived from Freebase with a focus on entity and relation types. | Used to remove specific relations in knowledge graphs, improving inference by eliminating redundant information and enhancing model performance. | Used to assess model performance on relation prediction, specifically addressing issues with inverse relations in validation and test sets. | Used to evaluate knowledge graph embedding models, focusing on relation prediction in a subset of WordNet. | Used to improve the evaluation of knowledge graph embeddings by removing inverse relations and reducing redundancy compared to FB15k.",
          "citing_paper_id": "221082536",
          "cited_paper_id": 4328400,
          "context_text": "We conducted the experiments on four benchmark datasets: WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), FB15k (Bordes et al., 2013) and FB15k-237 (Toutanova et al., 2015) (see Appendix B for the details, including best hyperparameters and additional experiments).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific datasets used for conducting experiments. These datasets are well-known in the field of knowledge graph reasoning and are clearly identified.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1609/aaai.v32i1.11573",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0126fce30b412d583f8e33714908dd09b86293d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9697d32ed0a16da167f2bdba05ef96d0da066eb5",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "WN18",
          "dataset_description": "Used in multimodal knowledge graph completion research, but noted for having a test leakage issue. | Used in multimodal knowledge graph completion research, serving as a benchmark dataset. | Proposed to resolve the test leakage issue found in WN18, used in multimodal knowledge graph completion research. | Used to compare processing times between VISTA and MKG-former, focusing on efficiency in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "266166905",
          "cited_paper_id": null,
          "context_text": "…WN18 (Bordes et al., 2013) and FB15K237 (Toutanova and Chen, 2015) are benchmark datasets used in other multimodal knowledge graph completion research (Zhao et al., 2022; Chen et al., 2022), WN18 has a test leakage issue, and WN18RR (Dettmers et al., 2018) has been proposed to resolve the issue.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used in multimodal knowledge graph completion research, including their issues and improvements.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        },
        {
          "dataset_name": "WN18",
          "dataset_description": "Used to evaluate knowledge graph embedding models, specifically focusing on multi-relational data and translational distance-based scoring functions. | Used to extend WN18 with 10 images per entity, enhancing multi-modal reasoning capabilities in knowledge graphs. | Used for KG embedding link prediction tasks, specifically to evaluate models' ability to predict missing links in knowledge graphs.",
          "citing_paper_id": "239011538",
          "cited_paper_id": 14941970,
          "context_text": "WN18-IMG: WN18 [4] is a well-known KG which is originally extracted from WordNet [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WN18, a knowledge graph derived from WordNet, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3474085.3475470",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2021,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "WN18",
          "dataset_description": "Used to evaluate the performance of MoSE against KG-BERT and MKGformer in knowledge graph completion tasks, focusing on relation prediction accuracy.",
          "citing_paper_id": "252918783",
          "cited_paper_id": 202539519,
          "context_text": "It is worth noting that even compared to the pre-trained language model methods, MoSE outperforms KG-BERT and MKGformer in all metrics on FB15K-237 and WN18 datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, FB15K-237 and WN18, which are used to evaluate the performance of different models in knowledge graph completion tasks.",
          "citing_paper_doi": "10.48550/arXiv.2210.08821",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f399dce9bbb2754d33c5aefb60bfe54f260a4b21",
          "cited_paper_url": "https://www.semanticscholar.org/paper/31184789ef4c3084af930b1e0dede3215b4a9240",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "3994012",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "Flickr30k"
      ],
      "dataset_details": [
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "Used to parse captions from images, focusing on multi-modal reasoning tasks that combine visual and textual information. | Used to evaluate multi-modal reasoning performance, focusing on image captioning and object grounding tasks. | Used to assess the ability to refer to objects in photographs of natural scenes, emphasizing natural language understanding and visual recognition. | Used to validate the model on image captioning tasks, focusing on the alignment between textual descriptions and visual content. | Used for multi-modal reasoning tasks, specifically for object referring in photographs of natural scenes, employing neural architectures. | Used to validate the model on referring expression comprehension tasks, focusing on identifying objects in photographs of natural scenes.",
          "citing_paper_id": "204972196",
          "cited_paper_id": 6308361,
          "context_text": ", Flickr30k [24] and ReferIt Game [14]) and neural architectures of various forms.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two datasets, Flickr30k and ReferIt Game, which are used in the context of multi-modal reasoning, likely for object referring tasks in images.",
          "citing_paper_doi": "10.1109/ICCV.2019.00438",
          "cited_paper_doi": "10.3115/v1/D14-1086",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0a7bb3c251498a7c700c5f0563a53aea54345653",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92c141447f51b6732242376164ff961e464731c8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "YAGO is utilized for encyclopedia knowledge, linking entities to time, space, and other ontological categories. | Probase is used for taxonomic knowledge, providing a large-scale taxonomy of concepts and their relationships. | Used to train detectors with supervised data, focusing on image-text pairs with entity annotations for multi-modal learning tasks. | ConceptNet is utilized for common sense knowledge, offering a large graph of semantic relationships between concepts. | WikiData is used for encyclopedia knowledge, providing a collaborative, multilingual knowledge base. | WordNet serves as a lexical knowledge base, providing a structured vocabulary and semantic relations among words. | Cyc is used as a source of common sense knowledge, providing a structured representation of everyday concepts and relationships. | BabelNet is used for lexical knowledge, integrating WordNet and Wikipedia to provide multilingual lexical information. | GeoNames is utilized for geographic knowledge, offering a comprehensive database of place names and locations. | Freebase is employed as an encyclopedia knowledge base, containing structured information from various sources. | DBpedia is used for encyclopedia knowledge, extracting structured information from Wikipedia articles. | Used to train detectors with supervised data, focusing on image-text pairs for multi-modal learning tasks. | CN-DBpedia is employed for Chinese-specific encyclopedia knowledge, extracting structured information from Chinese Wikipedia.",
          "citing_paper_id": "246823061",
          "cited_paper_id": null,
          "context_text": "These detectors are trained with supervised data from public images-text datasets [21] (such as MSCOCO [58], Flickr30k [59], Flick30k Entities [60] and Open Images [61]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image-text datasets used for training detectors, which are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "Used to evaluate multi-modal reasoning models under zero-shot and fine-tuned settings, focusing on image captioning and visual question answering tasks. | Used to evaluate the FILIP model in image-text retrieval tasks, focusing on region-to-phrase correspondences under zero-shot and fine-tuned settings. | Used to evaluate the FILIP model in image-text retrieval tasks, focusing on common objects in context under zero-shot and fine-tuned settings.",
          "citing_paper_id": "244117525",
          "cited_paper_id": 14113767,
          "context_text": "We evaluate our FILIP model on two retrieval benchmark datasets: Flickr30K (Plummer et al., 2015) and MSCOCO (Lin et al., 2014), under both zero-shot and ﬁne-tuned settings.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Flickr30K and MSCOCO, which are used to evaluate the FILIP model in retrieval tasks under different settings.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2021,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "Used for caption retrieval experiments to evaluate the effectiveness of the approach in matching images with their corresponding textual descriptions. | Used to evaluate the SGM model against other models, specifically for tasks involving image captioning and multi-modal reasoning. | Used for image-text retrieval experiments to evaluate the effectiveness of the approach, focusing on the alignment between images and their textual descriptions. | Used for image retrieval experiments to assess the performance of the method in identifying images based on textual queries. | Used to evaluate caption retrieval and image retrieval performance, comparing the model against state-of-the-art models using R@1 metrics. | Used for image-text retrieval experiments, focusing on evaluating the performance of the proposed method in aligning images with their textual descriptions. | Used for image-text retrieval, containing 123,287 images with multiple captions per image to assess cross-modal alignment. | Used to compare the performance of the SGM model with state-of-the-art models, focusing on image captioning and multi-modal reasoning. | Used for image-text retrieval, containing 31,783 images paired with descriptive captions to evaluate cross-modal matching. | Used to evaluate the performance of the proposed method on image captioning, comparing against other methods using 5k test images. | Used to train, validate, and test image-text matching models, specifically splitting 1,000 images for validation, 1,000 for testing, and the rest for training. | Used for training with an initial learning rate of 0.0005, focusing on image captioning and visual denotation tasks. | Used to evaluate the graph matching based approach for image-text retrieval, focusing on the accuracy of cross-modal matching.",
          "citing_paper_id": "204402762",
          "cited_paper_id": 3104920,
          "context_text": "Following [3, 17], we split Flickr30k as 1, 000 images for validation, 1, 000 images for testing and the rest for training.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the Flickr30k dataset, which is a well-known multi-modal dataset used for image-text matching tasks. The dataset is split into training, validation, and testing sets.",
          "citing_paper_doi": "10.1109/WACV45572.2020.9093614",
          "cited_paper_doi": "10.1162/tacl_a_00166",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c9f1b9e51c1d16c346e1ef679c243d61702a8a80",
          "cited_paper_url": "https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "Used to train, validate, and test image-text matching models, specifically splitting 1,000 images for validation, 1,000 for testing, and the rest for training.",
          "citing_paper_id": "204402762",
          "cited_paper_id": 3994012,
          "context_text": "Following [3, 17], we split Flickr30k as 1, 000 images for validation, 1, 000 images for testing and the rest for training.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Flickr30k dataset, which is a well-known multi-modal dataset used for image-text matching tasks. The dataset is split into training, validation, and testing sets.",
          "citing_paper_doi": "10.1109/WACV45572.2020.9093614",
          "cited_paper_doi": "10.1007/978-3-030-01225-0_13",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c9f1b9e51c1d16c346e1ef679c243d61702a8a80",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45dd2a3cd7c27f2e9509b023d702408f5ac11c9d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Flickr30k",
          "dataset_description": "Used to assess image-text retrieval capabilities, emphasizing caption generation and alignment between images and textual descriptions. | Used to evaluate image-text retrieval performance, focusing on region-to-phrase correspondences in image-to-sentence models.",
          "citing_paper_id": "252917745",
          "cited_paper_id": 6941275,
          "context_text": "We first conduct experiments on Flickr30k [37] and COCO Caption [8] dataset to show the performances of our model on image-text retrieval tasks.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Flickr30k and COCO Caption, which are used for image-text retrieval tasks.",
          "citing_paper_doi": "10.48550/arXiv.2210.08901",
          "cited_paper_doi": "10.1007/s11263-016-0965-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11c9c31dff70de92ada9160c78ff8bb46b2912d6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "3088903",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "FVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "FVQA",
          "dataset_description": "Used for fact-based visual question answering, evaluating models on 2,190 images, 5,286 questions, and a knowledge base of 193,449 facts, focusing on integrating visual and textual information. | Used to answer questions by integrating visual and factual knowledge, focusing on multi-modal reasoning and information complementary evidence from visual and knowledge perspectives. | Used to retrieve supporting knowledge for fact-based visual question answering, enhancing the model's ability to select the correct entity as the predicted answer.",
          "citing_paper_id": "221397171",
          "cited_paper_id": 7483388,
          "context_text": "e samples respectively. The entity corresponding to the concept with the largest probability is selected as the ﬁnal answer. 4. Experiments 4.1. Datasets and Evaluation Metrics FVQA: The FVQA dataset [5] consists of 2,190 images, 5,286 questions and a knowledge base of 193,449 facts. The knowledge base is constructed by extracting the top visual concepts from all the images in the dataset and queryin",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the FVQA dataset, which is a specific dataset used for fact-based visual question answering. It includes images, questions, and a knowledge base of facts.",
          "citing_paper_doi": "10.1016/j.patcog.2020.107563",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e526624783b3b5687da54b8cd4a7190a26a0b5e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FVQA",
          "dataset_description": "Contributes to the FVQA dataset by providing structured common-sense knowledge for fact-based visual question answering. | Serves as a structured database for fact-based visual question answering, providing semantic information. | Provided a part of the knowledge base for the FVQA dataset, supplying a large semantic network of common-sense knowledge. | Used as a source for the knowledge base in the FVQA dataset, providing commonsense knowledge extracted from the web. | Used to construct the knowledge base for the FVQA dataset, providing commonsense knowledge extracted from the web. | Introduced by Wang et al., this dataset contains images, questions, answers, and a knowledge base from multiple sources, designed for fact-based visual question answering. | Used to extract and organize commonsense knowledge for constructing a knowledge base of visual concepts, enhancing multi-modal reasoning. | Provides structured commonsense knowledge harvested from the web, enhancing the FVQA dataset for fact-based visual question answering. | Contributed to the knowledge base for the FVQA dataset, offering structured information from Wikipedia. | Provided a part of the knowledge base in the FVQA dataset, supplying a large semantic network of common-sense knowledge. | Contributed to the knowledge base in the FVQA dataset, offering structured information from Wikipedia. | Utilized to query and integrate commonsense knowledge for constructing a knowledge base of visual concepts, supporting multi-modal reasoning. | Used to evaluate fact-based visual question answering, focusing on integrating structured knowledge from multiple sources. | Employed to retrieve structured information from Wikipedia for constructing a knowledge base of visual concepts, aiding in multi-modal reasoning.",
          "citing_paper_id": "53199920",
          "cited_paper_id": 3088903,
          "context_text": "To bridge this discrepancy between human behavior and present day algorithmic design, Wang et al. [50] introduced a novel ‘fact-based’ VQA (FVQA) task, and an accompanying dataset containing images, questions with corresponding answers and a knowledge base (KB) of facts extracted from three different sources: WebChild [47], DBPedia [3] and ConceptNet [45].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a dataset created by Wang et al. that includes images, questions, answers, and a knowledge base from WebChild, DBPedia, and ConceptNet. These are specific, verifiable resources.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1145/2556195.2556245",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db95087540c956788a8560495bf03ded1239e062",
          "citing_paper_year": 2018,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "FVQA",
          "dataset_description": "Used to evaluate the model on factual visual question answering, focusing on integrating visual and textual information to answer questions accurately. | Contributes to the FVQA dataset by providing structured common-sense knowledge for fact-based visual question answering. | Serves as a structured database for fact-based visual question answering, providing semantic information. | Provided a part of the knowledge base for the FVQA dataset, supplying a large semantic network of common-sense knowledge. | Used to construct the knowledge base for the FVQA dataset, providing commonsense knowledge extracted from the web. | Provides structured commonsense knowledge harvested from the web, enhancing the FVQA dataset for fact-based visual question answering. | Used to demonstrate the proposed algorithm's performance in fact-based visual question answering, specifically comparing against state-of-the-art methods. | Contributed to the knowledge base for the FVQA dataset, offering structured information from Wikipedia. | Used to evaluate fact-based visual question answering, focusing on integrating structured knowledge from multiple sources.",
          "citing_paper_id": "53199920",
          "cited_paper_id": 7483388,
          "context_text": "We evaluate our method on the dataset released as part of the FVQA work, referred to as the FVQA dataset [50], which is a subset of three structured databases – DBpedia [3], ConceptNet [45], and WebChild [47].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions the FVQA dataset, which is a subset of three structured databases: DBpedia, ConceptNet, and WebChild. These are clearly identified as datasets used for evaluation.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FVQA",
          "dataset_description": "Used to provide external knowledge for fact-based visual question answering, specifically incorporating 225,434 factual triplets to enhance question answering accuracy.",
          "citing_paper_id": "269157470",
          "cited_paper_id": 7483388,
          "context_text": "The questions in this dataset use exter-nal knowledge from FVQA dataset 225434 factual triplets (Wang et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the FVQA dataset, which is a specific, verifiable resource used for fact-based visual question answering.",
          "citing_paper_doi": "10.48550/arXiv.2404.10226",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a712f22b419623590b5b121278b7709cd1d08a64",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FVQA",
          "dataset_description": "Used to provide image-text pairs for multi-modal reasoning tasks, focusing on the relationship between images and structured triples in a knowledge graph.",
          "citing_paper_id": "237453242",
          "cited_paper_id": 7483388,
          "context_text": "FVQA (Wang et al., 2017a) is a fact-based VQA dataset that provides image-question-answer-supporting fact tuples, where the supporting fact is a structured triple, e.g., (cid:104) Cat, CapableOf,ClimbingTrees (cid:105) .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FVQA as a dataset providing image-question-answer-supporting fact tuples, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/2021.emnlp-main.517",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4e92fec0a61972ae076707d0630d1333affccdfc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FVQA",
          "dataset_description": "Used to provide image-text pairs for multi-modal reasoning tasks, focusing on the relationship between images and structured triples in a knowledge graph. | Used to evaluate visual reasoning models, focusing on multi-modal reasoning tasks involving images and knowledge bases. | Used to evaluate multi-modal reasoning models, focusing on multi-modal data, including images and textual information, and knowledge graph reasoning.",
          "citing_paper_id": "237453242",
          "cited_paper_id": null,
          "context_text": "FVQA (Wang et al., 2017a) is a fact-based VQA dataset that provides image-question-answer-supporting fact tuples, where the supporting fact is a structured triple, e.g., (cid:104) Cat, CapableOf,ClimbingTrees (cid:105) .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions FVQA as a dataset providing image-question-answer-supporting fact tuples, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/2021.emnlp-main.517",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4e92fec0a61972ae076707d0630d1333affccdfc",
          "cited_paper_url": null,
          "citing_paper_year": 2021,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "9026666",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "LSMDC"
      ],
      "dataset_details": [
        {
          "dataset_name": "LSMDC",
          "dataset_description": "Used to evaluate methods for generating descriptions from multimodal data, comparing performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr. | Used to assess the effectiveness of KAGS and KAGS-T models in multi-modal captioning, emphasizing the integration of visual and textual information. | Used to evaluate the performance of KAGS and KAGS-T models on visual storytelling tasks, focusing on multi-modal reasoning and generation. | Used to evaluate the proposed KAGS model in multi-modal reasoning, specifically for image description tasks, achieving top scores in multiple metrics including BLEU, METEOR, ROUGE_L, and CIDEr.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 964287,
          "context_text": "As seen from the results, it is obvious that the statistical results of KAGS are better than the other four competing methods on\nTABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance\nMethods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr\nVIST\nseq2seq [31] (NAACL2016) - - - 3.5 31.4 - 6.8 BARNN [37] (AAAI2017) - - - - 33.3 - - h-attn-rank [38] (EMNLP2017) - - 21.0 - 34.1 29.5 7.5 XE-ss [1] (ACL2018) 62.3 38.2 22.5 13.7 34.8 29.7 8.7 AREL [1] (ACL2018) 63.7 39.0 23.1 14.0 35.0 29.6 9.5 HPSR [39] (AAAI2019) 61.9 37.8 21.5 12.2 34.4 31.2 8.0 HSRL [17] (AAAI2019) - - - 12.3 35.2 30.8 10.7 VSCMR [40] (ACMMM2019) 63.8 39.5 23.5 14.3 35.5 30.2 9.0 ReCO-RL [18] (AAAI2020) - - - 12.4 33.9 29.9 8.6 INet [3] (AAAI2020) 64.4 40.1 23.9 14.7 35.6 29.0 10.0 SGVST [13] (AAAI2020) 65.1 40.1 23.8 14.7 35.8 29.9 9.8 IRW [27] (AAAI2021) 66.7 41.6 25.0 15.4 35.6 29.6 11.0 TAPM [41] (CVPR2021) 64.2 39.9 24.5 14.3 35.1 29.7 10.5 KAGS 70.3 44.4 25.5 14.7 36.3 31.6 11.4\nLSMDC\nXE-ss [1] (ACL2018) 27.7 13.7 6.0 2.8 29.9 29.6 6.8 AREL [1] (ACL2018) 28.9 14.1 6.1 3.1 30.2 29.7 7.1 VSCMR [40] (ACMMM2019) 30.1 15.3 6.8 3.3 30.6 30.0 7.9 TAPM [41] (CVPR2021) 32.3 16.9 7.2 3.9 31.0 29.3 9.5 KAGS 34.6 17.5 7.9 4.3 31.7 30.3 10.3\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of various methods including KAGS.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        },
        {
          "dataset_name": "LSMDC",
          "dataset_description": "Used to evaluate methods for generating descriptions from multimodal data, comparing performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr. | Used to assess the effectiveness of KAGS and KAGS-T models in multi-modal captioning, emphasizing the integration of visual and textual information. | Used to evaluate the performance of KAGS and KAGS-T models on visual storytelling tasks, focusing on multi-modal reasoning and generation. | Used to evaluate the proposed KAGS model in multi-modal reasoning, specifically for image description tasks, achieving top scores in multiple metrics including BLEU, METEOR, ROUGE_L, and CIDEr.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 9026666,
          "context_text": "As seen from the results, it is obvious that the statistical results of KAGS are better than the other four competing methods on\nTABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance\nMethods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr\nVIST\nseq2seq [31] (NAACL2016) - - - 3.5 31.4 - 6.8 BARNN [37] (AAAI2017) - - - - 33.3 - - h-attn-rank [38] (EMNLP2017) - - 21.0 - 34.1 29.5 7.5 XE-ss [1] (ACL2018) 62.3 38.2 22.5 13.7 34.8 29.7 8.7 AREL [1] (ACL2018) 63.7 39.0 23.1 14.0 35.0 29.6 9.5 HPSR [39] (AAAI2019) 61.9 37.8 21.5 12.2 34.4 31.2 8.0 HSRL [17] (AAAI2019) - - - 12.3 35.2 30.8 10.7 VSCMR [40] (ACMMM2019) 63.8 39.5 23.5 14.3 35.5 30.2 9.0 ReCO-RL [18] (AAAI2020) - - - 12.4 33.9 29.9 8.6 INet [3] (AAAI2020) 64.4 40.1 23.9 14.7 35.6 29.0 10.0 SGVST [13] (AAAI2020) 65.1 40.1 23.8 14.7 35.8 29.9 9.8 IRW [27] (AAAI2021) 66.7 41.6 25.0 15.4 35.6 29.6 11.0 TAPM [41] (CVPR2021) 64.2 39.9 24.5 14.3 35.1 29.7 10.5 KAGS 70.3 44.4 25.5 14.7 36.3 31.6 11.4\nLSMDC\nXE-ss [1] (ACL2018) 27.7 13.7 6.0 2.8 29.9 29.6 6.8 AREL [1] (ACL2018) 28.9 14.1 6.1 3.1 30.2 29.7 7.1 VSCMR [40] (ACMMM2019) 30.1 15.3 6.8 3.3 30.6 30.0 7.9 TAPM [41] (CVPR2021) 32.3 16.9 7.2 3.9 31.0 29.3 9.5 KAGS 34.6 17.5 7.9 4.3 31.7 30.3 10.3\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of various methods including KAGS.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1109/CVPR.2015.7299087",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "LSMDC",
          "dataset_description": "Used to evaluate methods for generating descriptions from multimodal data, comparing performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr. | Used to evaluate the performance of KAGS and KAGS-T models on visual storytelling tasks, focusing on multi-modal reasoning and generation. | Used to assess the effectiveness of KAGS and KAGS-T models in multi-modal captioning, emphasizing the integration of visual and textual information. | Used to evaluate the proposed KAGS model in multi-modal reasoning, specifically for image description tasks, achieving top scores in multiple metrics including BLEU, METEOR, ROUGE_L, and CIDEr. | Used to evaluate the performance of KAGS-G with and without GSM, focusing on machine translation metrics such as BLEU-1 and BLEU-2.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 11080756,
          "context_text": "As seen from the results, it is obvious that the statistical results of KAGS are better than the other four competing methods on\nTABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance\nMethods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr\nVIST\nseq2seq [31] (NAACL2016) - - - 3.5 31.4 - 6.8 BARNN [37] (AAAI2017) - - - - 33.3 - - h-attn-rank [38] (EMNLP2017) - - 21.0 - 34.1 29.5 7.5 XE-ss [1] (ACL2018) 62.3 38.2 22.5 13.7 34.8 29.7 8.7 AREL [1] (ACL2018) 63.7 39.0 23.1 14.0 35.0 29.6 9.5 HPSR [39] (AAAI2019) 61.9 37.8 21.5 12.2 34.4 31.2 8.0 HSRL [17] (AAAI2019) - - - 12.3 35.2 30.8 10.7 VSCMR [40] (ACMMM2019) 63.8 39.5 23.5 14.3 35.5 30.2 9.0 ReCO-RL [18] (AAAI2020) - - - 12.4 33.9 29.9 8.6 INet [3] (AAAI2020) 64.4 40.1 23.9 14.7 35.6 29.0 10.0 SGVST [13] (AAAI2020) 65.1 40.1 23.8 14.7 35.8 29.9 9.8 IRW [27] (AAAI2021) 66.7 41.6 25.0 15.4 35.6 29.6 11.0 TAPM [41] (CVPR2021) 64.2 39.9 24.5 14.3 35.1 29.7 10.5 KAGS 70.3 44.4 25.5 14.7 36.3 31.6 11.4\nLSMDC\nXE-ss [1] (ACL2018) 27.7 13.7 6.0 2.8 29.9 29.6 6.8 AREL [1] (ACL2018) 28.9 14.1 6.1 3.1 30.2 29.7 7.1 VSCMR [40] (ACMMM2019) 30.1 15.3 6.8 3.3 30.6 30.0 7.9 TAPM [41] (CVPR2021) 32.3 16.9 7.2 3.9 31.0 29.3 9.5 KAGS 34.6 17.5 7.9 4.3 31.7 30.3 10.3\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of various methods including KAGS.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.3115/1073083.1073135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9",
          "citing_paper_year": 2022,
          "cited_paper_year": 2002
        },
        {
          "dataset_name": "LSMDC",
          "dataset_description": "Used to evaluate methods for generating descriptions from multimodal data, comparing performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 59599945,
          "context_text": "As seen from the results, it is obvious that the statistical results of KAGS are better than the other four competing methods on\nTABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance\nMethods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr\nVIST\nseq2seq [31] (NAACL2016) - - - 3.5 31.4 - 6.8 BARNN [37] (AAAI2017) - - - - 33.3 - - h-attn-rank [38] (EMNLP2017) - - 21.0 - 34.1 29.5 7.5 XE-ss [1] (ACL2018) 62.3 38.2 22.5 13.7 34.8 29.7 8.7 AREL [1] (ACL2018) 63.7 39.0 23.1 14.0 35.0 29.6 9.5 HPSR [39] (AAAI2019) 61.9 37.8 21.5 12.2 34.4 31.2 8.0 HSRL [17] (AAAI2019) - - - 12.3 35.2 30.8 10.7 VSCMR [40] (ACMMM2019) 63.8 39.5 23.5 14.3 35.5 30.2 9.0 ReCO-RL [18] (AAAI2020) - - - 12.4 33.9 29.9 8.6 INet [3] (AAAI2020) 64.4 40.1 23.9 14.7 35.6 29.0 10.0 SGVST [13] (AAAI2020) 65.1 40.1 23.8 14.7 35.8 29.9 9.8 IRW [27] (AAAI2021) 66.7 41.6 25.0 15.4 35.6 29.6 11.0 TAPM [41] (CVPR2021) 64.2 39.9 24.5 14.3 35.1 29.7 10.5 KAGS 70.3 44.4 25.5 14.7 36.3 31.6 11.4\nLSMDC\nXE-ss [1] (ACL2018) 27.7 13.7 6.0 2.8 29.9 29.6 6.8 AREL [1] (ACL2018) 28.9 14.1 6.1 3.1 30.2 29.7 7.1 VSCMR [40] (ACMMM2019) 30.1 15.3 6.8 3.3 30.6 30.0 7.9 TAPM [41] (CVPR2021) 32.3 16.9 7.2 3.9 31.0 29.3 9.5 KAGS 34.6 17.5 7.9 4.3 31.7 30.3 10.3\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of various methods including KAGS.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1609/aaai.v33i01.33018909",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fe33597affd4e99a5dc979ef4ed99ee6311fdc2b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "LSMDC",
          "dataset_description": "Used to evaluate methods for generating descriptions from multimodal data, comparing performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 210178945,
          "context_text": "As seen from the results, it is obvious that the statistical results of KAGS are better than the other four competing methods on\nTABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance\nMethods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr\nVIST\nseq2seq [31] (NAACL2016) - - - 3.5 31.4 - 6.8 BARNN [37] (AAAI2017) - - - - 33.3 - - h-attn-rank [38] (EMNLP2017) - - 21.0 - 34.1 29.5 7.5 XE-ss [1] (ACL2018) 62.3 38.2 22.5 13.7 34.8 29.7 8.7 AREL [1] (ACL2018) 63.7 39.0 23.1 14.0 35.0 29.6 9.5 HPSR [39] (AAAI2019) 61.9 37.8 21.5 12.2 34.4 31.2 8.0 HSRL [17] (AAAI2019) - - - 12.3 35.2 30.8 10.7 VSCMR [40] (ACMMM2019) 63.8 39.5 23.5 14.3 35.5 30.2 9.0 ReCO-RL [18] (AAAI2020) - - - 12.4 33.9 29.9 8.6 INet [3] (AAAI2020) 64.4 40.1 23.9 14.7 35.6 29.0 10.0 SGVST [13] (AAAI2020) 65.1 40.1 23.8 14.7 35.8 29.9 9.8 IRW [27] (AAAI2021) 66.7 41.6 25.0 15.4 35.6 29.6 11.0 TAPM [41] (CVPR2021) 64.2 39.9 24.5 14.3 35.1 29.7 10.5 KAGS 70.3 44.4 25.5 14.7 36.3 31.6 11.4\nLSMDC\nXE-ss [1] (ACL2018) 27.7 13.7 6.0 2.8 29.9 29.6 6.8 AREL [1] (ACL2018) 28.9 14.1 6.1 3.1 30.2 29.7 7.1 VSCMR [40] (ACMMM2019) 30.1 15.3 6.8 3.3 30.6 30.0 7.9 TAPM [41] (CVPR2021) 32.3 16.9 7.2 3.9 31.0 29.3 9.5 KAGS 34.6 17.5 7.9 4.3 31.7 30.3 10.3\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of various methods including KAGS.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1609/AAAI.V34I05.6455",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1f676946d1585f802d21272e2b3899a53e7b8187",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "LSMDC",
          "dataset_description": "Used to evaluate methods for generating descriptions from multimodal data, comparing performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr. | Used to evaluate KAGS against four competing methods in visual storytelling, demonstrating superior performance across all metrics compared to the runner-up method TAPM.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 235703153,
          "context_text": "As seen from the results, it is obvious that the statistical results of KAGS are better than the other four competing methods on\nTABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance\nMethods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE_L CIDEr\nVIST\nseq2seq [31] (NAACL2016) - - - 3.5 31.4 - 6.8 BARNN [37] (AAAI2017) - - - - 33.3 - - h-attn-rank [38] (EMNLP2017) - - 21.0 - 34.1 29.5 7.5 XE-ss [1] (ACL2018) 62.3 38.2 22.5 13.7 34.8 29.7 8.7 AREL [1] (ACL2018) 63.7 39.0 23.1 14.0 35.0 29.6 9.5 HPSR [39] (AAAI2019) 61.9 37.8 21.5 12.2 34.4 31.2 8.0 HSRL [17] (AAAI2019) - - - 12.3 35.2 30.8 10.7 VSCMR [40] (ACMMM2019) 63.8 39.5 23.5 14.3 35.5 30.2 9.0 ReCO-RL [18] (AAAI2020) - - - 12.4 33.9 29.9 8.6 INet [3] (AAAI2020) 64.4 40.1 23.9 14.7 35.6 29.0 10.0 SGVST [13] (AAAI2020) 65.1 40.1 23.8 14.7 35.8 29.9 9.8 IRW [27] (AAAI2021) 66.7 41.6 25.0 15.4 35.6 29.6 11.0 TAPM [41] (CVPR2021) 64.2 39.9 24.5 14.3 35.1 29.7 10.5 KAGS 70.3 44.4 25.5 14.7 36.3 31.6 11.4\nLSMDC\nXE-ss [1] (ACL2018) 27.7 13.7 6.0 2.8 29.9 29.6 6.8 AREL [1] (ACL2018) 28.9 14.1 6.1 3.1 30.2 29.7 7.1 VSCMR [40] (ACMMM2019) 30.1 15.3 6.8 3.3 30.6 30.0 7.9 TAPM [41] (CVPR2021) 32.3 16.9 7.2 3.9 31.0 29.3 9.5 KAGS 34.6 17.5 7.9 4.3 31.7 30.3 10.3\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of various methods including KAGS.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01247",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5c52e6a85cb3f68804bf0c875e88fa2f44160220",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "5378837",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "FB15k"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15k",
          "dataset_description": "Used as a benchmark dataset for KG completion, enriched with image data to enhance entity representation in multi-modal reasoning. | Used as a benchmark dataset to evaluate the relational structure of ImageGraph, focusing on visual-relational knowledge graph reasoning.",
          "citing_paper_id": "53957733",
          "cited_paper_id": 12161567,
          "context_text": "ImageGraph is a visual-relational KG whose relational structure is based on Freebase [Bollacker et al., 2008] and, more specifically, on FB15k, a subset of FreeBase and a popular benchmark data set [Nickel et al., 2016a].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FB15k', which is a specific dataset derived from Freebase, commonly used in knowledge graph research.",
          "citing_paper_doi": "10.24432/C56P45",
          "cited_paper_doi": "10.1109/JPROC.2015.2483592",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/033f25ad905ef2ed32a8331cf38b83953ff15922",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "FB15k",
          "dataset_description": "Used to evaluate the performance of TransE base score function in modeling multi-relational data, focusing on link prediction tasks in knowledge graphs.",
          "citing_paper_id": "258298672",
          "cited_paper_id": 14941970,
          "context_text": "The experiments are based on FB15K dataset and TransE base score function.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FB15K is a well-known dataset used for evaluating multi-relational data models, which aligns with the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/IJCNN54540.2023.10191314",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "FB15k",
          "dataset_description": "Used to compare the performance of TuckER and LowFER models, focusing on dimensionality and parameter efficiency in knowledge graph completion tasks.",
          "citing_paper_id": "221082536",
          "cited_paper_id": 52056218,
          "context_text": "In Balaˇzevi´c et al. (2019a), authors reported d e = d r = 200 as best choice of dimensions for TuckER on FB15k, however, we found using de = 300 and d r = 30 better with lesser number of parameters for LowFER.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions FB15k, which is a well-known dataset for knowledge graph completion tasks. The dataset is referenced in the context of comparing model performance.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-030-30493-5_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0126fce30b412d583f8e33714908dd09b86293d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7e9264aa7ac411cae5533029cbaeb87289639bce",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB15k",
          "dataset_description": "Used to compare the performance of TuckER and LowFER models, focusing on dimensionality and parameter efficiency in knowledge graph completion tasks. | Used to evaluate the performance of TuckER and LowFER models, focusing on dimensionality settings and parameter efficiency.",
          "citing_paper_id": "221082536",
          "cited_paper_id": 59316623,
          "context_text": "In Balaˇzevi´c et al. (2019a), authors reported d e = d r = 200 as best choice of dimensions for TuckER on FB15k, however, we found using de = 300 and d r = 30 better with lesser number of parameters for LowFER.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions FB15k, which is a well-known dataset for knowledge graph completion tasks. The dataset is referenced in the context of comparing model performance.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/D19-1522",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0126fce30b412d583f8e33714908dd09b86293d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FB15k",
          "dataset_description": "Used to evaluate knowledge graph completion methods, focusing on entity and relation prediction in a smaller subset of Freebase. | Used to evaluate the effectiveness of knowledge graph embedding techniques, emphasizing the ability to handle a larger and more complex subset of Freebase. | Used to assess the performance of knowledge graph embedding models, emphasizing the scalability and accuracy in a moderately sized subset of Freebase. | Used to assess the scalability of knowledge graph completion methods, focusing on a very large subset of Freebase with millions of entities and relations. | Used to evaluate the performance of knowledge graph completion models, specifically addressing the issue of inverse relations and improving the quality of predictions. | Used to benchmark knowledge graph completion algorithms, specifically evaluating the ability to predict missing links in a widely used subset of Freebase. | Used to test the robustness of knowledge graph reasoning models, focusing on a larger subset of Freebase with a diverse set of entities and relations. | Used to test the integration of knowledge graphs with textual data, focusing on linking entities from the New York Times corpus to a subset of Freebase.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 11223539,
          "context_text": "According to the entity set size, several subsets generated from it, including FB13 [201], FB122 [202], FB15k [203], FB20k [198], FB24k [204], FB5M [18], FB15k-237 [205], FB60k-NYT10 [206].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple Freebase-derived datasets, which are commonly used in knowledge graph research. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/01a858189394940d94ee00ee4285f3e84bff6f29",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FB15k",
          "dataset_description": "Used to evaluate knowledge graph embedding models, specifically comparing DistMult, Complex, and R-GCN+ performance. The dataset is a filtered version of FB15k, reducing redundancy and improving evaluation robustness. | Used to evaluate knowledge graph embedding models, specifically comparing DistMult, Complex, and R-GCN+ performance. The dataset supports multi-relational link prediction tasks. | Used to conduct experiments in multi-modal knowledge graph reasoning, focusing on relation prediction tasks with reverse relations removed.",
          "citing_paper_id": "3875633",
          "cited_paper_id": 5378837,
          "context_text": "For FB15k and FB15k-237, the results for DistMult, Complex, and R-GCN+ are taken from [28]; results for Node+LinkFeat are taken from [30]; and results for TransE were obtained with an implementation of the authors of the original paper 2 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets (FB15k and FB15k-237) used for evaluating knowledge graph embedding models. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/W15-4007",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c14347fa745a1f113fdbe8bf1c5ccfb71b5da296",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "59316623",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "UrbanKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "UrbanKG",
          "dataset_description": "Used to evaluate and compare the performance of various KRL models, focusing on multi-modal reasoning in knowledge graphs.",
          "citing_paper_id": "266469822",
          "cited_paper_id": 2768038,
          "context_text": "Second, we further compare with four typical KRL models on UrbanKG, TransE [3], DistMult [46], ComplEx [33], TuckER [2] and CompGCN [35].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'UrbanKG' as a dataset used for comparing KRL models. No other datasets are explicitly named or used in the context.",
          "citing_paper_doi": "10.1145/3589132.3625640",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/86682e30d86f4d1d3a8bf2aaf482783803a40efc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86412306b777ee35aba71d4795b02915cb8a04c3",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "UrbanKG",
          "dataset_description": "Used to evaluate and compare the performance of various KRL models, focusing on multi-modal reasoning in knowledge graphs.",
          "citing_paper_id": "266469822",
          "cited_paper_id": 14941970,
          "context_text": "Second, we further compare with four typical KRL models on UrbanKG, TransE [3], DistMult [46], ComplEx [33], TuckER [2] and CompGCN [35].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'UrbanKG' as a dataset used for comparing KRL models. No other datasets are explicitly named or used in the context.",
          "citing_paper_doi": "10.1145/3589132.3625640",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/86682e30d86f4d1d3a8bf2aaf482783803a40efc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "UrbanKG",
          "dataset_description": "Used to evaluate and compare the performance of various KRL models, focusing on multi-modal reasoning in knowledge graphs.",
          "citing_paper_id": "266469822",
          "cited_paper_id": 15150247,
          "context_text": "Second, we further compare with four typical KRL models on UrbanKG, TransE [3], DistMult [46], ComplEx [33], TuckER [2] and CompGCN [35].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'UrbanKG' as a dataset used for comparing KRL models. No other datasets are explicitly named or used in the context.",
          "citing_paper_doi": "10.1145/3589132.3625640",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/86682e30d86f4d1d3a8bf2aaf482783803a40efc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2218e2e1df2c3adfb70e0def2e326a39928aacfc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "UrbanKG",
          "dataset_description": "Used for pre-training the TuckER model to measure the plausibility of triplets, focusing on embedding learning for multi-modal knowledge graph reasoning. | Used to evaluate and compare the performance of various KRL models, focusing on multi-modal reasoning in knowledge graphs.",
          "citing_paper_id": "266469822",
          "cited_paper_id": 59316623,
          "context_text": "Second, we further compare with four typical KRL models on UrbanKG, TransE [3], DistMult [46], ComplEx [33], TuckER [2] and CompGCN [35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'UrbanKG' as a dataset used for comparing KRL models. No other datasets are explicitly named or used in the context.",
          "citing_paper_doi": "10.1145/3589132.3625640",
          "cited_paper_doi": "10.18653/v1/D19-1522",
          "citing_paper_url": "https://www.semanticscholar.org/paper/86682e30d86f4d1d3a8bf2aaf482783803a40efc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "UrbanKG",
          "dataset_description": "Used to obtain embeddings for multi-relational data, focusing on urban environments. The dataset is manually constructed and serves as the basis for applying classical embedding methods. | Used to obtain embeddings for multi-relational data, focusing on urban environments and their relationships. The dataset is manually constructed and serves as the basis for applying classical embedding methods.",
          "citing_paper_id": "259203574",
          "cited_paper_id": 14941970,
          "context_text": "Their common approach involves manually constructing a UrbanKG, and then obtaining embeddings using classical methods like TransE [30] or TuckER [34].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'UrbanKG' as a manually constructed knowledge graph, which is a specific dataset used for obtaining embeddings. The other terms (TransE, TuckER) are methods, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2306.11443",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3525e7911510e8b1adbb4290aefdaef7c0bd455",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "UrbanKG",
          "dataset_description": "Used to obtain embeddings for multi-relational data, focusing on urban environments. The dataset is manually constructed and serves as the basis for applying classical embedding methods. | Used to obtain embeddings for multi-relational data, focusing on urban environments and their relationships. The dataset is manually constructed and serves as the basis for applying classical embedding methods.",
          "citing_paper_id": "259203574",
          "cited_paper_id": 59316623,
          "context_text": "Their common approach involves manually constructing a UrbanKG, and then obtaining embeddings using classical methods like TransE [30] or TuckER [34].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'UrbanKG' as a manually constructed knowledge graph, which is a specific dataset used for obtaining embeddings. The other terms (TransE, TuckER) are methods, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2306.11443",
          "cited_paper_doi": "10.18653/v1/D19-1522",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3525e7911510e8b1adbb4290aefdaef7c0bd455",
          "cited_paper_url": "https://www.semanticscholar.org/paper/05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "265034288",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "SPIQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "SPIQA",
          "dataset_description": "Used to evaluate the comprehension abilities of closed and open-source multimodal models, focusing on their performance in understanding and reasoning across modalities.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 257532815,
          "context_text": "…on the SPIQA datasets evaluating the comprehension abilities of several closed large multimodal models, and state-of-the-art open-source models, including Gemini [63, 72], GPT4 [54, 1], Claude-3 [2], LLaVA 1.5 [40], InstructBLIP [13], XGen-MM [64], InternLM-XC [15], SPHINX-v2 [17] and CogVLM [80].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SPIQA datasets' which is a specific, verifiable dataset used for evaluating multimodal models. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SPIQA",
          "dataset_description": "Used to evaluate the comprehension abilities of closed and open-source multimodal models, focusing on their performance in understanding and reasoning across modalities. | Used to fine-tune InstructBLIP and LLaVA 1.5 models with simple QA prompts, focusing on multi-modal reasoning with reference images, questions, and answers.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 258615266,
          "context_text": "…on the SPIQA datasets evaluating the comprehension abilities of several closed large multimodal models, and state-of-the-art open-source models, including Gemini [63, 72], GPT4 [54, 1], Claude-3 [2], LLaVA 1.5 [40], InstructBLIP [13], XGen-MM [64], InternLM-XC [15], SPHINX-v2 [17] and CogVLM [80].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'SPIQA datasets' which is a specific, verifiable dataset used for evaluating multimodal models. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.48550/arXiv.2305.06500",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8bd6a2a89503be083176f2cc26fabedb79238cbd",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SPIQA",
          "dataset_description": "Used to evaluate the comprehension abilities of closed and open-source multimodal models, focusing on their performance in understanding and reasoning across modalities.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 265034288,
          "context_text": "…on the SPIQA datasets evaluating the comprehension abilities of several closed large multimodal models, and state-of-the-art open-source models, including Gemini [63, 72], GPT4 [54, 1], Claude-3 [2], LLaVA 1.5 [40], InstructBLIP [13], XGen-MM [64], InternLM-XC [15], SPHINX-v2 [17] and CogVLM [80].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SPIQA datasets' which is a specific, verifiable dataset used for evaluating multimodal models. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.48550/arXiv.2311.03079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2313afae52d98e569da2dedbf14daf9efc74e7cf",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SPIQA",
          "dataset_description": "Used to evaluate the comprehension abilities of closed and open-source multimodal models, focusing on their performance in understanding and reasoning across modalities.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 267547619,
          "context_text": "…on the SPIQA datasets evaluating the comprehension abilities of several closed large multimodal models, and state-of-the-art open-source models, including Gemini [63, 72], GPT4 [54, 1], Claude-3 [2], LLaVA 1.5 [40], InstructBLIP [13], XGen-MM [64], InternLM-XC [15], SPHINX-v2 [17] and CogVLM [80].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SPIQA datasets' which is a specific, verifiable dataset used for evaluating multimodal models. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.48550/arXiv.2402.05935",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec8e2b45c4601730015608a58e33409224a81228",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "SPIQA",
          "dataset_description": "Used to evaluate the comprehension abilities of closed and open-source multimodal models, focusing on their performance in understanding and reasoning across modalities.",
          "citing_paper_id": "271161780",
          "cited_paper_id": null,
          "context_text": "…on the SPIQA datasets evaluating the comprehension abilities of several closed large multimodal models, and state-of-the-art open-source models, including Gemini [63, 72], GPT4 [54, 1], Claude-3 [2], LLaVA 1.5 [40], InstructBLIP [13], XGen-MM [64], InternLM-XC [15], SPHINX-v2 [17] and CogVLM [80].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'SPIQA datasets' which is a specific, verifiable dataset used for evaluating multimodal models. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "3607155",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "VG150"
      ],
      "dataset_details": [
        {
          "dataset_name": "VG150",
          "dataset_description": "Used to train a scene graph generation method, focusing on iterative message passing to improve relationship detection in images.",
          "citing_paper_id": "269157470",
          "cited_paper_id": 1780254,
          "context_text": "We have trained our scene graph generation method using the training data introduced in (Xu et al., 2017) (VG150).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'training data introduced in (Xu et al., 2017) (VG150)', which is a specific dataset used for training a scene graph generation method.",
          "citing_paper_doi": "10.48550/arXiv.2404.10226",
          "cited_paper_doi": "10.1109/CVPR.2017.330",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a712f22b419623590b5b121278b7709cd1d08a64",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34b73c1aa158b892bbe41705b4ae5bf01ecaea86",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "VG150",
          "dataset_description": "Used as a benchmark for scene graph generation, evaluating models on cognitive tasks such as captioning and VQA, focusing on the integration of visual and textual information.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 3607155,
          "context_text": "In existing literature, VG150 serves as the most widely adopted benchmark on scene graph generation [38, 33, 35, 4, 20, 12], but was seldomly adopted on cognitive tasks such as captioning and VQA.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "VG150 is mentioned as a widely adopted benchmark for scene graph generation, which is relevant to multi-modal knowledge graph reasoning. It is used to evaluate models on cognitive tasks.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f268d317ad917447075767314dc48faf574fd4c8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "VG150",
          "dataset_description": "Used as a benchmark for scene graph generation, evaluating models on cognitive tasks such as captioning and VQA, focusing on the integration of visual and textual information. | Used to evaluate frequency-counting methods in scene graph generation, focusing on the prediction of relation labels using non-visual factors. | Used to evaluate and compare scene graph generation methods, focusing on the representability of relationships in visual scenes using various models.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 4379400,
          "context_text": "In existing literature, VG150 serves as the most widely adopted benchmark on scene graph generation [38, 33, 35, 4, 20, 12], but was seldomly adopted on cognitive tasks such as captioning and VQA.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "VG150 is mentioned as a widely adopted benchmark for scene graph generation, which is relevant to multi-modal knowledge graph reasoning. It is used to evaluate models on cognitive tasks.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1109/CVPR.2018.00611",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0da8af8d81e84381ffe656a0bbf2f3937ffac618",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "VG150",
          "dataset_description": "Used as a benchmark for scene graph generation, evaluating models on cognitive tasks such as captioning and VQA, focusing on the integration of visual and textual information.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 51894526,
          "context_text": "In existing literature, VG150 serves as the most widely adopted benchmark on scene graph generation [38, 33, 35, 4, 20, 12], but was seldomly adopted on cognitive tasks such as captioning and VQA.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "VG150 is mentioned as a widely adopted benchmark for scene graph generation, which is relevant to multi-modal knowledge graph reasoning. It is used to evaluate models on cognitive tasks.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1007/978-3-030-01246-5_41",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1fad7fe0a7a90a8470a0688ad26bab6ceb8a85b7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "VG150",
          "dataset_description": "Used as a benchmark for scene graph generation, evaluating models on cognitive tasks such as captioning and VQA, focusing on the integration of visual and textual information.",
          "citing_paper_id": "201881176",
          "cited_paper_id": null,
          "context_text": "In existing literature, VG150 serves as the most widely adopted benchmark on scene graph generation [38, 33, 35, 4, 20, 12], but was seldomly adopted on cognitive tasks such as captioning and VQA.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "VG150 is mentioned as a widely adopted benchmark for scene graph generation, which is relevant to multi-modal knowledge graph reasoning. It is used to evaluate models on cognitive tasks.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": null,
          "citing_paper_year": 2019,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "5378837",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "WN18RR"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN18RR",
          "dataset_description": "Used to create VTKG datasets, providing relational data for training and evaluation. | Used to evaluate the performance of VISTA on multi-relational data, focusing on the efficiency and effectiveness of the model with the best hyperparameters. | Used to construct VTKG datasets by merging with other visual commonsense datasets, requiring significant effort to align different vocabularies and contextual semantics. | Proposed to resolve the test leakage issue in WN18, enhancing the reliability of evaluation in multimodal knowledge graph completion research. | Used in multimodal knowledge graph completion research, serving as a benchmark dataset without the test leakage issues present in WN18. | Benchmark dataset used in multimodal knowledge graph completion research, known for its complexity and scale. | Used as a subset of WordNet to evaluate multi-relational data modeling, focusing on the performance of translational embeddings without additional post-processing. | Used in multimodal knowledge graph completion research, but noted for having a test leakage issue, affecting its reliability. | Used to evaluate multi-relational data modeling, specifically addressing issues in the original WN18RR dataset through fixes and improvements. | Used to enhance multi-relational data modeling, focusing on improving the performance of translational embedding methods. The dataset likely includes additional relations or entities compared to the original WN18RR. | Used to evaluate multi-relational embeddings, focusing on performance metrics such as MR, MRR, and Hit@1, Hit@3, Hit@10. The dataset is designed to test reasoning capabilities in knowledge graphs. | Used to create VTKG datasets, leveraging its structured knowledge for multi-relational data modeling. | Used in multimodal knowledge graph completion research, but noted for having a test leakage issue. | Proposed to address the test leakage issue in WN18, used for evaluating knowledge graph completion models.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 14941970,
          "context_text": "We used NVIDIA GeForce RTX 3090 and NVIDIA RTX A6000 to run VISTA on WN18RR++, and it took approximately 6 hours for a single run with the best hyperparameters.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions running experiments on WN18RR++, which is a specific dataset used for evaluating multi-relational data models.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "WN18RR",
          "dataset_description": "Used to evaluate multi-relational embeddings, focusing on performance metrics such as MR, MRR, and Hit@1, Hit@3, Hit@10. The dataset is designed to test reasoning capabilities in knowledge graphs.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 19370455,
          "context_text": "On the other hand, WN18RR++ and VTKG-I VTKG-C MR MRR Hit@1 Hit@3 Hit@10 MR MRR Hit@1 Hit@3 Hit@10 ANALOGY 39.5 0.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions WN18RR++, which is a dataset used for evaluating multi-relational embeddings. The context also lists evaluation metrics, indicating the dataset is used for performance evaluation.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e15d062ef07abab8fae65244f64ccd2aac8d2b94",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "WN18RR",
          "dataset_description": "Used for training and evaluating link prediction models, providing a large-scale dataset with diverse and complex relations from Wikidata. | Used for assessing the performance of link prediction algorithms, emphasizing the complexity of relations and entities in a large-scale knowledge graph. | Utilized for link prediction tasks, providing a large-scale dataset to test the scalability and performance of multi-relational models. | Used to evaluate multi-relational reasoning models, specifically setting the size of the rule set and the number of hidden dimensions to 8 pairs of reversible relational prompts. | Used as a benchmark for evaluating link prediction models, focusing on relational data and entity linking in knowledge graphs. | Used for evaluating link prediction models, focusing on relation types and entity interactions in a knowledge graph setting. | Serves as a benchmark for link prediction, specifically addressing the challenge of modeling complex relationships in large-scale knowledge graphs.",
          "citing_paper_id": "253117165",
          "cited_paper_id": 14941970,
          "context_text": "The WN18RR [2], FB15k-237 [29] and Wikidata5M [34]\nare standard benchmarks for link predictions as available in the literature.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for link prediction tasks, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.14463",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4259ee32a21153e62feb633ecc5d9ba48b615923",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2022,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "WN18RR",
          "dataset_description": "Used to evaluate the performance of the proposed method, focusing on multi-modal knowledge graph reasoning, assessing the ability to handle complex queries. | Used to evaluate the performance of the model, focusing on reasoning tasks in a multi-modal knowledge graph. | This dataset 'WN18RR' was mentioned in the citation context but no detailed description was generated. | Used to evaluate the performance of the proposed method, focusing on multi-modal knowledge graph reasoning, evaluating the ability to handle complex queries.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 5378837,
          "context_text": "We evaluate our HRGAT on four benchmark datasets, which are FB15k-237 [38], WN18RR [13], DB15K [24], and YAGO15K [24].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific datasets used to evaluate the HRGAT model. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.18653/v1/W15-4007",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "WN18RR",
          "dataset_description": "This dataset 'FB15k-237' was mentioned in the citation context but no detailed description was generated. | Used for evaluating multi-modal knowledge graph reasoning, focusing on relational graph convolutional networks, using the same splits and evaluation protocol as the original R-GCN paper. | Used for evaluating multi-modal knowledge graph reasoning, focusing on cross-lingual entity alignment, using the same splits and evaluation protocol as FB15k-237.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 5458500,
          "context_text": "We have followed the original division ratio in R-GCN [32] on datasets FB15k-237 and WN18RR, and split DB15K and YAGO15K with the same ratio as FB15k-237.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in multi-modal knowledge graph reasoning. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.1007/978-3-319-93417-4_38",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd8a9914d50b0ac63315872530274d158d6aff09",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "7865384",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "AM"
      ],
      "dataset_details": [
        {
          "dataset_name": "AM",
          "dataset_description": "Used to evaluate COMP GCN on relation prediction tasks, focusing on RDF triples and their semantic relationships.",
          "citing_paper_id": "207847719",
          "cited_paper_id": 35288341,
          "context_text": "Similar to Schlichtkrull et al. (2017), we evaluate C OMP GCN on MUTAG (Node) and AM (Ristoski & Paulheim, 2016) datasets.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions two specific datasets, MUTAG and AM, which are used for evaluating the performance of COMP GCN. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-319-46523-4_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4b244a6778c95b1df8e9e02332ff8d22e675f628",
          "cited_paper_url": "https://www.semanticscholar.org/paper/844d502387a3996f167b04e2e83117c30c22e752",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "AM",
          "dataset_description": "Used to follow the conventional train/test split for experiments, focusing on relational data modeling with graph convolutional networks.",
          "citing_paper_id": "235324797",
          "cited_paper_id": 5458500,
          "context_text": "For AM dataset, we follow the train/test split convention [20, 21].",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'AM dataset' which is a plausible dataset name, but lacks specific details about its usage or characteristics. The cited paper titles do not provide additional clarity.",
          "citing_paper_doi": "10.1145/3442381.3449925",
          "cited_paper_doi": "10.1007/978-3-319-93417-4_38",
          "citing_paper_url": "https://www.semanticscholar.org/paper/73965db326aee64123487676b6230bf417940698",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd8a9914d50b0ac63315872530274d158d6aff09",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "AM",
          "dataset_description": "Used for label collection in knowledge graph representation learning, focusing on Freebase relations and entities. | Used for label collection in knowledge graph representation learning, focusing on wordnet relations and entities. | Used to model multi-relational data from WordNet 3, consisting of triplets (synset, relation, synset) to evaluate multi-modal knowledge graph reasoning. | Used to model relationships between artifacts in the Amsterdam Museum, focusing on multi-modal reasoning and knowledge graph embedding techniques.",
          "citing_paper_id": "235324797",
          "cited_paper_id": 7865384,
          "context_text": "We conduct experiments on the following datasets: AM [20] which contains relationship between different artifacts in Amsterdam Museum, WN [3, 29] which consists of a collection of triplets (synset, relation, synset) extracted from WordNet 3.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, AM and WN, both of which are used in experiments. AM is a dataset containing relationships between artifacts in the Amsterdam Museum, and WN is a collection of triplets from WordNet 3.",
          "citing_paper_doi": "10.1145/3442381.3449925",
          "cited_paper_doi": "10.1609/aaai.v32i1.11266",
          "citing_paper_url": "https://www.semanticscholar.org/paper/73965db326aee64123487676b6230bf417940698",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7b9cdf953223aa27ea548fa3a62d77d67723b0e2",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "AM",
          "dataset_description": "Used to split labeled entities into train/valid/test sets for multi-relational data modeling, focusing on entity representation learning. | Used to model multi-relational data, focusing on entity and relation embeddings in knowledge graphs. The dataset is employed to train and evaluate models for link prediction tasks. | Used to study relationships between artifacts in the Amsterdam Museum, focusing on multi-modal reasoning across different types of cultural objects. | Consists of triplets extracted from WordNet 3.0, used to evaluate multi-relational data modeling and reasoning tasks. | Extracted from Freebase, used to test models on a large-scale knowledge graph, focusing on multi-relational reasoning and entity linking. | Used to model relationships between artifacts in the Amsterdam Museum, focusing on multi-modal reasoning and knowledge graph embedding techniques. | Used to demonstrate the handling of multiple labels per entity, contrasting with AM and WN's single-label constraint. | Used to collect labels for entity classification tasks, demonstrating the importance of relation modeling in graph convolutional networks. | Used to compare CompGCN and KE-GCN performance with TransE and QuatE embeddings, focusing on multi-relational data modeling in knowledge graphs. | Used to model multi-relational data from WordNet 3, consisting of triplets (synset, relation, synset) to evaluate multi-modal knowledge graph reasoning. | Used to evaluate knowledge graph embedding models, specifically measuring performance under P@1, P@5, and N@5 metrics.",
          "citing_paper_id": "235324797",
          "cited_paper_id": 14941970,
          "context_text": "We conduct experiments on the following datasets: AM [20] which contains relationship between different artifacts in Amsterdam Museum, WN [3, 29] which consists of a collection of triplets (synset, relation, synset) extracted from WordNet 3.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, AM and WN, both of which are used in experiments. AM is a dataset containing relationships between artifacts in the Amsterdam Museum, and WN is a collection of triplets from WordNet 3.",
          "citing_paper_doi": "10.1145/3442381.3449925",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/73965db326aee64123487676b6230bf417940698",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2021,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "AM",
          "dataset_description": "Extracted from Freebase, used to test models on a large-scale knowledge graph, focusing on multi-relational reasoning and entity linking. | Used to model multi-relational data, focusing on entity and relation embeddings in knowledge graphs. The dataset is employed to train and evaluate models for link prediction tasks. | Used to study relationships between artifacts in the Amsterdam Museum, focusing on multi-modal reasoning across different types of cultural objects. | Consists of triplets extracted from WordNet 3.0, used to evaluate multi-relational data modeling and reasoning tasks.",
          "citing_paper_id": "235324797",
          "cited_paper_id": 207167677,
          "context_text": "We conduct experiments on the following datasets: AM [20] which contains relationship between different artifacts in Amsterdam Museum, WN [3, 29] which consists of a collection of triplets (synset, relation, synset) extracted from WordNet 3.0 [15], FB15K [3, 39] which is extracted from a typical large-scale knowledge graph Freebase [2].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions three datasets used for experiments: AM, WN, and FB15K. These are clearly identified as datasets and their sources are referenced.",
          "citing_paper_doi": "10.1145/3442381.3449925",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/73965db326aee64123487676b6230bf417940698",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "235623770",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "FetaQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "FetaQA",
          "dataset_description": "Applied to evaluate fine-grained entity typing and question answering on tables, emphasizing the recognition of entities and their relationships.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 9027681,
          "context_text": "Additionally, there are datasets for QA purely on tabular data, including WTQ [60], TableQA [76], SQA [20], HiTab [11], AIT-QA [26], FetaQA [52], MultiTabQA [55].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets specifically designed for question answering on tabular data, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.3115/v1/P15-1142",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b41e95c8c97846d5ca4c11ef79d7814499cc9663",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "FetaQA",
          "dataset_description": "Applied to evaluate fine-grained entity typing and question answering on tables, emphasizing the recognition of entities and their relationships.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 235623770,
          "context_text": "Additionally, there are datasets for QA purely on tabular data, including WTQ [60], TableQA [76], SQA [20], HiTab [11], AIT-QA [26], FetaQA [52], MultiTabQA [55].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets specifically designed for question answering on tabular data, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.18653/v1/2022.naacl-industry.34",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bea8d75b7e78ba8becc691d89eb3b52a674272f0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "FetaQA",
          "dataset_description": "Applied to evaluate fine-grained entity typing and question answering on tables, emphasizing the recognition of entities and their relationships.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 237091377,
          "context_text": "Additionally, there are datasets for QA purely on tabular data, including WTQ [60], TableQA [76], SQA [20], HiTab [11], AIT-QA [26], FetaQA [52], MultiTabQA [55].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets specifically designed for question answering on tabular data, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.18653/v1/2022.acl-long.78",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a1364257028332760208827cd0c7af08d91e058b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "FetaQA",
          "dataset_description": "Applied to evaluate fine-grained entity typing and question answering on tables, emphasizing the recognition of entities and their relationships.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 258833465,
          "context_text": "Additionally, there are datasets for QA purely on tabular data, including WTQ [60], TableQA [76], SQA [20], HiTab [11], AIT-QA [26], FetaQA [52], MultiTabQA [55].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets specifically designed for question answering on tabular data, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.18653/v1/2023.acl-long.348",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c43537c9b9c7b99c2ad250c9d3f82e64c82ad62",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "67474824",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "IMGpedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Used to collect images and form a knowledge graph, focusing on content-based analysis of Wikimedia images for multi-modal reasoning.",
          "citing_paper_id": "264492337",
          "cited_paper_id": 3117929,
          "context_text": "IMGpedia [6] is one of the first attempts to collect images and form a KG, containing only image modality.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "IMGpedia is mentioned as a dataset that collects images and forms a knowledge graph, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3612266",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac7474000c867ea3b0ad64bf7301e102621a2afb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Serves as the source of images for IMGpedia, providing a large-scale collection of multimedia files for visual and semantic analysis. | Used as a linked dataset with content-based analysis of Wikimedia images, incorporating visual information for multi-modal reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 3117929,
          "context_text": "• IMGpedia [223], [238], [239], [240] is the KG, which incorporates visual information of the images from the Wikimedia Commons dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "IMGpedia is identified as a knowledge graph incorporating visual information from the Wikimedia Commons dataset. The context indicates that IMGpedia is used as a reusable resource.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Serves as the source of images for IMGpedia, providing a large-scale collection of multimedia files for visual and semantic analysis. | Used as a linked dataset with content-based analysis of Wikimedia images, incorporating visual information for multi-modal reasoning.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 67474824,
          "context_text": "• IMGpedia [223], [238], [239], [240] is the KG, which incorporates visual information of the images from the Wikimedia Commons dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "IMGpedia is identified as a knowledge graph incorporating visual information from the Wikimedia Commons dataset. The context indicates that IMGpedia is used as a reusable resource.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3a3d374a13e3cf4c69730e5c52138c0be57f6f2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Used as a large multi-modal knowledge graph with image-to-image and image-to-text relationships, supporting research in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 3117929,
          "context_text": "IMGpedia [9] is a large multi-modal knowledge graph which includes two types of relationships: image-to-image and image-to-text.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "IMGpedia is identified as a multi-modal knowledge graph, which fits the topic of multi-modal knowledge graph reasoning. It is used as a reusable resource.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "44145776",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "IKRL"
      ],
      "dataset_details": [
        {
          "dataset_name": "IKRL",
          "dataset_description": "Used to compare the current approach with a method that uses only images as extra attributes, focusing on multi-modal knowledge graph reasoning.",
          "citing_paper_id": "52160797",
          "cited_paper_id": 5440379,
          "context_text": "…et al., 2017) (we use IKRL from the ﬁrst work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'IKRL' which is likely a dataset or method used for comparison with the current approach. However, without more context, it is unclear if it is a dataset or a method. The other citations do not mention specific datasets.",
          "citing_paper_doi": "10.18653/v1/D18-1359",
          "cited_paper_doi": "10.18653/v1/D15-1031",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/318b558717ff9a4a996e45368b26a1233f03d1d7",
          "citing_paper_year": 2018,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "IKRL",
          "dataset_description": "Used to compare the current approach with a method that uses only images as extra attributes, focusing on multi-modal knowledge graph reasoning.",
          "citing_paper_id": "52160797",
          "cited_paper_id": 13621654,
          "context_text": "…et al., 2017) (we use IKRL from the ﬁrst work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'IKRL' which is likely a dataset or method used for comparison with the current approach. However, without more context, it is unclear if it is a dataset or a method. The other citations do not mention specific datasets.",
          "citing_paper_doi": "10.18653/v1/D18-1359",
          "cited_paper_doi": "10.18653/v1/P16-1136",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2072e4bc03c82697be667c265d728045712bc46",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "IKRL",
          "dataset_description": "Used to compare the current approach with a method that uses only images as extra attributes, focusing on multi-modal knowledge graph reasoning.",
          "citing_paper_id": "52160797",
          "cited_paper_id": 44145776,
          "context_text": "…et al., 2017) (we use IKRL from the ﬁrst work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'IKRL' which is likely a dataset or method used for comparison with the current approach. However, without more context, it is unclear if it is a dataset or a method. The other citations do not mention specific datasets.",
          "citing_paper_doi": "10.18653/v1/D18-1359",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2018,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "IKRL",
          "dataset_description": "Used to compare the current approach with a method that uses images as extra attributes, focusing on image-embodied knowledge representation learning.",
          "citing_paper_id": "52160797",
          "cited_paper_id": 9909815,
          "context_text": "ical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'IKRL' which is likely a dataset or resource used for comparison with the current approach. However, without more context, it is unclear if 'IKRL' is a specific dataset or a method.",
          "citing_paper_doi": "10.18653/v1/D18-1359",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2018,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "7204540",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "FB-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB-IMG",
          "dataset_description": "Used for image-embodied knowledge representation learning, integrating visual and textual information to enhance multimodal reasoning. | Utilized for multimodal translation-based knowledge graph representation learning, combining image and text data to improve model performance.",
          "citing_paper_id": "258509157",
          "cited_paper_id": 44145776,
          "context_text": "There includes multi-modal datasets: WN9-IMG [41] and FB-IMG [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, WN9-IMG and FB-IMG, which are relevant to multi-modal knowledge graph reasoning. These datasets are used for knowledge representation learning involving images.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB-IMG",
          "dataset_description": "Used to obtain textual and visual representations for multi-modal reasoning, combining pre-trained word2vec for text and VGG-m-128CNN embeddings for images.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 14124313,
          "context_text": "For the FB-IMG dataset, textual representations can be obtained by pre-trained word2vec and visual representations can be obtained by using embeddings of the VGG-m-128CNN [6] model.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the FB-IMG dataset, which is used to obtain textual and visual representations using pre-trained models. The dataset is clearly identified and used for multi-modal reasoning.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
          "citing_paper_year": 2021,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "FB-IMG",
          "dataset_description": "Mentioned as a representative knowledge graph, benefiting downstream tasks like semantic analysis, question-answer systems, and machine comprehension. | Used to construct a subset of FB15K, focusing on triples extracted from Freebase for multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 207167677,
          "context_text": "FB-IMG This dataset constructed by Hatem et al. [25] is the subset of FB15K [5], which consists of triples extracted from Freebase [3].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FB-IMG', which is a specific dataset derived from FB15K, a well-known knowledge graph dataset. The dataset is described as a subset of FB15K, containing triples extracted from Freebase.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "FB-IMG",
          "dataset_description": "Used to train a more compact VGG-m-128 CNN model, leveraging its larger size compared to WN9-IMG to speed up training and produce 128-dimensional image embeddings.",
          "citing_paper_id": "44145776",
          "cited_paper_id": 7204540,
          "context_text": "For the FB-IMG dataset, which contains much more data than WN9-IMG and in order to speed up the training, we used the more compact VGG-m-128 CNN model (Chatﬁeld et al., 2014), which produces 128-dimensional embedding vector for each image.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the FB-IMG dataset, which is used for training with a specific CNN model. The dataset is described as containing more data than another dataset (WN9-IMG) and is used to speed up training.",
          "citing_paper_doi": "10.18653/v1/S18-2027",
          "cited_paper_doi": "10.5244/C.28.6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/14d9be7962a4ec5a6e55755f4c7588ea00793652",
          "citing_paper_year": 2018,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "6866988",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "English Wikipedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "English Wikipedia",
          "dataset_description": "Used for pre-training to save time and ensure fair comparison with previous knowledge-enhanced PLMs, focusing on the effectiveness of the pre-training approach. | Used to evaluate models on entity typing, focusing on nine general entity types, enhancing multi-modal knowledge graph reasoning.",
          "citing_paper_id": "208006241",
          "cited_paper_id": 3782112,
          "context_text": "In our pre-training procedure, we only use the English Wikipedia corpus to save time and also for a fair comparison with previous knowledge-enhanced PLMs (Zhang et al., 2019; Peters et al., 2019",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the English Wikipedia corpus for pre-training, which is a specific and identifiable dataset. No other datasets are mentioned.",
          "citing_paper_doi": "10.1162/tacl_a_00360",
          "cited_paper_doi": "10.18653/v1/D17-1004",
          "citing_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/400e746bc8027c4b5f915cae6123cd1775484b4d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "English Wikipedia",
          "dataset_description": "Used for pretraining models with 2,500M words, enhancing the masked language modeling objective to capture diverse linguistic patterns. | Used as a pre-training corpus for the MLM objective, providing 2,500M words of text data.",
          "citing_paper_id": "208006241",
          "cited_paper_id": 6866988,
          "context_text": "For the MLM objective, we use the English Wikipedia (2,500M words) and BookCorpus (800M words) (Zhu et al., 2015) as our pre-training corpora (except KEPLEROnlyDesc).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'English Wikipedia' and 'BookCorpus' as pre-training corpora, which are specific and publicly accessible datasets.",
          "citing_paper_doi": "10.1162/tacl_a_00360",
          "cited_paper_doi": "10.1109/ICCV.2015.11",
          "citing_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "English Wikipedia",
          "dataset_description": "Used for training text-based models, contributing to a 160GB text corpus for natural language processing tasks.",
          "citing_paper_id": "251719655",
          "cited_paper_id": 6866988,
          "context_text": "For monomodal data, we use 14M images from ImageNet-21K and 160GB text corpora [BDW20] from English Wikipedia, BookCorpus [ZKZ15], OpenWebText3, CC-News [LOG19], and Stories [TL18].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions specific datasets used for training monomodal data, including ImageNet-21K and several text corpora. These are clearly identified and used for training models.",
          "citing_paper_doi": "10.48550/arXiv.2208.10442",
          "cited_paper_doi": "10.1109/ICCV.2015.11",
          "citing_paper_url": "https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "English Wikipedia",
          "dataset_description": "Used for training text-based models, contributing to a 160GB text corpus for natural language processing tasks.",
          "citing_paper_id": "251719655",
          "cited_paper_id": 198953378,
          "context_text": "For monomodal data, we use 14M images from ImageNet-21K and 160GB text corpora [BDW20] from English Wikipedia, BookCorpus [ZKZ15], OpenWebText3, CC-News [LOG19], and Stories [TL18].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions specific datasets used for training monomodal data, including ImageNet-21K and several text corpora. These are clearly identified and used for training models.",
          "citing_paper_doi": "10.48550/arXiv.2208.10442",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199",
          "cited_paper_url": "https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "161099",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "MIRFlickr"
      ],
      "dataset_details": [
        {
          "dataset_name": "MIRFlickr",
          "dataset_description": "Used to disambiguate concepts and relate them better to images, enhancing multi-modal knowledge graph reasoning through image tagging.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 14040310,
          "context_text": "Image Tagging NUS-WIDE [123] MIRFlickr [124] help disambiguation the concept and relate them better to images",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two image tagging datasets, NUS-WIDE and MIRFlickr, which are used to disambiguate concepts and relate them better to images in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/1460096.1460104",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f79131806747fce087d0fe73d0867cc621547b2a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "MIRFlickr",
          "dataset_description": "Used to evaluate image tagging, containing 25,000 images with an average of 17 tags per image, focusing on multi-label classification performance.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 161099,
          "context_text": "We consider two large-scale datasets to evaluate image tagging, NUS-WIDE [41] and MIRFlickr [42] (both of which are widely used [2], [6], [7], [43]) containing 259,333 and 25,000 images with an average number of tags per image as 17.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, NUS-WIDE and MIRFlickr, which are used for evaluating image tagging. Both datasets are described in terms of their size and tag distribution.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1109/CVPR.2016.644",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/299ef87826da0ae699c19cc9f79de9ddd94a9172",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "MIRFlickr",
          "dataset_description": "Used to evaluate image tagging, containing 25,000 images with an average of 17 tags per image, focusing on multi-label classification performance.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 12078302,
          "context_text": "We consider two large-scale datasets to evaluate image tagging, NUS-WIDE [41] and MIRFlickr [42] (both of which are widely used [2], [6], [7], [43]) containing 259,333 and 25,000 images with an average number of tags per image as 17.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, NUS-WIDE and MIRFlickr, which are used for evaluating image tagging. Both datasets are described in terms of their size and tag distribution.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1145/2502081.2502129",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/75996937696cce0804b5ac835443f5e28a3b7bdd",
          "citing_paper_year": 2020,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "MIRFlickr",
          "dataset_description": "Used to evaluate image tagging, containing 25,000 images with an average of 12.66 tags per image, focusing on multi-label classification performance.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 1573933,
          "context_text": "We consider two large-scale datasets to evaluate image tagging, NUS-WIDE [41] and MIRFlickr [42] (both of which are widely used [2], [6], [7], [43]) containing 259,333 and 25,000 images with an average number of tags per image as 17.8 and 12.66, respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, NUS-WIDE and MIRFlickr, which are used for evaluating image tagging. Both datasets are described in terms of their size and tag distribution, indicating their relevance to the research.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1007/s11042-016-3512-1",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1d841f1134b21ff18ce2d7a1796ef1328966c4cd",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "231879586",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "YFCC100M"
      ],
      "dataset_details": [
        {
          "dataset_name": "YFCC100M",
          "dataset_description": "Used to train vision-language models, focusing on large-scale image-text pairs for natural language supervision. | Used to train vision-language models, focusing on large-scale image-text pairs for noisy text supervision.",
          "citing_paper_id": "244117525",
          "cited_paper_id": 231591445,
          "context_text": "…of recent VLP models include publically available datasets like YFCC100M (Thomee et al., 2016) and CC12M (Chang-pinyo et al., 2021), as well as larger-scale datasets with more than 100M samples in CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which are shown to be even more powerful.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training vision-language models, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "YFCC100M",
          "dataset_description": "Used to train vision-language models, focusing on large-scale image-text pairs for natural language supervision. | Used to train vision-language models, focusing on large-scale image-text pairs for noisy text supervision. | Used to construct a large-scale dataset with 1.8 billion image-text pairs for visual and vision-language representation learning, focusing on noisy text supervision.",
          "citing_paper_id": "244117525",
          "cited_paper_id": 231879586,
          "context_text": "…of recent VLP models include publically available datasets like YFCC100M (Thomee et al., 2016) and CC12M (Chang-pinyo et al., 2021), as well as larger-scale datasets with more than 100M samples in CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), which are shown to be even more powerful.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for training vision-language models, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/141a5033d9994242b18bb3b217e79582f1ee9306",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "YFCC100M",
          "dataset_description": "Employed to improve video classification, integrating video, audio, and text modalities for comprehensive multi-modal learning. | Utilized to enhance multimedia research, offering a vast collection of images and videos for multi-modal analysis.",
          "citing_paper_id": "40114756",
          "cited_paper_id": 195345989,
          "context_text": "Signiﬁcant progress has been made especially since the release of large-scale benchmarks such as Sports-1M (Karpathy et al. 2014), YFCC-100M (Thomee et al. 2015) and YouTube-8M (Abu-El-Haija et al. 2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three large-scale benchmarks that have contributed to significant progress in the field. These benchmarks are clearly identified and are relevant to multi-modal learning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d81490b09cc99ab573d4ec73b43834a20c303244",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a6e695ddd07aad719001c0fc1129328452385949",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "YFCC100M",
          "dataset_description": "Employed to improve video classification, integrating video, audio, and text modalities for comprehensive multi-modal learning. | Utilized to enhance multimedia research, offering a vast collection of images and videos for multi-modal analysis.",
          "citing_paper_id": "40114756",
          "cited_paper_id": 206592218,
          "context_text": "Signiﬁcant progress has been made especially since the release of large-scale benchmarks such as Sports-1M (Karpathy et al. 2014), YFCC-100M (Thomee et al. 2015) and YouTube-8M (Abu-El-Haija et al. 2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three large-scale benchmarks that have contributed to significant progress in the field. These benchmarks are clearly identified and are relevant to multi-modal learning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2014.223",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d81490b09cc99ab573d4ec73b43834a20c303244",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253",
          "citing_paper_year": 2017,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "8081284",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "COCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "COCO",
          "dataset_description": "Used for pretraining in multi-modal reasoning, providing a large set of images annotated with captions, object segmentations, and keypoint annotations. | Used for pretraining, offering dense image annotations including objects, attributes, and relationships, enhancing multi-modal understanding. | Used for pretraining, containing images with single-sentence captions, supporting the training of models for image captioning and retrieval. | Used for pretraining, consisting of web images paired with descriptive captions, aiding in the development of image-text alignment models.",
          "citing_paper_id": "247011309",
          "cited_paper_id": 4492210,
          "context_text": "Following previous experimental protocols [7, 22], we use COCO [25], Visual Genome (VG) [21], Conceptual Captions (CC) [39], and SBU Captions [30] as the pretraining dataset in our study, where a total of 4.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four datasets used for pretraining in the study: COCO, Visual Genome, Conceptual Captions, and SBU Captions. These are all well-known datasets in the field of multi-modal learning.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01522",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0ad84c4bf7499df6945fc51b24ae2ac779f218ec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "COCO",
          "dataset_description": "Used to assess the model's effectiveness in cross-modal alignment, demonstrating significant performance gains over ViLT in image-caption matching tasks. | Used to evaluate the model's performance in cross-modal alignment, specifically comparing improvements over ViLT in image-caption matching tasks.",
          "citing_paper_id": "247011309",
          "cited_paper_id": 231839613,
          "context_text": "Compared with ViLT [24] which directly uses a transformer encoder to model the interaction between word and image patch embeddings, we improve +9.5% (average) on COCO and +12.2% (average) on Flickr30K, revealing the necessity of conducting cross-modal alignment before fusion.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions performance improvements on COCO and Flickr30K datasets, indicating their use for evaluating the model's effectiveness in cross-modal alignment.",
          "citing_paper_doi": "10.1109/CVPR52688.2022.01522",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0ad84c4bf7499df6945fc51b24ae2ac779f218ec",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0839722fb5369c0abaff8515bfc08299efc790a1",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "COCO",
          "dataset_description": "Used to build the VQA dataset, providing image captions and annotations for visual question answering tasks. | Used for visual question answering tasks, focusing on the integration of image and text modalities to answer questions about images. | Used for visual commonsense reasoning tasks, emphasizing the need for understanding complex visual scenes and reasoning about them.",
          "citing_paper_id": "201317624",
          "cited_paper_id": 8081284,
          "context_text": "0 dataset (Goyal et al., 2017), which is built based on the COCO (Lin et al.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'VQA' and 'COCO', both of which are known datasets in the field of multi-modal learning. However, only 'COCO' is fully specified in the citation context.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/s11263-018-1116-0",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "COCO",
          "dataset_description": "Used to evaluate the model's performance in image captioning and retrieval tasks, focusing on region-to-phrase correspondences.",
          "citing_paper_id": "251719655",
          "cited_paper_id": 6941275,
          "context_text": "Two popular retrieval benchmarks, i.e., COCO [LMB + 14], and Flickr30K [PWC + 15], are used to evaluate the model.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions two benchmarks, COCO and Flickr30K, which are used to evaluate the model. These are specific datasets used for evaluation in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2208.10442",
          "cited_paper_doi": "10.1007/s11263-016-0965-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11c9c31dff70de92ada9160c78ff8bb46b2912d6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "52160797",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "WN9"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN9",
          "dataset_description": "Used to expand the knowledge graph by adding images to entities, enhancing multimodal reasoning capabilities.",
          "citing_paper_id": "252089825",
          "cited_paper_id": 52160797,
          "context_text": "Similar studies [28] [41] expand the existing KGs WN-9 and FB15K respectively, only adding images for each entity to further explain them.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions expanding existing KGs WN-9 and FB15K by adding images, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.00015",
          "cited_paper_doi": "10.18653/v1/D18-1359",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79801d46d6c495f1db8deb4eda461706ae4820d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "WN9",
          "dataset_description": "Used to evaluate the performance of the VBKGC method, specifically comparing Hit@10 metric against baselines. | Used to evaluate knowledge graph reasoning models, focusing on wordnet relations and entity linking tasks. | Used to evaluate the performance of twins negative sampling in knowledge graph reasoning, specifically comparing against existing baselines. | Used to evaluate the VBKGC model with an embedding dimension of 128, focusing on multi-modal knowledge graph reasoning. | Employed to assess link prediction performance in knowledge graphs, specifically addressing relation prediction and entity disambiguation. | Used to determine optimal hyper-parameters for the VBKGC model with negative sampling methods, focusing on batch size, margin, and learning rate. | Used to evaluate the performance of the VBKGC model with twins, focusing on Hit@1 metric improvements. | Mentioned as a dataset enhanced in a multimodal scenario, but specific usage details are not provided. | Used to evaluate performance gains in knowledge representation learning, focusing on image-embodied representations and their impact on reasoning tasks.",
          "citing_paper_id": "252280329",
          "cited_paper_id": 9909815,
          "context_text": "The performance of twins negative sampling on the WN9 dataset exceeds the existing baselines in all aspects.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the WN9 dataset, which is a specific, verifiable dataset used for evaluating knowledge graph reasoning methods.",
          "citing_paper_doi": "10.48550/arXiv.2209.07084",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0bc258895dcd06c224d770139e872249c25374fd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9",
          "dataset_description": "Used to evaluate knowledge graph completion methods, specifically comparing twins negative sampling with other embedding alignment techniques. | Used to evaluate knowledge graph reasoning models, focusing on wordnet relations and entity linking tasks. | Used to measure the inference time of MMKGC models, specifically evaluating performance on knowledge graph completion tasks. | Employed to assess link prediction performance in knowledge graphs, specifically addressing relation prediction and entity disambiguation. | Used to determine optimal hyper-parameters for the VBKGC model with negative sampling methods, focusing on batch size, margin, and learning rate. | Used to evaluate multi-modal reasoning in knowledge graphs, focusing on image-based entity linking and relation prediction. | Used to evaluate the performance of the VBKGC model with twins, focusing on Hit@1 metric improvements. | Used to train VBKGC models with varying negative sampling strategies, focusing on knowledge graph completion performance. | Used to collect image resources for multi-modal knowledge graph reasoning, enhancing the visual aspects of entities in the knowledge graph.",
          "citing_paper_id": "252280329",
          "cited_paper_id": 202539519,
          "context_text": "In our experiments, we employ two public benchmarks WN9 [30] and FB15K-237 [32].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two public benchmarks, WN9 and FB15K-237, which are commonly used in knowledge graph reasoning tasks. These are specific datasets with clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2209.07084",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0bc258895dcd06c224d770139e872249c25374fd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/31184789ef4c3084af930b1e0dede3215b4a9240",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "248779998",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Wikipedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used to collect image-text pairs for early multi-modal research, focusing on the relationship between images and their textual descriptions. | Used to train and evaluate multi-modal models, focusing on image-text pairs to enhance cross-modal understanding and reasoning.",
          "citing_paper_id": "49867191",
          "cited_paper_id": 5583509,
          "context_text": "In the early times, some relatively small datasets were used, e.g., Wikipedia [1] and Pascal Sentence [2], which contain around 3,000 and 5,000 image-text pairs, respectively.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, Wikipedia and Pascal Sentence, both containing image-text pairs. These are specific, named datasets used in early research.",
          "citing_paper_doi": "10.1145/3383184",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/58555c7d168d1f50422ed9435d31ecd28d66eaa8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146",
          "citing_paper_year": 2017,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "The Wikipedia dataset is used to train the GloVe model, which provides feature representations for concepts' descriptions in the knowledge graph.",
          "citing_paper_id": "259165073",
          "cited_paper_id": 5958691,
          "context_text": "The GloVe model trained on the Wikipedia dataset is utilized to obtain the feature representation of concepts’ descriptions in the KG (we use the descriptions from WordNet [30] in this paper).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Wikipedia dataset and WordNet, but only Wikipedia is used as a dataset for training the GloVe model. WordNet is used for concept descriptions in the KG.",
          "citing_paper_doi": "10.1145/3580305.3599486",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/074c07282aedfd9a027bdea509df1d0490a9bbd3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d87ceda3042f781c341ac17109d1e94a717f5f60",
          "citing_paper_year": 2023,
          "cited_paper_year": 1999
        },
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used for event classification and relation extraction, contributing to the construction of the MMEKG multi-modal event knowledge graph. | Used for lexical data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph. | Used for news-related textual data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph. | Used for visual data to support object recognition and event relation extraction in the MMEKG multi-modal event knowledge graph. | Used for textual data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 248779998,
          "context_text": "MMEKG [24] N-MMKG event Wikipedia, BookCorpus, CC3M&CC12M, C4(news) WordNet event classification, object recognition, event relation extraction < 990K events, < 644 event relations < 863M instance events, < 934M instance events’ relations (including textual and visual ones)",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for constructing and evaluating the MMEKG multi-modal event knowledge graph, including Wikipedia, BookCorpus, CC3M, CC12M, and C4(news). These datasets are used for various tasks such as event classification, object recognition, and event relation extraction.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "14843884",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "KB-VQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "KB-VQA",
          "dataset_description": "Used as a source of images for visual question answering tasks, providing a diverse set of annotated images for training and evaluation. | Used to evaluate models on fact-based visual question answering, integrating textual and visual information to answer questions accurately. | Used for fact-based visual question answering, emphasizing the integration of factual knowledge with visual information to answer questions. | Used to evaluate fact-based visual question answering, focusing on commonsense reasoning and factual accuracy in responses. | Used for explicit knowledge-based reasoning in visual question answering, focusing on integrating external knowledge with image understanding.",
          "citing_paper_id": "85558018",
          "cited_paper_id": 7483388,
          "context_text": "In this context, two datasets, namely KB-VQA (Wang et al. 2017) and FVQA (Wang et al. 2018) were recently introduced.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, KB-VQA and FVQA, which are relevant to multi-modal knowledge graph reasoning. Both datasets are used for visual question answering tasks involving explicit knowledge.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "KB-VQA",
          "dataset_description": "Used for explicit knowledge-based reasoning in visual question answering, focusing on integrating external knowledge with image understanding. | Used for fact-based visual question answering, emphasizing the integration of factual knowledge with visual information to answer questions.",
          "citing_paper_id": "85558018",
          "cited_paper_id": 14843884,
          "context_text": "In this context, two datasets, namely KB-VQA (Wang et al. 2017) and FVQA (Wang et al. 2018) were recently introduced.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, KB-VQA and FVQA, which are relevant to multi-modal knowledge graph reasoning. Both datasets are used for visual question answering tasks involving explicit knowledge.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": "10.24963/IJCAI.2017/179",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "KB-VQA",
          "dataset_description": "Used to evaluate visual reasoning models, focusing on multi-modal reasoning tasks involving images and knowledge bases. | Used to evaluate multi-modal reasoning models, focusing on multi-modal data, including images and textual information, and knowledge graph reasoning.",
          "citing_paper_id": "237453242",
          "cited_paper_id": 14843884,
          "context_text": "KB-VQA (Wang et al., 2017b) dataset consists of three types of questions: “Visual\" question an-swerable using the visual concept in an image, “Common-sense\" questions answerable by adults without looking for an external source, and “KB-knowledge\" questions requiring higher-level knowledge, explicit reasoning, and external resource.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The KB-VQA dataset is explicitly mentioned and described in the context, indicating its use in visual question answering with different types of questions.",
          "citing_paper_doi": "10.18653/v1/2021.emnlp-main.517",
          "cited_paper_doi": "10.24963/IJCAI.2017/179",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4e92fec0a61972ae076707d0630d1333affccdfc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "218971783",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "OPENBG"
      ],
      "dataset_details": [
        {
          "dataset_name": "OPENBG",
          "dataset_description": "Used to develop and evaluate multi-modal knowledge graphs, focusing on large-scale integration of textual and visual data. | This dataset 'LSCOM' was mentioned in the citation context but no detailed description was generated. | Applied to assess multi-modal knowledge graph reasoning, emphasizing diverse and richly annotated data sources. | Utilized for evaluating multi-modal reasoning, specifically integrating textual and visual information in knowledge graphs. | Employed to benchmark relational and multimodal machine learning models, providing a collection of knowledge graph datasets.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 206477883,
          "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several multi-modal knowledge graphs, including OPENBG, COMM, LSCOM, WikiDiverse, and kgbench. These are specific datasets or collections used for evaluating multimodal machine learning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1109/MMUL.2006.63",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86dc975f9cbd9a205f8e82fb1db3b61c6b738fa5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2006
        },
        {
          "dataset_name": "OPENBG",
          "dataset_description": "Used to develop and evaluate multi-modal knowledge graphs, focusing on large-scale integration of textual and visual data. | This dataset 'LSCOM' was mentioned in the citation context but no detailed description was generated. | Applied to assess multi-modal knowledge graph reasoning, emphasizing diverse and richly annotated data sources. | Utilized for evaluating multi-modal reasoning, specifically integrating textual and visual information in knowledge graphs. | Employed to benchmark relational and multimodal machine learning models, providing a collection of knowledge graph datasets.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 218971783,
          "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several multi-modal knowledge graphs, including OPENBG, COMM, LSCOM, WikiDiverse, and kgbench. These are specific datasets or collections used for evaluating multimodal machine learning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "OPENBG",
          "dataset_description": "Used to develop and evaluate multi-modal knowledge graphs, focusing on large-scale integration of textual and visual data. | This dataset 'LSCOM' was mentioned in the citation context but no detailed description was generated. | Applied to assess multi-modal knowledge graph reasoning, emphasizing diverse and richly annotated data sources. | Utilized for evaluating multi-modal reasoning, specifically integrating textual and visual information in knowledge graphs. | Employed to benchmark relational and multimodal machine learning models, providing a collection of knowledge graph datasets.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 235271284,
          "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several multi-modal knowledge graphs, including OPENBG, COMM, LSCOM, WikiDiverse, and kgbench. These are specific datasets or collections used for evaluating multimodal machine learning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1007/978-3-030-77385-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dcde86c40b2480130d8d46380b3d83b1277b4c27",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "3144218",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "NUS-WIDE"
      ],
      "dataset_details": [
        {
          "dataset_name": "NUS-WIDE",
          "dataset_description": "Used for image captioning to integrate graph-structured information and improve the disambiguation of concepts in news images. | Employed for image captioning to enhance the reasoning capabilities of multi-modal data with graph-structured information. | Used to enhance reasoning capabilities in image tagging by constructing scene graphs and disambiguating concepts in a multi-modal context. | Utilized for image captioning to improve the relationship between textual and visual information using graph-structured data.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 6483070,
          "context_text": "…by constructing scene graphs 3.enhance the reasoning capabilities of multi-modal data with graph-structured information Image Tagging NUS-WIDE [129] help disambiguation the concept and relate them better to images Image Captioning MSVD [130] MSCOCO [58] GoodNews [131] 1.enable the…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for enhancing multi-modal reasoning through graph-structured information in image tagging and captioning tasks.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/1646396.1646452",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b80a580a6f2eca77524302acd944fd6edf0a0611",
          "citing_paper_year": 2022,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "NUS-WIDE",
          "dataset_description": "Used to assess the effectiveness of a GCN-based Multi-hop GNN model in visual concept detection, emphasizing multi-modal reasoning and retrieval. | Used to evaluate the performance of a GCN-based Multi-hop GNN model, focusing on multi-modal data integration and classification accuracy.",
          "citing_paper_id": "235306301",
          "cited_paper_id": 3144218,
          "context_text": "5 for the GCN-based Multi-hop GNN on both NUS-WIDE and MIRFlickr datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, NUS-WIDE and MIRFlickr, which are used for evaluating a GCN-based Multi-hop GNN model. These datasets are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1609/aaai.v35i3.16345",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e7b04687dc41ea7b2b603a0a6149dc258321301a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/36eff562f65125511b5dfab68ce7f7a943c27478",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "NUS-WIDE",
          "dataset_description": "Used to evaluate multi-modal reasoning systems, focusing on image-text pairs across 24 categories, enhancing understanding of visual and textual data integration. | Used to evaluate cross-modal retrieval performance, comparing DAGNN with ten baseline methods, focusing on visual and textual data integration. | Used to set a batch size of 1024 for experiments, focusing on visual concept detection and multi-modal learning. | Used to verify the superiority of DAGNN in multi-label cross-modal tasks, focusing on image and text label prediction. | Used to verify the superiority of DAGNN in multi-label cross-modal tasks, focusing on image and tag retrieval. | Used for cross-modal retrieval and representation learning, containing multiple labels per sample to capture semantic dependencies. | Used to assess the effectiveness of a GCN-based Multi-hop GNN model in visual concept detection, emphasizing multi-modal reasoning and retrieval. | Used to visualize learned classifiers in different learning manners, evaluating the preservation of meaningful semantic structure. | Used to set a batch size of 100 for experiments, focusing on visual concept detection and multi-modal learning. | Used to evaluate the performance of a GCN-based Multi-hop GNN model, focusing on multi-modal data integration and classification accuracy.",
          "citing_paper_id": "235306301",
          "cited_paper_id": 14761697,
          "context_text": "5 for the GCN-based Multi-hop GNN on both NUS-WIDE and MIRFlickr datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, NUS-WIDE and MIRFlickr, which are used for evaluating a GCN-based Multi-hop GNN model. These datasets are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.1609/aaai.v35i3.16345",
          "cited_paper_doi": "10.1145/1743384.1743475",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e7b04687dc41ea7b2b603a0a6149dc258321301a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3cfdd7b0dacef10a1578c0e6ed55aa3f3129a437",
          "citing_paper_year": 2021,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "173188134",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "ImageNet-V2"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet-V2",
          "dataset_description": "Used to evaluate the generalization capability of GraphAdapter with different visual backbones, focusing on robustness and out-of-distribution performance.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 173188134,
          "context_text": "We also investigate the generalization capability of our GraphAdapter on four commonly-used datasets i.e. , ImageNet-V2 [52], ImageNet-Sketch [62], ImageNet-A [22], ImageNet-R [21], with different visual backbones, including the pre-trained ResNet-101 [19], ViT-B/16 [13], ViT-B/32 [13].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions four specific datasets used to evaluate the generalization capability of GraphAdapter. These datasets are commonly used for evaluating robustness and out-of-distribution generalization in image recognition tasks.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4ae0c4a511697e960c477ea3e37b3e11bf3e0e02",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "ImageNet-V2",
          "dataset_description": "Used to evaluate the generalization capability of GraphAdapter with different visual backbones, focusing on robustness and out-of-distribution performance.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 206594692,
          "context_text": "We also investigate the generalization capability of our GraphAdapter on four commonly-used datasets i.e. , ImageNet-V2 [52], ImageNet-Sketch [62], ImageNet-A [22], ImageNet-R [21], with different visual backbones, including the pre-trained ResNet-101 [19], ViT-B/16 [13], ViT-B/32 [13].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions four specific datasets used to evaluate the generalization capability of GraphAdapter. These datasets are commonly used for evaluating robustness and out-of-distribution generalization in image recognition tasks.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ImageNet-V2",
          "dataset_description": "Used to evaluate the generalization capability of GraphAdapter with different visual backbones, focusing on robustness and out-of-distribution performance.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 220250257,
          "context_text": "We also investigate the generalization capability of our GraphAdapter on four commonly-used datasets i.e. , ImageNet-V2 [52], ImageNet-Sketch [62], ImageNet-A [22], ImageNet-R [21], with different visual backbones, including the pre-trained ResNet-101 [19], ViT-B/16 [13], ViT-B/32 [13].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions four specific datasets used to evaluate the generalization capability of GraphAdapter. These datasets are commonly used for evaluating robustness and out-of-distribution generalization in image recognition tasks.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00823",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/022622e024890d6e044ac50e2da6b44c59bdf418",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "18682",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "NLVR2"
      ],
      "dataset_details": [
        {
          "dataset_name": "NLVR2",
          "dataset_description": "Used to evaluate visual reasoning performance, specifically comparing existing approaches and the state-of-the-art method 'MaxEnt'. The dataset challenges models with complex visual and textual reasoning tasks.",
          "citing_paper_id": "201103729",
          "cited_paper_id": 18682,
          "context_text": "…is suitable and achieves a 4.6% improvement on opendomain questions (‘Open’ in Table 2).7\nNLVR2 NLVR2 (Suhr et al., 2019) is a challenging visual reasoning dataset where some existing approaches (Hu et al., 2017; Perez et al., 2018) fail, and the SotA method is ‘MaxEnt’ in Suhr et al. (2019).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'NLVR2' as a visual reasoning dataset, which is relevant to multi-modal knowledge graph reasoning. It is used to evaluate the performance of existing approaches and the state-of-the-art method.",
          "citing_paper_doi": "10.18653/v1/D19-1514",
          "cited_paper_doi": "10.1109/ICCV.2017.93",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a396a6febdacb84340d139096455e67049ac1e22",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "NLVR2",
          "dataset_description": "Used to evaluate the LXMERT model on natural language and vision reasoning tasks, focusing on the classification of concatenated cross-modality image representations.",
          "citing_paper_id": "201103729",
          "cited_paper_id": 2359786,
          "context_text": "To use our LXMERT model on NLVR 2 , we concatenate the cross-modality representations of the two images and then build the classiﬁer with GeLU activation(Hendrycks and Gimpel, 2016).",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'NLVR 2' which is a dataset used for evaluating multi-modal reasoning models. However, it does not provide details on how the dataset is used beyond mentioning it as part of the evaluation process.",
          "citing_paper_doi": "10.18653/v1/D19-1514",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "182952863",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MPII-MD"
      ],
      "dataset_details": [
        {
          "dataset_name": "MPII-MD",
          "dataset_description": "Used to learn text-video embeddings from a large corpus of instructional videos, focusing on narrated content. | Applied to analyze multimodal data in movies, focusing on action and event recognition.",
          "citing_paper_id": "233296845",
          "cited_paper_id": 8447479,
          "context_text": "Dataset # Videos # Sentences Duration Source Language YouCook [6] - 3K - YouTube English M-VAD [1] 92 56K 84h Movies English MPII-MD [33] 94 68K 41h Movies English MSR-VTT [43] 7,180 200K 40h Youtube English TGIF [22] 102,068 126K 103h Tumblr English AutoGIF [29] 163,183 165K - Web English HowTo100M [28] 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used in multi-modal video and text research, which are directly relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3474085.3475431",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e75ef4de1117db773b02b901f6d19a2ffd36a84a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MPII-MD",
          "dataset_description": "Used to train and evaluate text-video embedding models, focusing on the performance in various downstream tasks. | Applied to analyze multimodal data in movies, focusing on action and event recognition. | Employed for learning a text-video embedding by watching and narrating millions of instructional video clips. | Used to learn a text-video embedding by watching hundred million narrated video clips, focusing on multimodal representation learning. | Used to learn text-video embeddings from a large corpus of instructional videos, focusing on narrated content. | Utilized for training and evaluating text-video alignment models, specifically using automatically generated GIFs and their associated captions. | Used to evaluate multi-modal reasoning in GIFs, focusing on temporal and spatial understanding in short video clips. | Employed to learn a text-video embedding by watching and analyzing a large collection of narrated video clips, emphasizing the scale and diversity of the data. | Applied to study automatic generation of GIFs, emphasizing the alignment between visual content and textual descriptions. | Utilized to learn text-video embeddings from narrated video clips, addressing the challenge of aligning multimodal content at scale. | Used to train and evaluate text-video alignment models, focusing on short, animated GIFs with textual descriptions. | Proposed for pre-training the Victor model on large-scale English datasets, specifically to study performance in downstream tasks involving text-video embeddings. | Used to provide a large-scale dataset of short videos for multi-modal learning, emphasizing the scale and nature of the content compared to existing datasets.",
          "citing_paper_id": "233296845",
          "cited_paper_id": 182952863,
          "context_text": "Dataset # Videos # Sentences Duration Source Language YouCook [6] - 3K - YouTube English M-VAD [1] 92 56K 84h Movies English MPII-MD [33] 94 68K 41h Movies English MSR-VTT [43] 7,180 200K 40h Youtube English TGIF [22] 102,068 126K 103h Tumblr English AutoGIF [29] 163,183 165K - Web English HowTo100M [28] 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used in multi-modal video and text research, which are directly relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3474085.3475431",
          "cited_paper_doi": "10.1109/ICCV.2019.00272",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e75ef4de1117db773b02b901f6d19a2ffd36a84a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "8081284",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "GQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "GQA",
          "dataset_description": "Applied to evaluate natural language reasoning over paired images, emphasizing multi-modal alignment and logical inference. | Used to enhance image understanding in visual question answering, focusing on complex reasoning and compositional questions.",
          "citing_paper_id": "201103729",
          "cited_paper_id": 8081284,
          "context_text": "0 dataset (Goyal et al., 2017), GQA (Hudson and Manning, 2019), and NLVR 2 .",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three datasets: '0 dataset', 'GQA', and 'NLVR 2'. '0 dataset' is likely a placeholder or error, so it is excluded. 'GQA' and 'NLVR 2' are specific datasets used in visual question answering and natural language reasoning tasks.",
          "citing_paper_doi": "10.18653/v1/D19-1514",
          "cited_paper_doi": "10.1007/s11263-018-1116-0",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GQA",
          "dataset_description": "Used to report state-of-the-art results on the GQA leaderboard, focusing on visual reasoning and question answering tasks. | Used to evaluate state-of-the-art results in visual question answering, focusing on complex reasoning and compositional generalization.",
          "citing_paper_id": "201103729",
          "cited_paper_id": 29150617,
          "context_text": "GQA The GQA (Hudson and Manning, 2019) SotA result is taken from BAN (Kim et al., 2018) on the public leaderbaord.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions GQA, which is a dataset, and BAN, which is a method. However, BAN is excluded as per instructions. GQA is included because it is a specific, verifiable dataset.",
          "citing_paper_doi": "10.18653/v1/D19-1514",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a5d10341717c0519cf63151b496a6d2ed67aa05f",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "12758674",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "WebChild"
      ],
      "dataset_details": [
        {
          "dataset_name": "WebChild",
          "dataset_description": "Used to harvest and organize commonsense knowledge from the web, focusing on the automatic extraction and categorization of factual information. | Used to query visual concepts extracted from images, contributing to the construction of a multi-modal knowledge base.",
          "citing_paper_id": "221397171",
          "cited_paper_id": 3088903,
          "context_text": "[37] N. Tandon , G. De Melo , F. Suchanek , G. Weikum , WebChild: harvesting\nand organizing commonsense knowledge from the web, in: WSDM, 2014,\npp. 523–532 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'WebChild', which is a specific dataset used for harvesting and organizing commonsense knowledge from the web. The context indicates that it is a reusable resource.",
          "citing_paper_doi": "10.1016/j.patcog.2020.107563",
          "cited_paper_doi": "10.1145/2556195.2556245",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e526624783b3b5687da54b8cd4a7190a26a0b5e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db95087540c956788a8560495bf03ded1239e062",
          "citing_paper_year": 2020,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "WebChild",
          "dataset_description": "Used to acquire comparative commonsense knowledge, enhancing the reasoning capabilities of the models with web-derived information. | Provides practical commonsense reasoning, integrating a wide range of knowledge to support the models' reasoning tasks. | Used to provide a large, multilingual knowledge graph that enhances VQA models with semantic and relational information, improving reasoning accuracy. | Serves as a nucleus for a web of open data, providing structured information from Wikipedia for knowledge graph reasoning. | Used to enhance VQA models with comparative commonsense knowledge, specifically integrating web-derived information to improve accuracy. | Serves as a nucleus for a web of open data, providing structured information to enrich the knowledge base of the models. | Used to augment VQA models with structured knowledge from Wikipedia, enhancing the models' ability to reason about entities and their relationships. | Used to acquire comparative commonsense knowledge, focusing on relational and conceptual information for multi-modal reasoning. | Provides a large, multilingual knowledge graph of common-sense assertions, enhancing the models' ability to reason about everyday concepts.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 12758674,
          "context_text": "The relevant knowledge used in these models is obtained from various knowledge bases such as WebChild [37], DBpedia [2] and ConceptNet [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific knowledge bases that are used to obtain relevant knowledge for the models. These are WebChild, DBpedia, and ConceptNet.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1609/aaai.v28i1.8735",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/19b505d471c88708de949dffd788a3529b66d7c8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "53199920",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Visual7W-KB"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual7W-KB",
          "dataset_description": "Used to test the performance of the Out of the Box method on factual visual question answering, focusing on multi-modal reasoning with graph convolution networks.",
          "citing_paper_id": "221397171",
          "cited_paper_id": 53199920,
          "context_text": "We also test the performance of Out of the Box (OB) [7] on Visual7W-KB and report the results in Table 2.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual7W-KB' which is likely a dataset used for testing the performance of a method called 'Out of the Box'. The dataset is used in the context of factual visual question answering.",
          "citing_paper_doi": "10.1016/j.patcog.2020.107563",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e526624783b3b5687da54b8cd4a7190a26a0b5e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Visual7W-KB",
          "dataset_description": "Used to test the performance of the Out of the Box method on factual visual question answering, focusing on multi-modal reasoning with graph convolution networks. | Used to test the performance of the Out of the Box method, focusing on factual visual question answering using graph convolution networks.",
          "citing_paper_id": "219708313",
          "cited_paper_id": 53199920,
          "context_text": "We also test the performance of Out of the Box (OB) [ Narasimhan et al. , 2018 ] on Visual7W-KB and report the results in Table 5.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Visual7W-KB' which appears to be a specific dataset used for testing the performance of a method. The context indicates that this dataset is used for factual visual question answering.",
          "citing_paper_doi": "10.24963/ijcai.2020/153",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6b13065b4050800e30bb74e010b8aaba3355525d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "229153508",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "KRVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "KRVQA",
          "dataset_description": "Used to evaluate the CMRL model on knowledge-routed visual question reasoning, assessing the model's ability to handle deep representation embedding challenges. | Used to train and evaluate knowledge-routed visual question reasoning models, focusing on integrating external knowledge with visual and textual information. | Used to enhance visual question reasoning by integrating multi-source knowledge, focusing on challenges for deep representation embedding. | Provided the images used in the KRVQA dataset, serving as a rich source of visual content for multi-modal reasoning tasks. | Used to evaluate the CMRL model on visual question answering tasks requiring external knowledge, focusing on the integration of visual and textual information.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 229153508,
          "context_text": "Knowledge-Routed Visual Question Reasoning (KRVQA) [6] is the largest knowledge-based VQA dataset, which contains 32,910 images from Visual Genome [15] dataset and 157,201 QA pairs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets: KRVQA and Visual Genome. KRVQA is described as a knowledge-based VQA dataset, and Visual Genome is referenced as the source of images for KRVQA.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1109/TNNLS.2020.3045034",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e452c1aa68173d6a1d3aefd9f08f70e43b0c59f4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "KRVQA",
          "dataset_description": "Used to train a supervised knowledge retrieval model, focusing on integrating visual and textual information for question answering tasks.",
          "citing_paper_id": "269157470",
          "cited_paper_id": 229153508,
          "context_text": "In this work, we propose to train a supervised knowledge retrieval model using the provided meta-data in the KRVQA dataset (Cao et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the KRVQA dataset, which is a specific dataset used for training a supervised knowledge retrieval model. The dataset is directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2404.10226",
          "cited_paper_doi": "10.1109/TNNLS.2020.3045034",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a712f22b419623590b5b121278b7709cd1d08a64",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e452c1aa68173d6a1d3aefd9f08f70e43b0c59f4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "141406559",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ASER"
      ],
      "dataset_details": [
        {
          "dataset_name": "ASER",
          "dataset_description": "Used to advance the field of event knowledge graphs by providing a large-scale ontology with 990 thousand concept events and 644 relation types, covering most real-world happenings. | Leveraged to build a large-scale eventuality knowledge graph using defined patterns and an automatic pipeline, focusing on scalable construction of multi-modal reasoning resources.",
          "citing_paper_id": "248779998",
          "cited_paper_id": 141406559,
          "context_text": "ATOMIC (Sap et al., 2019) annotates manually and constructs high-quality knowledge bases, while ASER (Zhang et al., 2020) leverages defined patterns and automatic pipeline to build a large-scale graph.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ATOMIC and ASER, but only ASER is a dataset according to the cited paper title. ATOMIC is described as a method for constructing knowledge bases.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": "10.1145/3366423.3380107",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c5e77461f796ab460ae5daa97c1323d2e25c893",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "ASER",
          "dataset_description": "ASER is used as a large-scale eventuality knowledge graph, providing a rich source of event-related information for reasoning tasks. | ASER is used as a large-scale eventuality knowledge graph, providing a rich resource for semantic relations and eventualities in multi-modal reasoning contexts.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 141406559,
          "context_text": "Up to now, the largest event KG ASER [51] has reached the scale of 194 million events.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions ASER as a large-scale eventuality knowledge graph, which is relevant to multi-modal knowledge graph reasoning. However, it does not specify usage details.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": "10.1145/3366423.3380107",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c5e77461f796ab460ae5daa97c1323d2e25c893",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "201103729",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "VQAv2"
      ],
      "dataset_details": [
        {
          "dataset_name": "VQAv2",
          "dataset_description": "Used for training and testing a model with 14,031 images and 14,055 questions, divided into training and testing sets. Investigates multi-modal reasoning in image and text data. | Provides a large set of annotated images for training and evaluating image captioning and object detection models. | Used for visual question answering, containing 204,721 real images from MSCOCO, focusing on complex interactions between visual and textual information. | Serves as the image source for VQAv2, providing a diverse set of real-world images for visual question answering tasks. | Used to train and evaluate visual question answering models, focusing on complex reasoning over images and questions.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 14113767,
          "context_text": "VQAv2 [10] is a widely-used visual question answering dataset that contains 204,721 real images fromMSCOCO [18].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions VQAv2 and MSCOCO, both of which are datasets. VQAv2 is described as containing images from MSCOCO, indicating its use in visual question answering.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "VQAv2",
          "dataset_description": "Used for pretraining to accumulate basic visual-domain knowledge, focusing on cross-modality encoder representations using transformers.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 201103729,
          "context_text": "Therefore, like LXMERT [36] and MuKEA [7], we pretrain on the VQAv2 to accumulate basic visual-domain knowledge, i.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions VQAv2 as a dataset used for pretraining, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.18653/v1/D19-1514",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "264491155",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ArXivQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "ArXivQA",
          "dataset_description": "Used to improve scientific comprehension of large vision-language models by integrating scientific diagrams and multimodal data.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 264491155,
          "context_text": "More recently, MathVista [45], MathVerse [87], and ArXivQA [38] have integrated different scientiﬁc diagrams to develop benchmarks with a wider variety of tasks.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions MathVista, MathVerse, and ArXivQA as benchmarks integrating scientific diagrams. However, only ArXivQA is clearly identified as a dataset in the cited paper titles.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "ArXivQA",
          "dataset_description": "Used to improve scientific comprehension of large vision-language models by integrating scientific diagrams and multimodal data.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 268201930,
          "context_text": "More recently, MathVista [45], MathVerse [87], and ArXivQA [38] have integrated different scientiﬁc diagrams to develop benchmarks with a wider variety of tasks.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions MathVista, MathVerse, and ArXivQA as benchmarks integrating scientific diagrams. However, only ArXivQA is clearly identified as a dataset in the cited paper titles.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.48550/arXiv.2403.00231",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56994972adca9319577617345128e46803a4043f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "173991173",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "OKVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "OKVQA",
          "dataset_description": "Used to provide a real example for visual question answering that requires external knowledge, focusing on questions about objects on a wooden table and under dishes. | Used to evaluate knowledge-based visual question answering systems, focusing on the integration of external knowledge in multi-modal reasoning tasks. | Used to evaluate the CMRL model on knowledge-routed visual question reasoning, assessing the model's ability to handle deep representation embedding challenges. | Used to evaluate the CMRL model on visual question answering tasks requiring external knowledge, focusing on the integration of visual and textual information.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 173991173,
          "context_text": "In this section, we irst evaluate the CMRL model on the knowledge-based datasets OKVQA [25] and KRVQA [6] and compare it with existing works.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, OKVQA and KRVQA, which are used to evaluate the CMRL model. Both datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1109/CVPR.2019.00331",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/28ad018c39d1578bea84e7cedf94459e3dbe1e70",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "OKVQA",
          "dataset_description": "Used for vision-language understanding tasks, focusing on visual question answering with a balanced dataset. | Used for vision-language understanding tasks, focusing on natural language inference with visual entailment. | Used for entity linking tasks, focusing on linking entities in diverse Wikipedia articles. | Used for vision-language understanding tasks, focusing on visual question answering with world knowledge.",
          "citing_paper_id": "258352810",
          "cited_paper_id": 249375629,
          "context_text": "We finetune and evaluate our models on the vision-language understanding tasks (OK-VQA, AOK-VQA, VQA-v2, and SNLI-VE) (Marino et al., 2019; Schwenk et al., 2022; Antol et al., 2015; Xie et al., 2019) and entity linking tasks (WikiDiverse and WikiPerson) (Wang et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for vision-language understanding and entity linking tasks. These datasets are clearly named and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3581783.3613848",
          "cited_paper_doi": "10.48550/arXiv.2206.01718",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0fe1b1bfd634ee42846afbd64cef1c682e02e5e7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/47a67e76ed84260ff19f7a948d764005d1edf1c9",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "211123242",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Richpedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "Richpedia",
          "dataset_description": "Used to enhance KG-based applications by incorporating images as a visual modality, demonstrating significant potential in multi-modal reasoning. | Used to build a comprehensive multi-modal knowledge graph, leveraging abundant visual resources primarily composed of images to enhance entity alignment and representation.",
          "citing_paper_id": "264492372",
          "cited_paper_id": 225115084,
          "context_text": "The utilization of multi-modal formats like MMKG [18] and Richpedia [34] has been on the rise, resulting in the incorporation of images as a visual modality in KGs has shown significant potential in enhancing KG-based applications.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MMKG' and 'Richpedia' as multi-modal knowledge graphs, but only 'Richpedia' is a specific, verifiable dataset with a clear reference to a cited paper.",
          "citing_paper_doi": "10.1145/3581783.3612151",
          "cited_paper_doi": "10.1016/j.bdr.2020.100159",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8d09265120997e305c262b13e7a675f7c5a3ab9d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d39da9432ff7bd1d42349b96cbd194416c3d51e1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Richpedia",
          "dataset_description": "Used as a comprehensive multi-modal knowledge graph containing triplets, textual descriptions, and images, supporting research in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 211123242,
          "context_text": "• Richpedia [225] is the KGs, composed of the triplets, textual descriptions, and images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Richpedia is identified as a multi-modal knowledge graph, which aligns with the research topic of multi-modal knowledge graph reasoning. It is used as a comprehensive resource containing triplets, textual descriptions, and images.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-030-41407-8_9",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8a29f76fd7214431c84607d06d6257ff5def6a35",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "11241677",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "YouTube-8M"
      ],
      "dataset_details": [
        {
          "dataset_name": "YouTube-8M",
          "dataset_description": "Used to present challenges in large-scale video classification, focusing on the integration of multi-modal data including video, audio, and text. | Used to conduct extensive experiments demonstrating the superior performance of the approach in video classification, outperforming state-of-the-art knowledgeless models by up to 2.9% in MAP. | Employed to improve video classification, integrating video, audio, and text modalities for comprehensive multi-modal learning. | Used for multi-label video classification, focusing on large-scale benchmarking of video content. The dataset supports research into multi-modal learning by providing extensive video, audio, and text data. | Utilized to enhance multimedia research, offering a vast collection of images and videos for multi-modal analysis. | Used to explore multi-modal video classification, leveraging pre-extracted video and audio features for frame-level analysis.",
          "citing_paper_id": "40114756",
          "cited_paper_id": 11241677,
          "context_text": "Data We use the YouTube-8M benchmark 5 , the largest public dataset for multi-label video classiﬁcation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the YouTube-8M benchmark, which is a specific, publicly accessible dataset. The citation intent is to reference a reusable resource, and the resource type is a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d81490b09cc99ab573d4ec73b43834a20c303244",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "YouTube-8M",
          "dataset_description": "Used to present challenges in large-scale video classification, focusing on the integration of multi-modal data including video, audio, and text. | Used to conduct extensive experiments demonstrating the superior performance of the approach in video classification, outperforming state-of-the-art knowledgeless models by up to 2.9% in MAP. | Employed to improve video classification, integrating video, audio, and text modalities for comprehensive multi-modal learning. | Used for multi-label video classification, focusing on large-scale benchmarking of video content. The dataset supports research into multi-modal learning by providing extensive video, audio, and text data. | Utilized to enhance multimedia research, offering a vast collection of images and videos for multi-modal analysis. | Used to explore multi-modal video classification, leveraging pre-extracted video and audio features for frame-level analysis.",
          "citing_paper_id": "40114756",
          "cited_paper_id": 31387612,
          "context_text": "Data We use the YouTube-8M benchmark 5 , the largest public dataset for multi-label video classiﬁcation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the YouTube-8M benchmark, which is a specific, publicly accessible dataset. The citation intent is to reference a reusable resource, and the resource type is a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d81490b09cc99ab573d4ec73b43834a20c303244",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f67c665208b8d67555130cd3555a4a15f3940cfe",
          "citing_paper_year": 2017,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "44145776",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB-IMG-TXT"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB-IMG-TXT",
          "dataset_description": "Used to crawl 100 images per entity for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information. | Used to crawl 10 images per entity for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information. | Used to set the embedding dimension for image features to 128, focusing on multi-modal knowledge graph representation learning. | Used to enhance multimodal knowledge graphs by adding textual descriptions and images to entities, aiming to increase data diversity. | Used to set the embedding dimension for image features to 4096, focusing on multi-modal knowledge graph representation learning. | Used as a benchmark to compare against FB-IMG-TXT, demonstrating differences in sparsity and complexity in multimodal knowledge graph representation learning. | Used to compare sparsity and complexity in multimodal knowledge graph representation learning, highlighting challenges in handling sparse and complex data.",
          "citing_paper_id": "252089825",
          "cited_paper_id": 44145776,
          "context_text": "The embedding dimension d s of entity, relation and history is set to 200, the embedding dimension d i of image feature is set to 128 and 4096 on FB-IMG-TXT and WN9-IMG-TXT respectively, and the embedding dimension d t of textual feature is 1000 [45].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FB-IMG-TXT and WN9-IMG-TXT, which are used to set the embedding dimensions for image features. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.00015",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79801d46d6c495f1db8deb4eda461706ae4820d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB-IMG-TXT",
          "dataset_description": "Used to combine textual descriptions and images in a knowledge graph, specifically for multimodal translation-based representation learning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 44145776,
          "context_text": "• FB-IMG-TXT [173] is the KG combined with textual descriptions and images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FB-IMG-TXT' as a knowledge graph combined with textual descriptions and images, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "3986974",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CWQ"
      ],
      "dataset_details": [
        {
          "dataset_name": "CWQ",
          "dataset_description": "Used to assess the performance of the proposed method on complex web-based questions, emphasizing the handling of structured and unstructured data. | Used to evaluate the robustness of the proposed approach to complex questions, focusing on the ability to handle multi-hop reasoning and diverse query types.",
          "citing_paper_id": "252625996",
          "cited_paper_id": 3986974,
          "context_text": "To prove that our proposed approach has high robustness to complex questions, we separately analyze the complexity of the CWQ and WQSP.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions CWQ and WQSP, which are likely datasets used for evaluating the robustness of the proposed approach to complex questions.",
          "citing_paper_doi": "10.1109/IJCNN55064.2022.9892700",
          "cited_paper_doi": "10.18653/v1/N18-1059",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b2159e6ad8efb3e057e7d8f1d9e9ea1f5a3b5697",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8725f13be7434b69738491c66b45c9225258253",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "CWQ",
          "dataset_description": "Used to set the k-hop parameter to 2 in the candidate answers generation phase, focusing on simple question answering. | Used to set the k-hop parameter to 3 in the candidate answers generation phase, focusing on complex question answering.",
          "citing_paper_id": "252625996",
          "cited_paper_id": 52154304,
          "context_text": "In the candidate answers generation phase (Section III-A), we set the K (k-hop) is set to 3 in the CWQ dataset and 2 in the WQSP dataset 3 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for setting parameters in a multi-hop reasoning process.",
          "citing_paper_doi": "10.1109/IJCNN55064.2022.9892700",
          "cited_paper_doi": "10.18653/v1/D18-1455",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b2159e6ad8efb3e057e7d8f1d9e9ea1f5a3b5697",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8d17543c20f23b6a40bec9334d50e9c15a08c1c4",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "51864534",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CSM movie dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "CSM movie dataset",
          "dataset_description": "Used to pre-train the ResNet-50 backbone for extracting character features in video frames, focusing on visual and temporal links in person search tasks.",
          "citing_paper_id": "239011786",
          "cited_paper_id": 51864534,
          "context_text": "Afterwards, we adopt the ResNet-50 [7] backbone network, which is pre-trained on the CSM movie dataset1 via PPCC method [9], to extract character features Fc on each frame.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'CSM movie dataset' as a pre-training source for the ResNet-50 backbone. This dataset is specific and relevant to the research context.",
          "citing_paper_doi": "10.1145/3474085.3475684",
          "cited_paper_doi": "10.1007/978-3-030-01261-8_26",
          "citing_paper_url": "https://www.semanticscholar.org/paper/163c9313a8696142c36cbc92cc6cc382f9a1a1a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2320ce9e222dcbafa6d64790b88f28563ced3846",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "CSM movie dataset",
          "dataset_description": "Used to pre-train the ResNet-50 model for extracting character features in movie frames, focusing on multi-modal reasoning in video content.",
          "citing_paper_id": "239011786",
          "cited_paper_id": 206594692,
          "context_text": "Afterwards, we adopt the ResNet-50 [7] backbone network, which is pre-trained on the CSM movie dataset1 via PPCC method [9], to extract character features 𝐹𝑐 on each frame.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'CSM movie dataset' as a pre-training source for the ResNet-50 model. This dataset is used for extracting character features in the current research.",
          "citing_paper_doi": "10.1145/3474085.3475684",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/163c9313a8696142c36cbc92cc6cc382f9a1a1a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "6611164",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "YAGO3-10"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO3-10",
          "dataset_description": "Used to report additional results, focusing on a subset of YAGO3 with 123,182 entities and 37 relations, where each entity has at least 10 relations. | Used to compare the parameter efficiency of LowFER-k with RotatE, focusing on the number of parameters required for knowledge graph reasoning.",
          "citing_paper_id": "221082536",
          "cited_paper_id": 6611164,
          "context_text": "We report additional results on YAGO3-10, which is a sub-set of YAGO3 (Mahdisoltani et al., 2013), consisting of 123 , 182 entities and 37 relations such that have each entity has at least 10 relations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "YAGO3-10 is explicitly mentioned as a dataset used for reporting additional results, fitting the criteria for a specific, verifiable dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0126fce30b412d583f8e33714908dd09b86293d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c5b5adc3830ac45bf1d764603b1b71e5f729616",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "YAGO3-10",
          "dataset_description": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 53080423,
          "context_text": "According to the scopes of relations, YAGO3-10 [217], YAGO37 [218] and YAGO39k [219] are the subsets of it.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions YAGO3-10, YAGO37, and YAGO39k as subsets of a larger dataset, which are likely knowledge graphs used in multi-modal reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/D18-1222",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "4710439",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Charades"
      ],
      "dataset_details": [
        {
          "dataset_name": "Charades",
          "dataset_description": "Contains multiple concurrent actions, providing complex scenarios for multi-action recognition and reasoning.",
          "citing_paper_id": "209376177",
          "cited_paper_id": 4710439,
          "context_text": "In the mean time, other databases have provided more varieties of annotations: AVA [26] localizes the actors of actions, Charades [66] contains multiple actions happening at the same time, EPIC-Kitchen [15] localizes the interacted objects in ego-centric kitchen videos, DALY [75] provides object bounding boxes and upper body poses for 10 daily activities.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets with specific names and annotations, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/cvpr42600.2020.01025",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d1242ba8fdb994b82a0575dc92f30f7b26a75707",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fc50c9392fd23b6c88915177c6ae904a498aacea",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Charades",
          "dataset_description": "Contains multiple concurrent actions, providing complex scenarios for multi-action recognition and reasoning.",
          "citing_paper_id": "209376177",
          "cited_paper_id": 35269290,
          "context_text": "In the mean time, other databases have provided more varieties of annotations: AVA [26] localizes the actors of actions, Charades [66] contains multiple actions happening at the same time, EPIC-Kitchen [15] localizes the interacted objects in ego-centric kitchen videos, DALY [75] provides object bounding boxes and upper body poses for 10 daily activities.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets with specific names and annotations, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/cvpr42600.2020.01025",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d1242ba8fdb994b82a0575dc92f30f7b26a75707",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3daafe6389d877fe15d8823cdf5ac15fd919676f",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "14843884",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "KVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "KVQA",
          "dataset_description": "Used for fact-based visual question answering, integrating factual knowledge with visual information. | Focuses on open-ended questions that require external knowledge beyond the image content. | Utilizes a subset of VQA with wh-questions to evaluate fine-grained reasoning. | Utilizes knowledge bases to enhance visual question answering by incorporating external knowledge. | Used for visual question answering, focusing on image-text pairs to test reasoning capabilities. | Employs knowledge graphs to answer questions that require reasoning over structured data and images. | Employs complex questions requiring multi-step reasoning over images and knowledge graphs.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 14843884,
          "context_text": "VQA GQA [119] Visual7w [120] OK-VQA [56] FVQA [33] KVQA [121] KB-VQA [122] 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for visual question answering and knowledge-based reasoning, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.24963/IJCAI.2017/179",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "KVQA",
          "dataset_description": "Used to assess models that integrate knowledge bases with visual question answering, enhancing the accuracy and depth of responses. | Used to test the ability to reason about visual content using external knowledge, specifically in the context of knowledge-intensive VQA tasks. | Used to evaluate models that answer factual questions about images, requiring the integration of external knowledge and visual context. | Used to benchmark models that require external knowledge to answer visual questions, enhancing the depth of understanding in multi-modal reasoning. | Used to assess the need for external knowledge in visual question answering, emphasizing the integration of visual and textual information. | Used to test the reasoning capability of VQA models by providing questions that require external knowledge, focusing on the model's ability to integrate visual and textual information. | Used to train and evaluate models that answer questions about images, focusing on the integration of visual and textual information. | Used to assess the ability of models to reason about complex visual scenes and answer detailed questions, emphasizing compositional and logical reasoning. | Used to evaluate visual reasoning combined with external knowledge, focusing on complex question answering in the VQA domain. | Used to test the ability of models to answer knowledge-based questions about images, focusing on the combination of visual and textual information.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 173991173,
          "context_text": "In most recent VQA benchmark datasets such as GQA [125], OK-VQA [49] and KVQA [127], many questions require visual reasoning combined with external knowledge.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific VQA benchmark datasets that require visual reasoning combined with external knowledge, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/CVPR.2019.00331",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/28ad018c39d1578bea84e7cedf94459e3dbe1e70",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "19139252",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB13"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB13",
          "dataset_description": "Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets for efficient testing and validation. | Used to evaluate knowledge graph completion methods, focusing on entity and relation prediction in a smaller subset of Freebase. | Used to evaluate the effectiveness of knowledge graph embedding techniques, emphasizing the ability to handle a larger and more complex subset of Freebase. | Used to assess the performance of knowledge graph embedding models, emphasizing the scalability and accuracy in a moderately sized subset of Freebase. | Used to assess the scalability and performance of knowledge graph embedding models on medium-sized entity sets. | Used to assess the scalability of knowledge graph completion methods, focusing on a very large subset of Freebase with millions of entities and relations. | Used to evaluate the performance of knowledge graph completion models, specifically addressing the issue of inverse relations and improving the quality of predictions. | Used to benchmark knowledge graph completion algorithms, specifically evaluating the ability to predict missing links in a widely used subset of Freebase. | Used to test the robustness and generalization capabilities of knowledge graph reasoning algorithms on larger datasets. | Used to test the robustness of knowledge graph reasoning models, focusing on a larger subset of Freebase with a diverse set of entities and relations. | Used to test the integration of knowledge graphs with textual data, focusing on linking entities from the New York Times corpus to a subset of Freebase.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 19139252,
          "context_text": "According to the entity set size, several subsets generated from it, including FB13 [201], FB122 [202], FB15k [203], FB20k [198], FB24k [204], FB5M [18], FB15k-237 [205], FB60k-NYT10 [206].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple Freebase-derived datasets, which are commonly used in knowledge graph research. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1609/aaai.v32i1.11535",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3a5830800f7b471dafee2a1e6e070f45d9b3f7c7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB13",
          "dataset_description": "Used to evaluate knowledge graph completion methods, focusing on a larger subset of Freebase entities. | Used to evaluate knowledge graph completion methods, focusing on a very large subset of Freebase entities. | Used for knowledge graph embedding, focusing on large-scale entity and relation predictions in a specific domain. | Used to evaluate knowledge graph completion methods, focusing on a moderate subset of Freebase entities. | Used to evaluate knowledge graph completion methods, focusing on a smaller subset of Freebase entities. | Used for knowledge graph completion, focusing on entity linking and relation prediction in a larger subset of DBpedia. | Used to evaluate knowledge graph completion methods, combining Freebase entities with New York Times articles. | Used for knowledge graph completion, focusing on entity linking and relation prediction in a smaller subset of DBpedia. | Used to evaluate knowledge graph completion methods, focusing on a filtered subset of Freebase entities to reduce redundancy.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 19139252,
          "context_text": "According to the entity set size, we can derive several subsets from it, including FB13 [213], FB122 [214], FB15k [215], FB20k [210], FB24k [216], FB5M [19], FB15k-237 [217], FB60k-NYT10 [218].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple subsets derived from a larger entity set, all of which are specific datasets used in knowledge graph research.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1609/aaai.v32i1.11535",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3a5830800f7b471dafee2a1e6e070f45d9b3f7c7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "19187663",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "DBpedia50"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBpedia50",
          "dataset_description": "Used to assess the scalability and performance of knowledge graph embedding models on medium-sized entity sets. | Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets for efficient testing and validation. | Used to test the robustness and generalization capabilities of knowledge graph reasoning algorithms on larger datasets.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 19187663,
          "context_text": "According to the entity set size, we can derive several subsets from it, i.e., DBpedia50 [198], DBpedia500 [198] and DB100K [199].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific subsets derived from a larger entity set, which are likely datasets used in knowledge graph research.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/P18-1011",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/322aa32b2a409d2e135dbb14736d9aeb497f1c52",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DBpedia50",
          "dataset_description": "Used for knowledge graph completion, focusing on entity linking and relation prediction in a smaller subset of DBpedia. | Used for knowledge graph completion, focusing on entity linking and relation prediction in a larger subset of DBpedia. | Used for knowledge graph embedding, focusing on large-scale entity and relation predictions in a specific domain.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 19187663,
          "context_text": ", DBpedia50 [210], DBpedia500 [210] and DB100K [211].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for knowledge graph completion and embedding, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.18653/v1/P18-1011",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/322aa32b2a409d2e135dbb14736d9aeb497f1c52",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "221193809",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB15K-DB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15K-DB15K",
          "dataset_description": "Used to evaluate entity alignment methods with 20% alignment seeds, focusing on multi-modal knowledge graph reasoning. | Used to evaluate entity alignment methods in multi-modal knowledge graphs, specifically comparing MSNEA against PoE, MMEA, and EVA using Hits@1 metric. | Used for multi-modal entity alignment, focusing on aligning entities across different modalities in knowledge graphs.",
          "citing_paper_id": "251518434",
          "cited_paper_id": 221193809,
          "context_text": "• MMEA [5] first generates the entity representations of relational, visual, and numerical knowledge, and then migrates\n3https://pytorch.org/\nTable 3: The performance of entity alignment methods with 20% alignment seeds on FB15K-DB15K and FB15K-YG15K.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating entity alignment methods in multi-modal knowledge graphs.",
          "citing_paper_doi": "10.1145/3534678.3539244",
          "cited_paper_doi": "10.1007/978-3-030-55130-8_12",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1c29ae78cbaf67f8c3a9132ba97ebc771a176cd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ca3072dad2ee809c8fc3639e6fc1728b46f9ef66",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "FB15K-DB15K",
          "dataset_description": "Used in an ablation study for entity alignment, focusing on the impact of multi-modal knowledge on performance metrics such as Hits@1, Hits@10, and MRR.",
          "citing_paper_id": "251518434",
          "cited_paper_id": 232075612,
          "context_text": "This further implies the importance of exploiting multi-modal knowledge for entity 4https://github.com/nju-websoft/OpenEA 5https://github.com/nju-websoft/HyperKA 6https://github.com/zhurboo/RAGA 7https://github.com/DexterZeng/RAC 8https://github.com/cambridgeltl/eva\nHits@1\nMSNEA w/o R w/o A w/o V w/o VR w/o VA\n15.0 30.0 45.0 60.0 75.0\nHits@1 30.0 45.0 60.0 75.0 90.0\nHits@10 0.2 0.4 0.6 0.8\nMRR\n10.0 20.0 30.0 40.0 50.0\nHits@1 0.2 0.3 0.4 0.5 0.6 MRR 15.0 30.0 45.0 60.0 75.0 Hits@10\nFB15K-DB15K\nFB15K-YG15K\nFigure 2: Ablation study on two real-world datasets.\nalignment task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FB15K-DB15K' and 'FB15K-YG15K' as datasets used in an ablation study for entity alignment. These are specific, verifiable datasets.",
          "citing_paper_doi": "10.1145/3534678.3539244",
          "cited_paper_doi": "10.1007/978-3-030-75762-5_40",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1c29ae78cbaf67f8c3a9132ba97ebc771a176cd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/910795f54f970d327cbcd9eb943961e00800b08c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "3608725",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "VRD"
      ],
      "dataset_details": [
        {
          "dataset_name": "VRD",
          "dataset_description": "Employed for human-object interaction detection, providing annotations of interactions to enrich the visual commonsense knowledge base. | Used to provide image triplets for detecting human-object interactions, supporting multi-modal reasoning in complex scenes. | Used for visual relationship detection, contributing to the assembly of visual commonsense knowledge through structured relationships between objects. | Used for human-object interaction detection, enriching the visual commonsense knowledge with interactions between humans and objects. | Used for visual relationship detection, contributing to the assembly of visual commonsense knowledge through weakly-supervised learning. | Used for visual knowledge extraction, integrating diverse visual elements to build comprehensive visual commonsense knowledge. | Used to provide image triplets for visual relation detection, enhancing multi-modal reasoning in human-object interactions. | Applied for visual knowledge extraction, capturing a wide range of visual concepts and their relationships to build comprehensive visual commonsense knowledge. | Used to provide image triplets for visual relation detection, focusing on weakly-supervised learning of complex visual relations. | Used to provide triplets without images, focusing on knowledge extraction and representation in a multi-modal context. | Used to consolidate visual relation detection vocabularies, focusing on human-object interactions and weakly-supervised learning. | Used for visual relationship detection, providing examples of unusual or unexpected relationships to enhance visual commonsense understanding. | Utilized for visual relationship detection, focusing on unusual or unexpected relationships to enhance the diversity of visual commonsense knowledge.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 215806250,
          "context_text": "We assemble visual commonsense knowledge using four benchmark datasets from three different computer vision tasks: VRD (Lu et al., 2016) and UnRel (Peyre et al., 2017) for visual relationship detection, HICO-DET (Chao et al., 2018) for human-object interaction detection, and VisKE (Sadeghi et al.,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific datasets used for assembling visual commonsense knowledge across different computer vision tasks. Each dataset is clearly identified and relevant to the research topic.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": "10.1109/ICCV.2017.554",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ab64c8da40e5279c243cf18f06498cb2bfe0f7e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "VRD",
          "dataset_description": "Employed for human-object interaction detection, providing annotations of interactions to enrich the visual commonsense knowledge base. | Used for visual knowledge extraction, testing models' capability to extract and reason about visual knowledge. | Used to provide image triplets for detecting human-object interactions, supporting multi-modal reasoning in complex scenes. | Used for visual relationship detection, contributing to the assembly of visual commonsense knowledge through structured relationships between objects. | Applied for visual knowledge extraction, capturing a wide range of visual concepts and their relationships to build comprehensive visual commonsense knowledge. | Used for visual relationship detection, focusing on unusual or unexpected relationships between objects. | Used to provide image triplets for visual relation detection, enhancing multi-modal reasoning in human-object interactions. | Used for visual relationship detection, evaluating models' ability to identify relationships between objects in images. | Used to provide image triplets for visual relation detection, focusing on weakly-supervised learning of complex visual relations. | Used to provide triplets without images, focusing on knowledge extraction and representation in a multi-modal context. | Used to consolidate visual relation detection vocabularies, focusing on human-object interactions and weakly-supervised learning. | Utilized for visual relationship detection, focusing on unusual or unexpected relationships to enhance the diversity of visual commonsense knowledge.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 3608725,
          "context_text": "Among the four datasets, VRD, UnRel, and HICO-DET provide the triplets with images, whereas VisKE only provides triplets but not images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions four datasets by name, all of which are used to provide triplets, with three also providing images. These datasets are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": "10.1109/WACV.2018.00048",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/60542b1a857024c79db8b5b03db6e79f74ec8f9f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "204960716",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CM3KG"
      ],
      "dataset_details": [
        {
          "dataset_name": "CM3KG",
          "dataset_description": "Used as an open-sourced multi-modal knowledge graph to support research in multi-modal reasoning, providing a structured resource for integrating and analyzing diverse data types.",
          "citing_paper_id": "257697222",
          "cited_paper_id": 204960716,
          "context_text": "(Liu et al., 2020b) 3.22 3.12 3.17 HERD-Entity (Liu et al., 2020b) 3.83 3.77 3.74 BertGPT-Entity (Lewis et al., 2019) 3.71 3.78 3.82 CPM2-prompt (Zhang et al., 2021b) 4 CM3KG 10 is open-sourced multi-modal knowledge graphs.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CM3KG' as an open-sourced multi-modal knowledge graph, which fits the criteria for a dataset. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.18653/v1/2022.emnlp-demos.15",
          "cited_paper_doi": "10.18653/v1/2020.acl-main.703",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6a9d2d4e9c8b95a1f38d7561dbfd6af9586554a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "CM3KG",
          "dataset_description": "Used as an open-sourced multi-modal knowledge graph to support research in multi-modal reasoning, providing a structured resource for integrating and analyzing diverse data types.",
          "citing_paper_id": "257697222",
          "cited_paper_id": null,
          "context_text": "(Liu et al., 2020b) 3.22 3.12 3.17 HERD-Entity (Liu et al., 2020b) 3.83 3.77 3.74 BertGPT-Entity (Lewis et al., 2019) 3.71 3.78 3.82 CPM2-prompt (Zhang et al., 2021b) 4 CM3KG 10 is open-sourced multi-modal knowledge graphs.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CM3KG' as an open-sourced multi-modal knowledge graph, which fits the criteria for a dataset. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.18653/v1/2022.emnlp-demos.15",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6a9d2d4e9c8b95a1f38d7561dbfd6af9586554a9",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "8423494",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "NELL"
      ],
      "dataset_details": [
        {
          "dataset_name": "NELL",
          "dataset_description": "Used as a knowledge base for multi-modal reasoning, focusing on integrating textual and structured data to enhance knowledge graph construction.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 8423494,
          "context_text": "• NELL [229] is the knowledge base built based on NeverEnding Language Learner, which attempts to learn to read the web over time.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "NELL is mentioned as a knowledge base, but it is not a dataset in the traditional sense. It is a continuously evolving knowledge base built by a machine learning system.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1609/aaai.v24i1.7519",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ddf0f2226cc837750eb1eb57c43d8192ef0fc2b3",
          "citing_paper_year": 2022,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "NELL",
          "dataset_description": "Used to construct a knowledge graph from web data using semi-supervised methods, focusing on continuous learning and entity relationship extraction. | Used as a knowledge base for never-ending learning, continuously extracting and integrating new knowledge.",
          "citing_paper_id": "201871273",
          "cited_paper_id": 3201232,
          "context_text": "NELL [15] and WebChild [16] are constructed in a semi-supervised manner from web data.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions NELL and WebChild as constructed in a semi-supervised manner from web data, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TMM.2019.2937181",
          "cited_paper_doi": "10.1145/3191513",
          "citing_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0ba86604228b555475496e200f31878df3aabd6e",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "51880850",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Twitter15"
      ],
      "dataset_details": [
        {
          "dataset_name": "Twitter15",
          "dataset_description": "Used for multimodal named entity recognition, focusing on integrating visual and textual information to improve tagging accuracy.",
          "citing_paper_id": "239011558",
          "cited_paper_id": 51880850,
          "context_text": "The original corpus is built on three sources: two available multimodal named entity recognition datasets - Twitter15[15] and Twitter17[32], and crawling data from Twitter 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Twitter15 and Twitter17, which are used for multimodal named entity recognition. The third source, 'crawling data from Twitter', is too generic and lacks a specific identifier.",
          "citing_paper_doi": "10.1145/3474085.3476968",
          "cited_paper_doi": "10.18653/v1/P18-1185",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8852329d4108b24d160199a2fbe92e55999d74bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f81602b157899a785f49ca58ab99494a06c84bb9",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Twitter15",
          "dataset_description": "Used to gather a candidate set of 8357 instances for multi-modal social media analysis, focusing on visual attention models for name tagging. | Used to build a corpus for multimodal named entity recognition, focusing on visual attention models for tagging names in social media posts. | Used to gather a candidate set of 4819 instances for multi-modal social media analysis, focusing on visual attention models for name tagging.",
          "citing_paper_id": "236273668",
          "cited_paper_id": 51880850,
          "context_text": "We build the original corpus from three sources: two available multimodal named entity recognition datasets - Twitter15[9] and Twitter17[17], and crawling data from Twitter 1 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Twitter15 and Twitter17, which are used to build a corpus for multimodal named entity recognition. The third source, 'crawling data from Twitter', is too generic and lacks a specific identifier.",
          "citing_paper_doi": "10.1109/ICME51207.2021.9428274",
          "cited_paper_doi": "10.18653/v1/P18-1185",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b2741de002e915bbbcfb16ded1c59576b3735c5c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f81602b157899a785f49ca58ab99494a06c84bb9",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "7958862",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "WN18-rules"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN18-rules",
          "dataset_description": "Used to compare KB LRN with KB embedding methods and rule incorporation techniques, focusing on knowledge graph completion performance.",
          "citing_paper_id": "3875633",
          "cited_paper_id": 2949428,
          "context_text": "For the data sets WN18-rules and FB122-all we compared KB LRN to KB embedding methods TransE, TransR [17], TransH [33], and ComplEx [32] as well as state of the are approaches for incorporating logical rules into the learning process.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, WN18-rules and FB122-all, which are used to compare different knowledge graph embedding methods and rule incorporation techniques.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1609/aaai.v29i1.9491",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c14347fa745a1f113fdbe8bf1c5ccfb71b5da296",
          "cited_paper_url": "https://www.semanticscholar.org/paper/994afdf0db0cb0456f4f76468380822c2f532726",
          "citing_paper_year": 2017,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "WN18-rules",
          "dataset_description": "Used to evaluate knowledge graph reasoning models, focusing on performance metrics such as MR, MRR, and Hits@N. | Used to evaluate knowledge graph embedding models, specifically comparing TransE, TransH, TransR, and KALE against ComplEx and ASR-ComplEx. | Used to create and test a set of 47 logical rules for knowledge graph reasoning, focusing on embedding methods that incorporate logical constraints. | Used to evaluate logical formulas in knowledge graph reasoning, focusing on the performance of embedding methods and rule application.",
          "citing_paper_id": "3875633",
          "cited_paper_id": 7958862,
          "context_text": "For WN18-rules and FB122-all, the results for TransE, TransH, TransR, and KALE are taken from [10], and results for Com-plEx and ASR-ComplEx are taken from [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets (WN18-rules, FB122-all) used for evaluating knowledge graph embedding models. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/D16-1019",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c14347fa745a1f113fdbe8bf1c5ccfb71b5da296",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2cefe5adb11295b830ce27176c6d84b66fb20c2c",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "248524814",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "E-KAR"
      ],
      "dataset_details": [
        {
          "dataset_name": "E-KAR",
          "dataset_description": "Used to link external entities in Wikidata, providing a large-scale dataset of image-text pairs for multi-modal learning and reasoning. | Used to acquire image data by querying with text descriptions of entities, enhancing the multimodal knowledge graph reasoning process. | Used to link external entities in the collected data, enhancing the knowledge graph with additional information and context. | Used to provide images for multi-modal knowledge graph reasoning, integrating visual data with textual and relational information. | Used to collect and annotate seed entities and relations for multi-modal knowledge graph reasoning, focusing on entity linking and relation extraction.",
          "citing_paper_id": "252683295",
          "cited_paper_id": 241033103,
          "context_text": "These data are collected and annotated from seed entities and relations in E-KAR (Chen et al., 2022a) and BATs (Gladkova et al., 2016a), with linked external entities in Wikidata and images from Laion-5B (Schuhmann et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets and resources used for collecting and annotating data for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.00312",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8c43cbe3dff3f556bf09462a7bdbbb8a292af7f9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "E-KAR",
          "dataset_description": "Used to collect and annotate seed entities and relations for multi-modal knowledge graph reasoning, focusing on entity linking and relation extraction. | Used to develop a multimodal knowledge graph dataset, focusing on multi-level fusion for knowledge graph completion, derived from seed entities and relations in E-KAR. | Used as a target knowledge base for linking analogy entities from E-KAR and SAT, aiming to improve the accuracy and coverage of multimodal knowledge graph completion. | Used to collect analogy seed entities and relations for multi-modal knowledge graph completion, focusing on high-quality and semantically specific entities. | Used to create a multimodal analogical reasoning dataset, focusing on reasoning tasks that require understanding of analogies, derived from seed entities and relations in BATs. | Used to link external entities in the collected data, enhancing the knowledge graph with additional information and context. | Used to collect analogy entities for linking to Wikidata, focusing on enhancing multimodal knowledge graph completion through entity alignment. | Used to provide images for multi-modal knowledge graph reasoning, integrating visual data with textual and relational information. | Used to create a multimodal analogical reasoning dataset, derived from seed entities and relations for enhancing multimodal knowledge graph completion. | Used to develop a multimodal knowledge graph dataset, focusing on seed entities and relations for multi-level fusion in hybrid transformers.",
          "citing_paper_id": "252683295",
          "cited_paper_id": 248524814,
          "context_text": "These data are collected and annotated from seed entities and relations in E-KAR (Chen et al., 2022a) and BATs (Gladkova et al., 2016a), with linked external entities in Wikidata and images from Laion-5B (Schuhmann et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets and resources used for collecting and annotating data for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.00312",
          "cited_paper_doi": "10.1145/3477495.3531992",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8c43cbe3dff3f556bf09462a7bdbbb8a292af7f9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bedcfb163368f2d802de3e892acb34cc5a75a22d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "51605357",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "DBP-WD"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBP-WD",
          "dataset_description": "Used to experiment with entity alignment using knowledge graph embedding, focusing on the integration and alignment of Wikidata and DBpedia entities. | Used to experiment with entity alignment using knowledge graph embedding, focusing on the integration and alignment of YAGO and DBpedia entities.",
          "citing_paper_id": "174802832",
          "cited_paper_id": 51605357,
          "context_text": "In our experiments, we reused two datasets, namely DBP-WD and DBP-YG, recently proposed in [ Sun et al. , 2018 ] .",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DBP-WD and DBP-YG, which are used in the experiments. These datasets are likely related to knowledge graphs, aligning with the research topic.",
          "citing_paper_doi": "10.24963/ijcai.2019/754",
          "cited_paper_doi": "10.24963/ijcai.2018/611",
          "citing_paper_url": "https://www.semanticscholar.org/paper/11e402c699bcb54d57da1a5fdbc57076d7255baf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d899e434a7f2eecf33a90053df84cf32842fbca9",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DBP-WD",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of Wikipedia and YAGO entities. | Used to experiment with entity alignment using knowledge graph embedding, focusing on the integration and alignment of YAGO and DBpedia entities. | Used to experiment with entity alignment using knowledge graph embedding, focusing on the integration and alignment of Wikidata and DBpedia entities. | Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of Wikipedia and Wikidata entities.",
          "citing_paper_id": "174802832",
          "cited_paper_id": null,
          "context_text": "1 Datasets In our experiments, we reused two datasets, namely DBP-WD and DBP-YG, recently proposed in [Sun et al., 2018].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DBP-WD and DBP-YG, which are used in the experiments. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.24963/ijcai.2019/754",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/11e402c699bcb54d57da1a5fdbc57076d7255baf",
          "cited_paper_url": null,
          "citing_paper_year": 2019,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "15433626",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Visual Phrase"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual Phrase",
          "dataset_description": "Used to study object bounding box relationship triplets in images, focusing on visual and textual data integration. | Used to connect language and vision through dense image annotations, focusing on a subset of the original Visual Genome dataset for specific research needs. | Used to train and evaluate models on a large-scale dataset of annotated images, focusing on diverse object categories and relationships. | Our custom dataset used to evaluate multi-modal reasoning, focusing on visual and textual data integration. | Used to evaluate visual relationship detection, focusing on object interactions and relationships in images. | Used to connect language and vision using crowdsourced dense image annotations, focusing on complex scene understanding. | Used to provide large-scale relationship annotations for visual relationship tasks, enhancing the training and evaluation of multi-modal knowledge graph reasoning models. | Used to analyze scene graphs in images, providing dense annotations for connecting language and vision. | Used to provide large-scale and dense relationship annotations for visual relationship tasks, enhancing multi-modal knowledge graph reasoning with 2.3 million relationships and 21 relationships per image. | Used as the foundation for constructing the VrR-VG dataset, focusing on visually-relevant relationships in images to enhance multi-modal reasoning. | Used to study a subset of Visual Genome with 150 object categories, focusing on fine-grained object and relationship annotations. | Used to connect language and vision through crowdsourced dense image annotations, focusing on relation triplets and diverse object categories for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 4492210,
          "context_text": "Dataset object bbox relationship triplet image Visual Phrase [26] 8 3,271 9 1,796 2,769 Scene Graph [14] 266 69,009 68 109,535 5,000 VRD [19] 100 70 37993 5,000 Open Images [34] 57 3,290,070 10 374,768 Visual Genome [16] 33,877 3,843,636 40,480 2,347,187 108,077 VG150 [33] 150 738,945 50 413,269 87,670 VrR-VG (ours) 1,600 282,460 117 203,375 58,983 Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used for multi-modal knowledge graph reasoning, specifically focusing on visual and textual data. Each dataset is described with specific statistics, indicating their use in the research.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Phrase",
          "dataset_description": "Used to study object bounding box relationship triplets in images, focusing on visual and textual data integration. | Used to train and evaluate models on a large-scale dataset of annotated images, focusing on diverse object categories and relationships. | Our custom dataset used to evaluate multi-modal reasoning, focusing on visual and textual data integration. | Employed to measure VD-Net's effectiveness in relation predicate prediction, considering instances' locations and categories. | Used to evaluate visual relationship detection, focusing on object interactions and relationships in images. | Used to connect language and vision using crowdsourced dense image annotations, focusing on complex scene understanding. | Used to provide large-scale relationship annotations for visual relationship tasks, enhancing the training and evaluation of multi-modal knowledge graph reasoning models. | Utilized to test VD-Net's accuracy in relation predicate prediction, based on instances' locations and categories. | Used to analyze scene graphs in images, providing dense annotations for connecting language and vision. | Used for relation phrase recognition and detection, containing 8 object categories and 17 relation phrases with 9 different relationships. | Used to assess VD-Net's performance in predicting relation predicates, emphasizing instances' locations and categories. | Used to study a subset of Visual Genome with 150 object categories, focusing on fine-grained object and relationship annotations. | Used to evaluate VD-Net's accuracy in relation predicate prediction, focusing on instances' locations and categories.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 15433626,
          "context_text": "Dataset object bbox relationship triplet image Visual Phrase [26] 8 3,271 9 1,796 2,769 Scene Graph [14] 266 69,009 68 109,535 5,000 VRD [19] 100 70 37993 5,000 Open Images [34] 57 3,290,070 10 374,768 Visual Genome [16] 33,877 3,843,636 40,480 2,347,187 108,077 VG150 [33] 150 738,945 50 413,269 87,670 VrR-VG (ours) 1,600 282,460 117 203,375 58,983 Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used for multi-modal knowledge graph reasoning, specifically focusing on visual and textual data. Each dataset is described with specific statistics, indicating their use in the research.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1109/CVPR.2011.5995711",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec97294c1e5974c6b827f8fda67f2e96cf1d8339",
          "citing_paper_year": 2019,
          "cited_paper_year": 2011
        }
      ]
    },
    {
      "cited_paper_id": "16273722",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PropBank"
      ],
      "dataset_details": [
        {
          "dataset_name": "PropBank",
          "dataset_description": "Used to extend the size of the event knowledge graph by providing a large annotated corpus of semantic roles, enhancing the representation of verb-based events. | Used to extend the size of the event knowledge graph by providing an annotated corpus of nominal predicate argument structures, enhancing the representation of noun-based events.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 2486369,
          "context_text": "PropBank[31] and NomBank[27] further extend the size of event KG, with a number of 112,917",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions PropBank and NomBank, which are annotated corpora used for semantic role labeling and nominal predicate argument structure annotation, respectively. These are specific, verifiable resources.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": "10.1162/0891201053630264",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99d2dcdcf4cf05facaa101a48c7e31d140b4736d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2005
        },
        {
          "dataset_name": "PropBank",
          "dataset_description": "Used to extend the size of the event knowledge graph by providing a large annotated corpus of semantic roles, enhancing the representation of verb-based events. | Used to extend the size of the event knowledge graph by providing an annotated corpus of nominal predicate argument structures, enhancing the representation of noun-based events. | Used to extend the size of an event knowledge graph, contributing 114,576 events to enhance the graph's coverage and depth.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 16273722,
          "context_text": "PropBank[31] and NomBank[27] further extend the size of event KG, with a number of 112,917",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions PropBank and NomBank, which are annotated corpora used for semantic role labeling and nominal predicate argument structure annotation, respectively. These are specific, verifiable resources.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "219521208",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "GDELT"
      ],
      "dataset_details": [
        {
          "dataset_name": "GDELT",
          "dataset_description": "Used as a typical Temporal Knowledge Graph (TKG) to study relation codes following the CAMEO taxonomy, focusing on international crisis events and temporal dynamics. | Used as a typical Temporal Knowledge Graph (TKG) to study relation codes following the CAMEO taxonomy, focusing on event data and temporal dynamics. | Used as a temporal knowledge graph to describe events with actors, actions, and timestamps, following the CAMEO taxonomy for relation coding.",
          "citing_paper_id": "233486383",
          "cited_paper_id": null,
          "context_text": "GDELT [10] and ICEWS [11] are two typical TKGs describing who did what to whom, when, and their relation codes, and both follow the CAMEO taxonomy [12], which contains over 300 different types of coded relations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions GDELT and ICEWS as typical Temporal Knowledge Graphs (TKGs) and describes their content and structure, indicating they are used as datasets.",
          "citing_paper_doi": "10.1145/3443687",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6b97e2346cae752c809d089b99daa01d7bc1d22f",
          "cited_paper_url": null,
          "citing_paper_year": 2021,
          "cited_paper_year": null
        },
        {
          "dataset_name": "GDELT",
          "dataset_description": "Used to assess the efficiency of the Graph Hawkes Neural Network on a dataset of international crisis events, emphasizing temporal dynamics and forecasting accuracy. | Used to test the runtime efficiency of the Graph Hawkes Neural Network on a Wikipedia-based knowledge graph, evaluating its performance on a large-scale, diverse dataset. | Used to measure the computational efficiency of the Graph Hawkes Neural Network on a large semantic knowledge base, focusing on the scalability and accuracy of temporal predictions. | Used to evaluate the runtime performance of the Graph Hawkes Neural Network on a global event tracking dataset, focusing on temporal knowledge graph forecasting.",
          "citing_paper_id": "246828738",
          "cited_paper_id": 219521208,
          "context_text": "GHNN [14] is 10(1) 10(2) 10(3) 10(4) 10(5) Runtime (seconds) GDELT ICEWS18 WIKI YAGO",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets (GDELT, ICEWS18, WIKI, YAGO) in the context of runtime performance for a graph neural network model. These datasets are likely used for evaluation.",
          "citing_paper_doi": "10.1145/3488560.3498451",
          "cited_paper_doi": "10.24432/C50018",
          "citing_paper_url": "https://www.semanticscholar.org/paper/96dd81bda02205264fdd527bd8a150987220dbb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d412073051c81a39579a21799b8e639e4c38630e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "235306387",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "DRKGMM"
      ],
      "dataset_details": [
        {
          "dataset_name": "DRKGMM",
          "dataset_description": "Used to evaluate the performance of a-RotatE on the MR metric, focusing on multi-modal knowledge graph reasoning and comparing it with other models. | Used to evaluate the performance of DualE and PairRE models, focusing on multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "261329076",
          "cited_paper_id": 226281660,
          "context_text": "DualE and PairRE show inconsistent performance on DRKGMM and OMAHA-MM.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two datasets, DRKGMM and OMAHA-MM, which are used to evaluate the performance of DualE and PairRE models. These datasets are specific and relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.10231041",
          "cited_paper_doi": "10.18653/v1/2021.acl-long.336",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4cafc4dbd35d3a474609afc8834b9e273e41fac4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a8a8504722d9a39f17bbaa2968a89acc5cd0c48",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DRKGMM",
          "dataset_description": "Used to evaluate the performance of DualE and PairRE models, focusing on multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "261329076",
          "cited_paper_id": 235306387,
          "context_text": "DualE and PairRE show inconsistent performance on DRKGMM and OMAHA-MM.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two datasets, DRKGMM and OMAHA-MM, which are used to evaluate the performance of DualE and PairRE models. These datasets are specific and relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.10231041",
          "cited_paper_doi": "10.1609/aaai.v35i8.16850",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4cafc4dbd35d3a474609afc8834b9e273e41fac4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d947d696b55a3cca1010a4b61b561efc496fae4b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "3104920",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Flickr8K"
      ],
      "dataset_details": [
        {
          "dataset_name": "Flickr8K",
          "dataset_description": "Used to replicate experimental procedures for image description, focusing on ranking tasks and evaluation metrics in multi-modal learning.",
          "citing_paper_id": "7732372",
          "cited_paper_id": 928608,
          "context_text": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Flickr8K and Flickr30K, which are used for performing the same experimental procedure as in the cited papers. These datasets are relevant to multi-modal learning, particularly in the context of image description and visual denotation.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1613/jair.3994",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9814df8bd00ba999c4d1e305a7e9bca579dc7c75",
          "citing_paper_year": 2014,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "Flickr8K",
          "dataset_description": "Used to replicate experimental procedures for image description, focusing on ranking tasks and evaluation metrics in multi-modal learning.",
          "citing_paper_id": "7732372",
          "cited_paper_id": 3104920,
          "context_text": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Flickr8K and Flickr30K, which are used for performing the same experimental procedure as in the cited papers. These datasets are relevant to multi-modal learning, particularly in the context of image description and visual denotation.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1162/tacl_a_00166",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31",
          "citing_paper_year": 2014,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "202572622",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PubMedQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "PubMedQA",
          "dataset_description": "Used to evaluate D RAGON on biomedical research question answering, assessing performance on complex queries.",
          "citing_paper_id": "252968266",
          "cited_paper_id": 202572622,
          "context_text": "We ﬁnetune and evaluate D RAGON on three popular biomedical NLP and reasoning benchmarks: MedQA-USMLE ( MedQA ) [76], PubMedQA [77], and BioASQ [78].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions three benchmarks but does not specify them as downloadable datasets. However, 'MedQA-USMLE' and 'PubMedQA' are named and seem to be specific datasets used for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": "10.18653/v1/D19-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0c3c4c88c7b07596221ac640c7b7102686e3eae3",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "PubMedQA",
          "dataset_description": "Applied to assess DRAGON's performance on biomedical question answering, specifically using questions derived from PubMed abstracts. | Utilized to test DRAGON's capabilities in biomedical question answering, emphasizing complex queries and information retrieval.",
          "citing_paper_id": "252968266",
          "cited_paper_id": 221970190,
          "context_text": "We finetune and evaluate DRAGON on three popular biomedical NLP and reasoning benchmarks: MedQA-USMLE (MedQA) [76], PubMedQA [77], and BioASQ [78].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific benchmarks used for evaluating the DRAGON model. These benchmarks are clearly identified and are relevant to biomedical NLP and reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": "10.20944/PREPRINTS202105.0498.V1",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "436023",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MNRE"
      ],
      "dataset_details": [
        {
          "dataset_name": "MNRE",
          "dataset_description": "Used for multi-instance relation extraction, providing a dataset where each instance is a bag of sentences that collectively express a relation.",
          "citing_paper_id": "236273668",
          "cited_paper_id": 436023,
          "context_text": "SemEval-2010 Task 8 [11] - 205k 10,717 21,434 9 8,853 8,383 ACE 2003-2004 [12] - 297k 12,783 46,108 24 16,771 16,536 TACRED [5] - 1,823k 53,791 152,527 41 21,773 5,976 FewRel [6] - 1,397k 56,109 72,124 100 70,000 55,803 MNRE 10,089 172k 14,796 20,178 31 10,089 9,933",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context lists several datasets with their statistics, which are likely used for relation classification tasks. The cited papers confirm these are datasets.",
          "citing_paper_doi": "10.1109/ICME51207.2021.9428274",
          "cited_paper_doi": "10.3115/1621969.1621986",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b2741de002e915bbbcfb16ded1c59576b3735c5c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8ea8da551ef6b1c909ca5b37ba94be4cae02e9ac",
          "citing_paper_year": 2021,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "MNRE",
          "dataset_description": "Used for multi-instance relation extraction, providing a dataset where each instance is a bag of sentences that collectively express a relation.",
          "citing_paper_id": "236273668",
          "cited_paper_id": 3782112,
          "context_text": "SemEval-2010 Task 8 [11] - 205k 10,717 21,434 9 8,853 8,383 ACE 2003-2004 [12] - 297k 12,783 46,108 24 16,771 16,536 TACRED [5] - 1,823k 53,791 152,527 41 21,773 5,976 FewRel [6] - 1,397k 56,109 72,124 100 70,000 55,803 MNRE 10,089 172k 14,796 20,178 31 10,089 9,933",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context lists several datasets with their statistics, which are likely used for relation classification tasks. The cited papers confirm these are datasets.",
          "citing_paper_doi": "10.1109/ICME51207.2021.9428274",
          "cited_paper_doi": "10.18653/v1/D17-1004",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b2741de002e915bbbcfb16ded1c59576b3735c5c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/400e746bc8027c4b5f915cae6123cd1775484b4d",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "229280467",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "HacRED"
      ],
      "dataset_details": [
        {
          "dataset_name": "HacRED",
          "dataset_description": "Used for empirical evaluation in relation extraction, focusing on the performance of models in extracting relations from text. | Used for empirical evaluation in relation extraction, focusing on the performance of models in extracting biomedical relations from text. | Applied for large-scale biomedical relation extraction, using distant supervision to gather a comprehensive dataset for model training and evaluation. | Used for relation extraction, specifically employing distant supervision to collect data for training and evaluation.",
          "citing_paper_id": "268042282",
          "cited_paper_id": 229280467,
          "context_text": "We collected three benchmark KGs with corresponding text corpora from the field of relation extraction for empirical evaluation, which are HacRED 2 [7], DocRED 3 [48] and BioRel 4 [41].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for empirical evaluation in the field of relation extraction. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1186/s12859-020-03889-5",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126",
          "cited_paper_url": "https://www.semanticscholar.org/paper/188bc24b886ba4bf1ec23814a5e91a39de65579b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "HacRED",
          "dataset_description": "Used for training with 50 epochs, likely for relation extraction tasks in a specific domain. | Used for training with 50 epochs, likely for document-level relation extraction tasks.",
          "citing_paper_id": "268042282",
          "cited_paper_id": null,
          "context_text": "It was trained on an A100 GPU with 40GB memory by Adam [18] with 50 training epochs for HacRED and DocRED, and 20 for BioRel.",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions HacRED and DocRED, which are likely datasets used for training. However, there is no explicit mention of their usage or characteristics.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9df8a4f2ccd9ea572f65783840507de3c185126",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "248524814",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB15K-237-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15K-237-IMG",
          "dataset_description": "Used as a baseline dataset for knowledge graph completion, providing a set of triplets for evaluating the performance of the proposed method. | Used to extend the scope of triplets in multimodal knowledge graph completion, integrating image data with textual information. | Used to extend the scope of triplets for multimodal knowledge graph completion, focusing on integrating image data with textual information.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 248524814,
          "context_text": "Compared to it, FB15K-237-IMG [183] changes the scope of triplets to FB15k-237 [205].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions FB15K-237-IMG and FB15k-237, which are specific datasets used in multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/3477495.3531992",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bedcfb163368f2d802de3e892acb34cc5a75a22d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "252905085",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Global Database of Events, Language, and Tone"
      ],
      "dataset_details": [
        {
          "dataset_name": "Global Database of Events, Language, and Tone",
          "dataset_description": "Used to derive a dense knowledge graph for temporal knowledge graph completion, focusing on event and tone analysis over time.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 252905085,
          "context_text": "• GDELT [130] is a dense KG derived from the Global Database of Events, Language, and Tone.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "GDELT is mentioned as a dense KG derived from a specific database, which fits the criteria for a dataset. The context indicates it is used for research involving temporal knowledge graph completion.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/3511808.3557233",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "1181640",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DB15K",
          "dataset_description": "Used as a multi-modal knowledge graph with image, text, and numerical information to explore reasoning across different data types, specifically a subset of DBpedia.",
          "citing_paper_id": "270711106",
          "cited_paper_id": 1181640,
          "context_text": "• DB15K [32] is an MMKG with image, text, and numerical information proposed by [32], which is a subset of DBpedia [26].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DB15K is identified as a multi-modal knowledge graph (MMKG) containing image, text, and numerical information, which is a subset of DBpedia. It is relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3626772.3657800",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6667a975df5b0928c0b88582af9321922b91d402",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "252280329",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KVC16K"
      ],
      "dataset_details": [
        {
          "dataset_name": "KVC16K",
          "dataset_description": "Used to evaluate knowledge graph completion models, focusing on multi-modal reasoning and performance metrics such as MRR and Hit@1.",
          "citing_paper_id": "270711106",
          "cited_paper_id": 252280329,
          "context_text": "We report the MRR and Hit@1 results on the KVC16K/DB15K datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for reporting MRR and Hit@1 results, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3626772.3657800",
          "cited_paper_doi": "10.48550/arXiv.2209.07084",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6667a975df5b0928c0b88582af9321922b91d402",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0bc258895dcd06c224d770139e872249c25374fd",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "68049510",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ActivityNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ActivityNet",
          "dataset_description": "Used for evaluating multi-modal reasoning in human action recognition, focusing on large-scale video data with diverse actions and temporal annotations.",
          "citing_paper_id": "209376177",
          "cited_paper_id": 68049510,
          "context_text": "ActivityNet [8] 648 28K 200 HACS Clips [87] 833 0.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions two datasets, ActivityNet and HACS Clips, which are both relevant to multi-modal knowledge graph reasoning, particularly in the context of human action recognition and temporal localization.",
          "citing_paper_doi": "10.1109/cvpr42600.2020.01025",
          "cited_paper_doi": "10.1109/ICCV.2019.00876",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d1242ba8fdb994b82a0575dc92f30f7b26a75707",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5aeef2c4f3eb125ec1db9c20392f95e64ef62b41",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "224722163",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MIMIC-III"
      ],
      "dataset_details": [
        {
          "dataset_name": "MIMIC-III",
          "dataset_description": "Used to convert a relational database into a graph structure for knowledge graph-based question answering with electronic health records. | Used to construct a knowledge graph from electronic health records, focusing on diagnoses, procedures, prescriptions, and their descriptions to support question answering. | Used to extract triples from electronic health records, mapping columns to entities, literals, or relations to construct a knowledge graph.",
          "citing_paper_id": "247595191",
          "cited_paper_id": 224722163,
          "context_text": "We convert MIMIC-III to a graph from a relational database by following the approach in MIMICSQL* (Park et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions converting MIMIC-III into a graph, which is a specific dataset used in the research. The citation intent is to describe a reusable resource, and the resource type is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2203.09994",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/903e43969669a84f1c2e775408d17f4e49d12e80",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ccdbe266365ef7e80c15bbf05fafc939a7c8218",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "18217052",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VIST"
      ],
      "dataset_details": [
        {
          "dataset_name": "VIST",
          "dataset_description": "Used to assess the performance of the proposed method in multi-modal reasoning across diverse movie description tasks. | Used to evaluate the proposed method against state-of-the-art approaches, focusing on multi-modal reasoning in visual storytelling tasks. | Used to evaluate KAGS against four competing methods in visual storytelling, demonstrating superior performance across all metrics compared to the runner-up method TAPM. | Introduced to the visual storytelling task to test the generality of the proposed method, focusing on multi-modal reasoning with video and text data.",
          "citing_paper_id": "247362647",
          "cited_paper_id": 18217052,
          "context_text": "KAGS are better than the other four competing methods on TABLE 1 Comparison of the Proposed Method With Other State-of-the-Art Approaches on the VIST [31] and LSMDC [32] Datasets, Where the Bold Font Indicates the Best Performance",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, VIST and LSMDC, which are used to compare the performance of KAGS against other methods.",
          "citing_paper_doi": "10.1109/TPAMI.2022.3230934",
          "cited_paper_doi": "10.1007/s11263-016-0987-1",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7505f21e03ad0339f075ebbdaa3bd1e4d040c0d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "5583509",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Pascal1K"
      ],
      "dataset_details": [
        {
          "dataset_name": "Pascal1K",
          "dataset_description": "Used for image-sentence retrieval tasks, focusing on improving state-of-the-art performance in multi-modal learning. | Used to evaluate image-sentence retrieval performance, focusing on cross-modal matching between images and textual descriptions.",
          "citing_paper_id": "2315434",
          "cited_paper_id": 5583509,
          "context_text": "In particular, we report dramatic improvements over state of the art methods on image-sentence retrieval tasks on Pascal1K [2], Flickr8K [3] and Flickr30K [4] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for image-sentence retrieval tasks, which are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7f1b111f0bb703b0bd97aba505728a9b0d9b2a54",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146",
          "citing_paper_year": 2014,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "51876975",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CC3M&12M"
      ],
      "dataset_details": [
        {
          "dataset_name": "CC3M&12M",
          "dataset_description": "Used to train and evaluate multi-modal reasoning systems, focusing on image-caption pairs to enhance automatic image captioning.",
          "citing_paper_id": "248779998",
          "cited_paper_id": 51876975,
          "context_text": "To cover a variety of events, we apply our extraction system into multiple sources, including C4 News 2 , Wikipedia 3 , Bookcorpus 4 , and CC3M&12M (Sharma et al., 2018; Changpinyo et al., 2021).",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions multiple sources but does not specify their use as datasets. However, 'CC3M&12M' is a known dataset for multi-modal learning, which aligns with the research topic.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": "10.18653/v1/P18-1238",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "222310337",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MMEKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "MMEKG",
          "dataset_description": "Used to advance the field of event knowledge graphs by providing a large-scale ontology with 990 thousand concept events and 644 relation types, covering most real-world happenings.",
          "citing_paper_id": "248779998",
          "cited_paper_id": 222310337,
          "context_text": "Compared with existing event KGs (Speer et al., 2016; Zhang et al., 2020; Hwang et al., 2021), MMEKG advances this field in the following three aspects: (1) A large-scale ontology contains 990 thousand concept events and 644 relation types, which covers most types of real-world happenings.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MMEKG' as a dataset but does not provide a clear citation or reference to a specific paper. It describes the dataset's scale and content, which suggests it is a reusable resource.",
          "citing_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "cited_paper_doi": "10.1609/aaai.v35i7.16792",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f8a22859230e0ccafefc020dccc66b5a646fe0ac",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "16619709",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MovieLens-100k"
      ],
      "dataset_details": [
        {
          "dataset_name": "MovieLens-100k",
          "dataset_description": "Used to study user-movie interactions, incorporating user demographics and movie attributes for multi-modal reasoning and recommendation system evaluation.",
          "citing_paper_id": "245904709",
          "cited_paper_id": 16619709,
          "context_text": "ML100k+ MovieLens-100k is a well-known benchmark dataset about users, movies, and ratings given to these movies by the users, and contains various information that includes, amongst others, the genders and ages of users, and the release dates and titles of movies [11].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the MovieLens-100k dataset, which is a well-known benchmark dataset containing user, movie, and rating information. It is used for multi-modal reasoning involving user demographics and movie attributes.",
          "citing_paper_doi": "10.48550/arXiv.2309.01169",
          "cited_paper_doi": "10.1145/2827872",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3034935bf12190465f7dbee1c05d74abbbd7767",
          "cited_paper_url": "https://www.semanticscholar.org/paper/276ebc620a8976026bd2d03582b9ecfa3738d43c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "28930965",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AIFB"
      ],
      "dataset_details": [
        {
          "dataset_name": "AIFB",
          "dataset_description": "Used as a benchmark knowledge graph to evaluate machine learning methods on the Semantic Web, focusing on systematic evaluations and containing information about a museum collection. | Used as a benchmark knowledge graph to evaluate machine learning methods on the Semantic Web, focusing on scientific publications and people within a research group.",
          "citing_paper_id": "245904709",
          "cited_paper_id": 28930965,
          "context_text": "AIFB+ The AIFB dataset is a benchmark knowledge graph about scientiﬁc publications from a research group, and about the people working there [22].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context clearly identifies 'AIFB' as a dataset, which is a benchmark knowledge graph about scientific publications and people within a research group.",
          "citing_paper_doi": "10.48550/arXiv.2309.01169",
          "cited_paper_doi": "10.1007/978-3-319-46547-0_20",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3034935bf12190465f7dbee1c05d74abbbd7767",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4600f2b2a143b7bcdf5dae83e456649deb1908de",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "21692204",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "BioRead"
      ],
      "dataset_details": [
        {
          "dataset_name": "BioRead",
          "dataset_description": "Used to assess the multimodal and long-context capabilities of MLLMs in biomedical reading comprehension, focusing on cloze-style questions derived from PubMed papers.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 21692204,
          "context_text": "…to assess multimodal and long-context capabilities of several closed and open-sourced MLLMs. [75,30] Humanexperts 3.2K − PubMed Biomedical BioRead[58] Cloze-style 16.4M 3.4Mpapers PubMed Biomedical BioMRC[59] Cloze-style 812K 25Mabstracts Pubtator Biomedical emrQA[56] Cloze-style 455K…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions multiple datasets, including BioRead, BioMRC, and emrQA, which are all biomedical datasets used for reading comprehension tasks. These datasets are clearly identified and used in the research context.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/37cdc28d45c9e10af44d887bdc4160336c465683",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "235253782",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GeoQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "GeoQA",
          "dataset_description": "Used as a benchmark for multimodal numerical reasoning, specifically focusing on geometric question answering to evaluate MLLMs' performance.",
          "citing_paper_id": "271161780",
          "cited_paper_id": 235253782,
          "context_text": "Solving mathematical problems in a visual context has emerged as a complex reasoning task for MLLMs. Prior attempts, such as GeoQA [7], UniGeo [6], and Geometry3K [46], have exclusively focused on solving geometry-oriented questions.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions GeoQA, UniGeo, and Geometry3K as prior attempts in solving geometry-oriented questions. GeoQA is a benchmark, but the others are not clearly identified as datasets.",
          "citing_paper_doi": "10.48550/arXiv.2407.09413",
          "cited_paper_doi": "10.18653/v1/2021.findings-acl.46",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/291133a657498920451481d3bf784ebbafda8d6e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "8165330",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NEIL"
      ],
      "dataset_details": [
        {
          "dataset_name": "NEIL",
          "dataset_description": "Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on visual knowledge discovery and segmentation. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, focusing on general knowledge and entities. | Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on open information extraction from the web. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, focusing on providing a free, collaborative, multilingual, secondary database. | Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on continuous learning of knowledge from the web. | Used to construct a large-scale structured knowledge base through manual annotation and crowd-sourcing, focusing on extracting structured information from Wikipedia. | Used to construct a large-scale structured knowledge base through automatic extraction from unstructured or semi-structured data, focusing on linking Wikipedia and WordNet.",
          "citing_paper_id": "14843884",
          "cited_paper_id": 8165330,
          "context_text": "Popular large-scale structured KBs are constructed either by manual-annotation/crowd-sourcing (e.g ., DBpedia [2], Freebase [6] and Wikidata [39]), or by automatically extracting from unstructured/semistructured data (e.g ., YAGO [22, 28], OpenIE [3, 14, 15], NELL [9], NEIL [10, 11]).",
          "confidence_score": 0.6,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases and extraction systems, but does not specify their usage in a particular research context. The names are plausible and specific, but the actual use case is not clear.",
          "citing_paper_doi": "10.24963/IJCAI.2017/179",
          "cited_paper_doi": "10.1109/CVPR.2014.261",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bfb74e7d646b05105a81f4140bc9ac28ae925701",
          "citing_paper_year": 2015,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "212737039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OpenEA benchmarks"
      ],
      "dataset_details": [
        {
          "dataset_name": "OpenEA benchmarks",
          "dataset_description": "Used to evaluate multi-modal entity alignment methods, incorporating entity images obtained via Google search to enhance knowledge graph reasoning.",
          "citing_paper_id": "260334664",
          "cited_paper_id": 212737039,
          "context_text": "2 ) [19] are used, which are the multi-modal variants of the OpenEA benchmarks [33] with entity images achieved by searching the entity names through the Google search engine.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of multi-modal variants of the OpenEA benchmarks, which are specifically designed for entity alignment in knowledge graphs and include entity images.",
          "citing_paper_doi": "10.48550/arXiv.2307.16210",
          "cited_paper_doi": "10.14778/3407790.3407828",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65baa67a7cdb3b4b948d126ac5b41ca9c98b1f3b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "383200",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OxfordPets"
      ],
      "dataset_details": [
        {
          "dataset_name": "OxfordPets",
          "dataset_description": "Used to generate captions for pet images, focusing on class-specific descriptions in a multi-modal reasoning context.",
          "citing_paper_id": "262464639",
          "cited_paper_id": 383200,
          "context_text": "OxfordPets [47] 37 “a photo of a [class], a type of pet.”",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'OxfordPets' which appears to be a dataset used for generating captions for pet images. The context suggests it is used for multi-modal reasoning involving image and text.",
          "citing_paper_doi": "10.48550/arXiv.2309.13625",
          "cited_paper_doi": "10.1136/bmj.4.5674.53-c",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8fbe8c18f36f33314a3ee333cfe060ae9f790555",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db95b0785118db34b553e6b5f137ea2bee5a0639",
          "citing_paper_year": 2023,
          "cited_paper_year": 1969
        }
      ]
    },
    {
      "cited_paper_id": "13313217",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "People in Social Context (PISC)"
      ],
      "dataset_details": [
        {
          "dataset_name": "People in Social Context (PISC)",
          "dataset_description": "Used to improve person recognition by leveraging multiple cues in photo albums, enhancing multi-modal reasoning capabilities. | Utilized to enhance person recognition in social contexts, focusing on integrating visual and social information for improved accuracy.",
          "citing_paper_id": "239011786",
          "cited_paper_id": 13313217,
          "context_text": "Along this line, two large-scale datasets, namely The People in Photo Albums (PIPA) [37] and the People in Social Context (PISC) [15] are published for this task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, PIPA and PISC, which are relevant to multi-modal knowledge graph reasoning, particularly for person recognition in social contexts.",
          "citing_paper_doi": "10.1145/3474085.3475684",
          "cited_paper_doi": "10.1109/CVPR.2015.7299113",
          "citing_paper_url": "https://www.semanticscholar.org/paper/163c9313a8696142c36cbc92cc6cc382f9a1a1a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2150d110c1147155dab519150f4175830d5a9f1",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "4698877",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Mouse Genome Informatics (MGI)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Mouse Genome Informatics (MGI)",
          "dataset_description": "Used to build a dataset of mouse proteins and associated phenotypes, serving as out-of-domain entities for knowledge graph reasoning.",
          "citing_paper_id": "263671998",
          "cited_paper_id": 4698877,
          "context_text": "As PrimeKG only contains human proteins, we build a dataset of mouse proteins and the associated mouse phenotypes from the Mouse Genome Informatics (MGI) resource (Eppig et al., 2017), acting as out-of-domain entities.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions building a dataset from the Mouse Genome Informatics (MGI) resource, which is a specific, verifiable dataset. The dataset is used to provide out-of-domain entities (mouse proteins and phenotypes) for the research.",
          "citing_paper_doi": "10.48550/arXiv.2310.03320",
          "cited_paper_doi": "10.1007/978-1-4939-6427-7_3",
          "citing_paper_url": "https://www.semanticscholar.org/paper/16e0b8c878c75bb57ffb62c08ebf23b51ac10b99",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a7110ade2253e67f585c6e6083781bfb8555aa8c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "198118474",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ViSR"
      ],
      "dataset_details": [
        {
          "dataset_name": "ViSR",
          "dataset_description": "Used to conduct extensive experiments on video-based social relation recognition, focusing on multi-scale spatial-temporal reasoning. | Used to capture long-term and short-term temporal cues in videos, specifically for social relation recognition using a graph network.",
          "citing_paper_id": "239011786",
          "cited_paper_id": 198118474,
          "context_text": "[18] proposed a large-scale and high-quality Video dataset called as ViSR, and proposed a graph network to capture long-term and short-term temporal cues in the video.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions a specific dataset called ViSR, which is used for capturing long-term and short-term temporal cues in videos. This is directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3474085.3475684",
          "cited_paper_doi": "10.1109/CVPR.2019.00368",
          "citing_paper_url": "https://www.semanticscholar.org/paper/163c9313a8696142c36cbc92cc6cc382f9a1a1a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bdb5450be3a192034c989a0dbeaa24bccc4903ab",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "14124313",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FBDB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "FBDB15K",
          "dataset_description": "Used to evaluate multi-modal knowledge graph alignment methods, focusing on vision feature dimensions and model performance. | Used to evaluate the model's stability with VGG-16 vision encoder, focusing on vision feature dimension 4096 for entity alignment. | Used to evaluate the model's stability with ResNet-152 vision encoder, focusing on vision feature dimension 2048 for entity alignment.",
          "citing_paper_id": "255340818",
          "cited_paper_id": 14124313,
          "context_text": ", 2016] on DBP15K following EVA/MCLEA where the vision feature dimension dv is 2048, and set to VGG-16 [Simonyan and Zisserman, 2015] on FBDB15K/FBYG15K following MMEA [Chen et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions DBP15K and FBDB15K/FBYG15K, which are specific datasets used for multi-modal knowledge graph reasoning. VGG-16 is a model, not a dataset.",
          "citing_paper_doi": "10.1145/3581783.3611786",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2978e7e2159549b9b6be5a6d0fa456bb902aeed1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "53957733",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KBLN"
      ],
      "dataset_details": [
        {
          "dataset_name": "KBLN",
          "dataset_description": "Used to compare the authors' approach with image attributes, assessing effectiveness in integrating visual information into knowledge graphs. | Used to compare the authors' approach with numerical attributes, evaluating performance on multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "52160797",
          "cited_paper_id": 53957733,
          "context_text": "…and Niepert, 2017) (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions KBLN and IKRL as datasets used for comparison with the authors' approach, focusing on numerical attributes and images, respectively.",
          "citing_paper_doi": "10.18653/v1/D18-1359",
          "cited_paper_doi": "10.24432/C56P45",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "citing_paper_year": 2018,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "102352263",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MicrosoftCOCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MicrosoftCOCO",
          "dataset_description": "Used to compare the performance of MLGCN in multi-label image recognition, focusing on the best-performing method.",
          "citing_paper_id": "221819250",
          "cited_paper_id": 102352263,
          "context_text": "Because MLGCN [32] is the best-performing method on the MicrosoftCOCO dataset, we further follow its released code to train the model on VG-500 for comparison.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MicrosoftCOCO dataset and VG-500, both of which are specific datasets used for training and comparison in multi-label image recognition.",
          "citing_paper_doi": "10.1109/TPAMI.2020.3025814",
          "cited_paper_doi": "10.1109/CVPR.2019.00532",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6d74ebb937c463e8b6d7fa5ac6d79f9e877f79a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b1e245a304de66f6c6dcc8f5fb2254dab94de7d8",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Kinship dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Kinship dataset",
          "dataset_description": "Used to create a synthetic dataset for evaluating multi-modal knowledge graph reasoning, focusing on family relationships and kinship structures.",
          "citing_paper_id": "211003696",
          "cited_paper_id": null,
          "context_text": "• We introduce a synthetic dataset that resembles the popular Kinship dataset (Denham, 1973).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a synthetic dataset that resembles the Kinship dataset, which is a known dataset in the field of knowledge graphs and reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c77faa8577817b26c1a2e81914f19eecbe13ee1f",
          "cited_paper_url": null,
          "citing_paper_year": 2020,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "14779543",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "HMDB51"
      ],
      "dataset_details": [
        {
          "dataset_name": "HMDB51",
          "dataset_description": "Used to assess the TS-GCN method's performance on a diverse set of human actions, enhancing the evaluation of multi-modal reasoning capabilities.",
          "citing_paper_id": "69481030",
          "cited_paper_id": 14779543,
          "context_text": "In this section, we evaluate the performance of the proposed Two-Stream GCN (TS-GCN) method on three widely-used video datasets: Olympic Sports (Niebles, Chen, and Fei-Fei 2010), HMDB51 (Kuehne et al. 2011) and UCF101 (Soom-ro, Zamir, and Shah 2012).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions three specific video datasets used to evaluate the TS-GCN method. These datasets are clearly named and are relevant to the research topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1609/AAAI.V33I01.33018303",
          "cited_paper_doi": "10.1007/978-3-642-15552-9_29",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3cf367c96ea895473a26c580b4f1dfd168bd8c2c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/994a7b903b937f8b177c035db86852091fd26aa7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "4492210",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ImageNet2012"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet2012",
          "dataset_description": "Pre-trained CNN feature on ImageNet2012 used as input to calculate projection matrices, focusing on multi-modal feature integration.",
          "citing_paper_id": "27494872",
          "cited_paper_id": 4492210,
          "context_text": "Refer to the framework proposed in [25], we use CNN feature (Ca eNet pre-trained on ImageNet2012) and the average word2vec feature [15] as inputs to calculate the projection matrices.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ImageNet2012' as a pre-training dataset for the CNN feature. However, it does not specify how this dataset is used in the current research context beyond pre-training. No other datasets are mentioned.",
          "citing_paper_doi": "10.1145/3123266.3123443",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c54e00aadcdc8c4dcf556dbe4d30ff3952df94f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2017,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "15433626",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Visual Phrases"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual Phrases",
          "dataset_description": "Used to annotate visual relationships in images with bounding boxes, focusing on the recognition of complex visual scenes. | Used to detect and recognize visual relationships in images, focusing on compositional phrases and their occurrences in real-world scenes. | Used to understand complex visual scenes, including objects, attributes, and relationships, supporting multi-modal reasoning tasks. | Used to recognize visual relationships, specifically evaluating the model's performance on identifying composite object pairs and their interactions. | Used to fine-tune the image feature extractor, enhancing the model's ability to recognize complex visual scenes and relationships.",
          "citing_paper_id": "27494872",
          "cited_paper_id": 15433626,
          "context_text": "Two of the most famous visual relationship datasets are Visual Phrases and Visual Genome.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Visual Phrases and Visual Genome, which are relevant to multi-modal knowledge graph reasoning involving visual relationships.",
          "citing_paper_doi": "10.1145/3123266.3123443",
          "cited_paper_doi": "10.1109/CVPR.2011.5995711",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c54e00aadcdc8c4dcf556dbe4d30ff3952df94f5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec97294c1e5974c6b827f8fda67f2e96cf1d8339",
          "citing_paper_year": 2017,
          "cited_paper_year": 2011
        }
      ]
    },
    {
      "cited_paper_id": "207572116",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Chunyu"
      ],
      "dataset_details": [
        {
          "dataset_name": "Chunyu",
          "dataset_description": "Used to verify the high precision of KABLSTM in ranking question-answer pairs, highlighting the effectiveness of knowledge graph integration in medical applications. | Used to evaluate the performance of KABLSTM in ranking question-answer pairs, demonstrating the utility of incorporating knowledge graphs in the medical domain.",
          "citing_paper_id": "204837691",
          "cited_paper_id": 207572116,
          "context_text": "(3) KABLSTM beats SMatrix and K-NRM on Chunyu dataset and achieves high precision on Dingxiang dataset, verifying the incorporation of KG is useful in the medical domain.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two datasets, 'Chunyu' and 'Dingxiang', which are used to evaluate the performance of the KABLSTM model in the medical domain.",
          "citing_paper_doi": "10.1145/3343031.3351033",
          "cited_paper_doi": "10.1145/3209978.3210081",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80bfeda895ebc163bdb9957dacac29df2e4f0c58",
          "cited_paper_url": "https://www.semanticscholar.org/paper/acb19932620452deab36b24a24b139f17224f469",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "2768038",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MKG-W"
      ],
      "dataset_details": [
        {
          "dataset_name": "MKG-W",
          "dataset_description": "Used to evaluate the performance of MMRNS on knowledge graph embedding models (TransE, DistMult, ComplEx) compared to uniform sampling, focusing on multi-modal reasoning. | Used to evaluate the performance of KGC models (TransE and DistMult) by observing the change curve of performance when tuning hyperparameters. | Used to assess MMRNS performance on TransE, DistMult, and ComplEx, showing improvements in multi-modal knowledge graph reasoning tasks. | Used to evaluate MMRNS performance on TransE, DistMult, and ComplEx, demonstrating significant improvements over competitors in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "252783084",
          "cited_paper_id": 2768038,
          "context_text": "What’s more, MMRNS obtains a significant improvement of 6.4%, 2.0%, and 7.5% on TransE, DistMult, and ComplEx on the MKG-W dataset over its best competitors, and also obtains an improvement of 2.6%, 6.5%, and 1.6% on TransE, DistMult, and ComplEx on the MKG-Y dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, MKG-W and MKG-Y, which are used to evaluate the performance of MMRNS against other models. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3503161.3548388",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4ba9aab31d0a5e2af0147bd18e3381bdcfd15cd1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86412306b777ee35aba71d4795b02915cb8a04c3",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "53080736",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FewRel"
      ],
      "dataset_details": [
        {
          "dataset_name": "FewRel",
          "dataset_description": "Used to evaluate few-shot relation classification performance, focusing on general relation types. | Used for few-shot relation classification, specifically to construct a test set that avoids information leakage into the training data. | Used for few-shot relation classification, constructed with Wikipedia text and Wikidata facts, containing 100 relations and 70,000 instances. | Serves as a training source for multi-modal knowledge graph reasoning, with specific triplets removed to prevent information leakage from the FewRel test set. | Used to assess few-shot relation classification with a medical knowledge graph, highlighting improvements over FewRel 1.0.",
          "citing_paper_id": "208006241",
          "cited_paper_id": 53080736,
          "context_text": "Since FewRel is constructed on Wikidata, to avoid information leak of the test set, we delete all the triplets shown in the FewRel test set from our Wikidata5M training source.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FewRel' and 'Wikidata5M', both of which are datasets. FewRel is used for few-shot relation classification, and Wikidata5M is used as a training source, from which certain triplets are removed to prevent information leakage.",
          "citing_paper_doi": "10.1162/tacl_a_00360",
          "cited_paper_doi": "10.18653/v1/D18-1514",
          "citing_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "201871273",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VTKB"
      ],
      "dataset_details": [
        {
          "dataset_name": "VTKB",
          "dataset_description": "Used to construct a visio-textual knowledge base linking concepts to images and images by embedding similarities, enhancing the quality of image tagging.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 201871273,
          "context_text": "For example, [172] constructs an MMKG called VTKB containing hierarchical concepts, linking concepts of original tags to images and linking images by the similarities of embed-dings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the construction of a visio-textual knowledge base (VTKB) which is relevant to multi-modal knowledge graph reasoning. The VTKB links concepts to images and images by embedding similarities.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/TMM.2019.2937181",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "224291855",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "academic MMKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "academic MMKG",
          "dataset_description": "Used to offer retrieval on the implementation level, focusing on linking deep learning papers with their corresponding code implementations.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 224291855,
          "context_text": "[138] uses an academic MMKG about papers and codes to offer retrieval on the implementation level.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions an academic MMKG, which is a multimodal knowledge graph about papers and codes. This is directly relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/3340531.3417439",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/061c1a06d47170fa30f62018eee441a90f6967ce",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "236428934",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Wikipedia articles and images"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikipedia articles and images",
          "dataset_description": "Used to pre-train a cross-modal entity matching module, aligning textual and visual scene graphs extracted from input articles and images.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 236428934,
          "context_text": "In [18] the textual scene graph and visual scene graph extracted from the input article and images are aligned by the cross-modal entity matching module pre-trained on Wikipedia articles and images.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of Wikipedia articles and images for pre-training a cross-modal entity matching module, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/TMM.2023.3301279",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c5e6720fa4c3cd73fd915bc71dea5e78184b262",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "235271284",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "kgbench"
      ],
      "dataset_details": [
        {
          "dataset_name": "kgbench",
          "dataset_description": "Proposed as a collection of high-quality multi-modal benchmarks for evaluating node classification tasks on multi-modal knowledge graphs, focusing on relational and multimodal machine learning.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 235271284,
          "context_text": "Therefore, [140] proposes a collection of extensive and high-qualified multi-modal benchmarks for precisely evaluating node classification tasks on MMKGs.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'multi-modal benchmarks' but does not specify any particular dataset names. The title 'kgbench' suggests a collection of datasets, but it is not clear if it is a single dataset or a suite of datasets.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1007/978-3-030-77385-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dcde86c40b2480130d8d46380b3d83b1277b4c27",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "235503675",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "unearthed oracle bones’ photos"
      ],
      "dataset_details": [
        {
          "dataset_name": "unearthed oracle bones’ photos",
          "dataset_description": "Used to construct a multi-modal knowledge graph for oracle bone recognition, focusing on integrating visual and textual data to enhance information processing.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 235503675,
          "context_text": "If the multi-modal data are treated as first-class citizens in some scenarios, the multi-modal data labeling way is more preferred to construct the MMKG, such as unearthed oracle bones’ photos in oracle bones recognition system [136], teachers’ class audios in educa-",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'unearthed oracle bones’ photos' which is a specific type of data used in the construction of a multi-modal knowledge graph for oracle bone recognition.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1016/J.COMPELECENG.2021.107173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/70f304372e171c5f6ee61566d7943c46f1ed333d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "266028051",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "hasPart KB"
      ],
      "dataset_details": [
        {
          "dataset_name": "hasPart KB",
          "dataset_description": "Used to provide part-whole relationships for VQA, enhancing the system's ability to understand compositional structures in images. | Used to provide location triples of visual objects, integrating visual and textual information in the multi-modal knowledge graph. | Used as a baseline for commonsense reasoning, compared against MMKGs to evaluate tag generation and disambiguation capabilities. | Used to incorporate commonsense triples, enriching the multi-modal knowledge graph with general world knowledge. | Mentioned as a source of common sense knowledge, but not explicitly used in the research context. | Used to provide categorical knowledge for Visual Question Answering (VQA), enhancing the system's ability to understand and answer questions about images. | Mentioned as a source of lexical knowledge, but not explicitly used in the research context. | Used to provide has-Part triples, contributing to the construction of explicit knowledge in a multi-modal knowledge graph. | Mentioned as a source of encyclopedia knowledge, but not explicitly used in the research context. | Used as a baseline for knowledge representation, compared against MMKGs to assess performance in generating and disambiguating candidate tags. | Used as a baseline for image recognition, compared against MMKGs to evaluate the effectiveness in generating and disambiguating tags. | Used to extract hasPart/isA triples, enhancing the explicit knowledge in a multi-modal knowledge graph with structured information. | Used to provide commonsense knowledge for VQA, enhancing the system's ability to reason about everyday concepts and situations.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 266028051,
          "context_text": "For example, the explicit knowledge in [51] has four sources: has-Part triples from hasPart KB [162], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for constructing explicit knowledge in a knowledge graph, including hasPart KB, DBpedia, ConceptNet, and Visual Genome.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1023/B:BTTJ.0000047600.45421.6D",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b3fea597033a46d5ae282464a8f16d6715187e70",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "205692",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NELL-995"
      ],
      "dataset_details": [
        {
          "dataset_name": "NELL-995",
          "dataset_description": "Used to evaluate knowledge graph embedding methods, focusing on semantic smoothness and reasoning accuracy across a large set of entities and relations. | Utilized to assess the performance of knowledge graph reasoning models, specifically targeting a subset of 995 relations for detailed evaluation.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 205692,
          "context_text": "According to the different scopes, there are several subsets of it, e.g., Location [208], sports [208], NELL23k [210], NELL-995 [211].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'NELL23k' and 'NELL-995', which are specific subsets of a larger dataset. These are likely knowledge graphs used for reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.3115/v1/P15-1009",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/69418ff5d4eac106c72130e152b807004e2b979c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "3226443",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "electronic medical database"
      ],
      "dataset_details": [
        {
          "dataset_name": "electronic medical database",
          "dataset_description": "Used to construct a health knowledge graph for reasoning tasks, focusing on the integration and utilization of medical data in a structured format.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 3226443,
          "context_text": "For example, [246] and [247] both perform reasoning on the KG constructed from the electronic medical database.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions using an electronic medical database to construct a knowledge graph for reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1038/s41598-017-05778-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8b0de2074bb42a55395efd7e95503bd74736cdd7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "4437414",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Hetionet"
      ],
      "dataset_details": [
        {
          "dataset_name": "Hetionet",
          "dataset_description": "Used to integrate and prioritize biomedical knowledge for drug repurposing, leveraging a multi-modal structure derived from public resources.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 4437414,
          "context_text": "• Hetionet [207] is a knowledge graph derived from biomedical studies based on public resources.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Hetionet is mentioned as a knowledge graph derived from public resources, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.7554/eLife.26726",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1758eeac1b000571ee1eaf425abea72fc1ec6e9b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "6911541",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FAMILY"
      ],
      "dataset_details": [
        {
          "dataset_name": "FAMILY",
          "dataset_description": "Used to represent and reason about relations among family members, focusing on the structure and semantics of family relationships in a knowledge graph. | Used to represent relations among nations, specifically for reasoning about multi-modal knowledge graphs and their interconnected data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 6911541,
          "context_text": "• FAMILY [200] consists of relations among family members.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FAMILY' as a dataset consisting of relations among family members, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/1273496.1273551",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b6ce4ec0d28c050b99ec647a16e47116c939473c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "13955854",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Countries"
      ],
      "dataset_details": [
        {
          "dataset_name": "Countries",
          "dataset_description": "Used to represent and reason about geographical relations among countries, focusing on low-rank vector spaces for approximate reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 13955854,
          "context_text": "• Countries [194] consists of relations among countries based on public geographical data.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Countries' as a dataset consisting of relations among countries based on public geographical data, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03dfada96b88c741bb26bd4ce7b5ae4232157d37",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "53082197",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YOGA15k"
      ],
      "dataset_details": [
        {
          "dataset_name": "YOGA15k",
          "dataset_description": "Used to generate data for different periods, focusing on temporal aspects of knowledge graph reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 53082197,
          "context_text": "YOGA11k/YOGA [157], YOGA15k [222], YOGA-3SP [220] and YOGA1830 [116] are generated from it according to different periods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets with clear identifiers and versions, which are used to generate data according to different periods.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/D18-1225",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/83d58bc46b7adb92d8750da52313f060b10f201d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YAGO39k"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO39k",
          "dataset_description": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
          "citing_paper_id": "254564635",
          "cited_paper_id": null,
          "context_text": "According to the scopes of relations, YAGO3-10 [217], YAGO37 [218] and YAGO39k [219] are the subsets of it.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions YAGO3-10, YAGO37, and YAGO39k as subsets of a larger dataset, which are likely knowledge graphs used in multi-modal reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MMKG-FB15k-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "MMKG-FB15k-IMG",
          "dataset_description": "Used to integrate YAGO knowledge graph with images and text, facilitating comprehensive multi-modal reasoning. | Used to integrate Freebase knowledge graph with images, enhancing multi-modal reasoning capabilities. | Used to integrate DBpedia knowledge graph with numeric literals, supporting multi-modal reasoning with structured data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 76663467,
          "context_text": "• MMKG [224] provides three subsets, including MMKG-FB15k-IMG [224], MMKG-DB15k [224] and Yago15k-IMG-TXT [224], which integrates the speciﬁc KGs with numeric literals and images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used in the research, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "212737039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MKG-YAGO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MKG-YAGO",
          "dataset_description": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using Wikipedia data. | Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using YAGO data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 212737039,
          "context_text": "• MKG [181], [241] consists of two subsets, i.e., MKG-Wikipedia and MKG-YAGO .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MKG-Wikipedia' and 'MKG-YAGO' as subsets of MKG, which are specific datasets used in multimodal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.14778/3407790.3407828",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65baa67a7cdb3b4b948d126ac5b41ca9c98b1f3b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "252783084",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MKG-Wikipedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "MKG-Wikipedia",
          "dataset_description": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using Wikipedia data. | Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using YAGO data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 252783084,
          "context_text": "• MKG [181], [241] consists of two subsets, i.e., MKG-Wikipedia and MKG-YAGO .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MKG-Wikipedia' and 'MKG-YAGO' as subsets of MKG, which are specific datasets used in multimodal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/3503161.3548388",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4ba9aab31d0a5e2af0147bd18e3381bdcfd15cd1",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DBPedia50k"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBPedia50k",
          "dataset_description": "Used to evaluate the model on open-world tail prediction, focusing on the performance of the model in predicting unseen entities. | Used to assess the effect of relation information, showing higher performance gains attributed to extensive entity descriptions. | Used to evaluate the performance of ComplEx-RST and other approaches, focusing on knowledge graph reasoning tasks, particularly in Hits@10 metric. | Used to evaluate the performance of ComplEx-RCT and ComplEx-RST models, focusing on reducing the number of transformation functions while maintaining competitive results. | Used to assess the model's effectiveness in open-world tail prediction, specifically in handling large-scale knowledge graphs. | Used to test the model's capability in open-world tail prediction, emphasizing the challenge of predicting tails in an open-world setting. | Served as a comparative dataset, highlighting the lesser impact of relation information due to shorter descriptions. | Used to evaluate the impact of relation information on performance, noting significant improvements due to long entity descriptions.",
          "citing_paper_id": "227231162",
          "cited_paper_id": null,
          "context_text": "We also note that the improvement achieved by utilizing the relation information is higher in DBPedia50k and FB20k, both of which use very long descriptions compared to FB15k-237-OWE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: DBPedia50k, FB20k, and FB15k-237-OWE. These are specific, verifiable datasets used in the research.",
          "citing_paper_doi": "10.18653/v1/2020.textgraphs-1.9",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f61b704c38dd9d5c061fd79c8bc49fa2563d9ce9",
          "cited_paper_url": null,
          "citing_paper_year": 2020,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "31606602",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB15k-237-OWE"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15k-237-OWE",
          "dataset_description": "Used to create a validation set by removing 10% of test triples, addressing the lack of an open-world validation set for knowledge graph reasoning. | Used to evaluate the model on open-world tail prediction, focusing on the performance of the model in predicting unseen entities. | Used to assess the model's ability to reason over a large-scale knowledge graph, emphasizing entity and relation types. | Used to assess the effect of relation information, showing higher performance gains attributed to extensive entity descriptions. | Used to assess the model's effectiveness in open-world tail prediction, specifically in handling large-scale knowledge graphs. | Used to test the model's capability in handling complex relations and entities, particularly in the context of out-of-vocabulary entities. | Used to test the model's capability in open-world tail prediction, emphasizing the challenge of predicting tails in an open-world setting. | Used to evaluate the performance of ComplEx-RCT and ComplEx-RST models, focusing on reducing the number of transformation functions while maintaining competitive results. | Used to evaluate the model's performance on knowledge graph reasoning tasks, focusing on entity and relation prediction. | Served as a comparative dataset, highlighting the lesser impact of relation information due to shorter descriptions. | Used to evaluate the impact of relation information on performance, noting significant improvements due to long entity descriptions.",
          "citing_paper_id": "227231162",
          "cited_paper_id": 31606602,
          "context_text": "We also note that the improvement achieved by utilizing the relation information is higher in DBPedia50k and FB20k, both of which use very long descriptions compared to FB15k-237-OWE.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: DBPedia50k, FB20k, and FB15k-237-OWE. These are specific, verifiable datasets used in the research.",
          "citing_paper_doi": "10.18653/v1/2020.textgraphs-1.9",
          "cited_paper_doi": "10.1609/aaai.v30i1.10329",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f61b704c38dd9d5c061fd79c8bc49fa2563d9ce9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96acb1c882ad655c6b8459c2cd331803801446ca",
          "citing_paper_year": 2020,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "53734356",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Visual Commonsense Reasoning (VCR)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual Commonsense Reasoning (VCR)",
          "dataset_description": "Used to evaluate visual question answering performance, focusing on complex questions requiring both visual and textual understanding. | Used for image-text and text-image retrieval tasks, evaluating the model's ability to match images with corresponding captions. | Used to evaluate visual question answering and answer justification, focusing on multiple-choice problems to assess visual commonsense reasoning. | Used to test region-to-phrase grounding, focusing on identifying and localizing specific regions in images based on textual descriptions. | Used to assess visual commonsense reasoning, specifically the ability to understand and reason about social interactions and everyday scenarios.",
          "citing_paper_id": "220265934",
          "cited_paper_id": 53734356,
          "context_text": "The Visual Commonsense Reasoning (VCR) [12] task contains two sub-tasks: visual question answering (Q → A) and answer justiﬁcation (QA → R), which are both multiple choice problems.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the VCR task, which is a specific dataset used for visual commonsense reasoning, including visual question answering and answer justification.",
          "citing_paper_doi": "10.1609/aaai.v35i4.16431",
          "cited_paper_doi": "10.1109/CVPR.2019.00688",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6dfc2ff03534a4325d06c6f88c3144831996629b",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "395534",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NTU"
      ],
      "dataset_details": [
        {
          "dataset_name": "NTU",
          "dataset_description": "Used as a bimodal dataset for 3D model retrieval, focusing on visual similarity and multi-modal reasoning. | Used as a bimodal dataset, likely involving skeletal and RGB data, for multi-modal reasoning tasks. | Used as a trimodal dataset, incorporating audio, video, and text, for multi-modal emotion recognition.",
          "citing_paper_id": "221191635",
          "cited_paper_id": 395534,
          "context_text": "ModelNet40 and NTU are used as bimodal datasets, and IEMOCAP is used as trimodal dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for multi-modal reasoning, which aligns with the research topic.",
          "citing_paper_doi": "10.1145/3394486.3403182",
          "cited_paper_doi": "10.1111/1467-8659.00669",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5d24501a99d05306817171a744878315c31a880b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dc01194dca94f0cde2f98caa93f9740d7cbd1642",
          "citing_paper_year": 2020,
          "cited_paper_year": 2003
        }
      ]
    },
    {
      "cited_paper_id": "69934039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB20k"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB20k",
          "dataset_description": "Used to evaluate the model's performance on open-world tail prediction, focusing on the ability to predict unseen entities in a knowledge graph. | Used to evaluate the model on open-world tail prediction, focusing on the performance of the model in predicting unseen entities. | Used to assess the model's effectiveness in open-world tail prediction, specifically in handling large-scale knowledge graphs. | Used to assess the model's effectiveness in open-world tail prediction, specifically designed to test the model's capability to handle out-of-vocabulary entities. | Used to test the model's capability in open-world tail prediction, emphasizing the challenge of predicting tails in an open-world setting.",
          "citing_paper_id": "227231162",
          "cited_paper_id": 69934039,
          "context_text": "We evaluate our model on FB20k (Xie et al., 2016), DBPedia50k (Shi and Weninger, 2017a) and FB15k237-OWE (Shah et al., 2019) and compare our model with the state of the art on the task in open-world tail prediction.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for evaluating the model on the task of open-world tail prediction.",
          "citing_paper_doi": "10.18653/v1/2020.textgraphs-1.9",
          "cited_paper_doi": "10.1609/AAAI.V33I01.33013044",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f61b704c38dd9d5c061fd79c8bc49fa2563d9ce9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4e237df92dc6fd9a44db74e5d8e0ff2b11cd1c6a",
          "citing_paper_year": 2020,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "225039882",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VTKG-C"
      ],
      "dataset_details": [
        {
          "dataset_name": "VTKG-C",
          "dataset_description": "Used to qualitatively compare representation vectors created by BERT, ViT, and VISTA, focusing on multi-modal reasoning in a knowledge graph context.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 225039882,
          "context_text": "We qualitatively compare the representation vectors created by BERT (Devlin et al., 2019), ViT (Dosovitskiy et al., 2021), and VISTA in VTKG-C.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions VTKG-C, which appears to be a dataset or knowledge graph used for qualitative comparison of representation vectors. However, there is no explicit confirmation that VTKG-C is a dataset, and it could be a method or framework.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "232170356",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VTKG datasets"
      ],
      "dataset_details": [
        {
          "dataset_name": "VTKG datasets",
          "dataset_description": "Mentioned as reusable resources for diverse applications, including visual commonsense knowledge tasks, but specific usage details are not provided.",
          "citing_paper_id": "266166905",
          "cited_paper_id": 232170356,
          "context_text": "Our VTKG datasets and VISTA model can be utilized in diverse applications and scenarios (Sekuboyina et al., 2019; Kwak et al., 2022; Lee et al., 2023), including those requiring visual commonsense knowledge such as VQA (Marino et al., 2021) or scene graph generation (Chang et al., 2023; Zareian et…",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions VTKG datasets but does not provide specific details about their usage or characteristics. The context is too vague to confidently extract detailed usage descriptions.",
          "citing_paper_doi": "10.18653/v1/2023.findings-emnlp.488",
          "cited_paper_doi": "10.1109/ISBI48211.2021.9433786",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bf9154e5b7595eda4b379ae5ea30530152f2c2a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f2174870f1ed7e6aaffa84050144ece853ff3209",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "5583509",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Pascal Sentences dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Pascal Sentences dataset",
          "dataset_description": "Used for multi-modal reasoning, combining images with corresponding text descriptions, specifically five sentences per image, to explore the relationship between visual and textual data.",
          "citing_paper_id": "27837890",
          "cited_paper_id": 5583509,
          "context_text": "Pascal Sentences dataset [42] is selected from 2008 PASCAL development kit, which contains 1000 images with corresponding text descriptions as five sentences.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, Pascal Sentences dataset, which is part of the 2008 PASCAL development kit. It is used for multi-modal reasoning involving images and text.",
          "citing_paper_doi": "10.1109/TCYB.2018.2879846",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e5567742e53fd795cba76da8c23ca3123f9f8cee",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146",
          "citing_paper_year": 2017,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "189999659",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "entity linking corpus"
      ],
      "dataset_details": [
        {
          "dataset_name": "entity linking corpus",
          "dataset_description": "Used to evaluate pre-trained language models in a zero-shot setting, focusing on entity linking beyond factual knowledge bases.",
          "citing_paper_id": "253117165",
          "cited_paper_id": 189999659,
          "context_text": "Recently, Zeshel [16] has greatly supported the zero-shot evaluation of PLMs with an entity linking corpus beyond factual knowledge bases intensively used for pretrainig.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'entity linking corpus' which is a specific type of dataset used for evaluating pre-trained language models in a zero-shot setting.",
          "citing_paper_doi": "10.48550/arXiv.2210.14463",
          "cited_paper_doi": "10.18653/v1/p19-1335",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4259ee32a21153e62feb633ecc5d9ba48b615923",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45fe966219595e3a6d771c15f273efa171a9f53a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "8953884",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MetaQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MetaQA",
          "dataset_description": "Utilized to test the ability of models to handle complex, multi-hop questions over knowledge graphs, focusing on intricate query patterns. | Applied to assess the performance of multi-hop reasoning in knowledge graph question answering, emphasizing semantic parsing and complex queries. | Used for evaluating multi-hop question answering over knowledge graphs, focusing on complex query structures and reasoning capabilities.",
          "citing_paper_id": "258219651",
          "cited_paper_id": 8953884,
          "context_text": "we use three benchmark datasets for the multi-hop KGQA task: MetaQA [36], WebQuestionSP [18] and Complex We-bQuestions [37].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for the multi-hop KGQA task, which are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICTAI56018.2022.00074",
          "cited_paper_doi": "10.1609/aaai.v32i1.12057",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3b5c311ac38a03458614bfb411aee114214939e4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/95280565aa3d120c6d7e8d87ea3423f16977f19a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "15027084",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WN11"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN11",
          "dataset_description": "Used to evaluate the model's performance on wordnet relations, focusing on link prediction accuracy in a smaller subset of WordNet. | Used to assess the model's ability to predict links in a larger subset of WordNet, emphasizing scalability and robustness. | Detailed usage description focusing on specific research context and methodology (30-50 words) | Used to test the model's performance on Freebase relations, focusing on entity and relation prediction in a smaller subset of Freebase.",
          "citing_paper_id": "44113572",
          "cited_paper_id": 15027084,
          "context_text": "In this paper, we evaluate our model on four benchmark datasets: WN11, WN18, FB13 and FB15k (Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific datasets used for evaluating the model. These datasets are commonly used in knowledge graph reasoning tasks.",
          "citing_paper_doi": "10.18653/v1/N18-1068",
          "cited_paper_doi": "10.1609/aaai.v28i1.8870",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ce14b7a3c1b89c717eba10229d9d80d80bd0e04",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2a3f862199883ceff5e3c74126f0c80770653e05",
          "citing_paper_year": 2018,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "207167677",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Freebase 1"
      ],
      "dataset_details": [
        {
          "dataset_name": "Freebase 1",
          "dataset_description": "Mentioned as a collaboratively created graph database for structuring human knowledge, but specific usage in the research context is not detailed. | Used to evaluate the method on link prediction and triple classification tasks, focusing on lexical relationships and semantic hierarchies within the lexical database. | Used to evaluate the method on link prediction and triple classification tasks, focusing on the structure and relationships within the collaboratively created graph database.",
          "citing_paper_id": "44113572",
          "cited_paper_id": 207167677,
          "context_text": "We evaluate our method on both link prediction task and triple classiﬁcation task, using benchmark datasets from Freebase 1 and WordNet with the text corpus.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'benchmark datasets from Freebase 1 and WordNet', which are specific, verifiable datasets. These datasets are used for evaluating the method on link prediction and triple classification tasks.",
          "citing_paper_doi": "10.18653/v1/N18-1068",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ce14b7a3c1b89c717eba10229d9d80d80bd0e04",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2018,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "1017389",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Movie-QA"
      ],
      "dataset_details": [
        {
          "dataset_name": "Movie-QA",
          "dataset_description": "Used to understand stories in movies through question-answering, focusing on the integration of visual and textual information to answer questions about movie plots.",
          "citing_paper_id": "85558018",
          "cited_paper_id": 1017389,
          "context_text": "Movie-QA (Tapaswi et al. 2016) - 14,944 Movie videos 7 7",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Movie-QA' which is a specific dataset used for understanding stories in movies through question-answering. The dataset is clearly identified and used in the research context.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": "10.1109/CVPR.2016.501",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "60254729",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Nation"
      ],
      "dataset_details": [
        {
          "dataset_name": "Nation",
          "dataset_description": "Used to represent relations among nations, focusing on the structure and properties of international relationships in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 60254729,
          "context_text": "• Nation [212] contains relations among nations [221].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Nation' as a resource containing relations among nations, which appears to be a specific dataset. However, the title does not provide additional clarity to confirm it as a dataset.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.21236/ad0759998",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cc889a3ac654a1ad2226a4f3f0aca9a8c40a6983",
          "citing_paper_year": 2022,
          "cited_paper_year": 1968
        }
      ]
    },
    {
      "cited_paper_id": "8081284",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MS-Celebs"
      ],
      "dataset_details": [
        {
          "dataset_name": "MS-Celebs",
          "dataset_description": "Used to create a list of persons by combining entities from this visual question answering dataset with MS-Celebs, enhancing the entity set for multi-modal reasoning. | Used to create a list of persons by combining entities from this face recognition dataset with KVQA, enhancing the entity set for multi-modal reasoning. | Used to associate Freebase IDs with cropped faces of celebrities, enhancing multi-modal knowledge graph reasoning by linking visual data with structured world knowledge.",
          "citing_paper_id": "85558018",
          "cited_paper_id": 8081284,
          "context_text": "This list of persons is union of entities present in MS-Celebs (Guo et al. 2016) and KVQA.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MS-Celebs' and 'KVQA', both of which are specific datasets. MS-Celebs is a well-known face recognition dataset, and KVQA is a visual question answering dataset. The context indicates that these datasets are used to create a list of persons.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": "10.1007/s11263-018-1116-0",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "15458100",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CLEVR"
      ],
      "dataset_details": [
        {
          "dataset_name": "CLEVR",
          "dataset_description": "Used to evaluate visual named entity linking and Visual Question Answering, focusing on the integration of visual and textual information in multi-modal reasoning tasks. | Used to study Visual Question Answering over Knowledge Graphs, focusing on integrating visual and textual information to answer questions. | Used to evaluate compositional language and elementary visual reasoning, specifically through synthetic images and questions designed to test various reasoning capabilities. | Used to evaluate compositional language and elementary visual reasoning in Visual Question Answering, focusing on the ability to understand complex questions about images.",
          "citing_paper_id": "85558018",
          "cited_paper_id": 15458100,
          "context_text": "CLEVR (Johnson et al. 2017) 100,000 999,968 Synthetic images 7 7",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions CLEVR, which is a synthetic dataset used for visual reasoning tasks. The dataset is explicitly named and referenced in the citation.",
          "citing_paper_doi": "10.1609/aaai.v33i01.33018876",
          "cited_paper_doi": "10.1109/CVPR.2017.215",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0818dac77eee5b970736e57a478bcedfb1b15fe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03eb382e04cca8cca743f7799070869954f1402a",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "208006241",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WD-singer"
      ],
      "dataset_details": [
        {
          "dataset_name": "WD-singer",
          "dataset_description": "Mentioned as a dataset, but no specific usage or research context is provided in the citation. | Used to create a large-scale multi-modal knowledge graph dataset, focusing on knowledge embedding and pre-trained language representation. | Used to derive a subset of Wikidata for singer-related entities, focusing on multi-modal knowledge graph reasoning and entity linking.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 208006241,
          "context_text": "WD-singer [210] and wikidata5m [216] are derived from it according to different scopes.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, WD-singer and wikidata5m, which are derived from a larger dataset. These are specific, named datasets relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1162/tacl_a_00360",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "205228801",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "UMLS"
      ],
      "dataset_details": [
        {
          "dataset_name": "UMLS",
          "dataset_description": "Used to integrate drug information into a self-constructed knowledge graph for MedQA-USMLE, enhancing the representation of pharmaceutical data. | Used to integrate disease information into a self-constructed knowledge graph for MedQA-USMLE, enhancing the representation of medical concepts.",
          "citing_paper_id": "233219869",
          "cited_paper_id": 205228801,
          "context_text": "For MedQA-USMLE, we use a self-constructed knowledge graph that integrates the Disease Database portion of the Unified Medical Language System (UMLS; Bodenreider, 2004) and DrugBank (Wishart et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of the Disease Database portion of UMLS and DrugBank. Both are specific, verifiable resources used in constructing a knowledge graph for MedQA-USMLE.",
          "citing_paper_doi": "10.18653/V1/2021.NAACL-MAIN.45",
          "cited_paper_doi": "10.1093/nar/gkh061",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3950df97ea527009a32569cb7016bc3df1383dca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1f1eaf19e38b541eec8a02f099e3090536a4c936",
          "citing_paper_year": 2021,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "12740621",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Genia"
      ],
      "dataset_details": [
        {
          "dataset_name": "Genia",
          "dataset_description": "Used to evaluate the model's performance in multi-modal reasoning tasks, focusing on molecular biology entities and relations.",
          "citing_paper_id": "212827903",
          "cited_paper_id": 12740621,
          "context_text": "KAWR-Bi-LSTM-CRF) on 5 different data sets covering various domains, such as newswire (CoNLL2003), molecular biology (Genia), biomedical (NCBI), financial (SEC) and the noisy user-generated text (WNUT16).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions five specific datasets used for evaluating the KAWR-Bi-LSTM-CRF model across different domains.",
          "citing_paper_doi": "10.1609/AAAI.V34I05.6299",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/03549e327b9e896e00a544d61da36358478c522b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/af88ce6116c2cd2927a4198745e99e5465173783",
          "citing_paper_year": 2020,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "266469822",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KnowSite"
      ],
      "dataset_details": [
        {
          "dataset_name": "KnowSite",
          "dataset_description": "This dataset 'KnowSite' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "275054439",
          "cited_paper_id": 266469822,
          "context_text": "Besides, the relation path-based decoder is proposed to model site selection criteria for brands and stores so that the KnowSite [19] can not only achieve state-of-the-art (SOTA) performance but also provide explainable site decisions based on the relation path logic.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'KnowSite' in the context of a knowledge graph for site selection, which suggests it is a specific dataset or knowledge base. However, the term 'KnowSite' could also refer to a method or tool. Given the context, it is more likely a dataset or knowledge base used for site selection.",
          "citing_paper_doi": "10.1109/TMM.2024.3521742",
          "cited_paper_doi": "10.1145/3589132.3625640",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79d7dbb3afe6fb46f77607dd40b5ad89007ee61b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86682e30d86f4d1d3a8bf2aaf482783803a40efc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "248376906",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CheBI-20"
      ],
      "dataset_details": [
        {
          "dataset_name": "CheBI-20",
          "dataset_description": "Used to compare the performance of MolLM, MoMu, and MolT5 models using four captioning metrics, focusing on multi-modal reasoning in molecular representations.",
          "citing_paper_id": "265455405",
          "cited_paper_id": 248376906,
          "context_text": "Using the CheBI-20 dataset as in (Edwards et al. , 2022), we compare the performance of MolLM, MoMu, and MolT5 using four different captioning metrics, as shown in Fig 6.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of the CheBI-20 dataset for comparing the performance of different models using captioning metrics.",
          "citing_paper_doi": "10.1101/2023.11.25.568656",
          "cited_paper_doi": "10.48550/arXiv.2204.11817",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2f8f4530247e9bd43483cf28e1ebf5b3791d94d2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3b9b1aba877ecd3f7e508cbc78a41b623349902b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "254926709",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ZINC"
      ],
      "dataset_details": [
        {
          "dataset_name": "ZINC",
          "dataset_description": "Used to sample 200 molecules for input alongside text editing prompts, focusing on multi-modal molecule structure-text modeling for retrieval and editing.",
          "citing_paper_id": "265455405",
          "cited_paper_id": 254926709,
          "context_text": "Following the procedure and settings outlined in (Liu et al. , 2023b), we begin with 200 randomly sampled molecules from ZINC (Sterling and Irwin, 2015) along with a brief text editing prompt as inputs.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "ZINC is mentioned as a source of molecular data, which is used alongside text editing prompts for input in the research.",
          "citing_paper_doi": "10.1101/2023.11.25.568656",
          "cited_paper_doi": "10.48550/arXiv.2212.10789",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2f8f4530247e9bd43483cf28e1ebf5b3791d94d2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/958bb3831589246fe5b6b58cf99e3b65c58d027f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "21715202",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Babel-Net"
      ],
      "dataset_details": [
        {
          "dataset_name": "Babel-Net",
          "dataset_description": "Used to construct a knowledge graph by automatically labeling terms defined in natural language, enhancing interpretability in text entailment recognition. | Used to generate a metallic materials knowledge graph, integrating structured information from DBpedia and Wikipedia. | Used to build a multilingual knowledge graph, providing lexical and encyclopedic information for various languages. | Used to construct a knowledge graph based on WordNet, focusing on lexical and semantic relationships.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 21715202,
          "context_text": "One is to build knowledge graph based on structured or semi-structured information in Wikipedia or other existing knowledge base (e.g., MMKG [11], Babel-Net [38], and WordNetGraph [39]).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions MMKG, Babel-Net, and WordNetGraph as examples of knowledge bases used to build knowledge graphs. These are specific, verifiable resources.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/25cd421e0f999ef8151b02fe6c7660e059a898ce",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "3180429",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GQA balanced version"
      ],
      "dataset_details": [
        {
          "dataset_name": "GQA balanced version",
          "dataset_description": "GQA balanced version is used to evaluate models on real-world visual reasoning and compositional question answering, ensuring balanced distribution of questions. | Used to evaluate multi-modal reasoning models, focusing on visual and textual information fusion in multi-modal reasoning tasks. | VQA is used to train and evaluate models for visual question answering, focusing on the relationship between images and text. | VG-QA is used to evaluate models on visual question answering tasks using the Visual Genome dataset, focusing on complex scene understanding.",
          "citing_paper_id": "237453242",
          "cited_paper_id": 3180429,
          "context_text": "0 (Antol et al., 2015b), GQA balanced version (Hudson and Manning, 2019) and VG-QA (Zhu et al., 2016).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three datasets: VQA, GQA balanced version, and VG-QA. These are all visual question answering datasets, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/2021.emnlp-main.517",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4e92fec0a61972ae076707d0630d1333affccdfc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "6216506",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TAC KBP"
      ],
      "dataset_details": [
        {
          "dataset_name": "TAC KBP",
          "dataset_description": "Used to evaluate entity linking approaches, focusing on disambiguation in structured documents with rich context. | Used to assess entity linking performance, emphasizing disambiguation in structured documents with contextual information. | Used to test entity linking methods, particularly in structured documents that offer rich contextual details for disambiguation.",
          "citing_paper_id": "215746363",
          "cited_paper_id": 6216506,
          "context_text": "These approaches have shown their effectiveness on standard EL datasets such as TAC KBP [27] , CoNLL-YAGO [23] , and ACE [2] which contain structured documents that provide a rich context for disambiguation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating entity linking approaches, providing a clear context for their use.",
          "citing_paper_doi": "10.1007/978-3-030-45439-5_31",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f648706395e8a1a6c17801cf7bd2e350e800c73d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d95738f38d97a030d98508357e4d5c78a4a208ba",
          "citing_paper_year": 2020,
          "cited_paper_year": 2011
        }
      ]
    },
    {
      "cited_paper_id": "1671874",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WN18-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN18-IMG",
          "dataset_description": "Used for multimodal link prediction, specifically to evaluate models on knowledge graph reasoning tasks involving images and textual information.",
          "citing_paper_id": "248524814",
          "cited_paper_id": 1671874,
          "context_text": "We adopt two publicly available datasets for multimodal link prediction, including: 1)WN18-IMG:WN18 [3] is a knowledge graph originally extracted from WordNet [30].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions WN18-IMG, which is a specific dataset used for multimodal link prediction. It is derived from WordNet, a lexical database.",
          "citing_paper_doi": "10.1145/3477495.3531992",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bedcfb163368f2d802de3e892acb34cc5a75a22d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2022,
          "cited_paper_year": 1995
        }
      ]
    },
    {
      "cited_paper_id": "21277943",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VrR-VG"
      ],
      "dataset_details": [
        {
          "dataset_name": "VrR-VG",
          "dataset_description": "Used to evaluate and compare scene graph generation methods, focusing on the representability of relationships in visual scenes using various models.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 21277943,
          "context_text": "Since scene graph generation task points to the representability of relationships directly, we also evaluate and compare the task performances in VrR-VG with others datasets by using different widely used scene graph generation methods, including MSDN [17], Vtrans [39], Message Passing [33] and Neural-Motifs [38].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'VrR-VG' as a dataset used for evaluating scene graph generation methods. No other datasets are explicitly named.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1109/ICCV.2017.142",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cf2de559e5a6235783e0762862f6e42192f142a8",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "16414666",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Scene Graph"
      ],
      "dataset_details": [
        {
          "dataset_name": "Scene Graph",
          "dataset_description": "Used to study object bounding box relationship triplets in images, focusing on visual and textual data integration. | Used to train and evaluate models on a large-scale dataset of annotated images, focusing on diverse object categories and relationships. | Our custom dataset used to evaluate multi-modal reasoning, focusing on visual and textual data integration. | Employed to measure VD-Net's effectiveness in relation predicate prediction, considering instances' locations and categories. | Used to evaluate visual relationship detection, focusing on object interactions and relationships in images. | Used to connect language and vision using crowdsourced dense image annotations, focusing on complex scene understanding. | Utilized to test VD-Net's accuracy in relation predicate prediction, based on instances' locations and categories. | Used to explore image retrieval capabilities through scene graphs, focusing on the integration of visual and semantic information for improved retrieval accuracy. | Used to analyze scene graphs in images, providing dense annotations for connecting language and vision. | Used to assess VD-Net's performance in predicting relation predicates, emphasizing instances' locations and categories. | Used to study a subset of Visual Genome with 150 object categories, focusing on fine-grained object and relationship annotations. | Used to evaluate VD-Net's accuracy in relation predicate prediction, focusing on instances' locations and categories.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 16414666,
          "context_text": "Dataset object bbox relationship triplet image Visual Phrase [26] 8 3,271 9 1,796 2,769 Scene Graph [14] 266 69,009 68 109,535 5,000 VRD [19] 100 70 37993 5,000 Open Images [34] 57 3,290,070 10 374,768 Visual Genome [16] 33,877 3,843,636 40,480 2,347,187 108,077 VG150 [33] 150 738,945 50 413,269 87,670 VrR-VG (ours) 1,600 282,460 117 203,375 58,983 Table 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets used for multi-modal knowledge graph reasoning, specifically focusing on visual and textual data. Each dataset is described with specific statistics, indicating their use in the research.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1109/CVPR.2015.7298990",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/85ae705ef4353c6854f5be4a4664269d6317c66b",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "231951742",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CC12M"
      ],
      "dataset_details": [
        {
          "dataset_name": "CC12M",
          "dataset_description": "Used for pre-training VLP models, incorporating a large-scale dataset with over 100M samples to enhance multi-modal reasoning performance. | Used for pre-training VLP models, providing a large-scale image-text dataset to improve multi-modal understanding. | Used for pre-training VLP models, featuring a large-scale dataset with over 100M samples to improve multi-modal representation learning. | Used for pre-training VLP models, offering a web-scale image-text dataset to enhance multi-modal reasoning capabilities. | Used for web-scale image-text pre-training to recognize long-tail visual concepts, enhancing multi-modal reasoning capabilities. | Utilized for large-scale image-text pre-training, providing a diverse set of multimedia content for multi-modal learning tasks. | Used for web-scale image-text pre-training to recognize long-tail visual concepts, focusing on expanding the model's ability to understand diverse and rare visual elements.",
          "citing_paper_id": "244117525",
          "cited_paper_id": 231951742,
          "context_text": ", 2018), Conceptual 12M (CC12M) (Changpinyo et al., 2021) and Yahoo Flickr Creative Commons 100M (YFCC100M) (Thomee et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets: CC12M, YFCC100M, and potentially another dataset from Thomee et al. CC12M and YFCC100M are clearly identified as datasets.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR46437.2021.00356",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/394be105b87e9bfe72c20efe6338de10604e1a11",
          "citing_paper_year": 2021,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "14113767",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MSCOCO 2014 caption dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "MSCOCO 2014 caption dataset",
          "dataset_description": "Used to evaluate performance in image captioning, focusing on generating accurate textual descriptions of images.",
          "citing_paper_id": "201881176",
          "cited_paper_id": 14113767,
          "context_text": "We evaluate the performances in MSCOCO 2014 caption dataset [18].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'MSCOCO 2014 caption dataset', which is a specific, verifiable dataset used for evaluating performance in image captioning tasks.",
          "citing_paper_doi": "10.1109/ICCV.2019.01050",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2019,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FILIP300M"
      ],
      "dataset_details": [
        {
          "dataset_name": "FILIP300M",
          "dataset_description": "Resulting dataset from combining FILIP300M, YFCC100M, CC12M, and CC3M, used for multi-modal knowledge graph reasoning research. | Used for pre-training VLP models, incorporating a large-scale dataset with over 100M samples to enhance multi-modal reasoning performance. | Used for pre-training VLP models, providing a large-scale image-text dataset to improve multi-modal understanding. | Used for pre-training VLP models, featuring a large-scale dataset with over 100M samples to improve multi-modal representation learning. | Used as a large-scale multimedia dataset for training and evaluating multi-modal models, focusing on mixed-precision training. | Used for pre-training VLP models, providing a large-scale multimodal dataset with images and associated metadata. | Used for pre-training VLP models, offering a web-scale image-text dataset to enhance multi-modal reasoning capabilities. | Used for pre-training VLP models, offering a large-scale multimodal dataset with images and captions. | Used for zero-shot retrieval and classification tasks, focusing on multi-modal reasoning with a filtered subset of the dataset. | Used as part of the combination to create FILIP340M, contributing to multi-modal knowledge graph reasoning research. | Used to train the FILIP base model for fair comparison with CLIP’s ViT-B/32, focusing on multi-modal reasoning and visualization.",
          "citing_paper_id": "244117525",
          "cited_paper_id": null,
          "context_text": "FILIP340M is the combination of FILIP300M, YFCC100M, CC12M and CC3M.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets by name and describes their combination into a larger dataset. These datasets are clearly identified and used in the research.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6",
          "cited_paper_url": null,
          "citing_paper_year": 2021,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "3180429",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Visual Question Answering real-image dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual Question Answering real-image dataset",
          "dataset_description": "Used to train and evaluate models on visual question answering tasks, focusing on the integration of image and text modalities with approximately 200,000 MSCOCO images. | Used for visual question answering tasks, focusing on the integration of image and text modalities to answer questions about images. | Used to train and evaluate visual question answering models, specifically focusing on real-image data with 3 splits: train (80K images), validation (40K images), and test (80K images).",
          "citing_paper_id": "2840197",
          "cited_paper_id": 3180429,
          "context_text": "The Visual Question Answering real-image dataset (Antol et al., 2015) There are 3 data splits: train (80K images), validation (40K images), and test (80K images).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation clearly mentions the Visual Question Answering real-image dataset, which is a specific, verifiable dataset with a clear structure and purpose.",
          "citing_paper_doi": "10.18653/v1/D16-1044",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12f7de07f9b00315418e381b2bd797d21f12b419",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2016,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "14113767",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Motivation dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Motivation dataset",
          "dataset_description": "Used to conduct experiments on multi-modal reasoning, consisting of 10,191 images selected from Microsoft COCO, focusing on image-caption alignment. | Served as the source for images in the Motivation dataset, providing a rich set of annotated images for multi-modal reasoning tasks.",
          "citing_paper_id": "254097189",
          "cited_paper_id": 14113767,
          "context_text": "We conduct the experiments on Motivation dataset [54] which consists of 10,191 images selected from Microsoft COCO [34].",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Motivation dataset' and 'Microsoft COCO'. 'Microsoft COCO' is a well-known dataset, while 'Motivation dataset' is less clear but appears to be a specific dataset used in the research.",
          "citing_paper_doi": "10.1145/3572914",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/690fe20f24a3f78a241f8493e2f9ed5c74bc2849",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "11820063",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "IEMOCAP"
      ],
      "dataset_details": [
        {
          "dataset_name": "IEMOCAP",
          "dataset_description": "Used to evaluate the MMGCN model, focusing on multi-modal emotion recognition in conversational contexts. | Applied to analyze emotions in multiparty dialogues, focusing on multimodal data from movie scenes for emotion recognition. | Utilized for affective computing challenges, offering multimodal data for emotion and behavior analysis in various contexts. | Used to study emotional interactions in dyadic conversations, focusing on multimodal data including audio, video, and motion capture for emotion recognition. | Used to evaluate the MMGCN model, focusing on emotional dyadic interactions captured through motion and audio data. | Used to evaluate the MMGCN model, focusing on emotional dyadic interactions captured through audio, video, and physiological signals. | Used to evaluate the MMGCN model, focusing on multi-modal emotion recognition in conversational contexts, including text, audio, and visual modalities. | Used to study emotional interactions in dyadic conversations, providing multimodal data for emotion recognition and analysis.",
          "citing_paper_id": "235829068",
          "cited_paper_id": 11820063,
          "context_text": "We evaluate our proposed MMGCN model on two benchmark datasets, IEMOCAP(Busso et al., 2008) and MELD(Poria et al., 2018).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, IEMOCAP and MELD, which are used to evaluate the MMGCN model. Both are multi-modal datasets relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.18653/v1/2021.acl-long.440",
          "cited_paper_doi": "10.1007/S10579-008-9076-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/64374ad854dba86b087ed14d96a376e646103904",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5cf0d213f3253cd46673d955209f8463db73cc51",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "51876975",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CC3M"
      ],
      "dataset_details": [
        {
          "dataset_name": "CC3M",
          "dataset_description": "Used to enhance the training set for image captioning, focusing on improving the quality and diversity of generated captions. | Added to the training set to provide additional image-caption pairs, enhancing the model's ability to generate accurate and contextually relevant captions.",
          "citing_paper_id": "252917745",
          "cited_paper_id": 51876975,
          "context_text": "We practically add COCO Caption [8] and CC3M [42] to the training set, while large-scale datasets like CC12M [4] or YFCC [21] are not considered to maintain training efﬁciency.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training, which are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.08901",
          "cited_paper_doi": "10.18653/v1/P18-1238",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "231591445",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "400 million image-text pairs"
      ],
      "dataset_details": [
        {
          "dataset_name": "400 million image-text pairs",
          "dataset_description": "Used to pre-train CLIP, a model that learns to match raw text with corresponding images, enhancing multi-modal representation learning.",
          "citing_paper_id": "252917745",
          "cited_paper_id": 231591445,
          "context_text": "One of the pioneer works, CLIP [38], extends the scale of the pre-training dataset to 400 million image-text pairs, and learns representations by directly matching raw text with the corresponding image.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a large-scale pre-training dataset used by CLIP, which is relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2210.08901",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "2424223",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SituNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "SituNet",
          "dataset_description": "Used to define visual event schemas, contributing to the development of situation recognition systems that understand image content through visual semantic role labeling. | Used to train models in situation recognition tasks, focusing on visual semantic role labeling for image understanding.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 2424223,
          "context_text": "Schemas deﬁned in datasets of situation recognition tasks, such as SituNet [101] and SWiG [102], can be used to train models in this task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, SituNet and SWiG, which are used for training models in situation recognition tasks. These datasets are relevant to multi-modal knowledge graph reasoning as they involve visual and semantic information.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/CVPR.2016.597",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b65faba7088864e134e7eb3b68c8e2f18cc5b4f6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "67855617",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OMAHA-MM"
      ],
      "dataset_details": [
        {
          "dataset_name": "OMAHA-MM",
          "dataset_description": "Used to evaluate the performance of a-RotatE on the MR metric, focusing on multi-modal knowledge graph reasoning and comparing it with other models.",
          "citing_paper_id": "261329076",
          "cited_paper_id": 67855617,
          "context_text": "Compared with baselines, the only exception is that a-RotatE achieves the best performance on MR metric on OMAHA-MM dataset. a-RotatE (as well as PairRE) uses self-adversarial negative sampling technique, which automatically balances the updated degree of negative samples, making the model averaged in some situations.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the OMAHA-MM dataset, which is used to evaluate the performance of a-RotatE on the MR metric. The dataset is specific and relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.10231041",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4cafc4dbd35d3a474609afc8834b9e273e41fac4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8f096071a09701012c9c279aee2a88143a295935",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "236980279",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YAGO3"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO3",
          "dataset_description": "Used to create a knowledge graph comprising a collection of triplets, focusing on hierarchical-knowledge embedded meta-learning for visual reasoning in artistic domains.",
          "citing_paper_id": "261431541",
          "cited_paper_id": 236980279,
          "context_text": "Another method is Knowledge is Power [52], which uses the YAGO3 [32] to create a knowledge graph comprising a collection of triplets.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions YAGO3 as a source for creating a knowledge graph, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3618301",
          "cited_paper_doi": "10.1145/3447548.3467285",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb1bc9970a52566134280d2b01e0920c80d3d53e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8cee6b1e61b71b6af6086e442a6620c6bb8534bc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "59336197",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "HowNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "HowNet",
          "dataset_description": "Used to annotate sememes for candidate words in poetry, focusing on semantic information to enhance multi-modal reasoning.",
          "citing_paper_id": "256631072",
          "cited_paper_id": 59336197,
          "context_text": "We keep the words appearing more than 5 times in poetry as the candidate words (145,800 words), then crawl their definitions from HanDian 5 and annotations of sememes from HowNet (Dong and Dong, 2003; Qi et al., 2019).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'HanDian' and 'HowNet' as sources for definitions and sememe annotations. HowNet is a specific lexical knowledge base, while HanDian is likely a dictionary or similar resource.",
          "citing_paper_doi": "10.18653/v1/2022.findings-emnlp.171",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/94acf4682bce44810c377be075c20dd6de96b8e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6b1b5079dec9b1164d3415a925c1b4aca53a5e10",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "PKG",
          "dataset_description": "Used to construct a multi-modal knowledge graph for classical Chinese poetry, incorporating allusions and visual information to bridge the semantic gap between modalities.",
          "citing_paper_id": "256631072",
          "cited_paper_id": 76663467,
          "context_text": "To bridge the semantic gap between two modalities (Liu et al., 2019a; Wang et al., 2019), we construct a multi-modal knowledge graph for classical Chinese poetry ( PKG ), in which a wealth of information, such as the allusions and visual information of the words in poetry are incorporated.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the construction of a multi-modal knowledge graph for classical Chinese poetry, referred to as 'PKG'. This appears to be a specific, verifiable dataset.",
          "citing_paper_doi": "10.18653/v1/2022.findings-emnlp.171",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/94acf4682bce44810c377be075c20dd6de96b8e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "227257070",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KnowPoetry"
      ],
      "dataset_details": [
        {
          "dataset_name": "KnowPoetry",
          "dataset_description": "Used to construct a knowledge graph of Tang poetry and poets, applying inference rules to mine social relationships and support further study of Tang poetry.",
          "citing_paper_id": "256631072",
          "cited_paper_id": 227257070,
          "context_text": "KnowPoetry (Hong et al., 2020) semi-automatically constructs a knowledge graph of Tang poetry and poet, and uses inference rules from experts to help the further study of Tang poetry e.g., mining social relationship between poets.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'KnowPoetry' as a knowledge graph construction effort, which is a dataset-like resource. It is used for studying Tang poetry, including social relationships between poets.",
          "citing_paper_doi": "10.18653/v1/2022.findings-emnlp.171",
          "cited_paper_doi": "10.1353/lib.2020.0025",
          "citing_paper_url": "https://www.semanticscholar.org/paper/94acf4682bce44810c377be075c20dd6de96b8e5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8562bc1000a9fc8a6f2621300991caf568dffee7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "232257683",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MAG240M"
      ],
      "dataset_details": [
        {
          "dataset_name": "MAG240M",
          "dataset_description": "Used for training models in a large-scale graph learning challenge, focusing on transferability to related tasks. | Used for link-level tasks, offering a comprehensive dataset to train models for predicting links in multi-modal knowledge graphs. | Used alongside MAG240M for training and evaluating model transferability in graph-based machine learning tasks. | Used for node-level tasks, providing a large-scale graph dataset to train models for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "265871676",
          "cited_paper_id": 232257683,
          "context_text": "(2023) is mentioned as the baseline for node and link level tasks, it follows a different setting, being trained on the MAG240M or Wiki datasets (Hu et al., 2021) for corresponding node or link tasks.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for training in a different setting, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2310.00149",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ab22d54dd13876d25c6c8f46c40fb9ac41c61ec5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9389af659f14239319186dff1cef49e8ece742c8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "5714907",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "COCO Caption"
      ],
      "dataset_details": [
        {
          "dataset_name": "COCO Caption",
          "dataset_description": "Used for pre-training ZLXMERT, providing region-level captions to improve grounded language understanding. | Used for pre-training ZLXMERT, focusing on complex reasoning over visual scenes and associated questions. | Used for pre-training ZLXMERT, enhancing the model's ability to answer questions about visual scenes. | Used for pre-training ZLXMERT, improving the model's performance in visual question answering tasks. | Used for pre-training ZLXMERT, focusing on image captioning to enhance visual-linguistic understanding.",
          "citing_paper_id": "201317624",
          "cited_paper_id": 5714907,
          "context_text": "l commonsense reasoning 3) masked visual-feature classiﬁcation 3) grounding referring expressions zLXMERT is pre-trained on COCO Caption (Chen et al., 2015), VG Caption (Krishna et al., 2017), VG QA (Zhu et al., 2016), VQA (Antol et al., 2015) and GQA (Hudson &amp; Manning, 2019). Table 1: Comparison among our VL-BERT and other works seeking to derive pre-trainable generic representations for visual-linguistic tas",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for pre-training a multi-modal model, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2016.540",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca",
          "cited_paper_url": "https://www.semanticscholar.org/paper/def584565d05d6a8ba94de6621adab9e301d375d",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "53734356",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VCR"
      ],
      "dataset_details": [
        {
          "dataset_name": "VCR",
          "dataset_description": "Used for extensive experiments on visual commonsense reasoning, specifically addressing multiple choice QA problems derived from 110k movie scenes. | Used to validate and test models on visual commonsense reasoning tasks, focusing on state-of-the-art performance across multiple tasks.",
          "citing_paper_id": "202785103",
          "cited_paper_id": 53734356,
          "context_text": "We carry out extensive experiments on VCR [44] benchmark, a representative large-scale visual commonsense reasoning dataset containing a total of 290k multiple choice QA problems derived from 110k movie scenes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions VCR as a large-scale visual commonsense reasoning dataset, which is used for experiments involving multiple choice QA problems derived from movie scenes.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2019.00688",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef318e7ff0883e72d853c75736d20cc123b556d5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6dfc2ff03534a4325d06c6f88c3144831996629b",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MM-FB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "MM-FB15K",
          "dataset_description": "Used to construct a multi-modal knowledge graph, focusing on integrating visual and textual information to enhance reasoning capabilities.",
          "citing_paper_id": "252518772",
          "cited_paper_id": 76663467,
          "context_text": "Our constructed multi-modal datasets MM-FB15K and MM-DBpedia are based on FB15K [2,1] and DBpedia [14,11,23].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two constructed multi-modal datasets, MM-FB15K and MM-DBpedia, which are based on existing datasets FB15K and DBpedia. These are specific, verifiable datasets used in the research.",
          "citing_paper_doi": "10.1007/978-3-031-26390-3_11",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0ba984394512bbf32d7135716f8bf2e8984a996",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "8040343",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ICEWS05-15"
      ],
      "dataset_details": [
        {
          "dataset_name": "ICEWS05-15",
          "dataset_description": "Used to evaluate the performance of the method on temporal reasoning tasks in dynamic knowledge graphs, focusing on event prediction and entity resolution. | Used to test the model's capability in handling large-scale global event data, focusing on event prediction and temporal reasoning. | Used to test the method's effectiveness in handling large-scale global event data, evaluating its performance on entity linking and event prediction tasks. | Used to assess the model's ability to predict events over time in dynamic knowledge graphs, emphasizing temporal dynamics. | Used to evaluate the model's performance on temporal reasoning tasks in dynamic knowledge graphs, focusing on event prediction. | Used to study temporal reasoning in dynamic knowledge graphs, focusing on facts from April 2015 to March 2016, as provided by Trivedi et al. (2017). | Used to assess the method's ability to handle long-term temporal dependencies in dynamic knowledge graphs, specifically for event forecasting and relationship evolution. | Used to highlight the dataset's density and richness in structural and temporal contextual information, containing 2.7 million facts for 500 entities and 20 relations. | Used to evaluate the model's performance in multi-modal knowledge graph reasoning, specifically measuring improvements in MRR and Hits@1 over the best baseline.",
          "citing_paper_id": "259858848",
          "cited_paper_id": 8040343,
          "context_text": "ICEWS14 ICEWS05-15 GDELT Method MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 Table 2: Performance comparison on three benchmark datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'ICEWS14', 'ICEWS05-15', and 'GDELT' as benchmark datasets, which are commonly used in multi-modal knowledge graph reasoning. These datasets are used to evaluate the performance of the method.",
          "citing_paper_doi": "10.18653/v1/2023.findings-acl.28",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f9f2158801d2163658be943c43afcefb2f8f75b7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22f95fb555278ebe0ad2e5e18aaaf8be3e4e53fe",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "3922816",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ARC"
      ],
      "dataset_details": [
        {
          "dataset_name": "ARC",
          "dataset_description": "Used to evaluate multi-modal reasoning capabilities, focusing on complex question answering tasks that require both textual and visual understanding.",
          "citing_paper_id": "252968266",
          "cited_paper_id": 3922816,
          "context_text": "…benchmarks: Common-senseQA ( CSQA ) [56], OpenbookQA ( OBQA ) [57], RiddleSense ( Riddle ) [58], AI2 Reasoning Challenge–Challenge Set ( ARC ) [59], CosmosQA [60], HellaSwag [61], Physical Interaction QA ( PIQA ) [62], Social Interaction QA ( SIQA ) [63], and Abductive Natural Language…",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions several benchmarks, but they are primarily used for score comparison rather than as specific, reusable datasets. However, 'ARC' is included as it refers to a specific dataset beyond the leaderboard.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88bb0a28bb58d847183ec505dda89b63771bb495",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "198953378",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CSQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CSQA",
          "dataset_description": "Used to evaluate D RAGON's performance on commonsense reasoning tasks, specifically comparing it to RoBERTa with an 8% absolute accuracy gain.",
          "citing_paper_id": "252968266",
          "cited_paper_id": 198953378,
          "context_text": "For the general domain, D RAGON outperforms RoBERTa [18], our base LM without KGs, on various commonsense reasoning tasks such as CSQA, OBQA, RiddleSense and HellaSwag, with +8% absolute accuracy gain on average.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets (CSQA, OBQA, RiddleSense, HellaSwag) used to evaluate the performance of D RAGON compared to RoBERTa. These datasets are relevant to multi-modal knowledge graph reasoning as they involve commonsense reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CommonsenseQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CommonsenseQA",
          "dataset_description": "Used to evaluate D RAGON’s performance on commonsense reasoning questions, focusing on understanding everyday scenarios and concepts. | Mentioned as a large knowledge graph providing complementary information to text data, but specific usage in the research is not detailed. | This dataset 'Physical Interaction QA' was mentioned in the citation context but no detailed description was generated. | Used to challenge D RAGON with complex reasoning tasks, including multi-step inference and knowledge integration. | This dataset 'Abductive Natural Language Inference' was mentioned in the citation context but no detailed description was generated. | Used to test D RAGON’s capability in solving riddles, which require logical and creative thinking to deduce the correct answer.",
          "citing_paper_id": "252968266",
          "cited_paper_id": null,
          "context_text": "We ﬁnetune and evaluate D RAGON on nine diverse commonsense reasoning benchmarks: Common-senseQA ( CSQA ) [56], OpenbookQA ( OBQA ) [57], RiddleSense ( Riddle ) [58], AI2 Reasoning Challenge–Challenge Set ( ARC ) [59], CosmosQA [60], HellaSwag [61], Physical Interaction QA ( PIQA ) [62], Social Interaction QA ( SIQA ) [63], and Abductive Natural Language Inference ( aNLI ) [64].",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple benchmarks but does not specify them as reusable datasets. They are used for evaluating the performance of D RAGON on various commonsense reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2210.09338",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ad3dfb2514cb0c899fcb9a14d229ff2a6018892f",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "224270776",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "BioKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "BioKG",
          "dataset_description": "Used to facilitate relational learning on biomedical data, promoting the integration and analysis of diverse biological information in a multi-modal knowledge graph framework.",
          "citing_paper_id": "259203574",
          "cited_paper_id": 224270776,
          "context_text": "In the biomedical science domain, BioKG [54] helps the relational learning on biomedical data and recent proposed ProteinKG25 [55] is used to promote the research on protein language pre-training.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "BioKG is mentioned as a knowledge graph used for relational learning on biomedical data, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2306.11443",
          "cited_paper_doi": "10.1145/3340531.3412776",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3525e7911510e8b1adbb4290aefdaef7c0bd455",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8fa23ccde80e68d7c4f244189e01e26ea0618d76",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "229363322",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CUB"
      ],
      "dataset_details": [
        {
          "dataset_name": "CUB",
          "dataset_description": "Utilized for zero-shot learning tasks, combining visual and semantic information to recognize unseen categories. | Used to evaluate multi-modal reasoning, focusing on fine-grained classification tasks with images and textual descriptions. | Employed to enhance knowledge graph reasoning, integrating visual and textual data for improved zero-shot recognition.",
          "citing_paper_id": "250264976",
          "cited_paper_id": 229363322,
          "context_text": "Specifically, we use Swin-base [39] as the vision transformer encoder for SUN, and use DeiT-base [49] for CUB, AWA2 and AWA2-KG.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions datasets CUB, AWA2, and AWA2-KG, which are used for multi-modal reasoning tasks. These datasets are specific and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2207.01328",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/13d8ce3d2ac01cc5e108c1b89e79049428a53ad5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad7ddcc14984caae308c397f1a589aae75d4ab71",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "231786709",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Compositional ZSL"
      ],
      "dataset_details": [
        {
          "dataset_name": "Compositional ZSL",
          "dataset_description": "Used to explore feature-to-sequence transformation in compositional zero-shot learning, focusing on class descriptions composed of visual primitives (states and objects). | Used to explore the applicability of Feature-to-Sequence Transformation in zero-shot learning scenarios, focusing on class descriptions composed of visual primitives.",
          "citing_paper_id": "250264976",
          "cited_paper_id": 231786709,
          "context_text": "It is worth mentioning that our proposed Feature-to-Sequence Transformation technique could be further migrated to other ZSL datasets with different attribute formats or data types, such as Compositional ZSL [42] whose class descriptions are the composition of visual primitives (i.e., states and objects).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Compositional ZSL' as a dataset with class descriptions composed of visual primitives. This is a specific dataset used for compositional zero-shot learning.",
          "citing_paper_doi": "10.48550/arXiv.2207.01328",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00101",
          "citing_paper_url": "https://www.semanticscholar.org/paper/13d8ce3d2ac01cc5e108c1b89e79049428a53ad5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/480d14138c64f43c8b58a9ddd4e4ae08d2794396",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "231925372",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AWA2-KG"
      ],
      "dataset_details": [
        {
          "dataset_name": "AWA2-KG",
          "dataset_description": "Used to provide triples for prompting in ontology-enhanced zero-shot learning, focusing on relations between classes and attributes. | Used to validate DUET’s flexibility on various ZSL attribute formats, focusing on zero-shot learning with ontology-enhanced knowledge graphs. | Used to provide triples for prompting in ontology-enhanced zero-shot learning, specifically focusing on relations between entities and their attributes.",
          "citing_paper_id": "250264976",
          "cited_paper_id": 231925372,
          "context_text": "Weevaluate onAWA2-KG [24] to validate DUET’s flexibility on various ZSL attribute formats.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'AWA2-KG' as a dataset used for evaluation, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2207.01328",
          "cited_paper_doi": "10.1145/3442381.3450042",
          "citing_paper_url": "https://www.semanticscholar.org/paper/13d8ce3d2ac01cc5e108c1b89e79049428a53ad5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ed4449f065fa225dd7c08929c64e6356ff1264f6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "51876975",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Conceptual Captions"
      ],
      "dataset_details": [
        {
          "dataset_name": "Conceptual Captions",
          "dataset_description": "Used for image-text pre-training, containing 3.0M cleaned and hypernymed image descriptions, focusing on automatic image captioning. | Used as pre-training data for multi-modal tasks, specifically for training models to understand the relationship between text and image regions. | Used for pre-training ERNIE-ViL, focusing on image-text alignment and automatic image captioning using a large collection of image-caption pairs. | Used for pre-training ERNIE-ViL, focusing on image-text alignment and automatic image captioning using cleaned, hypernymed image alt-text. | Used for image-text pre-training, containing 1.0M image descriptions, serving as a benchmark for automatic image captioning.",
          "citing_paper_id": "220265934",
          "cited_paper_id": 51876975,
          "context_text": "Conceptual Captions [14] and SBU Captions [15] are the most widely-used datasets for image-text pre-training, with 3.0M and 1.0M image descriptions respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, Conceptual Captions and SBU Captions, which are used for image-text pre-training. Both datasets are clearly identified and their sizes are specified.",
          "citing_paper_doi": "10.1609/aaai.v35i4.16431",
          "cited_paper_doi": "10.18653/v1/P18-1238",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "216562512",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YAGO26K-906"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO26K-906",
          "dataset_description": "Used to conduct experiments on two-view KGs, leveraging YAGO as a base ontology to explore multi-modal reasoning approaches. | Used to conduct experiments on multi-modal knowledge graph reasoning, focusing on tasks related to knowledge abstraction, concretization, and completion. | Used to conduct experiments on multi-modal knowledge graph reasoning, focusing on two-view KGs derived from DBpedia. The dataset is part of the JOIE framework. | Employed for experiments on two-view KGs, incorporating Wikidata as a base ontology to assess multi-modal reasoning techniques. | Used to conduct experiments on multi-modal knowledge graph reasoning, focusing on two-view KGs derived from YAGO. The dataset is part of the JOIE framework. | Utilized for experiments on two-view KGs, using DBpedia as a base ontology to test multi-modal reasoning methodologies.",
          "citing_paper_id": "271961547",
          "cited_paper_id": 216562512,
          "context_text": "In this paper, we conducted experiments using three public datasets : YAGO26K-906[12], DB111K-174[12], and KACC-M[43] that contain information on two-view KGs. YAGO26K-906 and DB111K-174 are proposed by JOIE[12], utilizing YAGO[21] and DBpedia[18] as base ontologies, respectively, whereas KAAC-M utilizes Wikidata[35] as base ontology.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for experiments involving two-view KGs, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3637528.3671941",
          "cited_paper_doi": "10.18653/v1/2021.findings-acl.153",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e546dbcee7e0b88e200cbae08769ffbeacabe837",
          "cited_paper_url": "https://www.semanticscholar.org/paper/28d794bc8b8cf579745146b3fd3362942787dd10",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "438559",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Charades-STA"
      ],
      "dataset_details": [
        {
          "dataset_name": "Charades-STA",
          "dataset_description": "Used to compare cross-domain multi-modal content description alignment methods, evaluating performance across various metrics. | Used as the source dataset for multi-modal tasks involving action descriptions in videos, specifically noting slow convergence in T → C and T → A tasks. | Serves as the target domain where annotation knowledge from ActivityNet Captions is transferred, improving the quality of action descriptions in videos. | Used to evaluate the performance of multi-modal reasoning models on action localization and description in everyday activities. | Used to evaluate action captioning in videos, focusing on grounding textual descriptions with visual content using an inference threshold of 0.8. | Used for domain alignment to obtain annotation knowledge, speeding up the convergence of the learning curve in multi-modal context. | Used as the source domain to transfer annotation knowledge for action descriptions in videos, enhancing the target domain annotations. | Used to ground action descriptions in videos, focusing on aligning textual descriptions with corresponding video segments. | Utilized to test the generation of descriptive captions for video clips, evaluating the coherence and relevance of generated text to visual content.",
          "citing_paper_id": "252519575",
          "cited_paper_id": 438559,
          "context_text": "We conduct experiments on three challenging benchmark datasets: TACoS [71], Charades-STA [72], and ActivityNet Cap-tions [73], summarized in Table I. TACoS: TACoS [71] contains 127 videos, and the average video length is about 7 minutes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three specific datasets used for experiments in multi-modal knowledge graph reasoning, focusing on action descriptions in videos.",
          "citing_paper_doi": "10.1109/TMM.2022.3222965",
          "cited_paper_doi": "10.1162/tacl_a_00207",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2931d8103aaff3a2514eb29008b0da3e399e4bac",
          "cited_paper_url": "https://www.semanticscholar.org/paper/21b3007f967d39e1346bc91e0fc8b3f16121300c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2013
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "BIOSNAP-sub"
      ],
      "dataset_details": [
        {
          "dataset_name": "BIOSNAP-sub",
          "dataset_description": "Used to validate GENs for out-of-graph (OOG) drug-to-drug interaction prediction, focusing on the accuracy and generalizability of the model.",
          "citing_paper_id": "219573823",
          "cited_paper_id": null,
          "context_text": "We also validate GENs for OOG drug-to-drug interaction prediction task on DeepDDI [36] and BIOSNAP-sub [26].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions two specific datasets, DeepDDI and BIOSNAP-sub, which are used for validating GENs in the drug-to-drug interaction prediction task.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/225d571f97f9097be436626f225cff572007aaf4",
          "cited_paper_url": null,
          "citing_paper_year": 2020,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "232417264",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FashionGen"
      ],
      "dataset_details": [
        {
          "dataset_name": "FashionGen",
          "dataset_description": "Used to evaluate Cross-Modal Retrieval (CR) and Semantic Cross-Modal Retrieval (SCR) in the context of vision-language pre-training on the fashion domain. | Used to evaluate Cross-Modal Retrieval (CR) and Semantic Cross-Modal Retrieval (SCR) tasks, focusing on vision-language integration in the fashion domain. | Used for cross-modal retrieval experiments, focusing on fashion domain, employing the same evaluation protocol as KaleidoBERT.",
          "citing_paper_id": "250627337",
          "cited_paper_id": 232417264,
          "context_text": "Following KaleidoBERT [79], we evaluate CR and SCR on the FashionGen dataset [52].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the FashionGen dataset, which is a specific, verifiable dataset used for evaluation in the context of vision-language pre-training on the fashion domain.",
          "citing_paper_doi": "10.48550/arXiv.2207.08150",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9802779322febdb9fdd6177d3da41c1269a6e05c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/809b231e915c35db47cb81abfd8600f4c0f9fa10",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "15912887",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DBP ZH-EN"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBP ZH-EN",
          "dataset_description": "Used to evaluate cross-lingual knowledge alignment methods, focusing on Chinese-English entity linking performance. | Used to evaluate cross-lingual knowledge alignment methods, focusing on Japanese-English entity linking performance. | Used to evaluate cross-lingual knowledge alignment methods, focusing on French-English entity linking performance. | Used to evaluate cross-lingual knowledge alignment methods, focusing on alignment between DBpedia and Wikidata.",
          "citing_paper_id": "208176414",
          "cited_paper_id": 15912887,
          "context_text": "…observation to that Methods DBP ZH-EN DBP JA-EN DBP FR-EN DBP-WD DBP-YG H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR H@1 H@10 MRR MTransE (Chen et al. 2017) 0.308 0.614 0.364 0.279 0.575 0.349 0.244 0.556 0.335 0.281 0.520 0.363 0.252 0.493 0.334 IPTransE (Zhu et al. 2017) 0.406 0.735…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for evaluating cross-lingual knowledge alignment methods. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.1609/AAAI.V34I01.5354",
          "cited_paper_doi": "10.24963/ijcai.2017/209",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8f5bac59311dc32ed75d76587ff60df00e1be502",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2fb92d384a5416877d52f7c3b6b6b2889e86d680",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "16619709",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KuaiRec 1"
      ],
      "dataset_details": [
        {
          "dataset_name": "KuaiRec 1",
          "dataset_description": "Used for evaluating multi-modal knowledge graph reasoning, focusing on user-item interactions and recommendation accuracy. | Used for evaluating multi-modal knowledge graph reasoning, focusing on user-movie ratings and recommendation accuracy.",
          "citing_paper_id": "258756463",
          "cited_paper_id": 16619709,
          "context_text": "A. Experimental Setup 1) Datasets: We leverage two publicly accessible datasets, i.e., KuaiRec 1 [52] and Movielens-1M 2 [57] for evaluation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, KuaiRec 1 and Movielens-1M, which are used for evaluation in the experimental setup.",
          "citing_paper_doi": "10.1109/TMM.2023.3276505",
          "cited_paper_doi": "10.1145/2827872",
          "citing_paper_url": "https://www.semanticscholar.org/paper/29632ece419ac9537d8b3caadfa9edd3812392a2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/276ebc620a8976026bd2d03582b9ecfa3738d43c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "31606602",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WN"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN",
          "dataset_description": "Used to split labeled entities into train/valid/test sets for multi-relational data modeling, focusing on entity representation learning. | Used to model multi-relational data, focusing on entity and relation embeddings in knowledge graphs. The dataset is employed to train and evaluate models for link prediction tasks. | Used for label collection in knowledge graph representation learning, focusing on Freebase relations and entities. | Used to study relationships between artifacts in the Amsterdam Museum, focusing on multi-modal reasoning across different types of cultural objects. | Consists of triplets extracted from WordNet 3.0, used to evaluate multi-relational data modeling and reasoning tasks. | Extracted from Freebase, used to test models on a large-scale knowledge graph, focusing on multi-relational reasoning and entity linking. | Used for label collection in knowledge graph representation learning, focusing on wordnet relations and entities. | Used to demonstrate the handling of multiple labels per entity, contrasting with AM and WN's single-label constraint. | Used to collect labels for entity classification tasks, demonstrating the importance of relation modeling in graph convolutional networks. | Used to evaluate knowledge graph embedding models, specifically measuring performance under P@1, P@5, and N@5 metrics.",
          "citing_paper_id": "235324797",
          "cited_paper_id": 31606602,
          "context_text": "Additionally, we see 2Please refer to [29] and [39] for details of label collection in WN and FB15K datasets respectively.\nthat vanilla GCN performsworse than any relational GCNs in all the datasets, which demonstrates that relation modelling is significant for entity classification.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the WN and FB15K datasets, which are used to collect labels for entity classification tasks. The context indicates that these datasets are used to demonstrate the importance of relation modeling in graph convolutional networks.",
          "citing_paper_doi": "10.1145/3442381.3449925",
          "cited_paper_doi": "10.1609/aaai.v30i1.10329",
          "citing_paper_url": "https://www.semanticscholar.org/paper/73965db326aee64123487676b6230bf417940698",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96acb1c882ad655c6b8459c2cd331803801446ca",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "219573823",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB-Ext"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB-Ext",
          "dataset_description": "Used to evaluate link prediction models on extended Freebase entities, focusing on 'seen to unseen' and 'unseen to unseen' entity predictions. | Used to evaluate link prediction models on extended NELL entities, focusing on 'seen to unseen' and 'unseen to unseen' entity predictions.",
          "citing_paper_id": "256598084",
          "cited_paper_id": 219573823,
          "context_text": "For example, GEN [Baek et al. , 2020] and HRFN [Zhang et al. , 2021] develop datasets to show their link prediction ability on both ‘seen to unseen’ and ‘unseen to unseen’ entities; MaKEr [Chen et al. , 2022a] and RMPI [Geng et al. , 2023] provide FB-Ext/NELL-Ext and",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions datasets developed to demonstrate link prediction abilities, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2302.01859",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/76b042a91efd3d8f61e44f421b1839cb0d9fc309",
          "cited_paper_url": "https://www.semanticscholar.org/paper/225d571f97f9097be436626f225cff572007aaf4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "236956836",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LRS2-2Mix"
      ],
      "dataset_details": [
        {
          "dataset_name": "LRS2-2Mix",
          "dataset_description": "Used to train and validate audio-visual transformer models, comparing different methods' data usage for training and validation.",
          "citing_paper_id": "254926959",
          "cited_paper_id": 236956836,
          "context_text": "For instance, on LRS2-2Mix, the method in [38] used 29 hours data for training and 1 hours data for validation, while the method in [15] used 11 hours data for training and 3 hours data for validation.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'LRS2-2Mix' which appears to be a specific dataset used for training and validation in audio-visual transformer approaches. The dataset is referenced in the context of comparing different methods' data usage.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3384034",
          "cited_paper_doi": "10.1109/ICCV48922.2021.00114",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f7f217573449efb2b7be58ea87ad111bfc81748a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cfa9640d0dd0fec4d0ee87934106ffcb9875227a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    }
  ],
  "citation_count_distribution": {
    "2768038": 11,
    "4328400": 9,
    "5458500": 11,
    "6628106": 4,
    "15150247": 6,
    "152282269": 3,
    "199453025": 3,
    "199528533": 7,
    "201103729": 4,
    "216080982": 4,
    "220496043": 1,
    "235436185": 4,
    "236957384": 1,
    "259075619": 2,
    "259129314": 2,
    "259145427": 4,
    "267334765": 1,
    "267547866": 5,
    "272655172": 1,
    "1219941": 1,
    "2840197": 1,
    "5471519": 1,
    "8849206": 2,
    "9909815": 12,
    "13694791": 1,
    "13756489": 6,
    "14941970": 14,
    "18347865": 1,
    "44145776": 10,
    "52143467": 2,
    "52160797": 7,
    "73425227": 2,
    "201070367": 2,
    "201698166": 2,
    "210695009": 2,
    "211010433": 7,
    "211137418": 2,
    "213318844": 1,
    "218563659": 1,
    "226283804": 1,
    "233407536": 1,
    "235412627": 1,
    "235503675": 2,
    "260537889": 1,
    "262380035": 1,
    "1915014": 2,
    "3875633": 1,
    "3879949": 1,
    "3982237": 1,
    "4090850": 2,
    "4502993": 1,
    "4669223": 3,
    "6233645": 1,
    "6911541": 2,
    "8040343": 2,
    "8423494": 3,
    "8492900": 1,
    "10811631": 2,
    "10838787": 1,
    "18367155": 1,
    "18938726": 1,
    "19139252": 2,
    "19187663": 2,
    "20667722": 3,
    "26996000": 1,
    "38485677": 2,
    "54434537": 1,
    "56517517": 1,
    "59516071": 1,
    "60254729": 1,
    "67474824": 1,
    "67855617": 10,
    "67856459": 2,
    "85558018": 1,
    "117107657": 1,
    "118675981": 1,
    "168633605": 2,
    "201317624": 4,
    "202775885": 3,
    "202777324": 2,
    "203605587": 9,
    "203626690": 1,
    "206477883": 1,
    "206710454": 1,
    "207116476": 1,
    "207167677": 7,
    "207228784": 2,
    "208006241": 5,
    "211082667": 3,
    "211096730": 4,
    "214390104": 1,
    "214693100": 1,
    "214803074": 1,
    "218581196": 1,
    "218900866": 3,
    "218971783": 2,
    "219531264": 1,
    "221819250": 1,
    "222124934": 2,
    "222205878": 2,
    "226281660": 4,
    "231925372": 3,
    "232126110": 1,
    "232222958": 2,
    "233295959": 2,
    "233486383": 2,
    "235262529": 2,
    "235266233": 1,
    "235271284": 2,
    "235606453": 1,
    "236772282": 3,
    "237100866": 1,
    "238583240": 1,
    "239011538": 7,
    "239016536": 3,
    "240230810": 2,
    "245124050": 1,
    "245591477": 2,
    "245634466": 1,
    "246210481": 3,
    "246823061": 6,
    "246867470": 2,
    "248435922": 2,
    "248496482": 1,
    "248834330": 2,
    "249210042": 2,
    "249626322": 2,
    "250340389": 1,
    "250629390": 1,
    "252199918": 1,
    "252782076": 2,
    "252905025": 1,
    "252918783": 4,
    "252918814": 1,
    "253862877": 1,
    "254926959": 1,
    "257129854": 1,
    "257631615": 1,
    "257912512": 2,
    "257921533": 1,
    "258987428": 1,
    "259949958": 1,
    "260171695": 1,
    "261030303": 1,
    "263886314": 2,
    "263896114": 1,
    "268042282": 1,
    "269634664": 1,
    "261138": 3,
    "27263492": 2,
    "31606602": 5,
    "49644765": 1,
    "195351633": 1,
    "201058752": 3,
    "208201975": 3,
    "220730157": 1,
    "221376974": 2,
    "225039882": 6,
    "252089825": 4,
    "252783084": 4,
    "259165563": 2,
    "227151278": 1,
    "244521828": 1,
    "264057354": 1,
    "264325404": 1,
    "1033682": 2,
    "1157792": 2,
    "2949428": 3,
    "6071257": 4,
    "145832359": 1,
    "202539519": 3,
    "207775322": 1,
    "231591445": 5,
    "240241582": 1,
    "249440365": 1,
    "250118042": 2,
    "258509157": 4,
    "260887576": 2,
    "266939061": 1,
    "267841518": 1,
    "269684581": 1,
    "8429835": 2,
    "15027084": 5,
    "125343003": 2,
    "135465817": 1,
    "252917745": 2,
    "252968266": 1,
    "257168851": 2,
    "1671874": 5,
    "1926319": 2,
    "3656231": 1,
    "4703853": 1,
    "4852047": 1,
    "258220916": 1,
    "219708313": 1,
    "252780775": 1,
    "253309006": 1,
    "258352810": 1,
    "261562315": 1,
    "108300309": 1,
    "117223439": 1,
    "228861368": 1,
    "233863555": 1,
    "234826966": 1,
    "257118958": 1,
    "264439270": 1,
    "563473": 1,
    "2486369": 1,
    "3016223": 2,
    "3508727": 1,
    "5959482": 6,
    "12983389": 1,
    "15206880": 2,
    "15483870": 1,
    "16273722": 1,
    "17011026": 1,
    "21723549": 1,
    "27494872": 2,
    "38934160": 1,
    "49313245": 3,
    "54206179": 1,
    "54559476": 1,
    "69481030": 1,
    "141406559": 2,
    "159042183": 2,
    "204972196": 1,
    "206594692": 9,
    "213104909": 1,
    "215717250": 1,
    "219615799": 1,
    "236428227": 1,
    "218486837": 2,
    "233219869": 6,
    "3117929": 5,
    "7204540": 1,
    "16153365": 1,
    "16771371": 1,
    "57189444": 1,
    "210839653": 1,
    "219437602": 1,
    "224814368": 1,
    "225115084": 2,
    "231626386": 1,
    "234357689": 1,
    "236477903": 1,
    "245119728": 2,
    "247794106": 1,
    "249017794": 1,
    "258879983": 1,
    "205692": 2,
    "982761": 1,
    "1181640": 5,
    "1336493": 1,
    "1619841": 1,
    "1969092": 2,
    "3544741": 1,
    "3696627": 1,
    "3766110": 1,
    "3806582": 1,
    "3814153": 1,
    "3882054": 3,
    "4557963": 1,
    "4755450": 3,
    "6719686": 1,
    "7431082": 1,
    "7483388": 3,
    "7958862": 1,
    "10717843": 1,
    "12643399": 1,
    "13614891": 1,
    "13846713": 1,
    "19131678": 2,
    "19140125": 1,
    "19441281": 1,
    "25418227": 1,
    "25753806": 2,
    "37591724": 1,
    "51989311": 1,
    "52042083": 1,
    "52110037": 2,
    "53082197": 2,
    "53775725": 1,
    "54444869": 1,
    "56472583": 1,
    "73729352": 1,
    "86631164": 1,
    "88523916": 2,
    "128363050": 1,
    "195833229": 1,
    "196173551": 1,
    "196187271": 1,
    "196204964": 1,
    "201142785": 1,
    "202540096": 2,
    "202541491": 1,
    "202749877": 1,
    "202776155": 1,
    "203158029": 1,
    "203658320": 1,
    "204076452": 1,
    "204576077": 1,
    "204788875": 1,
    "207163173": 3,
    "207852450": 2,
    "207880490": 1,
    "208291464": 2,
    "209318312": 1,
    "210064217": 1,
    "212848342": 1,
    "214063723": 1,
    "214076868": 1,
    "214612503": 1,
    "214802539": 1,
    "218589276": 1,
    "218862816": 1,
    "219295273": 1,
    "220047862": 1,
    "220886470": 1,
    "221090697": 1,
    "221192811": 1,
    "221280346": 1,
    "221819493": 1,
    "221839825": 1,
    "221954184": 1,
    "222133165": 1,
    "222208985": 1,
    "224769806": 1,
    "226203042": 1,
    "227231162": 1,
    "227239576": 2,
    "229180723": 2,
    "229213013": 2,
    "229339845": 2,
    "229377193": 1,
    "229703490": 1,
    "233224480": 1,
    "233303189": 1,
    "233347189": 2,
    "233481294": 3,
    "235306387": 2,
    "235324797": 1,
    "235422273": 2,
    "235614395": 1,
    "235703270": 1,
    "235792443": 1,
    "236279449": 1,
    "236349787": 1,
    "236979995": 1,
    "237454564": 1,
    "237571654": 1,
    "237785556": 1,
    "240417169": 1,
    "243696417": 1,
    "244119616": 1,
    "244119769": 1,
    "244222941": 2,
    "244894903": 1,
    "244946393": 1,
    "245144534": 1,
    "245313643": 1,
    "245395720": 1,
    "245648744": 1,
    "246026190": 1,
    "246063616": 1,
    "246608097": 1,
    "246671483": 1,
    "246863638": 1,
    "247244896": 1,
    "248524814": 6,
    "248545885": 1,
    "248729692": 1,
    "248798765": 1,
    "248986385": 1,
    "249109773": 1,
    "250289207": 1,
    "250511662": 1,
    "250562885": 1,
    "250635180": 1,
    "251086467": 1,
    "251294990": 1,
    "251594533": 1,
    "251708766": 1,
    "251718718": 1,
    "251719495": 1,
    "251779418": 1,
    "252015770": 1,
    "252070677": 1,
    "252090252": 1,
    "252292167": 1,
    "252364875": 1,
    "252564311": 1,
    "252625996": 1,
    "252716986": 1,
    "252819373": 1,
    "252846570": 1,
    "253117165": 1,
    "253157669": 1,
    "253384318": 1,
    "253386566": 1,
    "253448309": 1,
    "253761315": 1,
    "254096435": 1,
    "254205278": 1,
    "256597814": 1,
    "256598084": 1,
    "256610018": 1,
    "257220329": 3,
    "257771370": 1,
    "258219651": 1,
    "258240832": 1,
    "258264587": 2,
    "258281574": 1,
    "258333655": 2,
    "258363396": 1,
    "258423071": 1,
    "258436828": 1,
    "258440205": 1,
    "258440388": 1,
    "258457252": 1,
    "258686648": 1,
    "258714753": 1,
    "258762793": 1,
    "259341618": 1,
    "259630548": 1,
    "259858848": 1,
    "259936842": 1,
    "260611397": 1,
    "261214582": 1,
    "261431541": 1,
    "262690390": 3,
    "263147330": 1,
    "263807455": 1,
    "263830580": 1,
    "264172465": 1,
    "264350156": 1,
    "266149723": 1,
    "266166905": 2,
    "266201568": 1,
    "266231035": 1,
    "266550719": 1,
    "266551134": 1,
    "266681126": 1,
    "266755771": 1,
    "266933295": 1,
    "267180466": 1,
    "267627414": 1,
    "267751000": 1,
    "267751414": 1,
    "268032370": 1,
    "268157585": 1,
    "268717837": 1,
    "268856632": 1,
    "268860898": 1,
    "269100293": 1,
    "269157470": 1,
    "269580299": 1,
    "270711106": 2,
    "270990971": 1,
    "271003412": 1,
    "271961547": 1,
    "272884970": 1,
    "273037266": 1,
    "10328909": 2,
    "52158102": 1,
    "53199920": 3,
    "221397171": 2,
    "237418303": 1,
    "237453242": 1,
    "263895473": 2,
    "577805": 2,
    "3226443": 1,
    "4437414": 1,
    "5958691": 1,
    "7278297": 3,
    "9095914": 1,
    "11223539": 1,
    "13955854": 1,
    "21883095": 1,
    "52056218": 1,
    "53080423": 1,
    "56657926": 1,
    "56915438": 1,
    "59316623": 2,
    "59821525": 1,
    "76663467": 9,
    "108296188": 1,
    "207847719": 3,
    "211003696": 1,
    "211123242": 1,
    "212737039": 3,
    "214198198": 1,
    "219573823": 1,
    "221082536": 1,
    "221083274": 1,
    "222177069": 1,
    "232135385": 1,
    "232269660": 1,
    "233324265": 1,
    "235845598": 2,
    "245502155": 1,
    "246828738": 1,
    "247451243": 1,
    "250630286": 1,
    "250631397": 1,
    "252683295": 1,
    "252904804": 1,
    "252905085": 1,
    "252992488": 1,
    "738850": 1,
    "3753452": 2,
    "9665943": 1,
    "14521054": 1,
    "14843884": 3,
    "29150617": 1,
    "46935302": 1,
    "57375753": 1,
    "67856593": 1,
    "208138178": 1,
    "2237901": 1,
    "7197134": 2,
    "8701238": 1,
    "26517743": 1,
    "40114756": 1,
    "51880810": 1,
    "57573854": 1,
    "102350405": 1,
    "204960442": 1,
    "206596979": 1,
    "208202400": 1,
    "209376177": 1,
    "214775220": 1,
    "216080778": 1,
    "231741093": 1,
    "233387838": 1,
    "235694438": 1,
    "246240170": 1,
    "1957433": 4,
    "2332513": 1,
    "5276660": 1,
    "5575601": 1,
    "11117517": 1,
    "16414666": 5,
    "40027675": 1,
    "54458106": 1,
    "108296442": 1,
    "195847902": 1,
    "209532101": 1,
    "215754564": 1,
    "2622646": 1,
    "26419490": 1,
    "127986044": 1,
    "215737187": 1,
    "220919723": 1,
    "224722163": 1,
    "225883893": 1,
    "244772950": 1,
    "245218982": 1,
    "247450969": 1,
    "249284566": 1,
    "251371732": 1,
    "251765079": 1,
    "253479599": 1,
    "254877499": 1,
    "258615731": 1,
    "258822888": 1,
    "259204112": 1,
    "259360665": 1,
    "262138540": 1,
    "263310448": 1,
    "263671998": 1,
    "265067168": 1,
    "265455405": 1,
    "265610070": 1,
    "267335111": 1,
    "267412232": 1,
    "267626929": 1,
    "268531479": 1,
    "269005493": 1,
    "269293311": 1,
    "269740933": 1,
    "270357492": 1,
    "271161780": 1,
    "271218596": 1,
    "273662191": 1,
    "274234014": 1,
    "276317958": 1,
    "276742400": 1,
    "278166534": 1,
    "220599074": 1,
    "224888856": 1,
    "2209131": 1,
    "3144218": 5,
    "3180429": 3,
    "3994012": 1,
    "7748515": 2,
    "10347107": 2,
    "21277943": 1,
    "52304560": 2,
    "3292002": 6,
    "3593775": 1,
    "9697423": 1,
    "12255087": 1,
    "26071662": 1,
    "52895589": 1,
    "95861012": 1,
    "133608068": 1,
    "169032532": 2,
    "201646309": 4,
    "212748233": 1,
    "221364718": 1,
    "231963511": 1,
    "231981018": 1,
    "235166820": 1,
    "247115730": 1,
    "247677346": 1,
    "254366618": 1,
    "255083414": 1,
    "256549502": 1,
    "257039063": 1,
    "257122091": 1,
    "257219404": 2,
    "257326550": 1,
    "257378479": 2,
    "257532815": 1,
    "258519932": 1,
    "258573462": 1,
    "259692010": 1,
    "260164663": 1,
    "260722930": 1,
    "263777406": 1,
    "264172683": 1,
    "265189794": 1,
    "265871676": 1,
    "266412890": 1,
    "267938234": 1,
    "271334161": 1,
    "271916060": 1,
    "274859421": 1,
    "1082740": 1,
    "1560943": 1,
    "4193919": 1,
    "5583509": 1,
    "8770925": 1,
    "9492646": 1,
    "9672033": 1,
    "14061443": 1,
    "15512280": 1,
    "27837890": 1,
    "122166830": 1,
    "212628625": 1,
    "213176935": 1,
    "219615944": 1,
    "222278217": 1,
    "233024902": 1,
    "255105361": 1,
    "2141740": 1,
    "3365209": 1,
    "3441497": 1,
    "4714433": 2,
    "6941275": 1,
    "12856358": 1,
    "14113767": 3,
    "49420146": 1,
    "53036732": 1,
    "201070217": 1,
    "266028051": 2,
    "14124313": 10,
    "52967399": 13,
    "249625869": 1,
    "253018586": 1,
    "264492337": 1,
    "265150374": 2,
    "267898006": 2,
    "270710718": 1,
    "275054439": 1,
    "9316331": 1,
    "52284222": 1,
    "237363704": 1,
    "246426909": 1,
    "254220898": 1,
    "257219550": 1,
    "259203574": 1,
    "261064713": 1,
    "266359151": 1,
    "267782436": 1,
    "268032947": 1,
    "4492210": 3,
    "85528899": 1,
    "158046772": 3,
    "206593880": 2,
    "207718082": 1,
    "219182395": 1,
    "235306349": 1,
    "238158425": 1,
    "248367577": 1,
    "268718050": 1,
    "272331125": 1,
    "2272015": 1,
    "14494942": 1,
    "14728290": 1,
    "52815006": 1,
    "52841398": 1,
    "54465873": 1,
    "85528598": 1,
    "173991173": 2,
    "189898023": 1,
    "202558968": 1,
    "209515395": 1,
    "211572655": 1,
    "218487288": 1,
    "218900797": 1,
    "220280200": 1,
    "229923949": 1,
    "232335877": 1,
    "235349192": 1,
    "235489690": 1,
    "235592814": 1,
    "758237": 1,
    "1399322": 1,
    "2924682": 1,
    "10363459": 1,
    "22177955": 1,
    "195347831": 1,
    "195908774": 1,
    "206594383": 1,
    "207061473": 1,
    "207178704": 1,
    "208248243": 1,
    "231572861": 1,
    "261076072": 1,
    "263605944": 1,
    "263610099": 1,
    "1430801": 1,
    "2210455": 1,
    "17682909": 1,
    "54460890": 2,
    "202719040": 1,
    "202734445": 1,
    "226096901": 3,
    "235097195": 1,
    "235692795": 3,
    "33389032": 1,
    "35467721": 1,
    "40176582": 1,
    "59291975": 1,
    "102352093": 3,
    "207997778": 1,
    "232061871": 1,
    "263873151": 1,
    "195657814": 1,
    "203626972": 1,
    "204960716": 3,
    "219182397": 1,
    "230435739": 1,
    "231573431": 1,
    "235417196": 1,
    "236635565": 1,
    "237416585": 1,
    "239016062": 1,
    "240420063": 1,
    "247618722": 1,
    "256503897": 1,
    "256868484": 1,
    "259095695": 1,
    "260435365": 1,
    "261016305": 1,
    "263227026": 1,
    "270764307": 1,
    "201666793": 1,
    "202573071": 1,
    "216641856": 1,
    "256436290": 1,
    "256827430": 1,
    "259309148": 1,
    "291713": 1,
    "11319376": 1,
    "20480879": 1,
    "21688777": 1,
    "21698461": 1,
    "212414835": 1,
    "1998416": 1,
    "10073982": 1,
    "31319559": 1,
    "52919654": 1,
    "54458229": 1,
    "623013": 1,
    "2239473": 1,
    "11202498": 4,
    "19135805": 2,
    "238419331": 1,
    "248525030": 1,
    "252280329": 1,
    "6706414": 1,
    "9059612": 1,
    "224281034": 3,
    "258298672": 2,
    "3104920": 2,
    "8355505": 1,
    "13145195": 1,
    "13618178": 1,
    "19153102": 1,
    "202888986": 1,
    "214743601": 1,
    "231879586": 1,
    "233444011": 1,
    "235306301": 1,
    "236428622": 1,
    "247011309": 1,
    "715463": 2,
    "2315434": 1,
    "2424223": 2,
    "2493017": 1,
    "6425394": 1,
    "6483070": 1,
    "7062707": 3,
    "7352553": 1,
    "7732372": 1,
    "7771402": 1,
    "8505367": 1,
    "10282227": 1,
    "13296639": 1,
    "13755946": 1,
    "14040310": 1,
    "14217450": 1,
    "14775471": 2,
    "15397918": 1,
    "16619709": 2,
    "38582742": 1,
    "49867191": 1,
    "91184120": 2,
    "201066287": 1,
    "201871273": 1,
    "202565460": 1,
    "202782699": 1,
    "204402762": 1,
    "206751218": 1,
    "208547698": 1,
    "211532586": 1,
    "213755659": 1,
    "215746363": 1,
    "215754208": 1,
    "218551030": 2,
    "220265934": 3,
    "221703022": 1,
    "222291117": 1,
    "224291855": 2,
    "229924402": 3,
    "233296711": 1,
    "233375020": 1,
    "233444273": 1,
    "234685749": 1,
    "235270502": 1,
    "236428934": 1,
    "239011558": 1,
    "244117525": 2,
    "244920947": 1,
    "246634906": 1,
    "246938066": 1,
    "248779998": 2,
    "254054622": 1,
    "3608725": 1,
    "53235839": 1,
    "201881176": 1,
    "230433941": 1,
    "236273668": 1,
    "265051720": 1,
    "202121966": 2,
    "221193809": 7,
    "221995513": 5,
    "245904709": 2,
    "251117920": 1,
    "251518434": 1,
    "251684399": 1,
    "255340818": 1,
    "260334664": 2,
    "262464639": 1,
    "652286": 1,
    "7164502": 1,
    "18268744": 1,
    "245634781": 2,
    "262466164": 1,
    "12161567": 1,
    "219530451": 1,
    "221655683": 1,
    "221970271": 1,
    "225040610": 1,
    "260124505": 1,
    "2887257": 1,
    "4929980": 1,
    "30164212": 1,
    "174802832": 1,
    "196205749": 1,
    "198354047": 1,
    "202583325": 2,
    "202770936": 1,
    "237420821": 1,
    "252819516": 1,
    "252907242": 1,
    "235358423": 1,
    "261329076": 1,
    "263830084": 1,
    "264492372": 1,
    "2407601": 2,
    "53082628": 1,
    "129946212": 1,
    "196172975": 1,
    "202712648": 1,
    "214714259": 1,
    "222177127": 1,
    "232404011": 1,
    "248227575": 1,
    "352650": 1,
    "198953378": 2,
    "226246289": 2,
    "233296292": 1,
    "236459932": 1,
    "252351757": 1,
    "259949770": 1,
    "259949942": 1,
    "264492774": 1,
    "265213194": 1,
    "268692718": 1,
    "269457193": 1,
    "273164062": 1,
    "276117175": 1,
    "271104595": 1,
    "221191635": 1,
    "237332868": 1,
    "259165073": 1,
    "261158362": 1,
    "264584799": 1,
    "4336741": 1,
    "16407324": 2,
    "204838007": 1,
    "237353134": 1,
    "253790338": 1,
    "5625629": 2,
    "20401422": 1,
    "49298988": 1,
    "51972201": 1,
    "189761996": 1,
    "201701022": 1,
    "207168823": 1,
    "207178741": 1,
    "211043589": 1,
    "220730030": 1,
    "235829175": 1,
    "245502839": 1,
    "246015673": 1,
    "251518175": 1,
    "252070586": 3,
    "252519575": 1,
    "258714901": 1,
    "3401524": 1,
    "84843937": 1,
    "255393926": 1,
    "12544231": 1,
    "14136367": 1,
    "15810061": 1,
    "211010432": 1,
    "224271811": 1,
    "244527692": 1,
    "256631072": 1,
    "10319744": 3,
    "14083350": 1,
    "51935625": 1,
    "216078090": 1,
    "220425719": 1,
    "231592822": 1,
    "54459095": 1,
    "214714330": 1,
    "216056360": 1,
    "116132630": 1,
    "211204736": 1,
    "213801097": 1,
    "243865362": 1,
    "249062918": 1,
    "15912887": 2,
    "57246310": 2,
    "69930495": 1,
    "86510052": 1,
    "8592977": 1,
    "11198605": 1,
    "62841652": 1,
    "86505256": 1,
    "174797737": 2,
    "222278475": 2,
    "5378837": 1,
    "11091552": 1,
    "19370455": 1,
    "44113572": 1,
    "215828509": 1,
    "226601508": 1,
    "232417873": 1,
    "238244608": 1,
    "245788482": 1,
    "1055111": 2,
    "2724321": 1,
    "3120635": 1,
    "8454173": 1,
    "8517067": 3,
    "10585115": 2,
    "11080756": 1,
    "14068874": 1,
    "108294843": 1,
    "206592766": 2,
    "215814392": 1,
    "222278716": 1,
    "231632752": 1,
    "233689007": 1,
    "237431500": 1,
    "252782756": 1,
    "67700681": 1,
    "85566873": 1,
    "203704895": 1,
    "213399202": 1,
    "216535344": 1,
    "219071361": 1,
    "219330527": 1,
    "220543075": 1,
    "225287688": 1,
    "228971082": 1,
    "232023070": 1,
    "179895": 1,
    "7079167": 1,
    "44075854": 1,
    "195779694": 1,
    "1900911": 1,
    "267802767": 1,
    "144167": 1,
    "4951598": 1,
    "4973991": 1,
    "7284112": 1,
    "8350875": 1,
    "14928728": 1,
    "36593804": 1,
    "46990556": 1,
    "52164698": 1,
    "53597419": 1,
    "57880229": 1,
    "60600755": 1,
    "159041743": 1,
    "195347125": 1,
    "195441316": 1,
    "195697699": 1,
    "201698324": 1,
    "202786261": 1,
    "203598995": 1,
    "204837691": 1,
    "207756951": 1,
    "209415079": 1,
    "210873091": 1,
    "245934348": 1,
    "216144439": 2,
    "2633248": 1,
    "3389583": 1,
    "3919301": 1,
    "9815689": 1,
    "13838003": 1,
    "14298338": 1,
    "29944424": 1,
    "49901898": 1,
    "52183483": 1,
    "67748132": 1,
    "69990369": 1,
    "70038990": 1,
    "146121289": 1,
    "198952485": 1,
    "203056539": 1,
    "204766590": 1,
    "211252408": 1,
    "211572518": 1,
    "214802049": 1,
    "215746373": 1,
    "216553528": 1,
    "216553689": 1,
    "218487793": 1,
    "218528456": 1,
    "218599990": 1,
    "219471191": 1,
    "220541874": 1,
    "221355139": 1,
    "227191214": 1,
    "235689895": 1,
    "706860": 1,
    "1023605": 2,
    "9505704": 1,
    "16447573": 1,
    "206592218": 1,
    "218470438": 1,
    "202785103": 1,
    "207758781": 1,
    "209376561": 1,
    "218889832": 1,
    "218923747": 1,
    "393948": 1,
    "588863": 1,
    "3582486": 1,
    "7547770": 1,
    "8722811": 1,
    "10442573": 1,
    "10910955": 1,
    "21715202": 1,
    "73728465": 1,
    "103802785": 1,
    "28913990": 1,
    "49299019": 1,
    "208176414": 1,
    "225076220": 1,
    "234950071": 2,
    "248108992": 1,
    "254564635": 1,
    "257697222": 1,
    "2127100": 2,
    "67413369": 2,
    "207930212": 2,
    "212827903": 1,
    "240070688": 1,
    "249538526": 1,
    "252518772": 1,
    "254221022": 1,
    "258333674": 1,
    "258333851": 1,
    "926364": 1,
    "6095318": 1,
    "6334682": 1,
    "51876975": 2,
    "202889175": 1,
    "219573512": 1,
    "232417264": 1,
    "237386166": 2,
    "246823486": 1,
    "250627337": 1,
    "252668458": 1,
    "13716346": 1,
    "18553623": 1,
    "28641153": 1,
    "36279912": 1,
    "57755729": 1,
    "202144421": 1,
    "214466795": 1,
    "220128101": 1,
    "221385483": 1,
    "226415469": 1,
    "229653947": 1,
    "235125956": 1,
    "238687946": 1,
    "247362647": 1,
    "248505867": 1,
    "252450639": 1,
    "252839708": 1,
    "253251385": 1,
    "253354759": 1,
    "253555095": 1,
    "253901615": 1,
    "254034416": 1,
    "254154806": 1,
    "256780457": 1,
    "258007950": 1,
    "207178809": 1,
    "231846408": 1,
    "250340301": 1,
    "2495502": 1,
    "7040223": 1,
    "7409058": 1,
    "12476041": 1,
    "13919896": 1,
    "18117752": 1,
    "247857460": 1,
    "251007195": 1,
    "265094990": 1,
    "199466173": 1,
    "210971227": 1,
    "218571035": 1,
    "189900": 1,
    "1012652": 1,
    "6392154": 1,
    "6655663": 1,
    "7847519": 1,
    "12726540": 1,
    "15152621": 1,
    "30698343": 1,
    "52901171": 1,
    "59291937": 1,
    "73489935": 1,
    "153313270": 1,
    "195892710": 1,
    "207190913": 1,
    "207198269": 1,
    "215827489": 1,
    "218870205": 1,
    "230770066": 1,
    "237494860": 1,
    "1780254": 1,
    "4379400": 1,
    "4593810": 1,
    "119284150": 1,
    "231802355": 1,
    "235593404": 1,
    "237291550": 1,
    "246411402": 1,
    "248512473": 1,
    "250264976": 1,
    "251710138": 1,
    "251719655": 1,
    "661123": 1,
    "1128753": 1,
    "15091093": 1,
    "70289226": 1,
    "102352623": 1,
    "203163196": 1,
    "232335647": 1,
    "232380138": 1,
    "234951345": 1,
    "235829068": 1,
    "236361803": 1,
    "249191733": 1,
    "252782215": 1,
    "253801768": 1,
    "256868818": 1,
    "257729173": 1,
    "258756463": 1,
    "258762390": 1,
    "258967414": 1,
    "266469822": 1,
    "268063320": 1,
    "268357641": 1,
    "1629541": 1,
    "1644335": 1,
    "3225424": 1,
    "3333648": 1,
    "10137425": 1,
    "12350611": 1,
    "17265929": 1,
    "17719760": 1,
    "33753227": 1,
    "44131945": 1,
    "53845347": 1,
    "53957733": 2,
    "56895382": 1,
    "196202217": 1,
    "206594738": 1,
    "207169186": 1,
    "209097551": 1,
    "221150486": 1,
    "225419066": 1,
    "235421836": 1,
    "236293464": 1,
    "237513894": 1,
    "239771829": 1,
    "52171640": 1,
    "201124533": 1,
    "225040315": 1,
    "232417534": 1,
    "248965170": 1,
    "257767382": 1,
    "258733774": 1,
    "7668308": 1,
    "10832728": 1,
    "23672393": 1,
    "30459525": 1,
    "218806820": 1,
    "221492015": 1,
    "222278304": 1,
    "222278652": 1,
    "235792531": 1,
    "246926460": 1,
    "950292": 2,
    "3864050": 1,
    "6659365": 1,
    "52115700": 1,
    "141321709": 1,
    "227238996": 1,
    "235235583": 1,
    "239054406": 1,
    "249192060": 1,
    "254853727": 1,
    "15928602": 1,
    "195440283": 1,
    "214728271": 1,
    "219964813": 1,
    "233004700": 1,
    "236234758": 1,
    "1103216": 1,
    "3896491": 1,
    "5083989": 1,
    "5205529": 1,
    "7900381": 1,
    "8281592": 1,
    "51608183": 1,
    "53172956": 1,
    "54447939": 1,
    "195441339": 1,
    "204837049": 1,
    "212725353": 1,
    "216080787": 1,
    "226202653": 1,
    "232153047": 1,
    "238419264": 1,
    "254097121": 1,
    "254097189": 1,
    "261514205": 1,
    "948039": 1,
    "11336213": 1,
    "50766964": 1,
    "53037206": 1,
    "54527549": 1,
    "59379420": 1,
    "205228801": 1,
    "220977071": 1,
    "235324796": 1,
    "235376996": 1,
    "237441141": 1,
    "247595191": 1,
    "253098669": 1,
    "2158023": 1,
    "2858079": 1,
    "3632923": 1,
    "8310135": 1,
    "12740621": 1,
    "15795805": 1,
    "16119010": 1,
    "102485964": 1,
    "206618067": 1,
    "208176418": 1,
    "215415914": 1,
    "340063": 1,
    "57814228": 1,
    "216562330": 1,
    "222310337": 1,
    "222341612": 1,
    "235248430": 1,
    "247084444": 1,
    "216009": 1,
    "1017389": 1,
    "3603249": 1,
    "6839244": 1,
    "102483628": 1,
    "145050804": 1,
    "233296845": 1,
    "239011786": 1,
    "236567496": 1,
    "259165040": 1,
    "259837088": 1,
    "261065787": 1,
    "261101015": 1,
    "265656168": 1,
    "309759": 1
  },
  "merged_dataset_groups": [
    {
      "display_name": "DBpedia",
      "normalized_name": "dbpedia",
      "name_variants": [
        "DBPEDIA",
        "DBPedia",
        "DBpedia"
      ],
      "mention_count": 38,
      "cited_papers_count": 25,
      "topic_summary": "DBpedia is used to construct and enhance multi-modal knowledge graphs by integrating structured information from Wikipedia, including numeric literals and image data. It supports multi-modal reasoning by linking visual and textual information, and is employed in knowledge-driven applications such as recommendation systems, information retrieval, and machine learning. The dataset helps address the incompleteness in knowledge graphs, particularly the lack of associated images for entities, and enriches the semantic network with commonsense knowledge."
    },
    {
      "display_name": "Visual Genome",
      "normalized_name": "visualgenome",
      "name_variants": [
        "Visual Genome",
        "Visual-Genome",
        "VisualGenome"
      ],
      "mention_count": 21,
      "cited_papers_count": 13,
      "topic_summary": "The Visual Genome dataset is used extensively in multi-modal reasoning tasks, particularly for visual-relational reasoning. It provides dense image annotations, including objects, attributes, and relationships, which are used to train models for object recognition, part detection, and attribute identification. The dataset supports the generation of scene graphs and the extraction of visual features using pretrained models like Faster-RCNN and ResNet-152. It enhances the ability of models to detect and recognize visual relationships, compositional phrases, and complex scenes, often through fine-tuning and iterative message passing techniques. The dataset also serves as a benchmark for evaluating scene graph generation and addressing dataset biases in relationship prediction."
    },
    {
      "display_name": "Freebase",
      "normalized_name": "freebase",
      "name_variants": [
        "FreeBASE",
        "FreeBase",
        "Freebase"
      ],
      "mention_count": 20,
      "cited_papers_count": 15,
      "topic_summary": "Freebase is used to construct large-scale structured knowledge bases through both manual annotation and automatic extraction from unstructured data, focusing on integrating structured data with textual and visual information. It supports AI applications, including multi-modal knowledge graph reasoning, by providing a rich, interconnected dataset of human knowledge. Freebase facilitates the development and evaluation of question answering systems and enhances semantic understanding and reasoning over entities, addressing issues such as sparsity in knowledge graphs."
    },
    {
      "display_name": "ImageNet",
      "normalized_name": "imagenet",
      "name_variants": [
        "ImageNet"
      ],
      "mention_count": 16,
      "cited_papers_count": 13,
      "topic_summary": "ImageNet is primarily used for pretraining deep learning models like ResNet and Faster-RCNN for convolutional feature extraction, focusing on large-scale hierarchical image classification and regional-level feature extraction. It enhances multi-modal reasoning by connecting language and vision through dense image annotations. Additionally, it serves as a robust dataset for few-shot classification tasks across various domains, including action recognition, pet species, aircraft, car, and food category recognition, validating models like GraphAdapter. Pretrained models on ImageNet are also utilized for image-text matching and cross-modal representation learning, leveraging existing visual recognition capabilities."
    },
    {
      "display_name": "FB15K-237",
      "normalized_name": "fb15k237",
      "name_variants": [
        "FB15K-237",
        "FB15k-237"
      ],
      "mention_count": 16,
      "cited_papers_count": 15,
      "topic_summary": "The FB15K-237 dataset is primarily used to evaluate and train models for knowledge graph completion tasks, focusing on link prediction, entity linking, and relation prediction. It is employed in both single-modal and multi-modal settings, where it enhances models with visual and textual data for improved reasoning. The dataset is used to assess model performance using metrics like MRR and hits at k=10, and it supports the evaluation of various knowledge graph embedding models, including R-GCN, RotatE, DistMult, and ComplEx. It is also utilized to test zero-shot learning capabilities and to benchmark against baselines in multi-relational data modeling."
    },
    {
      "display_name": "ConceptNet",
      "normalized_name": "conceptnet",
      "name_variants": [
        "ConceptNet"
      ],
      "mention_count": 15,
      "cited_papers_count": 13,
      "topic_summary": "ConceptNet is used to enhance multi-modal reasoning in visual question answering (VQA) systems by providing structured knowledge and dense image annotations. It connects visual and textual data, offering scene graphs and part-whole relationships to enrich the models' understanding of entities and their interactions. The dataset serves as a knowledge base, integrating commonsense and factual information from Wikipedia, and is used to train and evaluate VQA models, improving their ability to answer complex questions accurately."
    },
    {
      "display_name": "DBP15K",
      "normalized_name": "dbp15k",
      "name_variants": [
        "DBP15K",
        "DBP15k"
      ],
      "mention_count": 14,
      "cited_papers_count": 10,
      "topic_summary": "The DBP15K dataset is primarily used for evaluating and demonstrating the stability and performance of entity alignment methods in multi-modal knowledge graphs. It focuses on aligning entities across different languages and modalities, using joint attribute-preserving embedding techniques. The dataset is utilized to train and evaluate models with various vision encoders (e.g., ResNet-152, CLIP, VGG-16) and different vision feature dimensions (512, 2048, 4096). Research applications include cross-lingual entity alignment, monolingual entity alignment, and link prediction tasks, with performance assessed through metrics like Hits@N, precision, recall, and F1 scores. The dataset supports the evaluation of model stability and alignment accuracy across bilingual, monolingual, and high-degree categories, and is used to optimize embedding dimensions for better alignment performance."
    },
    {
      "display_name": "MSCOCO",
      "normalized_name": "mscoco",
      "name_variants": [
        "MS COCO",
        "MS-COCO",
        "MSCOCO"
      ],
      "mention_count": 13,
      "cited_papers_count": 8,
      "topic_summary": "The MSCOCO dataset is extensively used for pre-training and evaluating vision-and-language models, focusing on tasks such as image captioning, object detection, and visual question answering. It provides a large set of annotated images and captions, enabling multi-modal reasoning by linking textual descriptions to visual content. The dataset supports rich contextual information and diverse object classes, facilitating the training of detectors and the evaluation of cross-modal alignment and retrieval performance."
    },
    {
      "display_name": "WN9-IMG",
      "normalized_name": "wn9img",
      "name_variants": [
        "WN9-IMG",
        "WN9IMG"
      ],
      "mention_count": 12,
      "cited_papers_count": 8,
      "topic_summary": "The WN9-IMG dataset is primarily used for multi-modal knowledge graph reasoning, focusing on integrating visual and textual information to enhance model performance. It is utilized for tasks such as image-embodied knowledge representation learning, multimodal translation-based knowledge graph representation, and generating negative instances for evaluation. The dataset supports the training and evaluation of models like TransE, TransR, and IKRL, often comparing their performance on metrics like Raw Hits@10. It also aids in establishing baseline performance and comparing statistical properties with other datasets like MarKG."
    },
    {
      "display_name": "Wikidata",
      "normalized_name": "wikidata",
      "name_variants": [
        "WikiData",
        "Wikidata"
      ],
      "mention_count": 11,
      "cited_papers_count": 9,
      "topic_summary": "Wikidata is used as a structured knowledge base to enhance multi-modal reasoning by providing rich entity and relationship information. It integrates diverse human knowledge, supporting tasks such as question answering, recommendation systems, and multimedia reasoning. Specifically, it is used to store and query relational facts, extract entity descriptions, and enhance entity descriptions and relationships in vision/text data. Wikidata's large-scale, collaboratively edited nature, containing 50 million items, makes it a valuable resource for training and evaluating multi-modal reasoning models."
    },
    {
      "display_name": "WordNet",
      "normalized_name": "wordnet",
      "name_variants": [
        "WordNet"
      ],
      "mention_count": 10,
      "cited_papers_count": 8,
      "topic_summary": "WordNet is primarily used as a lexical database to provide semantic relationships and structured vocabulary, enhancing multi-modal reasoning and knowledge graph construction. It initializes ontologies, enriches knowledge graphs, and supports visual semantic role labeling, improving image understanding and context. WordNet's hierarchical structure aids in tasks like image recognition, link prediction, and entity categorization, and it serves as a foundational resource for semantic understanding and never-ending learning."
    },
    {
      "display_name": "ICEWS14",
      "normalized_name": "icews14",
      "name_variants": [
        "ICEWS14"
      ],
      "mention_count": 10,
      "cited_papers_count": 8,
      "topic_summary": "The ICEWS14 dataset is primarily used to evaluate and enhance temporal knowledge graph reasoning models, focusing on event prediction, temporal dynamics, and entity linking. It is employed to test the accuracy of models in forecasting events, handling large-scale global event data, and integrating structured information from multilingual Wikipedia. The dataset's frequent updates (15-minute intervals) and extensive temporal coverage (2005-2015) enable researchers to assess model performance in dynamic environments, emphasizing historical context and long-term trends."
    },
    {
      "display_name": "YAGO",
      "normalized_name": "yago",
      "name_variants": [
        "YAGO"
      ],
      "mention_count": 8,
      "cited_papers_count": 7,
      "topic_summary": "YAGO is used as a general domain knowledge graph, integrating information from WordNet, Wikipedia, and Geonames. It is employed for structured information extraction, entity linking, and enhancing multi-modal reasoning by providing rich, fine-grained entity types and relationships. YAGO supports research in semantic knowledge integration, temporal knowledge graph reasoning, and entity matching across knowledge graphs, serving as a multilingual, extensible ontology."
    },
    {
      "display_name": "VQA",
      "normalized_name": "vqa",
      "name_variants": [
        "VQA",
        "VQA dataset"
      ],
      "mention_count": 8,
      "cited_papers_count": 4,
      "topic_summary": "The VQA dataset is primarily used to train and evaluate visual question answering (VQA) models, focusing on the integration of image and text modalities. It contains 50,000 images and 150,000 questions, with subsets like GQA and VG-QA used for balanced and complex scene understanding tasks. Research applications include multi-modal reasoning, fact-based VQA, and visual commonsense reasoning, emphasizing the model's ability to answer open-ended questions and perform multi-step reasoning over images and knowledge graphs."
    },
    {
      "display_name": "WN18",
      "normalized_name": "wn18",
      "name_variants": [
        "WN18"
      ],
      "mention_count": 7,
      "cited_papers_count": 5,
      "topic_summary": "The WN18 dataset is primarily used to evaluate knowledge graph embedding models, focusing on link and relation prediction tasks. It highlights issues such as test leakage and inverse relations, which researchers address by filtering out redundant information. The dataset is also extended with visual data for multi-modal reasoning and serves as a benchmark for assessing model robustness, scalability, and performance in knowledge graph completion tasks."
    },
    {
      "display_name": "Flickr30k",
      "normalized_name": "flickr30k",
      "name_variants": [
        "Flickr30K",
        "Flickr30k"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "The Flickr30k dataset is primarily used for multi-modal reasoning tasks, focusing on image captioning, object grounding, and referring expression comprehension. It evaluates models' ability to align textual descriptions with visual content, using neural architectures. The dataset contains 31,783 images with multiple captions each, enabling researchers to train, validate, and test image-text matching models, particularly in zero-shot and fine-tuned settings. It is also used for caption retrieval and image retrieval experiments, assessing cross-modal alignment and performance using metrics like R@1."
    },
    {
      "display_name": "FVQA",
      "normalized_name": "fvqa",
      "name_variants": [
        "FVQA",
        "FVQA dataset"
      ],
      "mention_count": 6,
      "cited_papers_count": 3,
      "topic_summary": "The FVQA dataset is used for fact-based visual question answering, integrating visual and textual information to answer questions accurately. It evaluates models on 2,190 images, 5,286 questions, and a knowledge base of 193,449 facts, focusing on multi-modal reasoning and the integration of structured knowledge from multiple sources, including Wikipedia and web-extracted commonsense knowledge. The dataset enhances the model's ability to select correct entities and supports the construction of a knowledge base for visual concepts, aiding in multi-modal reasoning tasks."
    },
    {
      "display_name": "LSMDC",
      "normalized_name": "lsmdc",
      "name_variants": [
        "LSMDC"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "The LSMDC dataset is primarily used to evaluate methods for generating descriptions from multimodal data, focusing on performance metrics such as BLEU, METEOR, ROUGE_L, and CIDEr. It is extensively used to assess the effectiveness of KAGS and KAGS-T models in multi-modal captioning and visual storytelling, emphasizing the integration of visual and textual information. The dataset enables researchers to compare different models and approaches, demonstrating their capabilities in multi-modal reasoning and generation tasks."
    },
    {
      "display_name": "FB15k",
      "normalized_name": "fb15k",
      "name_variants": [
        "FB15K",
        "FB15k"
      ],
      "mention_count": 6,
      "cited_papers_count": 6,
      "topic_summary": "FB15k is primarily used as a benchmark dataset for evaluating knowledge graph completion methods, focusing on link prediction, entity, and relation prediction tasks. It is employed to assess the performance of various models like TransE, TuckER, LowFER, DistMult, Complex, and R-GCN+ in terms of dimensionality, parameter efficiency, and scalability. The dataset supports multi-relational data and is often used in a filtered form to reduce redundancy and improve robustness. It is also utilized for integrating knowledge graphs with textual data and enhancing entity representation through multi-modal reasoning."
    },
    {
      "display_name": "UrbanKG",
      "normalized_name": "urbankg",
      "name_variants": [
        "UrbanKG"
      ],
      "mention_count": 6,
      "cited_papers_count": 4,
      "topic_summary": "The UrbanKG dataset is used for evaluating and comparing the performance of various Knowledge Representation Learning (KRL) models, particularly in multi-modal reasoning within knowledge graphs. It is also utilized for pre-training models like TuckER to assess triplet plausibility and for obtaining embeddings of multi-relational data in urban environments. The dataset, which is manually constructed, supports classical embedding methods and focuses on urban contexts and their relationships."
    },
    {
      "display_name": "SPIQA",
      "normalized_name": "spiqa",
      "name_variants": [
        "SPIQA"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The SPIQA dataset is used to evaluate and fine-tune multimodal models, such as InstructBLIP and LLaVA 1.5, focusing on their comprehension and reasoning abilities across different modalities. It assesses model performance in understanding and reasoning with reference images, questions, and answers, using simple QA prompts. This dataset enables researchers to benchmark and improve the multi-modal reasoning capabilities of AI models."
    },
    {
      "display_name": "VG150",
      "normalized_name": "vg150",
      "name_variants": [
        "VG150"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The VG150 dataset is primarily used for training and evaluating scene graph generation methods, focusing on improving relationship detection in images through iterative message passing and frequency-counting techniques. It serves as a benchmark for cognitive tasks like captioning and VQA, emphasizing the integration of visual and textual information. The dataset enables researchers to assess and compare the performance of different models in representing relationships within visual scenes."
    },
    {
      "display_name": "WN18RR",
      "normalized_name": "wn18rr",
      "name_variants": [
        "WN18RR",
        "WN18RR++"
      ],
      "mention_count": 5,
      "cited_papers_count": 4,
      "topic_summary": "The WN18RR dataset is primarily used in multimodal knowledge graph completion research, serving as a benchmark to evaluate the performance of multi-relational data modeling and link prediction models. It addresses test leakage issues present in WN18, enhancing reliability. Researchers use it to assess performance metrics like MR, MRR, and Hit@1, Hit@3, Hit@10, focusing on reasoning capabilities and the complexity of relations in knowledge graphs. The dataset is also utilized to construct VTKG datasets by merging with other visual commonsense datasets, requiring significant effort to align vocabularies and contextual semantics."
    },
    {
      "display_name": "AM",
      "normalized_name": "am",
      "name_variants": [
        "AM",
        "AM dataset"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The AM dataset is used for evaluating and training models in multi-relational data modeling and knowledge graph reasoning. It focuses on tasks such as relation prediction, entity and relation embeddings, and link prediction. The dataset includes RDF triples, Freebase relations, and WordNet 3.0 triplets, enabling researchers to assess models like CompGCN, KE-GCN, TransE, and QuatE using metrics like P@1, P@5, and N@5. It is also used to study multi-modal reasoning in cultural artifacts at the Amsterdam Museum, highlighting its versatility in both semantic and cultural contexts."
    },
    {
      "display_name": "FetaQA",
      "normalized_name": "fetaqa",
      "name_variants": [
        "FetaQA"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The FetaQA dataset is used to evaluate fine-grained entity typing and question answering on tables. It emphasizes the recognition of entities and their relationships within tabular data, enabling researchers to assess the accuracy and effectiveness of models in understanding and processing complex table structures. This dataset supports research in improving entity recognition and relational reasoning in table-based information retrieval systems."
    },
    {
      "display_name": "IMGpedia",
      "normalized_name": "imgpedia",
      "name_variants": [
        "IMGpedia"
      ],
      "mention_count": 4,
      "cited_papers_count": 2,
      "topic_summary": "IMGpedia is used to form a large-scale multi-modal knowledge graph, primarily focusing on content-based analysis of Wikimedia images. It supports multi-modal reasoning by incorporating visual and textual information, enabling research that explores image-to-image and image-to-text relationships. This dataset facilitates visual and semantic analysis, enhancing understanding in multi-modal knowledge graph reasoning."
    },
    {
      "display_name": "IKRL",
      "normalized_name": "ikrl",
      "name_variants": [
        "IKRL"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The IKRL dataset is used in research to compare methods for multi-modal knowledge graph reasoning, specifically focusing on the integration of images as extra attributes. It enables researchers to evaluate and contrast approaches that incorporate visual data in knowledge representation learning, enhancing the understanding of image-embodied knowledge."
    },
    {
      "display_name": "FB-IMG",
      "normalized_name": "fbimg",
      "name_variants": [
        "FB-IMG"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The FB-IMG dataset is used for image-embodied knowledge representation learning, integrating visual and textual information to enhance multimodal reasoning. It combines pre-trained word2vec for text and VGG-m-128CNN embeddings for images, facilitating multimodal translation-based knowledge graph representation learning. This dataset benefits downstream tasks such as semantic analysis, question-answer systems, and machine comprehension by providing a rich integration of visual and textual data. Its larger size compared to WN9-IMG speeds up training and produces 128-dimensional image embeddings, enhancing model performance."
    },
    {
      "display_name": "English Wikipedia",
      "normalized_name": "englishwikipedia",
      "name_variants": [
        "English Wikipedia",
        "English Wikipedia corpus"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The English Wikipedia dataset is primarily used for pre-training models in natural language processing tasks, providing a large corpus of 2,500M words and 160GB of text data. It enhances masked language modeling objectives, capturing diverse linguistic patterns. The dataset is also used to evaluate models on entity typing, focusing on nine general entity types, which contributes to multi-modal knowledge graph reasoning. This extensive text resource ensures fair comparisons with previous models and improves the effectiveness of pre-training approaches."
    },
    {
      "display_name": "MIRFlickr",
      "normalized_name": "mirflickr",
      "name_variants": [
        "MIRFlickr"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The MIRFlickr dataset is primarily used for evaluating image tagging and enhancing multi-modal knowledge graph reasoning. It contains 25,000 images with an average of 12.66 to 17 tags per image, which researchers use to focus on multi-label classification performance and to disambiguate concepts, improving the relationship between tags and images. This dataset enables detailed assessments of image tagging accuracy and the effectiveness of multi-modal approaches in knowledge graph reasoning."
    },
    {
      "display_name": "YFCC100M",
      "normalized_name": "yfcc100m",
      "name_variants": [
        "YFCC-100M",
        "YFCC100M"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The YFCC100M dataset is primarily used to train vision-language models, leveraging large-scale image-text pairs for both natural language and noisy text supervision. It is also employed to improve video classification by integrating video, audio, and text modalities, enhancing comprehensive multi-modal learning. The dataset's extensive size and multi-modal content enable robust representation learning and multimedia research."
    },
    {
      "display_name": "COCO",
      "normalized_name": "coco",
      "name_variants": [
        "COCO"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The COCO dataset is extensively used for pretraining and evaluating models in multi-modal reasoning tasks. It provides a rich set of images annotated with captions, object segmentations, and keypoint annotations, which support the development of models for image captioning, retrieval, and cross-modal alignment. The dataset is also crucial for building and assessing visual question answering (VQA) systems and visual commonsense reasoning tasks, where models must integrate and reason about both image and text data. Its dense annotations enable researchers to train and test models on complex visual scenes and their textual descriptions, facilitating advancements in multi-modal understanding and alignment."
    },
    {
      "display_name": "WN9",
      "normalized_name": "wn9",
      "name_variants": [
        "WN-9",
        "WN9"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The WN9 dataset is primarily used to evaluate and enhance multi-modal knowledge graph reasoning models. It is employed to assess performance metrics such as Hit@10 and Hit@1, compare different negative sampling techniques, and optimize hyper-parameters like batch size, margin, and learning rate. The dataset supports research on wordnet relations, entity linking, and link prediction, particularly in scenarios involving image-embodied representations and visual enhancements to entities."
    },
    {
      "display_name": "Wikipedia",
      "normalized_name": "wikipedia",
      "name_variants": [
        "Wikipedia"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Wikipedia dataset is utilized for training and evaluating multi-modal models, particularly focusing on image-text pairs to enhance cross-modal understanding and reasoning. It is also used to train the GloVe model for generating feature representations of concepts. Additionally, Wikipedia supports event classification and relation extraction, contributing to the construction of the MMEKG multi-modal event knowledge graph by providing lexical, textual, and visual data."
    },
    {
      "display_name": "KB-VQA",
      "normalized_name": "kbvqa",
      "name_variants": [
        "KB-VQA"
      ],
      "mention_count": 3,
      "cited_papers_count": 2,
      "topic_summary": "The KB-VQA dataset is primarily used for fact-based visual question answering, where it provides a diverse set of annotated images for training and evaluating models. It emphasizes the integration of textual and visual information, focusing on commonsense reasoning and factual accuracy. The dataset supports explicit knowledge-based reasoning, combining external knowledge with image understanding. It is also used to evaluate multi-modal reasoning models, particularly those involving images and knowledge bases."
    },
    {
      "display_name": "OPENBG",
      "normalized_name": "openbg",
      "name_variants": [
        "OPENBG"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The OPENBG dataset is used to develop and evaluate multi-modal knowledge graphs, focusing on the large-scale integration of textual and visual data. It is applied to assess multi-modal knowledge graph reasoning, emphasizing diverse and richly annotated data sources. The dataset enables researchers to benchmark relational and multimodal machine learning models, facilitating the evaluation of systems that integrate textual and visual information."
    },
    {
      "display_name": "NUS-WIDE",
      "normalized_name": "nuswide",
      "name_variants": [
        "NUS-WIDE"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The NUS-WIDE dataset is primarily used for evaluating and enhancing multi-modal reasoning systems, particularly in image captioning and tagging. It integrates graph-structured information to improve the disambiguation and relationship between textual and visual data. Research focuses on visual concept detection, cross-modal retrieval, and multi-label classification, often employing GCN-based Multi-hop GNN models to assess performance and accuracy in multi-modal data integration. The dataset's 24 categories and multiple labels per sample facilitate comprehensive evaluation and visualization of learned classifiers, ensuring meaningful semantic structure preservation."
    },
    {
      "display_name": "ImageNet-V2",
      "normalized_name": "imagenetv2",
      "name_variants": [
        "ImageNet-V2"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "ImageNet-V2 is used to evaluate the generalization and robustness of GraphAdapter models with various visual backbones, focusing on out-of-distribution performance. This dataset helps researchers assess how well these models can adapt and maintain accuracy when faced with data that differs from their training set."
    },
    {
      "display_name": "NLVR2",
      "normalized_name": "nlvr2",
      "name_variants": [
        "NLVR 2",
        "NLVR2"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The NLVR2 dataset is used to evaluate visual reasoning performance, particularly in comparing existing approaches with state-of-the-art methods like MaxEnt and LXMERT. It challenges models with complex tasks involving both visual and textual reasoning, focusing on the classification of concatenated cross-modality image representations. This dataset enables researchers to assess and improve the capabilities of models in understanding and reasoning about multi-modal data."
    },
    {
      "display_name": "MPII-MD",
      "normalized_name": "mpiimd",
      "name_variants": [
        "MPII-MD"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MPII-MD dataset is primarily used for training and evaluating text-video embedding models, focusing on multimodal representation learning from large corpora of narrated instructional videos and short, animated GIFs. It emphasizes the scale and diversity of the data, enabling research in text-video alignment, action and event recognition, and automatic GIF generation. The dataset supports the development of models that can understand and align visual and textual content, enhancing performance in downstream tasks such as multi-modal reasoning and temporal-spatial understanding in short video clips."
    },
    {
      "display_name": "GQA",
      "normalized_name": "gqa",
      "name_variants": [
        "GQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The GQA dataset is used to evaluate and enhance natural language reasoning over paired images, focusing on multi-modal alignment, logical inference, and complex reasoning. It is applied in visual question answering tasks to improve image understanding and compositional generalization, enabling researchers to report state-of-the-art results on the GQA leaderboard."
    },
    {
      "display_name": "WebChild",
      "normalized_name": "webchild",
      "name_variants": [
        "WebChild"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The WebChild dataset is used to harvest and organize commonsense knowledge from the web, focusing on the automatic extraction and categorization of factual information. It serves as a nucleus for a web of open data, providing structured information from Wikipedia to enhance VQA models with semantic and relational knowledge. The dataset is employed to acquire comparative commonsense knowledge, integrating web-derived information to improve reasoning accuracy and support practical commonsense reasoning tasks. It provides a large, multilingual knowledge graph that enriches the knowledge base of models, specifically enhancing their ability to reason about entities, relationships, and everyday concepts."
    },
    {
      "display_name": "Visual7W-KB",
      "normalized_name": "visual7wkb",
      "name_variants": [
        "Visual7W-KB"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The Visual7W-KB dataset is used to test the performance of the Out of the Box method on factual visual question answering, focusing on multi-modal reasoning with graph convolution networks. It enables researchers to evaluate how effectively these networks can integrate visual and textual information to answer factual questions."
    },
    {
      "display_name": "KRVQA",
      "normalized_name": "krvqa",
      "name_variants": [
        "KRVQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The KRVQA dataset is used to train and evaluate models for knowledge-routed visual question reasoning, focusing on integrating external knowledge with visual and textual information. It assesses the models' ability to handle deep representation embedding challenges and enhances visual question answering by incorporating multi-source knowledge. The dataset provides rich visual content and supports supervised learning approaches for question answering tasks requiring external knowledge."
    },
    {
      "display_name": "ASER",
      "normalized_name": "aser",
      "name_variants": [
        "ASER"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "ASER is used to construct and enhance large-scale event knowledge graphs, providing a rich resource of 990 thousand concept events and 644 relation types. It supports scalable construction and reasoning tasks, particularly in multi-modal contexts, by offering a comprehensive ontology and automatic pipeline for building multi-modal reasoning resources."
    },
    {
      "display_name": "VQAv2",
      "normalized_name": "vqav2",
      "name_variants": [
        "VQAv2"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The VQAv2 dataset is primarily used for visual question answering, featuring 204,721 real images from MSCOCO and 14,055 questions. It supports training and evaluating models that require complex interactions between visual and textual information. The dataset is also utilized for pretraining cross-modality encoders using transformers, enabling the accumulation of basic visual-domain knowledge. Its large, annotated image set facilitates multi-modal reasoning and enhances image captioning and object detection models."
    },
    {
      "display_name": "ArXivQA",
      "normalized_name": "arxivqa",
      "name_variants": [
        "ArXivQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The ArXivQA dataset is used to enhance the scientific comprehension capabilities of large vision-language models by integrating scientific diagrams and multimodal data. This integration allows researchers to explore how these models can better understand and reason about complex scientific content, improving their performance in tasks that require both textual and visual understanding."
    },
    {
      "display_name": "OKVQA",
      "normalized_name": "okvqa",
      "name_variants": [
        "OK-VQA",
        "OKVQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The OKVQA dataset is primarily used for evaluating visual question answering (VQA) systems that require external knowledge, focusing on the integration of visual and textual information. It is employed to assess models like CMRL on their ability to handle deep representation embedding challenges and perform knowledge-routed visual question reasoning. The dataset supports vision-language understanding tasks, including natural language inference with visual entailment and entity linking in diverse Wikipedia articles, providing a balanced set of examples that require both visual and world knowledge."
    },
    {
      "display_name": "Richpedia",
      "normalized_name": "richpedia",
      "name_variants": [
        "Richpedia"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Richpedia dataset is used to enhance KG-based applications by integrating images as a visual modality, which supports multi-modal reasoning. It contains triplets, textual descriptions, and images, enabling the construction of a comprehensive multi-modal knowledge graph. This dataset is leveraged to improve entity alignment and representation, demonstrating significant potential in multi-modal knowledge graph reasoning."
    },
    {
      "display_name": "YouTube-8M",
      "normalized_name": "youtube8m",
      "name_variants": [
        "YouTube-8M"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The YouTube-8M dataset is primarily used for large-scale video classification, focusing on the integration of multi-modal data such as video, audio, and text. It supports extensive experiments and benchmarking, enabling researchers to improve video classification accuracy and outperform state-of-the-art models. The dataset facilitates multi-modal learning by providing pre-extracted features for frame-level analysis, enhancing multimedia research through comprehensive data integration."
    },
    {
      "display_name": "FB-IMG-TXT",
      "normalized_name": "fbimgtxt",
      "name_variants": [
        "FB-IMG-TXT"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The FB-IMG-TXT dataset is used for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information. It involves crawling images (ranging from 10 to 100 per entity) and setting image feature embedding dimensions (128 to 4096). The dataset enhances multimodal knowledge graphs by adding textual descriptions and images, increasing data diversity. It serves as a benchmark to compare sparsity and complexity in multimodal knowledge graph representation learning, addressing challenges in handling sparse and complex data."
    },
    {
      "display_name": "CWQ",
      "normalized_name": "cwq",
      "name_variants": [
        "CWQ"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CWQ dataset is used to evaluate methods for handling complex web-based questions, focusing on the performance and robustness of approaches in multi-hop reasoning and diverse query types. It is employed to assess the effectiveness of candidate answer generation, with k-hop parameters set to 2 or 3, depending on the complexity of the questions being addressed. This dataset enables researchers to test and refine algorithms for both simple and complex question answering tasks."
    },
    {
      "display_name": "CSM movie dataset",
      "normalized_name": "csmmovie",
      "name_variants": [
        "CSM movie dataset"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CSM movie dataset is used to pre-train the ResNet-50 model for extracting character features from video and movie frames. This pre-training focuses on enhancing visual and temporal links, which are crucial for person search tasks and multi-modal reasoning in video content. The dataset's visual data enables researchers to improve feature extraction and reasoning capabilities in dynamic media."
    },
    {
      "display_name": "YAGO3-10",
      "normalized_name": "yago310",
      "name_variants": [
        "YAGO3-10"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The YAGO3-10 dataset is used in research to evaluate and compare knowledge graph reasoning models, particularly focusing on relation prediction tasks. It is utilized to assess parameter efficiency, such as comparing LowFER-k with RotatE, and to report results on a subset of YAGO3 with 123,182 entities and 37 relations, where each entity has at least 10 relations. This subset enables detailed analysis and benchmarking of model performance and efficiency."
    },
    {
      "display_name": "Charades",
      "normalized_name": "charades",
      "name_variants": [
        "Charades"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Charades dataset is primarily used for multi-action recognition and reasoning in complex scenarios, where multiple concurrent actions are present. It enables researchers to develop and evaluate models that can accurately identify and reason about multiple actions occurring simultaneously in video data. This dataset's complexity and rich annotation support the advancement of algorithms in action recognition and understanding in real-world settings."
    },
    {
      "display_name": "KVQA",
      "normalized_name": "kvqa",
      "name_variants": [
        "KVQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The KVQA dataset is used for fact-based visual question answering, integrating external knowledge with visual information. It focuses on open-ended questions that require reasoning beyond the image content, utilizing knowledge bases and graphs to enhance accuracy and depth. The dataset evaluates models' ability to integrate visual and textual information, particularly in complex, multi-step reasoning tasks, and benchmarks their performance in knowledge-intensive VQA scenarios."
    },
    {
      "display_name": "FB13",
      "normalized_name": "fb13",
      "name_variants": [
        "FB13"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The FB13 dataset is primarily used to evaluate knowledge graph completion methods, focusing on entity and relation prediction across various scales of Freebase subsets. It is employed to assess the performance, scalability, and robustness of knowledge graph embedding models, particularly in predicting missing links and handling inverse relations. The dataset's moderate size makes it suitable for efficient testing and validation, while its diversity supports the evaluation of generalization capabilities and the integration of knowledge graphs with textual data, such as linking entities from the New York Times corpus."
    },
    {
      "display_name": "DBpedia50",
      "normalized_name": "dbpedia50",
      "name_variants": [
        "DBpedia50"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "DBpedia50 is used to evaluate the performance, scalability, and robustness of knowledge graph embedding and completion methods, particularly on medium to large-sized entity sets. It supports research in entity linking, relation prediction, and generalization capabilities of knowledge graph reasoning algorithms. The dataset's size and structure enable efficient testing and validation, making it suitable for both smaller-scale and domain-specific studies."
    },
    {
      "display_name": "FB15K-DB15K",
      "normalized_name": "fb15kdb15k",
      "name_variants": [
        "FB15K-DB15K"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The FB15K-DB15K dataset is primarily used for evaluating entity alignment methods in multi-modal knowledge graphs. It focuses on aligning entities across different modalities, using 20% alignment seeds. Research employs this dataset to compare methods like MSNEA, PoE, MMEA, and EVA, assessing their performance through metrics such as Hits@1, Hits@10, and MRR. The dataset facilitates ablation studies to analyze the impact of multi-modal knowledge on these performance metrics."
    },
    {
      "display_name": "VRD",
      "normalized_name": "vrd",
      "name_variants": [
        "VRD"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The VRD dataset is primarily used for visual relationship detection and human-object interaction detection, providing annotated image triplets that support multi-modal reasoning in complex scenes. It enriches visual commonsense knowledge by capturing structured relationships between objects, including unusual or unexpected interactions. The dataset facilitates both supervised and weakly-supervised learning, enabling researchers to test and evaluate models' capabilities in extracting and reasoning about visual knowledge."
    },
    {
      "display_name": "CM3KG",
      "normalized_name": "cm3kg",
      "name_variants": [
        "CM3KG"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CM3KG dataset is used as an open-sourced multi-modal knowledge graph to support research in multi-modal reasoning. It provides a structured resource for integrating and analyzing diverse data types, enabling researchers to explore complex relationships and patterns across different modalities. This dataset facilitates the development and evaluation of methods for combining textual, visual, and other data sources in reasoning tasks."
    },
    {
      "display_name": "NELL",
      "normalized_name": "nell",
      "name_variants": [
        "NELL"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The NELL dataset is used as a knowledge base for multi-modal reasoning, integrating textual and structured data to enhance knowledge graph construction. It supports continuous learning and entity relationship extraction through semi-supervised methods, enabling the construction and expansion of knowledge graphs from web data. This dataset facilitates never-ending learning by continuously extracting and integrating new knowledge."
    },
    {
      "display_name": "Twitter15",
      "normalized_name": "twitter15",
      "name_variants": [
        "Twitter15"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The Twitter15 dataset is primarily used for multimodal named entity recognition, focusing on integrating visual and textual information to improve tagging accuracy in social media posts. It has been employed to gather large candidate sets (8357 and 4819 instances) for multi-modal social media analysis, specifically using visual attention models to enhance name tagging. This dataset enables researchers to develop and test models that combine visual and textual data, advancing the field of multimodal NER."
    },
    {
      "display_name": "WN18-rules",
      "normalized_name": "wn18rules",
      "name_variants": [
        "WN18-rules"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The WN18-rules dataset is used to evaluate and compare knowledge graph reasoning models, focusing on performance metrics like MR, MRR, and Hits@N. It assesses knowledge graph completion and embedding methods, including TransE, TransH, TransR, KALE, ComplEx, and ASR-ComplEx. The dataset also supports the creation and testing of 47 logical rules to incorporate logical constraints into embedding methods, enhancing the evaluation of logical formulas in knowledge graph reasoning."
    },
    {
      "display_name": "E-KAR",
      "normalized_name": "ekar",
      "name_variants": [
        "E-KAR"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The E-KAR dataset is used to enhance multi-modal knowledge graph reasoning by linking external entities in Wikidata and collecting annotated seed entities and relations. It provides a large-scale dataset of image-text pairs, enabling the integration of visual, textual, and relational information. The dataset supports tasks such as entity linking, relation extraction, and multimodal analogical reasoning, focusing on high-quality, semantically specific entities to improve knowledge graph completion and reasoning accuracy."
    },
    {
      "display_name": "DBP-WD",
      "normalized_name": "dbpwd",
      "name_variants": [
        "DBP-WD"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The DBP-WD dataset is primarily used for entity alignment experiments using knowledge graph embedding, focusing on integrating and aligning entities from DBpedia with either Wikidata or YAGO. It is also used to evaluate multi-modal knowledge graph reasoning, particularly in the context of integrating Wikipedia and YAGO or Wikidata entities. This dataset enables researchers to test and improve methods for aligning and integrating diverse knowledge sources, enhancing the accuracy and comprehensiveness of knowledge graphs."
    },
    {
      "display_name": "Visual Phrase",
      "normalized_name": "visualphrase",
      "name_variants": [
        "Visual Phrase"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Visual Phrase dataset is used to study and evaluate visual relationship detection, focusing on object interactions and relationships in images. It integrates visual and textual data through dense annotations, enhancing multi-modal reasoning. The dataset supports the training and evaluation of models on diverse object categories and relationships, providing large-scale annotations for tasks such as relation predicate prediction and scene graph analysis. It is particularly useful for fine-grained annotations and complex scene understanding, with applications in multi-modal knowledge graph reasoning."
    },
    {
      "display_name": "PropBank",
      "normalized_name": "propbank",
      "name_variants": [
        "PropBank"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "PropBank is used to extend the size and depth of event knowledge graphs by providing large annotated corpora of semantic roles and nominal predicate argument structures. This enhances the representation of both verb-based and noun-based events, contributing over 114,576 events to improve the graph's coverage and detail. The dataset's extensive annotations enable researchers to enrich event representations, supporting more nuanced and comprehensive knowledge graphs."
    },
    {
      "display_name": "GDELT",
      "normalized_name": "gdelt",
      "name_variants": [
        "GDELT"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The GDELT dataset is primarily used as a Temporal Knowledge Graph (TKG) to study relation codes following the CAMEO taxonomy, focusing on international crisis events and event data with temporal dynamics. It is utilized to assess the efficiency and accuracy of the Graph Hawkes Neural Network in forecasting and runtime performance, particularly in large-scale, diverse datasets. This enables researchers to analyze and predict temporal patterns in global events and crises."
    },
    {
      "display_name": "DRKGMM",
      "normalized_name": "drkgmm",
      "name_variants": [
        "DRKGMM"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The DRKGMM dataset is used to evaluate the performance of various models, including a-RotatE, DualE, and PairRE, on multi-modal knowledge graph reasoning tasks. It focuses on assessing these models using the Mean Rank (MR) metric, enabling researchers to compare their effectiveness in reasoning across different modalities within knowledge graphs."
    },
    {
      "display_name": "Flickr8K",
      "normalized_name": "flickr8k",
      "name_variants": [
        "Flickr8K"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Flickr8K dataset is primarily used for replicating experimental procedures in image description tasks, particularly focusing on ranking tasks and evaluation metrics in multi-modal learning. It enables researchers to assess and compare different models and algorithms by providing a standardized set of images and corresponding textual descriptions. This facilitates the evaluation of multi-modal learning approaches in generating accurate and contextually relevant image captions."
    },
    {
      "display_name": "PubMedQA",
      "normalized_name": "pubmedqa",
      "name_variants": [
        "PubMedQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The PubMedQA dataset is used to evaluate the performance of the DRAGON model on biomedical question answering, focusing on complex queries derived from PubMed abstracts. It assesses the model's capabilities in handling intricate information retrieval tasks, emphasizing the accuracy and efficiency of responses to biomedical research questions."
    },
    {
      "display_name": "MNRE",
      "normalized_name": "mnre",
      "name_variants": [
        "MNRE"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MNRE dataset is used for multi-instance relation extraction, where each instance consists of a bag of sentences that collectively express a relation. This approach allows researchers to capture the nuances of relational information across multiple contexts, enhancing the accuracy of relation extraction tasks. The dataset's structure supports the development and evaluation of models that can effectively aggregate and reason over multiple instances to identify and extract relations."
    },
    {
      "display_name": "HacRED",
      "normalized_name": "hacred",
      "name_variants": [
        "HacRED"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The HacRED dataset is primarily used for empirical evaluation and training in relation extraction tasks, particularly in the biomedical domain. It employs distant supervision to gather and train on a comprehensive dataset, enabling researchers to assess model performance in extracting relations from text. The dataset supports both sentence-level and document-level relation extraction, often trained over 50 epochs."
    },
    {
      "display_name": "FB15K-237-IMG",
      "normalized_name": "fb15k237img",
      "name_variants": [
        "FB15K-237-IMG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB15K-237-IMG dataset is primarily used for knowledge graph completion, serving as a baseline to evaluate the performance of proposed methods. It extends the scope of triplets by integrating image data with textual information, enhancing multimodal knowledge graph completion. This integration allows researchers to explore how visual and textual data can be combined to improve the accuracy and robustness of knowledge graph reasoning tasks."
    },
    {
      "display_name": "Global Database of Events, Language, and Tone",
      "normalized_name": "globaldatabaseofeventslanguageandtone",
      "name_variants": [
        "Global Database of Events, Language, and Tone"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Global Database of Events, Language, and Tone is used to construct a dense knowledge graph for temporal knowledge graph completion. Researchers focus on analyzing events and tones over time, employing methodologies that leverage the dataset's extensive temporal and linguistic data to enhance understanding of dynamic social and political phenomena. This enables detailed event and tone analysis, supporting research into temporal patterns and trends."
    },
    {
      "display_name": "DB15K",
      "normalized_name": "db15k",
      "name_variants": [
        "DB15K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DB15K dataset is used as a multi-modal knowledge graph containing image, text, and numerical information, specifically a subset of DBpedia. It is employed to explore reasoning across different data types, enabling researchers to investigate how various modalities can be integrated and reasoned over in knowledge graphs. This dataset facilitates the development and evaluation of multi-modal reasoning techniques, enhancing the understanding of complex, interconnected data."
    },
    {
      "display_name": "KVC16K",
      "normalized_name": "kvc16k",
      "name_variants": [
        "KVC16K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KVC16K dataset is used to evaluate knowledge graph completion models, specifically focusing on multi-modal reasoning. Researchers employ performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1 to assess model effectiveness. This dataset enables the testing and comparison of models designed to integrate and reason over multiple data modalities, enhancing the accuracy and robustness of knowledge graph completion tasks."
    },
    {
      "display_name": "ActivityNet",
      "normalized_name": "activitynet",
      "name_variants": [
        "ActivityNet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ActivityNet dataset is used for evaluating multi-modal reasoning in human action recognition, leveraging large-scale video data with diverse actions and precise temporal annotations. Researchers employ this dataset to assess models' ability to recognize and reason about complex human activities in videos, focusing on the integration of visual and temporal information. This enables advancements in understanding and interpreting dynamic human behaviors in various contexts."
    },
    {
      "display_name": "MIMIC-III",
      "normalized_name": "mimiciii",
      "name_variants": [
        "MIMIC-III"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MIMIC-III dataset is used to construct knowledge graphs from electronic health records, focusing on diagnoses, procedures, prescriptions, and their descriptions. Researchers convert the relational database into a graph structure to support knowledge graph-based question answering. This involves extracting triples by mapping columns to entities, literals, or relations, enabling more sophisticated and context-aware query capabilities."
    },
    {
      "display_name": "VIST",
      "normalized_name": "vist",
      "name_variants": [
        "VIST"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VIST dataset is used to evaluate and assess the performance of methods in multi-modal reasoning, particularly in visual storytelling tasks involving video and text data. It is employed to compare proposed methods against state-of-the-art techniques, focusing on tasks such as generating coherent narratives from visual content. The dataset's diverse movie descriptions enable researchers to test the generality and effectiveness of their models in complex, multi-modal reasoning scenarios."
    },
    {
      "display_name": "Pascal1K",
      "normalized_name": "pascal1k",
      "name_variants": [
        "Pascal1K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Pascal1K dataset is primarily used for image-sentence retrieval tasks, focusing on enhancing cross-modal matching between images and textual descriptions. It is employed to evaluate and improve state-of-the-art performance in multi-modal learning, specifically in the context of retrieving sentences that accurately describe images and vice versa. This dataset facilitates research by providing a benchmark for assessing cross-modal retrieval algorithms."
    },
    {
      "display_name": "CC3M&12M",
      "normalized_name": "cc3m12m",
      "name_variants": [
        "CC3M&12M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CC3M&12M dataset is used to train and evaluate multi-modal reasoning systems, specifically focusing on image-caption pairs. It enhances automatic image captioning by providing a large corpus of aligned images and captions, enabling researchers to develop and test algorithms that generate more accurate and contextually relevant captions."
    },
    {
      "display_name": "MMEKG",
      "normalized_name": "mmekg",
      "name_variants": [
        "MMEKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MMEKG dataset is used to advance the field of event knowledge graphs by providing a large-scale ontology encompassing 990,000 concept events and 644 relation types. This comprehensive coverage of real-world happenings enables researchers to develop and test methods for reasoning over complex, multi-faceted events, enhancing the representation and understanding of dynamic scenarios in knowledge graphs."
    },
    {
      "display_name": "MovieLens-100k",
      "normalized_name": "movielens100k",
      "name_variants": [
        "MovieLens-100k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MovieLens-100k dataset is used to study user-movie interactions by incorporating user demographics and movie attributes. It supports multi-modal reasoning and is employed to evaluate recommendation systems. This dataset enables researchers to analyze how different user and movie features influence recommendation accuracy and effectiveness."
    },
    {
      "display_name": "AIFB",
      "normalized_name": "aifb",
      "name_variants": [
        "AIFB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AIFB dataset is used as a benchmark knowledge graph to evaluate machine learning methods on the Semantic Web. It supports systematic evaluations in two primary contexts: museum collections and scientific publications within a research group. The dataset's structured information enables researchers to assess the performance of machine learning algorithms in these specific domains, facilitating advancements in knowledge graph reasoning and semantic web technologies."
    },
    {
      "display_name": "BioRead",
      "normalized_name": "bioread",
      "name_variants": [
        "BioRead"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The BioRead dataset is used to evaluate the multimodal and long-context capabilities of Multimodal Large Language Models (MLLMs) in biomedical reading comprehension. Specifically, it focuses on assessing these models' performance on cloze-style questions derived from PubMed papers, enabling researchers to test and improve the models' ability to understand complex biomedical texts."
    },
    {
      "display_name": "GeoQA",
      "normalized_name": "geoqa",
      "name_variants": [
        "GeoQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GeoQA dataset is used as a benchmark for evaluating multimodal numerical reasoning, particularly in the context of geometric question answering. It assesses the performance of Multimodal Large Language Models (MLLMs) by challenging them with tasks that require understanding and reasoning about geometric concepts. This dataset enables researchers to test and improve the numerical and spatial reasoning capabilities of MLLMs."
    },
    {
      "display_name": "NEIL",
      "normalized_name": "neil",
      "name_variants": [
        "NEIL"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NEIL dataset is primarily used to construct large-scale structured knowledge bases through both automatic extraction from unstructured or semi-structured data and manual annotation via crowd-sourcing. It focuses on various aspects such as visual knowledge discovery, segmentation, open information extraction from the web, continuous learning, and linking Wikipedia and WordNet. This dataset enables researchers to create comprehensive, multilingual, and collaboratively-built databases, facilitating the extraction and structuring of general knowledge and entities."
    },
    {
      "display_name": "OpenEA benchmarks",
      "normalized_name": "openeabenchmarks",
      "name_variants": [
        "OpenEA benchmarks"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OpenEA benchmarks dataset is used to evaluate multi-modal entity alignment methods by incorporating entity images obtained through Google search. This enhances knowledge graph reasoning, focusing on aligning entities across different modalities. The dataset enables researchers to test and improve the accuracy of entity alignment systems that utilize both textual and visual data."
    },
    {
      "display_name": "OxfordPets",
      "normalized_name": "oxfordpets",
      "name_variants": [
        "OxfordPets"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OxfordPets dataset is used to generate class-specific captions for pet images, integrating visual and textual data in a multi-modal reasoning context. This approach focuses on enhancing the accuracy and relevance of image captions by leveraging the dataset's rich image and label resources, enabling research in automated image captioning and multi-modal understanding."
    },
    {
      "display_name": "People in Social Context (PISC)",
      "normalized_name": "peopleinsocialcontextpisc",
      "name_variants": [
        "People in Social Context (PISC)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The People in Social Context (PISC) dataset is used to enhance person recognition in social contexts by integrating visual and social information. It leverages multiple cues in photo albums to improve multi-modal reasoning capabilities, focusing on the integration of diverse data types to achieve higher accuracy in person recognition tasks."
    },
    {
      "display_name": "Mouse Genome Informatics (MGI)",
      "normalized_name": "mousegenomeinformaticsmgi",
      "name_variants": [
        "Mouse Genome Informatics (MGI)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Mouse Genome Informatics (MGI) dataset is used to compile a collection of mouse proteins and their associated phenotypes, which serve as out-of-domain entities for knowledge graph reasoning. This compilation aids in expanding the scope and robustness of knowledge graphs by integrating biological data, enhancing the ability to reason about complex biological relationships and phenotypic associations."
    },
    {
      "display_name": "ViSR",
      "normalized_name": "visr",
      "name_variants": [
        "ViSR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ViSR dataset is used for video-based social relation recognition, employing multi-scale spatial-temporal reasoning to capture both long-term and short-term temporal cues. It leverages a graph network to analyze these cues, enabling researchers to conduct extensive experiments on understanding and recognizing social interactions in videos."
    },
    {
      "display_name": "FBDB15K",
      "normalized_name": "fbdb15k",
      "name_variants": [
        "FBDB15K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FBDB15K dataset is used to evaluate multi-modal knowledge graph alignment methods, specifically focusing on the impact of different vision feature dimensions on model performance. Researchers use VGG-16 and ResNet-152 vision encoders, examining feature dimensions of 4096 and 2048 respectively, to assess model stability and entity alignment accuracy. This dataset enables detailed analysis of how visual features influence the alignment of entities in multi-modal knowledge graphs."
    },
    {
      "display_name": "KBLN",
      "normalized_name": "kbln",
      "name_variants": [
        "KBLN"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KBLN dataset is used to evaluate and compare different approaches to integrating visual and numerical attributes into knowledge graphs. It assesses the effectiveness of these methods in multi-modal knowledge graph reasoning tasks, focusing on how well visual and numerical data can be integrated to enhance reasoning capabilities. This dataset enables researchers to benchmark their models against established methods, providing insights into the performance and limitations of various integration techniques."
    },
    {
      "display_name": "MicrosoftCOCO",
      "normalized_name": "microsoftcoco",
      "name_variants": [
        "MicrosoftCOCO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MicrosoftCOCO dataset is primarily used for evaluating multi-label image recognition models, such as MLGCN. It enables researchers to compare the performance of different methods, focusing on identifying the best-performing techniques. The dataset's rich annotations and diverse image content facilitate robust testing and validation of these models in recognizing multiple labels within images."
    },
    {
      "display_name": "Kinship dataset",
      "normalized_name": "kinship",
      "name_variants": [
        "Kinship dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Kinship dataset is used to create a synthetic dataset for evaluating multi-modal knowledge graph reasoning, specifically focusing on family relationships and kinship structures. It enables researchers to test and validate algorithms designed to reason about complex relational data, ensuring they can accurately infer and predict kinship connections."
    },
    {
      "display_name": "HMDB51",
      "normalized_name": "hmdb51",
      "name_variants": [
        "HMDB51"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The HMDB51 dataset is used to evaluate the performance of the TS-GCN method on a diverse set of human actions, specifically enhancing the assessment of multi-modal reasoning capabilities. This dataset provides a rich set of video data that allows researchers to test and refine algorithms designed to recognize and reason about complex human activities."
    },
    {
      "display_name": "ImageNet2012",
      "normalized_name": "imagenet2012",
      "name_variants": [
        "ImageNet2012"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ImageNet2012 dataset is primarily used for pre-training Convolutional Neural Networks (CNNs), which are then utilized to extract feature representations. These pre-trained CNN features serve as inputs to calculate projection matrices, enabling multi-modal feature integration. This approach facilitates the development of models that can effectively combine visual and other modalities, enhancing tasks such as multi-modal reasoning and feature alignment."
    },
    {
      "display_name": "Visual Phrases",
      "normalized_name": "visualphrases",
      "name_variants": [
        "Visual Phrases"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Visual Phrases dataset is used to annotate and detect visual relationships in images, focusing on complex visual scenes and compositional phrases. It supports the recognition of objects, attributes, and relationships, enabling the evaluation of models' performance in identifying composite object pairs and their interactions. The dataset is also utilized to fine-tune image feature extractors, enhancing the model's ability to understand and reason about multi-modal data in real-world scenes."
    },
    {
      "display_name": "Chunyu",
      "normalized_name": "chunyu",
      "name_variants": [
        "Chunyu"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Chunyu dataset is used to evaluate and verify the performance of KABLSTM in ranking question-answer pairs, particularly in medical applications. It highlights the effectiveness and utility of integrating knowledge graphs to enhance the precision and relevance of medical Q&A systems. This dataset enables researchers to test and demonstrate the benefits of knowledge graph integration in improving the accuracy of medical information retrieval."
    },
    {
      "display_name": "MKG-W",
      "normalized_name": "mkgw",
      "name_variants": [
        "MKG-W"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MKG-W dataset is used to evaluate the performance of multi-modal knowledge graph reasoning systems, particularly focusing on knowledge graph embedding models such as TransE, DistMult, and ComplEx. It is employed to compare the effectiveness of MMRNS against uniform sampling and to observe performance changes with hyperparameter tuning. The dataset demonstrates significant improvements in multi-modal reasoning tasks, highlighting its utility in advancing these models."
    },
    {
      "display_name": "FewRel",
      "normalized_name": "fewrel",
      "name_variants": [
        "FewRel"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FewRel dataset is primarily used for evaluating few-shot relation classification, particularly in constructing test sets that avoid information leakage into training data. It contains 100 relations and 70,000 instances derived from Wikipedia and Wikidata. Researchers use it to assess model performance on general relation types and to improve few-shot classification in specialized domains like medical knowledge graphs. The dataset also serves as a training source for multi-modal knowledge graph reasoning, with specific triplets removed to prevent information leakage."
    },
    {
      "display_name": "VTKB",
      "normalized_name": "vtkb",
      "name_variants": [
        "VTKB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VTKB dataset is used to construct a visio-textual knowledge base that links concepts to images through embedding similarities, enhancing the quality of image tagging. This involves creating a rich, interconnected database where textual and visual data are aligned, improving the accuracy and relevance of image annotations. The dataset's ability to link concepts and images via embeddings is crucial for this application."
    },
    {
      "display_name": "academic MMKG",
      "normalized_name": "academicmmkg",
      "name_variants": [
        "academic MMKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The academic MMKG dataset is used to facilitate retrieval at the implementation level by linking deep learning papers with their corresponding code implementations. This linkage supports researchers in efficiently finding and verifying code associated with specific academic publications, enhancing reproducibility and practical application of research findings."
    },
    {
      "display_name": "Wikipedia articles and images",
      "normalized_name": "wikipediaarticlesandimages",
      "name_variants": [
        "Wikipedia articles and images"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Wikipedia articles and images dataset is used to pre-train a cross-modal entity matching module, which aligns textual and visual scene graphs extracted from the articles and images. This approach facilitates the integration of textual and visual information, enabling more robust multi-modal understanding and entity alignment in research."
    },
    {
      "display_name": "kgbench",
      "normalized_name": "kgbench",
      "name_variants": [
        "kgbench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The kgbench dataset is used for evaluating node classification tasks on multi-modal knowledge graphs. It serves as a benchmark for relational and multimodal machine learning, focusing on high-quality data to assess the performance of models in these specific tasks. This dataset enables researchers to test and compare algorithms designed for multi-modal reasoning and classification, ensuring robust evaluation through its diverse and high-quality content."
    },
    {
      "display_name": "unearthed oracle bones’ photos",
      "normalized_name": "unearthedoraclebonesphotos",
      "name_variants": [
        "unearthed oracle bones’ photos"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'unearthed oracle bones’ photos' dataset is used to construct a multi-modal knowledge graph for oracle bone recognition. This involves integrating visual and textual data to enhance information processing. The dataset enables researchers to develop and refine methods for recognizing and interpreting oracle bone inscriptions by leveraging both image and text data, thereby improving the accuracy and efficiency of information extraction from these historical artifacts."
    },
    {
      "display_name": "hasPart KB",
      "normalized_name": "haspartkb",
      "name_variants": [
        "hasPart KB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'hasPart KB' dataset is primarily used to enhance multi-modal knowledge graphs by providing part-whole relationships, location triples, and commonsense knowledge. It enriches systems for Visual Question Answering (VQA) and image recognition, improving their ability to understand compositional structures, integrate visual and textual information, and reason about everyday concepts. The dataset serves as a baseline for evaluating tag generation, disambiguation, and commonsense reasoning capabilities, often compared against more complex multi-modal knowledge graphs (MMKGs)."
    },
    {
      "display_name": "NELL-995",
      "normalized_name": "nell995",
      "name_variants": [
        "NELL-995"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NELL-995 dataset is used to evaluate knowledge graph embedding methods and reasoning models, focusing on semantic smoothness and accuracy. It targets a subset of 995 relations for detailed performance assessment, enabling researchers to analyze how well these models handle a large set of entities and relations. This dataset facilitates the comparison and improvement of knowledge graph reasoning techniques."
    },
    {
      "display_name": "electronic medical database",
      "normalized_name": "electronicmedicaldatabase",
      "name_variants": [
        "electronic medical database"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The electronic medical database is used to construct a health knowledge graph, enabling reasoning tasks through the integration and structured utilization of medical data. This approach supports research focused on enhancing the representation and application of medical information in a structured format, facilitating more effective data-driven healthcare solutions."
    },
    {
      "display_name": "Hetionet",
      "normalized_name": "hetionet",
      "name_variants": [
        "Hetionet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Hetionet is used to integrate and prioritize biomedical knowledge for drug repurposing by leveraging a multi-modal structure derived from public resources. This dataset enables researchers to systematically analyze and identify potential new uses for existing drugs, enhancing the efficiency and effectiveness of drug discovery processes."
    },
    {
      "display_name": "FAMILY",
      "normalized_name": "family",
      "name_variants": [
        "FAMILY"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FAMILY dataset is used to represent and reason about relations among family members and nations, focusing on the structure and semantics of these relationships within a knowledge graph. It enables research on multi-modal knowledge graph reasoning, specifically addressing how interconnected data can be represented and analyzed. The dataset's ability to capture complex relational structures supports advanced reasoning tasks."
    },
    {
      "display_name": "Countries",
      "normalized_name": "countries",
      "name_variants": [
        "Countries"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Countries' dataset is used to represent and reason about geographical relations among countries, employing low-rank vector spaces for approximate reasoning. This approach facilitates the analysis of spatial relationships and supports research focused on understanding and modeling geographical data in a computationally efficient manner."
    },
    {
      "display_name": "YOGA15k",
      "normalized_name": "yoga15k",
      "name_variants": [
        "YOGA15k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YOGA15k dataset is used to generate data for different periods, focusing on the temporal aspects of knowledge graph reasoning. This involves creating and analyzing temporal knowledge graphs to understand how relationships and entities evolve over time. The dataset's temporal characteristics enable researchers to address questions related to dynamic changes in multi-modal data, enhancing the accuracy and relevance of knowledge graph reasoning over time."
    },
    {
      "display_name": "YAGO39k",
      "normalized_name": "yago39k",
      "name_variants": [
        "YAGO39k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YAGO39k dataset is used as a subset of a larger knowledge graph to evaluate relation prediction tasks. It focuses on assessing the scope and accuracy of relation predictions within the dataset. This evaluation helps researchers understand the effectiveness of their models in predicting relationships, contributing to advancements in knowledge graph reasoning methodologies."
    },
    {
      "display_name": "MMKG-FB15k-IMG",
      "normalized_name": "mmkgfb15kimg",
      "name_variants": [
        "MMKG-FB15k-IMG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MMKG-FB15k-IMG dataset is used to integrate various knowledge graphs (YAGO, Freebase, DBpedia) with images and other data types, enhancing multi-modal reasoning capabilities. It facilitates comprehensive reasoning by combining textual, visual, and structured data, enabling researchers to explore complex relationships and integrate diverse information sources effectively."
    },
    {
      "display_name": "MKG-YAGO",
      "normalized_name": "mkgyago",
      "name_variants": [
        "MKG-YAGO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MKG-YAGO dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. It leverages Wikipedia and YAGO data to assess the performance of these methods, enabling researchers to enhance the accuracy and robustness of knowledge graph reasoning systems."
    },
    {
      "display_name": "MKG-Wikipedia",
      "normalized_name": "mkgwikipedia",
      "name_variants": [
        "MKG-Wikipedia"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MKG-Wikipedia dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. It leverages Wikipedia data to assess the performance of these methods, enabling researchers to improve the accuracy and robustness of knowledge graph reasoning systems. The dataset's rich textual and structural information supports the development and testing of advanced multimodal reasoning techniques."
    },
    {
      "display_name": "DBPedia50k",
      "normalized_name": "dbpedia50k",
      "name_variants": [
        "DBPedia50k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DBPedia50k dataset is primarily used to evaluate models in open-world tail prediction, focusing on their performance in predicting unseen entities within large-scale knowledge graphs. It assesses the impact of relation information and entity descriptions on model performance, with studies highlighting significant improvements from extensive entity descriptions. The dataset is also used to compare different models, such as ComplEx-RST and ComplEx-RCT, emphasizing the balance between reducing transformation functions and maintaining competitive results."
    },
    {
      "display_name": "FB15k-237-OWE",
      "normalized_name": "fb15k237owe",
      "name_variants": [
        "FB15k-237-OWE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB15k-237-OWE dataset is primarily used to evaluate and validate models for open-world knowledge graph reasoning tasks, focusing on tail prediction and handling out-of-vocabulary entities. It is employed to assess the model's ability to reason over large-scale knowledge graphs, particularly in predicting unseen entities and managing complex relations. The dataset facilitates the evaluation of relation information's impact, often showing performance gains with extensive entity descriptions. It serves as a benchmark for comparing different models, such as ComplEx-RCT and ComplEx-RST, and highlights the importance of entity and relation types in enhancing model performance."
    },
    {
      "display_name": "Visual Commonsense Reasoning (VCR)",
      "normalized_name": "visualcommonsensereasoningvcr",
      "name_variants": [
        "Visual Commonsense Reasoning (VCR)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Visual Commonsense Reasoning (VCR) dataset is used to evaluate models in visual question answering, image-text and text-image retrieval, and region-to-phrase grounding. It focuses on complex questions and tasks that require both visual and textual understanding, particularly in assessing visual commonsense reasoning, including the ability to justify answers and understand social interactions and everyday scenarios."
    },
    {
      "display_name": "NTU",
      "normalized_name": "ntu",
      "name_variants": [
        "NTU"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NTU dataset is utilized in multi-modal reasoning tasks, primarily for 3D model retrieval and emotion recognition. It serves as a bimodal dataset, integrating visual and skeletal data for 3D model retrieval, and as a trimodal dataset, combining audio, video, and text for emotion recognition. These applications leverage the dataset's multi-modal characteristics to enhance reasoning and recognition accuracy."
    },
    {
      "display_name": "FB20k",
      "normalized_name": "fb20k",
      "name_variants": [
        "FB20k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB20k dataset is primarily used to evaluate models on open-world tail prediction, focusing on their ability to predict unseen or out-of-vocabulary entities in large-scale knowledge graphs. This dataset enables researchers to assess the performance and effectiveness of models in handling the challenges of open-world settings, where entities may not be present in the training data."
    },
    {
      "display_name": "VTKG-C",
      "normalized_name": "vtkgc",
      "name_variants": [
        "VTKG-C"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VTKG-C dataset is used to qualitatively compare representation vectors generated by BERT, ViT, and VISTA, specifically focusing on multi-modal reasoning within a knowledge graph context. This involves analyzing how these models integrate textual and visual information to enhance reasoning tasks. The dataset enables researchers to evaluate and contrast the effectiveness of different multi-modal approaches in knowledge graph applications."
    },
    {
      "display_name": "VTKG datasets",
      "normalized_name": "vtkg",
      "name_variants": [
        "VTKG datasets"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VTKG datasets are used as reusable resources for diverse applications, particularly in visual commonsense knowledge tasks. While specific methodologies and research questions are not detailed, the datasets support tasks that require integrating visual and textual information to infer commonsense knowledge. This enables researchers to develop and test models that can reason about real-world scenarios using multi-modal data."
    },
    {
      "display_name": "Pascal Sentences dataset",
      "normalized_name": "pascalsentences",
      "name_variants": [
        "Pascal Sentences dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Pascal Sentences dataset is used for multi-modal reasoning, integrating images with their corresponding text descriptions, typically five sentences per image. This integration helps researchers explore the relationship between visual and textual data, enabling studies that focus on understanding how textual descriptions can complement and enhance the interpretation of visual content."
    },
    {
      "display_name": "entity linking corpus",
      "normalized_name": "entitylinking",
      "name_variants": [
        "entity linking corpus"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The entity linking corpus is used to evaluate pre-trained language models in a zero-shot setting, specifically focusing on their ability to perform entity linking tasks that extend beyond traditional factual knowledge bases. This dataset enables researchers to assess model performance in identifying and linking entities without prior training on the specific dataset, highlighting its utility in evaluating the generalization capabilities of these models."
    },
    {
      "display_name": "MetaQA",
      "normalized_name": "metaqa",
      "name_variants": [
        "MetaQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MetaQA dataset is used to evaluate the performance of models in handling complex, multi-hop questions over knowledge graphs. It focuses on intricate query patterns and complex query structures, assessing semantic parsing and reasoning capabilities. This dataset enables researchers to test and improve multi-hop reasoning in knowledge graph question answering systems."
    },
    {
      "display_name": "WN11",
      "normalized_name": "wn11",
      "name_variants": [
        "WN11"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WN11 dataset is used to evaluate model performance on wordnet relations, specifically for link prediction accuracy. It assesses the model's ability to predict links in a smaller subset of WordNet, emphasizing scalability and robustness. This dataset is also utilized to test performance on Freebase relations, focusing on entity and relation prediction."
    },
    {
      "display_name": "Freebase 1",
      "normalized_name": "freebase1",
      "name_variants": [
        "Freebase 1"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Freebase 1 is used to evaluate methods on link prediction and triple classification tasks, focusing on lexical relationships and semantic hierarchies within the collaboratively created graph database. The dataset's structured nature and rich lexical content enable researchers to assess the performance of their models in understanding and predicting relationships within complex knowledge graphs."
    },
    {
      "display_name": "Movie-QA",
      "normalized_name": "movieqa",
      "name_variants": [
        "Movie-QA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Movie-QA dataset is used to understand stories in movies through question-answering, integrating visual and textual information to answer questions about movie plots. This involves methodologies that combine both modalities to enhance comprehension and accuracy in answering plot-related questions, enabling research on multi-modal narrative understanding."
    },
    {
      "display_name": "Nation",
      "normalized_name": "nation",
      "name_variants": [
        "Nation"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Nation' dataset is used to represent relations among nations, focusing on the structure and properties of international relationships within multi-modal knowledge graph reasoning. It enables researchers to analyze complex interactions and dependencies between countries, facilitating a deeper understanding of global dynamics through structured data representation."
    },
    {
      "display_name": "MS-Celebs",
      "normalized_name": "mscelebs",
      "name_variants": [
        "MS-Celebs"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MS-Celebs dataset is used to enhance multi-modal reasoning by associating Freebase IDs with cropped celebrity faces, linking visual data with structured world knowledge. It is also combined with other datasets like KVQA to create enhanced lists of persons, improving entity sets for multi-modal knowledge graph reasoning tasks. This integration enriches the representation of individuals in multi-modal systems, facilitating more accurate and contextually rich reasoning."
    },
    {
      "display_name": "CLEVR",
      "normalized_name": "clevr",
      "name_variants": [
        "CLEVR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CLEVR dataset is primarily used to evaluate and study visual question answering (VQA) and compositional language understanding in the context of elementary visual reasoning. It employs synthetic images and questions to test the integration of visual and textual information, focusing on the ability to answer complex questions and perform multi-modal reasoning tasks. This dataset enables researchers to assess models' capabilities in understanding and reasoning about visual scenes, particularly in VQA over knowledge graphs and visual named entity linking."
    },
    {
      "display_name": "WD-singer",
      "normalized_name": "wdsinger",
      "name_variants": [
        "WD-singer"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WD-singer dataset is used to create a large-scale multi-modal knowledge graph, focusing on knowledge embedding and pre-trained language representation. It derives a subset of Wikidata for singer-related entities, enabling research in multi-modal knowledge graph reasoning and entity linking. This dataset supports the development and evaluation of models that integrate textual and visual data for enhanced reasoning and linking tasks."
    },
    {
      "display_name": "UMLS",
      "normalized_name": "umls",
      "name_variants": [
        "UMLS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The UMLS dataset is used to enhance the representation of pharmaceutical and medical concepts in a self-constructed knowledge graph for MedQA-USMLE. It integrates drug and disease information, improving the accuracy and comprehensiveness of medical question answering systems. This integration supports the development of more robust and contextually rich knowledge graphs, specifically tailored for medical education and clinical reasoning tasks."
    },
    {
      "display_name": "Genia",
      "normalized_name": "genia",
      "name_variants": [
        "Genia"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Genia dataset is used to evaluate models in multi-modal reasoning tasks, specifically focusing on molecular biology entities and relations. It enables researchers to assess the performance of their models in understanding and reasoning about complex biological data, enhancing the accuracy and reliability of computational methods in molecular biology."
    },
    {
      "display_name": "KnowSite",
      "normalized_name": "knowsite",
      "name_variants": [
        "KnowSite"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The dataset 'KnowSite' is mentioned in the citation context but lacks detailed descriptions of its usage, methodology, research questions, or specific characteristics. Therefore, there is insufficient evidence to provide a comprehensive description of how it is actually used in research."
    },
    {
      "display_name": "CheBI-20",
      "normalized_name": "chebi20",
      "name_variants": [
        "CheBI-20"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CheBI-20 dataset is used to evaluate the performance of MolLM, MoMu, and MolT5 models in multi-modal reasoning tasks, specifically focusing on molecular representations. It employs four captioning metrics to compare these models, enabling researchers to assess their effectiveness in generating accurate and meaningful captions for molecular structures. This dataset facilitates the advancement of multi-modal knowledge graph reasoning by providing a standardized benchmark for model evaluation."
    },
    {
      "display_name": "ZINC",
      "normalized_name": "zinc",
      "name_variants": [
        "ZINC"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ZINC dataset is used for sampling molecules to pair with text editing prompts, specifically for multi-modal molecule structure-text modeling. This approach facilitates tasks such as retrieval and editing, leveraging the dataset's molecular structures to enhance the integration of textual and structural data in machine learning models."
    },
    {
      "display_name": "Babel-Net",
      "normalized_name": "babelnet",
      "name_variants": [
        "Babel-Net"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Babel-Net is used to construct knowledge graphs by automatically labeling terms in natural language, enhancing interpretability in text entailment recognition. It integrates structured information from sources like DBpedia and Wikipedia to generate specialized knowledge graphs, such as those for metallic materials. Babel-Net also supports the creation of multilingual knowledge graphs, providing lexical and encyclopedic information across various languages, and focuses on lexical and semantic relationships."
    },
    {
      "display_name": "GQA balanced version",
      "normalized_name": "gqabalancedversion",
      "name_variants": [
        "GQA balanced version"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GQA balanced version dataset is used to evaluate models on real-world visual reasoning and compositional question answering, ensuring a balanced distribution of questions. It focuses on multi-modal reasoning tasks, integrating visual and textual information. This dataset is specifically employed to assess the performance of models in understanding complex scenes and the relationships between images and text, enhancing the evaluation of visual question answering systems."
    },
    {
      "display_name": "TAC KBP",
      "normalized_name": "tackbp",
      "name_variants": [
        "TAC KBP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TAC KBP dataset is primarily used to evaluate and assess entity linking approaches, with a focus on disambiguation in structured documents that provide rich contextual information. This dataset enables researchers to test and improve entity linking methods by leveraging detailed contextual details, enhancing the accuracy of disambiguation in complex textual environments."
    },
    {
      "display_name": "WN18-IMG",
      "normalized_name": "wn18img",
      "name_variants": [
        "WN18-IMG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WN18-IMG dataset is used for multimodal link prediction, focusing on evaluating models in knowledge graph reasoning tasks that integrate images and textual information. This dataset enables researchers to assess the performance of models in predicting relationships within multimodal knowledge graphs, enhancing the understanding of how visual and textual data can be effectively combined and reasoned over."
    },
    {
      "display_name": "VrR-VG",
      "normalized_name": "vrrvg",
      "name_variants": [
        "VrR-VG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VrR-VG dataset is used to evaluate and compare scene graph generation methods, specifically focusing on the representability of relationships in visual scenes. Researchers employ this dataset to assess various models' effectiveness in generating accurate and meaningful scene graphs, which are crucial for understanding complex visual data. This evaluation helps in advancing the methodologies for scene graph generation and improving the representation of visual relationships."
    },
    {
      "display_name": "Scene Graph",
      "normalized_name": "scenegraph",
      "name_variants": [
        "Scene Graph"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Scene Graph dataset is primarily used to train and evaluate models on visual relationship detection, focusing on object interactions and relationships in images. It integrates visual and textual data, providing dense annotations for connecting language and vision. The dataset supports research on multi-modal reasoning, relation predicate prediction, and image retrieval, emphasizing fine-grained object and relationship annotations. It is particularly useful for assessing model accuracy in predicting relation predicates based on instances' locations and categories."
    },
    {
      "display_name": "CC12M",
      "normalized_name": "cc12m",
      "name_variants": [
        "CC12M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CC12M dataset is primarily used for pre-training Vision-Language Pre-training (VLP) models, leveraging its large-scale image-text pairs to enhance multi-modal reasoning and representation learning. It provides over 100M samples, enabling web-scale pre-training that improves the recognition of long-tail visual concepts and expands the model's ability to understand diverse and rare visual elements. This dataset is crucial for improving multi-modal understanding and reasoning capabilities in VLP models."
    },
    {
      "display_name": "MSCOCO 2014 caption dataset",
      "normalized_name": "mscoco2014caption",
      "name_variants": [
        "MSCOCO 2014 caption dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MSCOCO 2014 caption dataset is primarily used to evaluate performance in image captioning tasks, where the focus is on generating accurate textual descriptions of images. Researchers employ this dataset to test and compare different models and algorithms, assessing their ability to produce coherent and contextually relevant captions. The dataset's rich collection of images and corresponding human-generated captions enables robust evaluation and benchmarking in this domain."
    },
    {
      "display_name": "FILIP300M",
      "normalized_name": "filip300m",
      "name_variants": [
        "FILIP300M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FILIP300M dataset is primarily used for pre-training Vision-Language Pre-training (VLP) models, providing a large-scale image-text dataset with over 100M samples. It enhances multi-modal reasoning performance, improves multi-modal understanding, and supports mixed-precision training. The dataset is also utilized for zero-shot retrieval and classification tasks, focusing on multi-modal reasoning. Additionally, it contributes to creating larger datasets like FILIP340M for multi-modal knowledge graph reasoning research."
    },
    {
      "display_name": "Visual Question Answering real-image dataset",
      "normalized_name": "visualquestionansweringrealimage",
      "name_variants": [
        "Visual Question Answering real-image dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Visual Question Answering real-image dataset is used to train and evaluate models on visual question answering tasks, integrating image and text modalities. It consists of approximately 200,000 MSCOCO images, divided into train (80K), validation (40K), and test (80K) splits. This dataset enables researchers to focus on developing models that can accurately answer questions about real images, enhancing the integration of visual and textual information."
    },
    {
      "display_name": "Motivation dataset",
      "normalized_name": "motivation",
      "name_variants": [
        "Motivation dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Motivation dataset is used for conducting experiments on multi-modal reasoning, specifically focusing on image-caption alignment. It consists of 10,191 images from Microsoft COCO, providing a rich set of annotated images. This dataset enables researchers to evaluate and develop models for aligning textual captions with visual content, enhancing multi-modal reasoning capabilities."
    },
    {
      "display_name": "IEMOCAP",
      "normalized_name": "iemocap",
      "name_variants": [
        "IEMOCAP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The IEMOCAP dataset is primarily used for multi-modal emotion recognition in conversational contexts, including dyadic and multiparty dialogues. It is employed to evaluate models like MMGCN, focusing on emotional interactions captured through audio, video, motion capture, and physiological signals. The dataset supports affective computing challenges by providing multimodal data for emotion and behavior analysis, enabling researchers to analyze emotions in various conversational settings."
    },
    {
      "display_name": "CC3M",
      "normalized_name": "cc3m",
      "name_variants": [
        "CC3M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CC3M dataset is used to enhance the training set for image captioning models, specifically to improve the quality and diversity of generated captions. It provides additional image-caption pairs, which help in training models to generate more accurate and contextually relevant captions. This dataset is crucial for expanding the training data, thereby enabling better performance in image captioning tasks."
    },
    {
      "display_name": "400 million image-text pairs",
      "normalized_name": "400millionimagetextpairs",
      "name_variants": [
        "400 million image-text pairs"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 400 million image-text pairs dataset is primarily used to pre-train CLIP, a model designed to learn the correspondence between raw text and images. This enhances multi-modal representation learning, enabling the model to effectively match textual descriptions with visual content. The large scale and diverse nature of the dataset are crucial for training robust and versatile multi-modal models."
    },
    {
      "display_name": "SituNet",
      "normalized_name": "situnet",
      "name_variants": [
        "SituNet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "SituNet is used to develop and train models for situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It contributes to defining visual event schemas, enabling systems to interpret image content more effectively. The dataset's emphasis on visual semantics supports the creation of robust situation recognition systems."
    },
    {
      "display_name": "OMAHA-MM",
      "normalized_name": "omahamm",
      "name_variants": [
        "OMAHA-MM"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OMAHA-MM dataset is used to evaluate the performance of the a-RotatE model on the Mean Rank (MR) metric, specifically in the context of multi-modal knowledge graph reasoning. It is employed to compare the effectiveness of a-RotatE against other models, focusing on reasoning tasks that integrate multiple data modalities."
    },
    {
      "display_name": "YAGO3",
      "normalized_name": "yago3",
      "name_variants": [
        "YAGO3"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YAGO3 dataset is used to construct a knowledge graph consisting of triplets, which supports hierarchical-knowledge embedded meta-learning for visual reasoning in artistic domains. This approach leverages the structured data in YAGO3 to enhance the understanding and reasoning capabilities in visual tasks, particularly those involving art. The dataset's rich hierarchical structure and extensive entity relationships enable researchers to develop more sophisticated and context-aware models for visual reasoning."
    },
    {
      "display_name": "HowNet",
      "normalized_name": "hownet",
      "name_variants": [
        "HowNet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "HowNet is used to annotate sememes for candidate words in poetry, enhancing multi-modal reasoning by providing rich semantic information. This dataset supports research focused on integrating semantic details into multi-modal systems, improving the understanding and processing of poetic language. The specific characteristic of HowNet that enables this research is its detailed sememe annotations, which capture nuanced meanings."
    },
    {
      "display_name": "PKG",
      "normalized_name": "pkg",
      "name_variants": [
        "PKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PKG dataset is used to construct a multi-modal knowledge graph for classical Chinese poetry, integrating allusions and visual information. This integration bridges the semantic gap between textual and visual modalities, enabling researchers to explore and analyze the complex relationships within classical Chinese poetry more effectively."
    },
    {
      "display_name": "KnowPoetry",
      "normalized_name": "knowpoetry",
      "name_variants": [
        "KnowPoetry"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KnowPoetry dataset is used to construct a knowledge graph of Tang poetry and poets, employing inference rules to mine social relationships. This supports further study of Tang poetry by enabling researchers to explore and analyze the interconnectedness of poets and their works, enhancing understanding of historical and cultural contexts."
    },
    {
      "display_name": "MAG240M",
      "normalized_name": "mag240m",
      "name_variants": [
        "MAG240M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MAG240M dataset is used for training and evaluating models in large-scale graph learning challenges, focusing on both node-level and link-level tasks. It supports research on model transferability and is particularly relevant for predicting links and nodes in multi-modal knowledge graphs, enabling comprehensive training and evaluation in graph-based machine learning tasks."
    },
    {
      "display_name": "COCO Caption",
      "normalized_name": "cococaption",
      "name_variants": [
        "COCO Caption"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The COCO Caption dataset is primarily used for pre-training ZLXMERT, a model focused on improving grounded language understanding. It provides region-level captions, enhancing the model's ability to perform complex reasoning over visual scenes, answer questions about them, and generate accurate image captions. This dataset is crucial for advancing visual-linguistic understanding and visual question answering tasks."
    },
    {
      "display_name": "VCR",
      "normalized_name": "vcr",
      "name_variants": [
        "VCR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VCR dataset is used for extensive experiments and validation of models on visual commonsense reasoning, particularly through multiple-choice QA problems derived from 110k movie scenes. It focuses on testing and improving state-of-the-art performance in visual reasoning tasks, enabling researchers to address complex visual understanding challenges."
    },
    {
      "display_name": "MM-FB15K",
      "normalized_name": "mmfb15k",
      "name_variants": [
        "MM-FB15K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MM-FB15K dataset is used to construct a multi-modal knowledge graph by integrating visual and textual information. This integration enhances reasoning capabilities, allowing researchers to explore complex relationships between different data types. The dataset's focus on combining modalities supports advanced reasoning tasks, making it valuable for developing and testing multi-modal reasoning systems."
    },
    {
      "display_name": "ICEWS05-15",
      "normalized_name": "icews0515",
      "name_variants": [
        "ICEWS05-15"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ICEWS05-15 dataset is primarily used to evaluate models on temporal reasoning tasks in dynamic knowledge graphs, focusing on event prediction, entity resolution, and entity linking. It is utilized to test the capability of models in handling large-scale global event data, assessing their performance on predicting events over time and managing long-term temporal dependencies. The dataset's density and richness, with 2.7 million facts for 500 entities and 20 relations, enable detailed analysis and improvement in metrics like MRR and Hits@1."
    },
    {
      "display_name": "ARC",
      "normalized_name": "arc",
      "name_variants": [
        "ARC"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ARC dataset is used to evaluate multi-modal reasoning capabilities, particularly in complex question answering tasks that require integrating textual and visual understanding. It enables researchers to assess systems' ability to reason across different modalities, focusing on the accuracy and effectiveness of multi-modal knowledge graph reasoning."
    },
    {
      "display_name": "CSQA",
      "normalized_name": "csqa",
      "name_variants": [
        "CSQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CSQA dataset is used to evaluate the performance of models like D RAGON on commonsense reasoning tasks, specifically by comparing them to other models such as RoBERTa. It enables researchers to measure and demonstrate significant accuracy gains, highlighting its utility in benchmarking and advancing natural language processing models in commonsense reasoning."
    },
    {
      "display_name": "CommonsenseQA",
      "normalized_name": "commonsenseqa",
      "name_variants": [
        "CommonsenseQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CommonsenseQA dataset is used to evaluate models like D RAGON on commonsense reasoning tasks, focusing on understanding everyday scenarios, concepts, and solving riddles that require logical and creative thinking. It challenges models with complex reasoning tasks, including multi-step inference and knowledge integration, thereby assessing their ability to handle nuanced and practical problems."
    },
    {
      "display_name": "BioKG",
      "normalized_name": "biokg",
      "name_variants": [
        "BioKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The BioKG dataset is used to facilitate relational learning on biomedical data, enabling the integration and analysis of diverse biological information within a multi-modal knowledge graph framework. This approach supports the exploration of complex relationships and patterns in biomedical data, enhancing the understanding of biological systems and processes."
    },
    {
      "display_name": "CUB",
      "normalized_name": "cub",
      "name_variants": [
        "CUB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CUB dataset is utilized for zero-shot learning tasks, integrating visual and semantic information to recognize unseen categories. It evaluates multi-modal reasoning, particularly in fine-grained classification tasks, using images and textual descriptions. The dataset enhances knowledge graph reasoning by combining visual and textual data, improving zero-shot recognition accuracy."
    },
    {
      "display_name": "Compositional ZSL",
      "normalized_name": "compositionalzsl",
      "name_variants": [
        "Compositional ZSL"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Compositional ZSL dataset is used to explore feature-to-sequence transformation in compositional zero-shot learning, specifically focusing on class descriptions composed of visual primitives such as states and objects. This methodology helps researchers address the challenge of recognizing unseen classes by leveraging structured class descriptions, enabling advancements in zero-shot learning scenarios."
    },
    {
      "display_name": "AWA2-KG",
      "normalized_name": "awa2kg",
      "name_variants": [
        "AWA2-KG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AWA2-KG dataset is primarily used for ontology-enhanced zero-shot learning, providing triples that represent relations between classes and attributes. It is utilized to validate the flexibility of models like DUET on various zero-shot learning attribute formats, focusing on enhancing model performance through knowledge graph integration. This dataset enables researchers to explore and improve zero-shot learning methodologies by leveraging structured relational data."
    },
    {
      "display_name": "Conceptual Captions",
      "normalized_name": "conceptualcaptions",
      "name_variants": [
        "Conceptual Captions"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Conceptual Captions dataset is primarily used for image-text pre-training, containing millions of cleaned and hypernymed image descriptions. It serves as a benchmark and pre-training data for multi-modal tasks, specifically focusing on automatic image captioning and image-text alignment. This dataset enables the development of models like ERNIE-ViL, enhancing their ability to understand and generate captions for image regions."
    },
    {
      "display_name": "YAGO26K-906",
      "normalized_name": "yago26k906",
      "name_variants": [
        "YAGO26K-906"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YAGO26K-906 dataset is primarily used for conducting experiments on multi-modal knowledge graph reasoning, particularly focusing on two-view KGs. It leverages YAGO, DBpedia, and Wikidata as base ontologies to explore tasks such as knowledge abstraction, concretization, and completion. The dataset is integral to the JOIE framework, enabling researchers to assess and develop multi-modal reasoning techniques."
    },
    {
      "display_name": "Charades-STA",
      "normalized_name": "charadessta",
      "name_variants": [
        "Charades-STA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Charades-STA dataset is primarily used for evaluating and improving multi-modal tasks involving action descriptions in videos. It serves as both a source and target domain for transferring annotation knowledge, enhancing the quality and coherence of action descriptions. Researchers use it to compare cross-domain alignment methods, assess performance in action localization and captioning, and ground textual descriptions with visual content, often focusing on specific metrics and convergence rates."
    },
    {
      "display_name": "BIOSNAP-sub",
      "normalized_name": "biosnapsub",
      "name_variants": [
        "BIOSNAP-sub"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The BIOSNAP-sub dataset is used to validate GENs (Graph Embedding Networks) for predicting out-of-graph (OOG) drug-to-drug interactions. It focuses on assessing the accuracy and generalizability of these models, enabling researchers to improve the reliability of drug interaction predictions. The dataset's characteristics support rigorous testing of model performance in unseen scenarios."
    },
    {
      "display_name": "FashionGen",
      "normalized_name": "fashiongen",
      "name_variants": [
        "FashionGen"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FashionGen dataset is primarily used to evaluate Cross-Modal Retrieval (CR) and Semantic Cross-Modal Retrieval (SCR) tasks, focusing on vision-language integration within the fashion domain. It supports research in vision-language pre-training and cross-modal retrieval experiments, employing consistent evaluation protocols such as those used in KaleidoBERT. This dataset enables researchers to assess models' ability to understand and retrieve information across different modalities, specifically images and text, in the context of fashion."
    },
    {
      "display_name": "DBP ZH-EN",
      "normalized_name": "dbpzhen",
      "name_variants": [
        "DBP ZH-EN"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DBP ZH-EN dataset is primarily used to evaluate cross-lingual knowledge alignment methods, specifically focusing on entity linking performance between Chinese and English. It is also utilized to assess alignment between DBpedia and Wikidata, enabling researchers to compare and improve the accuracy of cross-lingual entity linking across different language pairs and knowledge bases."
    },
    {
      "display_name": "KuaiRec 1",
      "normalized_name": "kuairec1",
      "name_variants": [
        "KuaiRec 1"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KuaiRec 1 dataset is used for evaluating multi-modal knowledge graph reasoning, specifically focusing on user-item and user-movie interactions to enhance recommendation accuracy. It enables researchers to assess the effectiveness of recommendation systems by analyzing user ratings and interactions, thereby improving the precision of recommendations."
    },
    {
      "display_name": "WN",
      "normalized_name": "wn",
      "name_variants": [
        "WN"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WN dataset, consisting of triplets from WordNet 3.0, is primarily used for multi-relational data modeling and knowledge graph representation learning. It is employed to train and evaluate models for tasks such as entity and relation embeddings, link prediction, and entity classification. The dataset supports the study of multi-modal reasoning and the handling of multiple labels per entity, enabling researchers to measure model performance using metrics like P@1, P@5, and N@5. It is also used for label collection and to contrast single-label constraints with more complex labeling schemes."
    },
    {
      "display_name": "FB-Ext",
      "normalized_name": "fbext",
      "name_variants": [
        "FB-Ext"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB-Ext dataset is used to evaluate link prediction models, specifically focusing on 'seen to unseen' and 'unseen to unseen' entity predictions. It extends entities from Freebase and NELL, enabling researchers to assess model performance in predicting relationships between known and unknown entities. This dataset facilitates the development and testing of algorithms designed to handle and predict links in knowledge graphs with extended entity sets."
    },
    {
      "display_name": "LRS2-2Mix",
      "normalized_name": "lrs22mix",
      "name_variants": [
        "LRS2-2Mix"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LRS2-2Mix dataset is used to train and validate audio-visual transformer models, focusing on the comparison of different methods' data usage for training and validation. This dataset enables researchers to evaluate the effectiveness of various training strategies and model architectures in handling multi-modal data, specifically audio and visual streams."
    }
  ]
}