{
  "summary": {
    "total_unique_datasets": 107,
    "total_dataset_mentions": 211,
    "unique_dataset_names": 107,
    "extraction_successful": 359,
    "extraction_failed": 2990,
    "unique_contexts_processed": 2592,
    "total_citation_instances": 3349,
    "total_processing_time": 165.54203724861145
  },
  "datasets_sorted_by_citation_count": [
    {
      "cited_paper_id": "254221022",
      "citation_count": 0,
      "total_dataset_mentions": 15,
      "unique_datasets": [
        "Freebase"
      ],
      "dataset_details": [
        {
          "dataset_name": "Freebase",
          "dataset_description": "Mentioned as a collaboratively created graph database for structuring human knowledge, but not explicitly used in the research context. | Mentioned as an encyclopedia knowledge source, but not explicitly used in the research context.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 207167677,
          "context_text": "…common sense knowledge (e.g., Cyc [1], ConceptNet [2]), lexical knowledge (e.g., WordNet [3], BabelNet [4]), encyclopedia knowledge (e.g., Freebase [5], DBpedia [6], YAGO [7], WikiData [8], CN-DBpedia [9]), taxonomic knowledge (e.g., Probase [10]) and geographic knowledge (e.g., GeoNames [11]).",
          "confidence_score": 0.5,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases and databases, which are relevant to multi-modal knowledge graph reasoning. However, none of these are explicitly used in the research context provided.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a large, collaboratively created graph database for structuring human knowledge, integrating data from diverse sources like Wikipedia and NNDB.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 207167677,
          "context_text": "• FreeBASE [227] is a large knowledge base generated from multiple sources, such as Wikipedia, NNDB, Fashion Model Directory, etc.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FreeBASE is identified as a large knowledge base used in the context of multi-modal knowledge graph reasoning, which aligns with the research topic.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Mentioned as a reusable resource, but specific usage details are not specified. | Mentioned as a reusable knowledge graph, but specific usage in the research context is not detailed.",
          "citing_paper_id": "269075665",
          "cited_paper_id": 7278297,
          "context_text": "Several KGs are available for commercial and academic use, including Free-base [6], DBpedia [2], and YAGO [88].",
          "confidence_score": 0.5,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several knowledge graphs, but does not specify their usage in the research context. The names are plausible and specific, but the actual use is not detailed.",
          "citing_paper_doi": "10.1145/3656579",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/af5e3a307546ff583195a70b3c1ad51dec41614b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations. | Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure. | Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 1181640,
          "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2025,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations. | Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure. | Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 1671874,
          "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2025,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations. | Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure. | Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 8423494,
          "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1609/aaai.v24i1.7519",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ddf0f2226cc837750eb1eb57c43d8192ef0fc2b3",
          "citing_paper_year": 2025,
          "cited_paper_year": 2010
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations. | Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure. | Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 207163173,
          "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1145/1242572.1242667",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff",
          "citing_paper_year": 2025,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a core of semantic knowledge unifying WordNet and Wikipedia, enhancing the representation of entities and their relations. | Used as a lexical database for English, providing a structured representation of words and their meanings for knowledge graph construction. | Used as a large-scale, multilingual knowledge base extracted from Wikipedia, providing a rich source of structured information for knowledge graphs. | Used as a collaboratively created graph database for structuring human knowledge, representing entities and their relations in a graph structure. | Used for never-ending language learning, continuously extracting and representing knowledge about the world in a graph structure.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 207167677,
          "context_text": "In contrast, knowledge graphs (KGs) have emerged as a powerful technique for modeling and utilizing knowledge by representing entities and their relations through a directed graph structure [1], [2], such as some notable examples Freebase [3], YAGO [4], WordNet [5], NELL [6], and DBpedia [7].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge bases that are used to represent entities and their relations in a graph structure, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2025,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as a general knowledge graph to provide structured information about entities and their relationships. | Used as a domain-specific knowledge graph to provide structured information about musical artists and recordings. | Used as a domain-specific knowledge graph to study drug-side effect relationships in medical contexts. | Used as a general knowledge graph to enhance semantic understanding and reasoning capabilities. | Used as a large-scale knowledge graph to integrate diverse sources of information. | This dataset 'IMDB' was mentioned in the citation context but no detailed description was generated. | Used as a general knowledge graph to support concept-based search and recommendation systems.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 10442573,
          "context_text": "…graph can be clas-siﬁed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB [37].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, including Freebase, YAGO, Knowledge Vault, Microsoft Concept Graph, SIDER, MusicBrainz, and IMDB. These are all specific, verifiable datasets or knowledge bases.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1093/nar/gkv1075",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1e62b537f3d30018e7979a89b0e0f15e2b6eecc",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as an example of a multi-modal knowledge graph, specifically for reasoning tasks involving music and multimedia data. | Used as an example of a general knowledge graph, derived from web-scale data to represent concepts and their relationships. | Used as an example of a domain-specific knowledge graph, focusing on drug side effects and indications. | Used as an example of a domain-specific knowledge graph, focusing on movies and their attributes. | Used as an example of a general knowledge graph, emphasizing large-scale extraction and integration of facts. | Used as an example of a general knowledge graph, structured collaboratively to represent human knowledge. | Used as an example of a general knowledge graph, focusing on linking entities to Wikipedia and GeoNames.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 14775471,
          "context_text": "The knowledge graph can be clas-siﬁed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, which are specific, verifiable resources. These are used as examples of general and domain-specific knowledge graphs.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1145/2213836.2213891",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/760103b363b1557372e048c4c31b5f01162bfcfa",
          "citing_paper_year": 2019,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as an example of a general knowledge graph, derived from web-scale data to represent concepts and their relationships. | Used as an example of a multi-modal knowledge graph, specifically for reasoning tasks involving music and multimedia data. | Used as an example of a domain-specific knowledge graph, focusing on drug side effects and indications. | Used as an example of a domain-specific knowledge graph, focusing on movies and their attributes. | Used as an example of a general knowledge graph, emphasizing large-scale extraction and integration of facts. | Used as an example of a general knowledge graph, structured collaboratively to represent human knowledge. | Used as an example of a general knowledge graph, focusing on linking entities to Wikipedia and GeoNames.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 207167677,
          "context_text": "The knowledge graph can be clas-siﬁed into general knowledge graph, such as Freebase [30], YAGO [31], Knowledge Vault [32], Microsoft Concept Graph [33], [34], and domain knowledge graph, such as medical knowledge graph SIDER [35], music knowledge graph MusicBrainz [36], movie knowledge graph IMDB…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs, which are specific, verifiable resources. These are used as examples of general and domain-specific knowledge graphs.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2019,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data. | Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information. | Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph.",
          "citing_paper_id": "271405981",
          "cited_paper_id": 212827903,
          "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
          "citing_paper_doi": "10.1109/ICDE60146.2024.00061",
          "cited_paper_doi": "10.1609/AAAI.V34I05.6299",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6fcb7f9fb1dab34affe2a3129c9c58536e1c01c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03549e327b9e896e00a544d61da36358478c522b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data. | Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information. | Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph.",
          "citing_paper_id": "271405981",
          "cited_paper_id": 254221022,
          "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
          "citing_paper_doi": "10.1109/ICDE60146.2024.00061",
          "cited_paper_doi": "10.48550/arXiv.2212.00959",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6fcb7f9fb1dab34affe2a3129c9c58536e1c01c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data. | Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information. | Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph.",
          "citing_paper_id": "271405981",
          "cited_paper_id": 258333655,
          "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
          "citing_paper_doi": "10.1109/ICDE60146.2024.00061",
          "cited_paper_doi": "10.1145/3543507.3583376",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6fcb7f9fb1dab34affe2a3129c9c58536e1c01c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8be9e2ad4696eac6168d3d0ed490f124eb100078",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Utilized to improve the performance of downstream tasks like semantic search, question answering, and entity recognition through its comprehensive structured data. | Used to evaluate the performance of the MULTIFORM method, specifically focusing on the enhancement of entity representations through multi-modal data. | Used to enhance the performance of downstream tasks such as semantic search, question answering, and entity recognition by providing structured information. | Employed to boost the performance of downstream tasks including semantic search, question answering, and entity recognition by leveraging its rich knowledge graph.",
          "citing_paper_id": "271405981",
          "cited_paper_id": 258333851,
          "context_text": "Knowledge graphs (KGs) like FreeBase [1], Wikidata [2], and YAGO [3] are widely used to improve the performance of downstream tasks, e.g., semantic search [4] [5] [6], question answering [7] [8], and entity recognition [9] [10].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several knowledge graphs (FreeBase, Wikidata, YAGO) and their applications in various tasks. These are specific, verifiable resources that are used to enhance downstream tasks.",
          "citing_paper_doi": "10.1109/ICDE60146.2024.00061",
          "cited_paper_doi": "10.1145/3543507.3583429",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6fcb7f9fb1dab34affe2a3129c9c58536e1c01c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/93b60a789d097c0b64d45e4de6505a9dcfb996e2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "18268744",
      "citation_count": 0,
      "total_dataset_mentions": 9,
      "unique_datasets": [
        "MSCOCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to evaluate cross-modal hashing methods, specifically comparing CMGCH with 16 state-of-the-art methods using Mean Average Precision (MAP). | Used to evaluate different algorithms with varying hash code lengths, focusing on multi-modal reasoning and image captioning tasks. | Used to set hyperparameters for a momentum encoder, focusing on queue length and update mechanisms in multi-modal learning. | Used to evaluate the proposed method's performance in cross-modal retrieval tasks, specifically measuring mean average precision (MAP) for image-to-text and text-to-image retrieval at various bit lengths. | Used for feature extraction with Faster RCNN and BUTD mechanism, investigating the impact of data augmentation on intra-modal contrastive learning. | Used to compare and analyze different variants of CMGCH, focusing on the effectiveness of various learning components in a multi-modal context. | Used to evaluate cross-modal hashing retrieval tasks, focusing on image-text matching and retrieval performance using MAP and R@K metrics.",
          "citing_paper_id": "268712378",
          "cited_paper_id": 14113767,
          "context_text": "In order to verify the effectiveness of different learning components, six types of CMGCH variants are compared and analyzed on MSCOCO.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MSCOCO' which is a well-known dataset in the field of computer vision and multi-modal learning. It is used here to compare and analyze different variants of CMGCH.",
          "citing_paper_doi": "10.1609/aaai.v38i12.29280",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/186a8a64a58832cd1b950adcb9fe8932b5af830d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used for image captioning to enhance the accuracy and context of generated captions through graph-structured information. | Used for image captioning to improve the generation of textual descriptions by leveraging graph-structured information. | Used for image captioning to improve the generation of captions for news images by incorporating graph-structured information. | Used for image tagging to enhance reasoning capabilities by constructing scene graphs and disambiguating concepts in web images.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 6483070,
          "context_text": "…by constructing scene graphs 3.enhance the reasoning capabilities of multi-modal data with graph-structured information Image Tagging NUS-WIDE [129] help disambiguation the concept and relate them better to images Image Captioning MSVD [130] MSCOCO [58] GoodNews [131] 1.enable the…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for enhancing multi-modal reasoning through graph-structured information. These datasets are used for image tagging and captioning tasks.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/1646396.1646452",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b80a580a6f2eca77524302acd944fd6edf0a0611",
          "citing_paper_year": 2022,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Leveraged for training on a diverse set of labeled images and annotations, expanding the system's multi-modal reasoning across various domains. | Used to train detectors on annotated images and captions, enhancing multi-modal understanding and reasoning capabilities. | Used to evaluate the precision of the visual object extraction model in GAIA, focusing on fine-grained multimedia knowledge extraction. | Employed to enhance entity recognition in images and associated text, supporting fine-grained multimedia knowledge extraction. | Used to evaluate the accuracy of visual grounding methods in the GAIA system, focusing on fine-grained multimedia knowledge extraction. | Utilized for training detectors on image-caption pairs, improving the system's ability to recognize and describe visual content accurately.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 218551030,
          "context_text": "These detectors are trained with supervised data from public images-text datasets [21] (such as MSCOCO [58], Flickr30k [59], Flick30k Entities [60] and Open Images [61]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image-text datasets used for training detectors. These datasets are specific and publicly accessible, making them relevant for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.18653/v1/2020.acl-demos.11",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/51c8975d88aa66781300e8ca88272ab3112445c0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Leveraged for training on a diverse set of labeled images and annotations, expanding the system's multi-modal reasoning across various domains. | Noted as a taxonomic knowledge base, but its application in the research is not described. | Referenced as an encyclopedia knowledge base, though its specific use in the research is not specified. | Used to train detectors on annotated images and captions, enhancing multi-modal understanding and reasoning capabilities. | Identified as an encyclopedia knowledge base, though its specific contribution to the research is unclear. | Listed as an encyclopedia knowledge base, but its use in the research is not described. | Cited as an encyclopedia knowledge base, with no specific mention of its application in the study. | Mentioned as a common sense knowledge base, but its application in the current research is not specified. | Noted as a lexical knowledge base, but its role in the research is not elaborated. | Employed to enhance entity recognition in images and associated text, supporting fine-grained multimedia knowledge extraction. | Mentioned as an encyclopedia knowledge base, but its role in the research is not detailed. | Utilized for training detectors on image-caption pairs, improving the system's ability to recognize and describe visual content accurately.",
          "citing_paper_id": "246823061",
          "cited_paper_id": null,
          "context_text": "These detectors are trained with supervised data from public images-text datasets [21] (such as MSCOCO [58], Flickr30k [59], Flick30k Entities [60] and Open Images [61]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several image-text datasets used for training detectors. These datasets are specific and publicly accessible, making them relevant for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to train object detection models, specifically for recognizing instances of objects and returning their bounding boxes and categories in multi-modal knowledge graph construction.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 14113767,
          "context_text": "Constructing MMKG We employ detectron2 (Wu et al. 2019) trained on the MSCOCO object detection classes (Lin et al. 2014) to recognize instances of objects and return their bounding boxes and object categories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of MSCOCO object detection classes for recognizing objects and returning bounding boxes and categories. This indicates the use of a specific dataset.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to train object detection models, specifically for recognizing instances of objects and returning their bounding boxes and categories in multi-modal knowledge graph construction.",
          "citing_paper_id": "250301328",
          "cited_paper_id": null,
          "context_text": "Constructing MMKG We employ detectron2 (Wu et al. 2019) trained on the MSCOCO object detection classes (Lin et al. 2014) to recognize instances of objects and return their bounding boxes and object categories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of MSCOCO object detection classes for recognizing objects and returning bounding boxes and categories. This indicates the use of a specific dataset.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object classification and segmentation in visual scenes. | Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object detection and annotation in diverse images. | Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on small-scale image recognition and object localization.",
          "citing_paper_id": "271962901",
          "cited_paper_id": 18268744,
          "context_text": "Vi-sionKG [29] is an MMKG containing bounding boxes of objects extracted from various image datasets such as MS-COCO [16], CI-FAR [13], and PASCAL VOC [9].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used to create Vi-sionKG, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3627673.3679175",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3472db260af3cc01602db3ae2d6ef02a3ee937c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086",
          "citing_paper_year": 2024,
          "cited_paper_year": 2009
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to evaluate cross-modal hashing methods, specifically comparing CMGCH with 16 state-of-the-art methods using Mean Average Precision (MAP).",
          "citing_paper_id": "268712378",
          "cited_paper_id": 13145195,
          "context_text": "We compare CMGCH with 16 state-of-the-art methods on MSCOCO dataset by MAP, including twelve unsupervised approaches (UCCH(Hu et al. 2022), DGCPN(Yu et al. 2021), LSSH(Wang et al. 2020), UKD-SS(Hu et al. 2020) and DSAH(Tu et al. 2022), CMFH(Lu et al. 2019), UCH(Li et al. 2019), FSH(Hong et al. 2017",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the MSCOCO dataset for comparing CMGCH with other methods using MAP. The dataset is used for evaluating cross-modal hashing methods.",
          "citing_paper_doi": "10.1609/aaai.v38i12.29280",
          "cited_paper_doi": "10.1109/CVPR.2017.672",
          "citing_paper_url": "https://www.semanticscholar.org/paper/186a8a64a58832cd1b950adcb9fe8932b5af830d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/aae669f2cba3d51641d95b1abca5a9a56ece309c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "MSCOCO",
          "dataset_description": "Used to evaluate cross-modal hashing methods, specifically comparing CMGCH with 16 state-of-the-art methods using Mean Average Precision (MAP).",
          "citing_paper_id": "268712378",
          "cited_paper_id": 214743601,
          "context_text": "We compare CMGCH with 16 state-of-the-art methods on MSCOCO dataset by MAP, including twelve unsupervised approaches (UCCH(Hu et al. 2022), DGCPN(Yu et al. 2021), LSSH(Wang et al. 2020), UKD-SS(Hu et al. 2020) and DSAH(Tu et al. 2022), CMFH(Lu et al. 2019), UCH(Li et al. 2019), FSH(Hong et al. 2017",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the MSCOCO dataset for comparing CMGCH with other methods using MAP. The dataset is used for evaluating cross-modal hashing methods.",
          "citing_paper_doi": "10.1609/aaai.v38i12.29280",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00319",
          "citing_paper_url": "https://www.semanticscholar.org/paper/186a8a64a58832cd1b950adcb9fe8932b5af830d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f669a973d3ac83f9dea52b8a629a2ae48be4532f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "14124313",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "ImageNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pre-train the VGG16 and ResNet-50 networks for the backbone image encoder, enhancing feature extraction capabilities.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 2141740,
          "context_text": "Speciﬁcally, we use the Glove embedding (Pennington, Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on ImageNet (Deng et al. 2009) for backbone image encoder.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet as a pre-training dataset for the backbone image encoder. No other datasets are mentioned as specific, reusable resources.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1007/978-3-319-46448-0_2",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pre-train the VGG16 and ResNet-50 networks for the backbone image encoder, enhancing feature extraction capabilities. | Used to pre-train the backbone image encoder (VGG16 or ResNet-50) for feature extraction in a multi-modal learning setup, enhancing the model's ability to recognize and reason about visual content.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 206594692,
          "context_text": "Speciﬁcally, we use the Glove embedding (Pennington, Socher, and Manning 2014) and Bi-LSTM (hidden dimension 256) for query features, and SSD (Liu et al. 2016) with VGG16 network or RetinaNet (Lin et al. 2017b) with Resnet-50 (He et al. 2016) network initialized with features pre-trained on ImageNet (Deng et al. 2009) for backbone image encoder.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet as a pre-training dataset for the backbone image encoder. No other datasets are mentioned as specific, reusable resources.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1109/cvpr.2016.90",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to pre-train ResNet-152 and VGG-16 models for learning image embeddings in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "271961581",
          "cited_paper_id": 14124313,
          "context_text": "We use ResNet-152 [20] or VGG-16 [42], pre-trained visual models that has been trained on ImageNet [12], to learn image embeddings for entities adopted by the previous MMKG models [26].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ImageNet as a dataset used for pre-training visual models. No other datasets are mentioned.",
          "citing_paper_doi": "10.1145/3637528.3671769",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d4d60a5c303cec0f8df9138eb2dd7a535f970294",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
          "citing_paper_year": 2024,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and DBpedia subsets. | Used to pre-train ResNet-152 and VGG-16 models for learning image embeddings in multi-modal knowledge graph reasoning. | Subset of Freebase used for cross-KG reasoning experiments, focusing on entity and relation alignment. | Subset of DBpedia used for cross-KG reasoning experiments, focusing on entity and relation alignment. | Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and YAGO subsets. | Subset of YAGO used for cross-KG reasoning experiments, focusing on entity and relation alignment.",
          "citing_paper_id": "271961581",
          "cited_paper_id": null,
          "context_text": "We use ResNet-152 [20] or VGG-16 [42], pre-trained visual models that has been trained on ImageNet [12], to learn image embeddings for entities adopted by the previous MMKG models [26].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet as a dataset used for pre-training visual models. No other datasets are mentioned.",
          "citing_paper_doi": "10.1145/3637528.3671769",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d4d60a5c303cec0f8df9138eb2dd7a535f970294",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used in multi-modal reasoning experiments, though specific details about its usage are not provided in the context. | Used to construct a semantic tree of millions of images, aligning with WordNet's 80,000 synonyms, with an average of 500-1000 clean, high-resolution images per synonym.",
          "citing_paper_id": "276928279",
          "cited_paper_id": 1671874,
          "context_text": "…namely ImageNet [6], AWA2 [7], and Attribute Pascal and Yahoo (aPY) [8] ImageNet:This dataset aims to use an average of 500-1000 clean and high-resolution images to form the majority of WordNet [9]’s 80000 synonyms, and to construct a semantic tree of millions of ImageNet images using WordNet.",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: ImageNet, AWA2, and aPY. ImageNet is described in detail, while AWA2 and aPY are mentioned without additional details.",
          "citing_paper_doi": "10.48550/arXiv.2503.07202",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/253a5e04cb17c2490175a2718ff29cf2bc924971",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2025,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used to test the main experiment, focusing on zero-shot learning by leveraging attributes and class labels for animal categories. | Used to test the main experiment, focusing on zero-shot learning by constructing a semantic tree from high-resolution images representing WordNet synonyms. | Used to test the main experiment, focusing on zero-shot learning by combining attributes and class labels from Pascal VOC and Yahoo datasets.",
          "citing_paper_id": "276928279",
          "cited_paper_id": 4852047,
          "context_text": "Our main experiment was tested on three datasets, namely ImageNet [6], AWA2 [7], and Attribute Pascal and Yahoo (aPY) [8] ImageNet:This dataset aims to use an average of 500-1000 clean and high-resolution images to form the majority of WordNet [9]’s 80000 synonyms, and to construct a semantic tree…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for testing the main experiment. These datasets are clearly identified and are relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2503.07202",
          "cited_paper_doi": "10.1109/TPAMI.2018.2857768",
          "citing_paper_url": "https://www.semanticscholar.org/paper/253a5e04cb17c2490175a2718ff29cf2bc924971",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ffe28f6bf0e9e6bbca6313319aa1a5409d283d9b",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ImageNet",
          "dataset_description": "Used for pretraining ResNet to extract 2D visual features from video frames, enhancing the model's ability to recognize objects and scenes.",
          "citing_paper_id": "252782878",
          "cited_paper_id": null,
          "context_text": "For video embedding, we extract 2D and 3D visual features for each video frame by ResNet pretrained on ImageNet[4] and SlowFast[6] pretrained on Kinetics[10] respectively.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ImageNet and Kinetics, which are well-known datasets used for pretraining models. However, they are not used directly in the research described but rather as sources for pretrained models.",
          "citing_paper_doi": "10.1145/3503161.3551604",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2da4666811d0095ab0a71e01e238277a1d908c02",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "67474824",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "IMGpedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Used to create a linked dataset with content-based analysis of Wikimedia images, enhancing multi-modal knowledge graph reasoning through image-text integration.",
          "citing_paper_id": "268697475",
          "cited_paper_id": 3117929,
          "context_text": "MMKGs along this line include IMGpedia (Ferrada, Bustos, and Hogan 2017), Richpedia (Wang et al. 2020), and Visu-alSem (Alberts et al. 2020).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions IMGpedia, Richpedia, and VisualSem as examples of multi-modal knowledge graphs (MMKGs). IMGpedia is a linked dataset with content-based analysis of Wikimedia images, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1609/aaai.v38i17.29828",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80ef8e5aa7df5c96fd0f0e9d847e0617be743c14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Used to ground Wikimedia Commons images into DBpedia entities, focusing on content-based analysis and linking images to structured data.",
          "citing_paper_id": "271962901",
          "cited_paper_id": 3117929,
          "context_text": "IMGpedia [10] is an MMKG that grounds Wikimedia Commons images into DBpedia [2] entities.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "IMGpedia is identified as a multi-modal knowledge graph that links images to DBpedia entities, which is directly relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3627673.3679175",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3472db260af3cc01602db3ae2d6ef02a3ee937c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Utilized as a large-scale, comprehensive multi-modal knowledge graph to support reasoning across various data types and modalities. | Serves as a foundational dataset for multi-modal knowledge graph reasoning, though specific details on its structure and use are not provided. | Used to provide content-based analysis of Wikimedia images, enhancing multi-modal reasoning capabilities in knowledge graphs.",
          "citing_paper_id": "259203022",
          "cited_paper_id": 3117929,
          "context_text": "Common MKGs are IMGpedia [9], Rich-pedia [40], and FB-Des [36].",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2025.3546686",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3f1473dd6c0f159a255b16f44998663e605b4878",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Utilized as a large-scale, comprehensive multi-modal knowledge graph to support reasoning across various data types and modalities. | Serves as a foundational dataset for multi-modal knowledge graph reasoning, though specific details on its structure and use are not provided. | Used to provide content-based analysis of Wikimedia images, enhancing multi-modal reasoning capabilities in knowledge graphs.",
          "citing_paper_id": "259203022",
          "cited_paper_id": 225115084,
          "context_text": "Common MKGs are IMGpedia [9], Rich-pedia [40], and FB-Des [36].",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2025.3546686",
          "cited_paper_doi": "10.1016/j.bdr.2020.100159",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3f1473dd6c0f159a255b16f44998663e605b4878",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d39da9432ff7bd1d42349b96cbd194416c3d51e1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Used as a large multi-modal knowledge graph with image-to-image and image-to-text relationships, supporting research in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 3117929,
          "context_text": "IMGpedia [9] is a large multi-modal knowledge graph which includes two types of relationships: image-to-image and image-to-text.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "IMGpedia is identified as a multi-modal knowledge graph, which fits the topic of multi-modal knowledge graph reasoning. It is used as a reusable resource.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Serves as the source of images for IMGpedia, providing a large-scale collection of multimedia files for content-based analysis and visuo-semantic queries. | Used as a knowledge graph to incorporate visual information from Wikimedia Commons images, enabling visuo-semantic queries and content-based analysis.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 67474824,
          "context_text": "• IMGpedia [223], [238], [239], [240] is the KG, which incorporates visual information of the images from the Wikimedia Commons dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "IMGpedia is identified as a knowledge graph incorporating visual information from the Wikimedia Commons dataset. The context indicates that IMGpedia is used as a reusable resource.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3a3d374a13e3cf4c69730e5c52138c0be57f6f2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "IMGpedia",
          "dataset_description": "Serves as the source of images for IMGpedia, providing a large-scale collection of multimedia files for content-based analysis and visuo-semantic queries. | Used as a knowledge graph to incorporate visual information from Wikimedia Commons images, enabling visuo-semantic queries and content-based analysis.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 3117929,
          "context_text": "• IMGpedia [223], [238], [239], [240] is the KG, which incorporates visual information of the images from the Wikimedia Commons dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "IMGpedia is identified as a knowledge graph incorporating visual information from the Wikimedia Commons dataset. The context indicates that IMGpedia is used as a reusable resource.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-319-68204-4_8",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5a19c57e29b5ac537e53d033be2c62db0e1f2e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "14124313",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "WN9-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph representation learning, focusing on image-embodied knowledge representation and translation-based approaches.",
          "citing_paper_id": "258509157",
          "cited_paper_id": 9909815,
          "context_text": "There includes multi-modal datasets: WN9-IMG [41] and FB-IMG [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, WN9-IMG and FB-IMG, which are used in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph representation learning, focusing on image-embodied knowledge representation and translation-based approaches.",
          "citing_paper_id": "258509157",
          "cited_paper_id": 44145776,
          "context_text": "There includes multi-modal datasets: WN9-IMG [41] and FB-IMG [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, WN9-IMG and FB-IMG, which are used in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to learn visual embeddings using the VGG19 model, focusing on multi-modal reasoning tasks involving image and word pairs.",
          "citing_paper_id": "258509157",
          "cited_paper_id": 14124313,
          "context_text": "As for the WN9-IMG dataset, we take the VGG19 [45] model to learn visual embeddings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for multi-modal learning tasks. The citation intent is to describe the use of a reusable resource.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to construct an image-embodied knowledge representation, focusing on integrating visual and textual information for multi-modal reasoning.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 9909815,
          "context_text": "WN9-IMG This dataset constructed by Xie et al. [37] is the subset of WN18 [4], which comes from the large lexical knowledge base WordNet [21].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'WN9-IMG', which is a specific dataset derived from WN18, a subset of WordNet. It is clearly identified and used in the research.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on raw Hits@10.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 31606602,
          "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1609/aaai.v30i1.10329",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96acb1c882ad655c6b8459c2cd331803801446ca",
          "citing_paper_year": 2021,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on raw Hits@10. | Used as a subset for training and evaluating knowledge graph representation models, focusing on multimodal translation-based approaches.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 44145776,
          "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "WN9-IMG",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning models, specifically comparing performance on various metrics, with a focus on raw Hits@10.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 211137418,
          "context_text": "First, our proposed MMKRL outperforms the other multi-modal KRL models [25, 32, 35, 36] on all metrics apart from Raw Hits@10 on the WN9-IMG dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the WN9-IMG dataset, which is a specific, verifiable dataset used for evaluating multi-modal knowledge graph reasoning models.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1109/TCDS.2019.2906685",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3f3bba81ab55d7ca7d3064241d7595592bc9dc86",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "21277943",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "Visual Genome"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to incorporate commonsense triples, enriching the knowledge graph with everyday reasoning and conceptual relationships. | Used to provide has-Part triples, contributing to the construction of a knowledge graph with part-whole relationships. | Used to connect language and vision using crowdsourced dense image annotations, specifically for visual data in multi-modal knowledge graph reasoning. | Applied to connect language and vision through dense image annotations, supporting complex multi-modal reasoning tasks. | Used to generate more fluent stories through logical inference between images, demonstrating superior performance compared to non-KG methods. | Used to supply location triples of visual objects, integrating spatial information into the knowledge graph. | Utilized for training and evaluating image-text alignment models, emphasizing rich caption annotations and diverse visual content. | Used for evaluating image-text matching, focusing on aligning captions with images using multi-modal representations. | Used to extract hasPart/isA triples, enhancing the knowledge graph with hierarchical and part-whole relationships. | Used to label objects with bounding boxes, connecting language and vision using crowdsourced dense image annotations.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 4492210,
          "context_text": "2: Examples of labeling images: (a) labeling components after image segmentation in Visipedia [58]; (b) labeling objects with bounding boxes in Visual Genome [59]; (c) labeling two objects where one is a part of the other in NEIL [19].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Visual Genome' as an example of labeling objects with bounding boxes, which is a dataset used for connecting language and vision.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to train a relation detection model for predicting existing relations between object nodes in a knowledge graph, enhancing multi-modal reasoning capabilities.",
          "citing_paper_id": "56895321",
          "cited_paper_id": 52304560,
          "context_text": "Speciﬁcally, there are only connections be-tween two object nodes when the relation detection trained on Visual Genome [32] predicts existing relations, regardless of the relation types.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for training a relation detection model. The dataset is used to predict existing relations between object nodes in a knowledge graph.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-030-01264-9_42",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd2f6fe2cd8e96ca62a9c1c9e12973b8e13d5609",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0000fcfd467a19cf0e59169c2f07d730a0f3a8b9",
          "citing_paper_year": 2018,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to train a relation classifier for generating relationship embeddings, focusing on visual relationship detection and scene graph generation.",
          "citing_paper_id": "56895321",
          "cited_paper_id": 21277943,
          "context_text": "Inspired by recent works in Visual Relationship Detection [6, 13] and Scene Graph Generation [20], we train a relation classiﬁer to generate relationship embeddings on Visual Genome [16].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Visual Genome' as a dataset used for training a relation classifier to generate relationship embeddings, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/ICCV.2017.142",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd2f6fe2cd8e96ca62a9c1c9e12973b8e13d5609",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cf2de559e5a6235783e0762862f6e42192f142a8",
          "citing_paper_year": 2018,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Sub-sampled split of Flickr30K Entities used for training and evaluation, focusing on region-phrase correspondences. | Used for zero-shot grounding experiments, providing a rich set of annotations for images to test model performance. | Used for zero-shot phrase grounding experiments, focusing on region-to-phrase correspondences in images. | Used for zero-shot phrase grounding experiments, providing region-to-phrase correspondences to enhance image-to-sentence models.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 6941275,
          "context_text": "Extensive experiments were performed on zero-shot phrase grounding splits introduced by (Sadhu, Chen, and Nevatia 2019), which were developed on Visual Genome (Krishna et al. 2017) and Flickr30K Entities (Plummer et al. 2015; Young et al. 2014).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for zero-shot phrase grounding experiments, including Visual Genome and Flickr30K Entities.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1007/s11263-016-0965-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11c9c31dff70de92ada9160c78ff8bb46b2912d6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used to pre-train a Faster R-CNN model for extracting localized object-level features, enhancing multi-modal reasoning capabilities.",
          "citing_paper_id": "278782132",
          "cited_paper_id": 4492210,
          "context_text": "To extract localized object-level features, we utilize a bottom-up attention mechanism based on Faster R-CNN [22], pre-trained on the Visual Genome dataset [23].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Visual Genome dataset, which is a specific, verifiable resource used for pre-training a model to extract localized object-level features.",
          "citing_paper_doi": "10.48550/arXiv.2505.14714",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/868dffeabaf0128614622ec27914c7328a97355d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Visual Genome",
          "dataset_description": "Used for predicate classification tasks to verify the effectiveness of EDET, focusing on connecting language and vision using dense image annotations.",
          "citing_paper_id": "259180790",
          "cited_paper_id": 4492210,
          "context_text": "Finally, the effectiveness of EDET was veriﬁed by predicate classiﬁcation and scene description tasks on Visual Genome dataset [17] and Flickr30k dataset [18], respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the use of two specific datasets, Visual Genome and Flickr30k, for verifying the effectiveness of EDET through predicate classification and scene description tasks.",
          "citing_paper_doi": "10.3390/app13127115",
          "cited_paper_doi": "10.1007/s11263-016-0981-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3309cc53ccc37ec5f26f04632bedcf018c83db2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "2924682",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "ConceptNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Employed to leverage structured information extracted from Wikipedia, enriching the knowledge base for reasoning tasks. | Used to integrate common-sense knowledge into multi-modal reasoning systems, enhancing the understanding of complex relationships. | Utilized to incorporate structured data and entity relationships, supporting the construction of comprehensive knowledge graphs. | Applied to access a collaborative, multilingual knowledge base, providing a rich source of structured data for reasoning.",
          "citing_paper_id": "252782478",
          "cited_paper_id": 14494942,
          "context_text": "At this point, we can use public knowledge bases such as ConceptNet [33], Freebase [4], DBPedia [18], and Wiki-Data [36].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several public knowledge bases, which are relevant to multi-modal knowledge graph reasoning. These resources are explicitly named and are likely to be used for integrating structured knowledge into reasoning systems.",
          "citing_paper_doi": "10.1145/3503161.3548321",
          "cited_paper_doi": "10.1145/2629489",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3959ad38bea60eaeaaf415977030b11d385f49d8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dab7e605237ad4f4fe56dcba2861b8f0a57112be",
          "citing_paper_year": 2022,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to construct a knowledge graph by combining entities from document images and keywords from QA pairs, enhancing factual visual question answering.",
          "citing_paper_id": "252782478",
          "cited_paper_id": 53199920,
          "context_text": "Similar to previous work [30], a knowledge graph represented as a subset RQ,I of all ConceptNet RDF triplesR can be constructed by combining entities in document image and keywords in QA pairs from the dataset.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions constructing a knowledge graph using ConceptNet RDF triples, which suggests the use of a specific dataset. However, the exact name 'ConceptNet' is not mentioned in the context, only inferred from the cited paper title.",
          "citing_paper_doi": "10.1145/3503161.3548321",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3959ad38bea60eaeaaf415977030b11d385f49d8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to retrieve first-order subgraphs for multi-modal knowledge graph reasoning, focusing on edges connecting candidate nodes. | Used to mine common-sense relationships for aiding reasoning in open-domain visual question answering, enhancing the model's ability to understand and answer questions.",
          "citing_paper_id": "20059796",
          "cited_paper_id": 2924682,
          "context_text": "In this paper, we adopt external knowledge mined from Concept-Net [23], an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ConceptNet is identified as a knowledge graph used to aid reasoning in open-domain VQA, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/23ed7f18100717ba814b2859196e10c5d4fed216",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1b97b4623cf2f183340e548e0aa53abf0f2963d8",
          "citing_paper_year": 2017,
          "cited_paper_year": 2012
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to extract semantic relations for multi-modal knowledge graph reasoning, enhancing the understanding of object interactions and related concepts. | Used to represent commonsense knowledge in a graph format, focusing on nodes as concepts and edges as relations, enhancing multi-modal reasoning capabilities. | Leveraged as an external knowledge corpus to enhance relation learning, providing abundant object and relation knowledge for multi-modal reasoning. | Leveraged as an external knowledge corpus to understand and enhance object relations in a graph neural network framework, focusing on general knowledge across multiple languages. | Used to build the object graph by exploring intra-event object relations, enhancing fine-grained event information through external knowledge integration.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 15206880,
          "context_text": "To better understand the relations, we leverage an external knowledge corpus ConceptNet [41] which has abundant object and relation knowledge to conduct the relation learning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ConceptNet is mentioned as an external knowledge corpus used for relation learning, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to enhance an initial scene graph generated by Faster R-CNN, integrating general knowledge to improve multi-modal reasoning in images. | Used to enhance an initial scene graph generated by Faster R-CNN, providing lexical information to improve multi-modal reasoning in images.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 15206880,
          "context_text": "…relations among them, representing the image as a directed graph with entities as nodes and relations as directed edges similar to a KG. GB-Net [417] uses Faster R-CNN to generate an initial scene graph, which is then enhanced by bridging it with external KGs such as ConceptNet [418] and WordNet.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of external KGs such as ConceptNet and WordNet to enhance an initial scene graph. ConceptNet is a specific, verifiable dataset, while WordNet is a lexical database.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2025,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to enhance an initial scene graph generated by Faster R-CNN, specifically by adding external knowledge to improve the representation of entities and relations in images.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 210064217,
          "context_text": "…relations among them, representing the image as a directed graph with entities as nodes and relations as directed edges similar to a KG. GB-Net [417] uses Faster R-CNN to generate an initial scene graph, which is then enhanced by bridging it with external KGs such as ConceptNet [418] and…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of external KGs such as ConceptNet to enhance an initial scene graph generated by Faster R-CNN. ConceptNet is a specific, verifiable dataset.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1007/978-3-030-58592-1_36",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3e5f240d634be018536e6419672daf38dd4d24bb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "244527692",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "Wikipedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used to generate 2,866 imagetext pairs from Wikipedia’s ‘featured articles’ for cross-modal multimedia retrieval, focusing on correlation and abstraction in the data. | Used to test the GCR model's capabilities in cross-modal retrieval, specifically in handling diverse multimedia content. | Used to evaluate the GCR model's performance in cross-modal retrieval, focusing on the integration of textual and visual information. | Used to assess the GCR model's effectiveness in cross-modal multimedia retrieval, emphasizing the correlation and abstraction in image-text pairs. | Used to evaluate the GCR model's ability to align images with descriptive sentences, focusing on fine-grained visual and textual matching. | Used for cross-modal retrieval experiments with 1,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval. | Used for cross-modal retrieval experiments with 3,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval.",
          "citing_paper_id": "239011700",
          "cited_paper_id": 15512280,
          "context_text": "Following [17], the 3,000-dimensional, 1,000-dimensonal, and 3,000-dimensional BoW features are separately used for Wikipedia, NUS-WIDE-10K, and XMedia datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: Wikipedia, NUS-WIDE-10K, and XMedia. These are used for cross-modal multimedia retrieval experiments with different dimensional BoW features.",
          "citing_paper_doi": "10.1145/3474085.3475567",
          "cited_paper_doi": "10.1109/TPAMI.2013.142",
          "citing_paper_url": "https://www.semanticscholar.org/paper/50ee08270ff4204b5d348c6d654d0aa57b29fe66",
          "cited_paper_url": "https://www.semanticscholar.org/paper/13edf531cf6367c6a994f9a2b3e527f438433cdb",
          "citing_paper_year": 2021,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used for cross-modal retrieval experiments with 1,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval. | Used for cross-modal retrieval experiments with 3,000-dimensional BoW features, focusing on the role of correlation and abstraction in multimedia retrieval.",
          "citing_paper_id": "239011700",
          "cited_paper_id": 27837890,
          "context_text": "Following [17], the 3,000-dimensional, 1,000-dimensonal, and 3,000-dimensional BoW features are separately used for Wikipedia, NUS-WIDE-10K, and XMedia datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: Wikipedia, NUS-WIDE-10K, and XMedia. These are used for cross-modal multimedia retrieval experiments with different dimensional BoW features.",
          "citing_paper_doi": "10.1145/3474085.3475567",
          "cited_paper_doi": "10.1109/TCYB.2018.2879846",
          "citing_paper_url": "https://www.semanticscholar.org/paper/50ee08270ff4204b5d348c6d654d0aa57b29fe66",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e5567742e53fd795cba76da8c23ca3123f9f8cee",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used for event classification and relation extraction, contributing to the construction of the MMEKG multi-modal event knowledge graph. | Used for lexical data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph. | Used for news-related textual data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph. | Used for visual data to support object recognition and event relation extraction in the MMEKG multi-modal event knowledge graph. | Used for textual data to support event classification and relation extraction in the MMEKG multi-modal event knowledge graph.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 248779998,
          "context_text": "MMEKG [24] N-MMKG event Wikipedia, BookCorpus, CC3M&CC12M, C4(news) WordNet event classification, object recognition, event relation extraction < 990K events, < 644 event relations < 863M instance events, < 934M instance events’ relations (including textual and visual ones)",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for constructing and evaluating the MMEKG multi-modal event knowledge graph, including Wikipedia, BookCorpus, CC3M, CC12M, and C4(news). These datasets are used for various tasks such as event classification, object recognition, and event relation extraction.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.18653/v1/2022.acl-demo.23",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9367e642fa47c844834e4415c8cac2a315ea5be6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used to collect entities and their aspect labels by treating section names in Wikipedia pages as aspect labels, focusing on entity representation and aspect labeling.",
          "citing_paper_id": "260735729",
          "cited_paper_id": 244527692,
          "context_text": "Following previous work [12, 16, 21], we collect entities and their aspect labels from Wikipedia by regarding the section names in each entity’s Wikipedia page as its aspect labels.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions collecting entities and aspect labels from Wikipedia, which suggests the use of Wikipedia as a dataset. However, it does not specify a particular dataset or version of Wikipedia.",
          "citing_paper_doi": "10.1145/3583780.3614782",
          "cited_paper_doi": "10.1145/3488560.3498405",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0356abf345662110b6d6a818304c886ce45b6f04",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be5ab2e3e4871d514919a17c266861d7e73aaf69",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Wikipedia",
          "dataset_description": "Used to extract structured data for constructing a knowledge graph, focusing on the integration of multi-modal information from web sources. | Used to generate a metallic materials knowledge graph, integrating structured information from DBpedia and Wikipedia. | Used to extract textual content for constructing a knowledge graph, focusing on the integration of multi-modal information from web sources. | Used to construct a knowledge graph based on WordNet, focusing on lexical and semantic relationships. | Used to build a multilingual knowledge graph, providing a large-scale lexicalized semantic network.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 588863,
          "context_text": "MMKG [11] extracts data from DBpedia and Wikipedia to construct knowledge graph.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions DBpedia and Wikipedia as sources for constructing a knowledge graph, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.1016/J.CPC.2016.07.005",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34f3dc2393f5bc166c66ceed5b915d816ad78e64",
          "citing_paper_year": 2019,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "202539519",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "DBpedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to extract raw triplet facts for constructing a knowledge graph, focusing on the integration of multi-modal data and reasoning over the extracted facts.",
          "citing_paper_id": "268697475",
          "cited_paper_id": 1181640,
          "context_text": "In this paper, we obtain the raw triplet facts from DBpedia 1 (Lehmann et al. 2015).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DBpedia is a well-known knowledge base extracted from Wikipedia, which fits the criteria for a dataset. The context indicates it is used as a source of raw triplet facts.",
          "citing_paper_doi": "10.1609/aaai.v38i17.29828",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80ef8e5aa7df5c96fd0f0e9d847e0617be743c14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used to collect textual descriptions for the latter two datasets, providing structured information for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "274397704",
          "cited_paper_id": 202539519,
          "context_text": "The textual descriptions of the first two datasets are collected by [56], while the textual descriptions of the latter two datasets are collected from DBpedia [57].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions two sources for textual descriptions, but does not specify the names of the datasets. DBpedia is referenced, but it is not clear if it is used as a dataset or just a source of information.",
          "citing_paper_doi": "10.3390/s24237605",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/46bc47ade14a95d8d51da938292f0ececbd855e0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/31184789ef4c3084af930b1e0dede3215b4a9240",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 213399202,
          "context_text": "The most representative KGs, e.g., DBpedia [15], Freebase [3], and WordNet [21], benefit various kinds of downstream tasks, such as semantic analysis [29], question-answer systems [14], and machine comprehension [16].",
          "confidence_score": 0.5,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions DBpedia, Freebase, and WordNet as representative KGs, but does not specify their usage in a particular research context. These are general references to well-known knowledge graphs.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1016/j.neucom.2020.01.056",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2721cd5c2afe83ec3d81bc3a216b0bb9ce16b986",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Mentioned as a large-scale, multilingual knowledge base extracted from Wikipedia, but the specific usage in the research context is not detailed.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 1181640,
          "context_text": "And most of the text resources are associated with DBpedia [10].",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions DBpedia, which is a well-known knowledge base. However, it does not specify how it is used in the research context.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2019,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DBpedia",
          "dataset_description": "Used as a structured content source from Wikimedia projects, providing a nucleus for a web of open data, enhancing multi-modal knowledge graph reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 7278297,
          "context_text": "• DBpedia [226] consists of structured content from the information created in various Wikimedia projects.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DBpedia is mentioned as a structured content source from Wikimedia projects, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "266028051",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "WordNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used as a lexical database for English, providing a structured vocabulary for semantic relationships in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "252782967",
          "cited_paper_id": 1671874,
          "context_text": "A number of large-scale knowledge graphs are available commercially or in open source, which are generally constructed based on common sense [50], Wikipedia[3], English word [40].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Wikipedia' and 'English word', which could refer to datasets. However, 'Wikipedia' is not a specific dataset but a general source, and 'English word' is too generic. 'WordNet' is a specific lexical database mentioned in the cited paper title, which is relevant.",
          "citing_paper_doi": "10.1145/3503161.3548257",
          "cited_paper_doi": "10.1145/219717.219748",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8db962faf0ec4708f0e14410eabd23521d244619",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677",
          "citing_paper_year": 2022,
          "cited_paper_year": 1995
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used as a lexical database for English, providing a structured vocabulary for semantic relationships in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "252782967",
          "cited_paper_id": 7278297,
          "context_text": "A number of large-scale knowledge graphs are available commercially or in open source, which are generally constructed based on common sense [50], Wikipedia[3], English word [40].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Wikipedia' and 'English word', which could refer to datasets. However, 'Wikipedia' is not a specific dataset but a general source, and 'English word' is too generic. 'WordNet' is a specific lexical database mentioned in the cited paper title, which is relevant.",
          "citing_paper_doi": "10.1145/3503161.3548257",
          "cited_paper_doi": "10.1007/978-3-540-76298-0_52",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8db962faf0ec4708f0e14410eabd23521d244619",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Used to build an undirected graph for commonsense knowledge, providing entity and relation nodes for a given image and query. | Used to build an undirected graph for commonsense knowledge, specifically incorporating entity and relation nodes for a given image and query.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 266028051,
          "context_text": "…Knowledge We ﬁrst consider building an undirected graph G cms for a given image I and query q based on the commonsense knowledge from WordNet (Fell-baum 2012) and ConceptNet (Liu and Singh 2004), where the node set includes three types of nodes: entity nodes , relation nodes , and object nodes .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions using WordNet and ConceptNet for building a graph based on commonsense knowledge. These are specific resources used for their knowledge bases.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1023/B:BTTJ.0000047600.45421.6D",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b3fea597033a46d5ae282464a8f16d6715187e70",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        },
        {
          "dataset_name": "WordNet",
          "dataset_description": "Serves as a large lexical knowledge base for constructing and evaluating knowledge graphs, providing a rich structure of semantic relations. | Used as a subset of WordNet for evaluating link prediction in knowledge graphs, focusing on relational patterns and entity types.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 67700681,
          "context_text": "[37] is the subset of WN18 [4], which comes from the large lexical knowledge base WordNet [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions WN18, which is derived from WordNet. Both are specific, verifiable datasets used in knowledge graph research.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2fd46c6acedb9fb68c59c9ec435eb192ec0cd914",
          "citing_paper_year": 2021,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "221193809",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "DB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DB15K",
          "dataset_description": "Used as a multi-modal knowledge graph with image, text, and numerical information for reasoning tasks, serving as a subset of DBpedia.",
          "citing_paper_id": "270711106",
          "cited_paper_id": 1181640,
          "context_text": "• DB15K [32] is an MMKG with image, text, and numerical information proposed by [32], which is a subset of DBpedia [26].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "DB15K is identified as a multi-modal knowledge graph (MMKG) containing image, text, and numerical information, which is a subset of DBpedia. It is relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3626772.3657800",
          "cited_paper_doi": "10.3233/SW-140134",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6667a975df5b0928c0b88582af9321922b91d402",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d2946a868682e4141beabc288d79253ae254c6e1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "DB15K",
          "dataset_description": "Used to enhance representation learning with multimodal data, specifically for evaluating the integration of textual and visual information in knowledge graphs. | Used to evaluate multimodal representation learning, focusing on cross-modal entity alignment and reasoning in knowledge graphs.",
          "citing_paper_id": "280003374",
          "cited_paper_id": 9909815,
          "context_text": "The experiments are performed on three datasets, such as DB15K [17], MKG-W, and MKG-Y [18].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets by name, two of which are acronyms and one is a hyphenated name, all of which meet the criteria for inclusion.",
          "citing_paper_doi": "10.1109/KSE63888.2024.11063656",
          "cited_paper_doi": "10.24963/ijcai.2017/438",
          "citing_paper_url": "https://www.semanticscholar.org/paper/805102e1783b51edc9888ea1c2407ede7fe166a4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/657703c9914ce785649c67374a0e8860a1b4321c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "DB15K",
          "dataset_description": "Used to enhance representation learning with multimodal data, specifically for evaluating the integration of textual and visual information in knowledge graphs. | Used to evaluate multimodal representation learning, focusing on cross-modal entity alignment and reasoning in knowledge graphs.",
          "citing_paper_id": "280003374",
          "cited_paper_id": 203605587,
          "context_text": "The experiments are performed on three datasets, such as DB15K [17], MKG-W, and MKG-Y [18].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets by name, two of which are acronyms and one is a hyphenated name, all of which meet the criteria for inclusion.",
          "citing_paper_doi": "10.1109/KSE63888.2024.11063656",
          "cited_paper_doi": "10.1109/IJCNN.2019.8852079",
          "citing_paper_url": "https://www.semanticscholar.org/paper/805102e1783b51edc9888ea1c2407ede7fe166a4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/02ea29150d2526641bad18a0bc1932e6235c69a3",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "DB15K",
          "dataset_description": "Used for entity alignment in multi-modal knowledge graphs, specifically to align entities between FB15K and DB15K. The dataset is split into training and testing sets in proportions of 2:8, 5:5, and 8:2. | Used for entity alignment in multi-modal knowledge graphs, split into training and testing sets in various proportions to evaluate model performance. | Used for entity alignment in multi-modal knowledge graphs, specifically to align entities between FB15K and YAGO15K. The dataset is split into training and testing sets in proportions of 2:8, 5:5, and 8:2.",
          "citing_paper_id": "263829023",
          "cited_paper_id": 221193809,
          "context_text": "The FB15K-DB15K dataset 2 is an entity alignment dataset of FB15K and DB15K MMKGs, while the latter is a dataset of FB15K and YAGO15K MMKGs. Consistent with prior works (Chen et al., 2020; Guo et al., 2021), we split each dataset into training and testing sets in proportions of 2:8, 5:5, and 8:2, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FB15K-DB15K and DB15K, which are used for entity alignment in multi-modal knowledge graphs. The datasets are split into training and testing sets in various proportions.",
          "citing_paper_doi": "10.48550/arXiv.2310.06365",
          "cited_paper_doi": "10.1007/978-3-030-55130-8_12",
          "citing_paper_url": "https://www.semanticscholar.org/paper/49933d4602c7a51fccefc1c83f6cb10c7a0546f7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ca3072dad2ee809c8fc3639e6fc1728b46f9ef66",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "53957733",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "FB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15K",
          "dataset_description": "Used to evaluate the performance of TransE in modeling multi-relational data, focusing on link prediction tasks in knowledge graphs.",
          "citing_paper_id": "258298672",
          "cited_paper_id": 14941970,
          "context_text": "The experiments are based on FB15K dataset and TransE base score function.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "FB15K is a well-known dataset used for evaluating multi-relational data models, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/IJCNN54540.2023.10191314",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "FB15K",
          "dataset_description": "ImageGraph extends FB15K with 1,330 relation types, 14,870 entities, and 829 images, used to answer visual-relational queries in web-extracted knowledge graphs. | FB15K is extended by ImageGraph to include images and additional relation types, enhancing multi-modal reasoning capabilities.",
          "citing_paper_id": "269075665",
          "cited_paper_id": 53957733,
          "context_text": "ImageGraph [67] is an MMKG that extends the FB15K dataset we cited before; it contains 1,330 relation types, 14,870 entities, and 829,931 images crawled from the Web.",
          "confidence_score": 0.95,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FB15K' and 'ImageGraph', both of which are datasets. FB15K is extended by ImageGraph, which adds images and relations.",
          "citing_paper_doi": "10.1145/3656579",
          "cited_paper_doi": "10.24432/C56P45",
          "citing_paper_url": "https://www.semanticscholar.org/paper/af5e3a307546ff583195a70b3c1ad51dec41614b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12d64afc8a19b1234a766aba5684036ce7937d0d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB15K",
          "dataset_description": "Utilized as a medical knowledge graph to enhance multi-modal reasoning in healthcare applications, specifically for disease diagnosis and treatment recommendation. | Used as a general knowledge graph for evaluating multi-modal reasoning models, focusing on entity and relation prediction tasks. | Applied as a comprehensive dataset of COVID-19 research articles to support multi-modal reasoning in pandemic-related studies, focusing on information extraction and synthesis.",
          "citing_paper_id": "259180790",
          "cited_paper_id": 216056360,
          "context_text": "The established knowledge graphs include general knowledge graph FB15K [1], medical knowledge graph DiaKG [2], CORD-19 [3], etc.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific knowledge graphs, which are considered datasets in the context of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.3390/app13127115",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b3309cc53ccc37ec5f26f04632bedcf018c83db2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bc411487f305e451d7485e53202ec241fcc97d3b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "FB15K",
          "dataset_description": "Used to train and evaluate models on relational reasoning tasks, specifically focusing on the projection of an arbitrary relation from the relation set.",
          "citing_paper_id": "271915694",
          "cited_paper_id": 248965170,
          "context_text": "We select an arbitrary relation, r i , from the relation set in the FB15k dataset for the projection.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the FB15k dataset, which is a well-known knowledge graph dataset used for training and evaluating models on relational reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2408.11526",
          "cited_paper_doi": "10.48550/arXiv.2205.10128",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1534872453944130329d5d6b4cf3ced007bfb31e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/53eecd3d126a11911b53c639a7a808cac4fbf127",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "262690390",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "FB15k-237"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15k-237",
          "dataset_description": "Used to validate the proposed method's performance on multi-modal knowledge graph reasoning, demonstrating state-of-the-art results and improved convergence speed. | Used to test the HRGAT model's effectiveness in cross-lingual knowledge graph alignment, specifically between English and Japanese. | Used to evaluate the HRGAT model on a large-scale knowledge graph, focusing on entity and relation prediction in a diverse and richly structured dataset. | Used to assess the performance of HRGAT on wordnet relations, emphasizing the model's ability to handle hierarchical and semantic relationships. | Used to test the proposed method's capabilities in multi-modal knowledge graph reasoning, highlighting state-of-the-art results and faster convergence. | Used to evaluate the HRGAT model, focusing on link prediction in knowledge graphs with complex relational structures. | Used to assess the effectiveness of the proposed method in multi-modal knowledge graph reasoning, emphasizing state-of-the-art performance and convergence speed. | Used to evaluate the proposed method's performance on multi-modal knowledge graph reasoning, focusing on state-of-the-art results and faster convergence.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 5378837,
          "context_text": "We evaluate our HRGAT on four benchmark datasets, which are FB15k-237 [38], WN18RR [13], DB15K [24], and YAGO15K [24].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific datasets used to evaluate the HRGAT model. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.18653/v1/W15-4007",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "FB15k-237",
          "dataset_description": "Split using the same ratio as FB15k-237 to train and evaluate relational graph convolutional networks, focusing on entity and relation prediction tasks. | Used to train and evaluate relational graph convolutional networks, focusing on entity and relation prediction tasks with a specific division ratio.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 5458500,
          "context_text": "We have followed the original division ratio in R-GCN [32] on datasets FB15k-237 and WN18RR, and split DB15K and YAGO15K with the same ratio as FB15k-237.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of graph convolutional networks. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.1007/978-3-319-93417-4_38",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd8a9914d50b0ac63315872530274d158d6aff09",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "FB15k-237",
          "dataset_description": "Used to collect textual information for entities, focusing on improving knowledge graph reasoning through multi-modal data integration.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 52967399,
          "context_text": "We collect the textual information for entities of FB15k-237, WN18RR, DB15K, and YAGO15K from KG-BERT [46].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used to collect textual information for entities. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.18653/v1/N19-1423",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "FB15k-237",
          "dataset_description": "Used to compare convergence speeds of the proposed method with CNN-based ConvE and GNN-based WGCN models, focusing on knowledge graph reasoning tasks.",
          "citing_paper_id": "250118042",
          "cited_paper_id": 262690390,
          "context_text": "We also find out that our method gets a faster convergence speed compared with the CNN-based model ConvE [13] and the GNN-based model WGCN [33] on the FB15k-237 dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the FB15k-237 dataset for comparing convergence speeds of different models. The dataset is a well-known benchmark for knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3545573",
          "cited_paper_doi": "10.1609/AAAI.V33I01.33013060",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d7681420e5751a8e8ea588b3533947594d13d9d0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8f3812a4f2093b488fa4bd3adda3cea0e770dc3d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "14843884",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "GQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "GQA",
          "dataset_description": "Used to address shortcomings in previous VQA datasets, focusing on real-world visual reasoning and compositional question answering. | Used to develop and evaluate models for real-world visual reasoning and compositional question answering, addressing shortcomings in existing datasets. | Used to evaluate real-world visual reasoning and compositional question answering, focusing on complex interactions between images and questions. | Compared against GQA to highlight improvements in visual reasoning and compositional question answering.",
          "citing_paper_id": "235829220",
          "cited_paper_id": 152282269,
          "context_text": "challenging, real-world datasets such as GQA (see [17]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "GQA is identified as a dataset in the citation context, and the title confirms it is a dataset for visual reasoning and compositional question answering.",
          "citing_paper_doi": "10.1007/978-3-030-88361-4_7",
          "cited_paper_doi": "10.1109/CVPR.2019.00686",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ec08bf2dd3be70a6508793e94b717838bc4c7679",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a7ac99d7cf3f568ab1a741392144b646b856ae0c",
          "citing_paper_year": 2021,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GQA",
          "dataset_description": "Used for fact-based visual question answering, integrating factual knowledge with visual information. | Focuses on open-ended questions that require external knowledge beyond the image content. | Utilizes a subset of VQA with wh-questions to evaluate fine-grained reasoning. | Used for visual question answering, focusing on image-text pairs to test reasoning capabilities. | Utilizes knowledge bases to enhance visual question answering with external information. | Employs knowledge graphs to answer questions that require reasoning over structured data and images. | Employs complex questions requiring multi-step reasoning over images and knowledge graphs.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 7483388,
          "context_text": "VQA GQA [119] Visual7w [120] OK-VQA [56] FVQA [33] KVQA [121] KB-VQA [122] 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for visual question answering and knowledge-based reasoning, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GQA",
          "dataset_description": "Used for fact-based visual question answering, integrating factual knowledge with visual information. | Focuses on open-ended questions that require external knowledge beyond the image content. | Utilizes a subset of VQA with wh-questions to evaluate fine-grained reasoning. | Used for visual question answering, focusing on image-text pairs to test reasoning capabilities. | Utilizes knowledge bases to enhance visual question answering with external information. | Employs knowledge graphs to answer questions that require reasoning over structured data and images. | Employs complex questions requiring multi-step reasoning over images and knowledge graphs.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 14843884,
          "context_text": "VQA GQA [119] Visual7w [120] OK-VQA [56] FVQA [33] KVQA [121] KB-VQA [122] 1.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for visual question answering and knowledge-based reasoning, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.24963/IJCAI.2017/179",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "85528899",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Fakeddit"
      ],
      "dataset_details": [
        {
          "dataset_name": "Fakeddit",
          "dataset_description": "Used to evaluate fine-grained fake news detection, focusing on multimodal content and social context in a controlled environment.",
          "citing_paper_id": "278782132",
          "cited_paper_id": 85528899,
          "context_text": "To assess the effectiveness of our approach, we conduct experiments on two real-world datasets: Fakeddit [28] and Gossipcop [29].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, Fakeddit and Gossipcop, which are used to assess the effectiveness of the approach in fake news detection.",
          "citing_paper_doi": "10.48550/arXiv.2505.14714",
          "cited_paper_doi": "10.1089/big.2020.0062",
          "citing_paper_url": "https://www.semanticscholar.org/paper/868dffeabaf0128614622ec27914c7328a97355d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eed1a4b3ec3b6de0fd1f0b8b2ec969b540fe41a0",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Fakeddit",
          "dataset_description": "Used to evaluate fine-grained fake news detection, focusing on multimodal content and social context in a controlled environment.",
          "citing_paper_id": "278782132",
          "cited_paper_id": 238158425,
          "context_text": "To assess the effectiveness of our approach, we conduct experiments on two real-world datasets: Fakeddit [28] and Gossipcop [29].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, Fakeddit and Gossipcop, which are used to assess the effectiveness of the approach in fake news detection.",
          "citing_paper_doi": "10.48550/arXiv.2505.14714",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/868dffeabaf0128614622ec27914c7328a97355d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/87e450815569eb65b78c47c79d4faee3c1a61b9c",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Fakeddit",
          "dataset_description": "Used to analyze millions of fauxtography posts from Reddit, focusing on visual and textual elements to understand the spread of manipulated images.",
          "citing_paper_id": "253460330",
          "cited_paper_id": 52164698,
          "context_text": "FakeNewsNet contains thousands of multi-modal celebrity-related news articles collected from GossipCop 5 while Fakeddit consists of millions of fauxtography posts from Reddit 6 .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, FakeNewsNet and Fakeddit, which are used to study fake news and fauxtography, respectively. Both are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3555178",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c080f98ee5debd9c026254329ed22a4a89daa707",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad6f321c4bf36ddfff1b18b0e1d31aa8c836217b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "54460890",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "MSCOCO captions"
      ],
      "dataset_details": [
        {
          "dataset_name": "MSCOCO captions",
          "dataset_description": "Used for fine-tuning on image captioning, providing a large set of images paired with descriptive captions. | Used for fine-tuning on visual question answering, enhancing the model's ability to answer questions about images. | Used for fine-tuning on image captioning, supplementing the training with diverse image-caption pairs.",
          "citing_paper_id": "275357711",
          "cited_paper_id": 2210455,
          "context_text": "It is is also finetuned on VQA and Image captioning on MSCOCO captions (Chen et al., 2015), VQA 2.0 (Goyal et al., 2017b) and Flickr30k (Yang et al., 2018a).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for fine-tuning models on visual question answering and image captioning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2501.04173",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef993f751f072ea9defcf6ed29a9836c4b7f383a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628",
          "citing_paper_year": 2025,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MSCOCO captions",
          "dataset_description": "Used for fine-tuning on image captioning, supplementing the training with diverse image-caption pairs. | Used to develop and evaluate models that answer questions about images, focusing on the integration of visual and textual information in a multi-modal setting. | Used for fine-tuning on image captioning, providing a large set of images paired with descriptive captions. | Used for fine-tuning on visual question answering, enhancing the model's ability to answer questions about images. | Used to develop and evaluate models that answer questions about images, focusing on the integration of visual and textual information in multi-modal reasoning. | Used to balance the dataset by increasing the number of image-question pairs, enhancing the diversity and complexity of the visual question answering task.",
          "citing_paper_id": "275357711",
          "cited_paper_id": 3180429,
          "context_text": "It is is also finetuned on VQA and Image captioning on MSCOCO captions (Chen et al., 2015), VQA 2.0 (Goyal et al., 2017b) and Flickr30k (Yang et al., 2018a).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for fine-tuning models on visual question answering and image captioning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2501.04173",
          "cited_paper_doi": "10.1007/s11263-016-0966-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef993f751f072ea9defcf6ed29a9836c4b7f383a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "citing_paper_year": 2025,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MSCOCO captions",
          "dataset_description": "Used for fine-tuning on image captioning, providing a large set of images paired with descriptive captions. | Used to develop and evaluate text-based QA systems requiring reasoning over multiple documents, enhancing multi-modal knowledge graph reasoning capabilities. | Used for fine-tuning on visual question answering, enhancing the model's ability to answer questions about images. | Used for fine-tuning on image captioning, supplementing the training with diverse image-caption pairs.",
          "citing_paper_id": "275357711",
          "cited_paper_id": 54460890,
          "context_text": "It is is also finetuned on VQA and Image captioning on MSCOCO captions (Chen et al., 2015), VQA 2.0 (Goyal et al., 2017b) and Flickr30k (Yang et al., 2018a).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for fine-tuning models on visual question answering and image captioning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2501.04173",
          "cited_paper_doi": "10.1109/CVPR.2019.01094",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef993f751f072ea9defcf6ed29a9836c4b7f383a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f6feb1af1809dfd872d868dfcc13021cc42f496c",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "218971783",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "kgbench"
      ],
      "dataset_details": [
        {
          "dataset_name": "kgbench",
          "dataset_description": "Used to evaluate relational and multimodal machine learning models, providing a collection of knowledge graph datasets for benchmarking.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 206477883,
          "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning. However, only 'kgbench' is clearly identified as a dataset in the cited paper titles.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1109/MMUL.2006.63",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86dc975f9cbd9a205f8e82fb1db3b61c6b738fa5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2006
        },
        {
          "dataset_name": "kgbench",
          "dataset_description": "Used to evaluate relational and multimodal machine learning models, providing a collection of knowledge graph datasets for benchmarking.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 218971783,
          "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning. However, only 'kgbench' is clearly identified as a dataset in the cited paper titles.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "kgbench",
          "dataset_description": "Used to evaluate relational and multimodal machine learning models, providing a collection of knowledge graph datasets for benchmarking.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 235271284,
          "context_text": "Until recently, with the development of technologies such as GPT [288], some more practical multi-modal knowledge graphs have gradually emerged, such as OPENBG [289], COMM [290], LSCOM [291] WikiDiverse [292], kgbench [293].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several multi-modal knowledge graphs, which are relevant to the topic of multi-modal knowledge graph reasoning. However, only 'kgbench' is clearly identified as a dataset in the cited paper titles.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1007/978-3-030-77385-4_37",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dcde86c40b2480130d8d46380b3d83b1277b4c27",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "230770066",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Recipe1M+"
      ],
      "dataset_details": [
        {
          "dataset_name": "Recipe1M+",
          "dataset_description": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual data in a large-scale structured dataset. | Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual information in recipe recommendation systems.",
          "citing_paper_id": "260735757",
          "cited_paper_id": 195892710,
          "context_text": "Recipe1M+ [26] is a large-scale and structured dataset with over 1M cooking recipes and 13M food images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions Recipe1M+ as a large-scale and structured dataset with cooking recipes and food images, which is relevant to multi-modal learning and knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2308.04579",
          "cited_paper_doi": "10.1109/TPAMI.2019.2927476",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ecd1a69b31a4ed3f43675966a3f0afa6c6393f24",
          "cited_paper_url": "https://www.semanticscholar.org/paper/02c009f41b66d2f977fb663f3cb69329f0f03d3f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Recipe1M+",
          "dataset_description": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual information in recipe recommendation systems.",
          "citing_paper_id": "260735757",
          "cited_paper_id": 207198269,
          "context_text": "Although the NLP methods were used in settings such as ingredient networks [44], Recipe1M+ [26], and pFoodREQ [11], we found no fully text-based query-recipe and profile-recipe matching solutions in the literature.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions three resources: 'ingredient networks', 'Recipe1M+', and 'pFoodREQ'. Based on the cited paper titles, 'Recipe1M+' is a dataset, while 'ingredient networks' and 'pFoodREQ' are likely methods or tools.",
          "citing_paper_doi": "10.48550/arXiv.2308.04579",
          "cited_paper_doi": "10.1145/2380718.2380757",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ecd1a69b31a4ed3f43675966a3f0afa6c6393f24",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e7c90c985077ebb256bdc39a7181dacfa0a43eec",
          "citing_paper_year": 2023,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "Recipe1M+",
          "dataset_description": "Used to learn cross-modal embeddings for cooking recipes and food images, focusing on the integration of textual and visual information in recipe recommendation systems.",
          "citing_paper_id": "260735757",
          "cited_paper_id": 230770066,
          "context_text": "Although the NLP methods were used in settings such as ingredient networks [44], Recipe1M+ [26], and pFoodREQ [11], we found no fully text-based query-recipe and profile-recipe matching solutions in the literature.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions three resources: 'ingredient networks', 'Recipe1M+', and 'pFoodREQ'. Based on the cited paper titles, 'Recipe1M+' is a dataset, while 'ingredient networks' and 'pFoodREQ' are likely methods or tools.",
          "citing_paper_doi": "10.48550/arXiv.2308.04579",
          "cited_paper_doi": "10.1145/3437963.3441816",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ecd1a69b31a4ed3f43675966a3f0afa6c6393f24",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fb72dd373cc88a4e4300742a986799ece457f836",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "239011538",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "FB15k-237-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15k-237-IMG",
          "dataset_description": "Used to extend entities in WN18 with images for multi-modal knowledge graph representation learning, integrating visual information into lexical semantic relationships. | Used to extend entities in FB15k-237 with images for multi-modal knowledge graph representation learning, enhancing the visual aspect of entity relationships.",
          "citing_paper_id": "259375740",
          "cited_paper_id": 44145776,
          "context_text": "Specifically, both FB15k-237-IMG [38] and WN18 [38] datasets are constructed by extending ten images for each entity based on FB15k-237 and WN18, which are the subset of the large-scale knowledge graph Freebase [4] and WordNet [37], separately.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FB15k-237-IMG and WN18, which are extensions of FB15k-237 and WN18, respectively. These datasets are used for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.03591",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd1271b05230267f114502eccdc84b1e15a4313b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB15k-237-IMG",
          "dataset_description": "Used to test the model's effectiveness in multi-modal reasoning across different knowledge bases, incorporating visual information to enhance entity linking. | Used to assess the model's ability to handle relational reasoning with visual context, emphasizing the impact of image data on knowledge graph embeddings. | Used to evaluate the model's performance on multi-modal knowledge graph reasoning, focusing on visual context integration and representation learning.",
          "citing_paper_id": "274397704",
          "cited_paper_id": 239011538,
          "context_text": "…model is evaluated on four publicly available datasets: FB15k-237-IMG [55] (https://github.com/mniepert/mmkb, accessed on 11 May 2018), WN18RR-IMG [34] (https://github.com/wangmengsd/RSME, accessed on 14 January 2022), DB15K-IMG [55] (https://github.com/mniepert/mmkb, accessed on 11 May 2018),…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific datasets used for evaluating a model. These datasets are clearly named and have public access points, making them verifiable resources.",
          "citing_paper_doi": "10.3390/s24237605",
          "cited_paper_doi": "10.1145/3474085.3475470",
          "citing_paper_url": "https://www.semanticscholar.org/paper/46bc47ade14a95d8d51da938292f0ececbd855e0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1007b5a8d0af5b39d061eb0ac45a0700fe47bd1e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "FB15k-237-IMG",
          "dataset_description": "Used to extend the scope of triplets in multimodal knowledge graph completion, integrating image data with textual information. | Used to extend the scope of triplets in multimodal knowledge graph completion, focusing on integrating image data with textual information.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 248524814,
          "context_text": "Compared to it, FB15K-237-IMG [183] changes the scope of triplets to FB15k-237 [205].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, FB15K-237-IMG and FB15k-237, which are specific and relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/3477495.3531992",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bedcfb163368f2d802de3e892acb34cc5a75a22d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "6839244",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "MovieQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MovieQA",
          "dataset_description": "Used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, focusing on understanding movie stories.",
          "citing_paper_id": "252782878",
          "cited_paper_id": 1017389,
          "context_text": "For instance, MovieQA [23] attempts to comprehend story both from video and text via question answering paradigm, [24] tries to predict social relation among characters and [22] aligns book chapters with video scenes in order to better understand video contents.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions MovieQA, which is a specific dataset used for question-answering tasks involving movies. The other two citations refer to methodologies or approaches rather than datasets.",
          "citing_paper_doi": "10.1145/3503161.3551604",
          "cited_paper_doi": "10.1109/CVPR.2016.501",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2da4666811d0095ab0a71e01e238277a1d908c02",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MovieQA",
          "dataset_description": "Used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, focusing on understanding movie stories.",
          "citing_paper_id": "252782878",
          "cited_paper_id": 6839244,
          "context_text": "For instance, MovieQA [23] attempts to comprehend story both from video and text via question answering paradigm, [24] tries to predict social relation among characters and [22] aligns book chapters with video scenes in order to better understand video contents.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions MovieQA, which is a specific dataset used for question-answering tasks involving movies. The other two citations refer to methodologies or approaches rather than datasets.",
          "citing_paper_doi": "10.1145/3503161.3551604",
          "cited_paper_doi": "10.1109/CVPR.2015.7298792",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2da4666811d0095ab0a71e01e238277a1d908c02",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45ca387a4080b6aee610783ed03d19bd1891503f",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "MovieQA",
          "dataset_description": "Used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, focusing on understanding movie stories.",
          "citing_paper_id": "252782878",
          "cited_paper_id": 239011786,
          "context_text": "For instance, MovieQA [23] attempts to comprehend story both from video and text via question answering paradigm, [24] tries to predict social relation among characters and [22] aligns book chapters with video scenes in order to better understand video contents.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions MovieQA, which is a specific dataset used for question-answering tasks involving movies. The other two citations refer to methodologies or approaches rather than datasets.",
          "citing_paper_doi": "10.1145/3503161.3551604",
          "cited_paper_doi": "10.1145/3474085.3475684",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2da4666811d0095ab0a71e01e238277a1d908c02",
          "cited_paper_url": "https://www.semanticscholar.org/paper/163c9313a8696142c36cbc92cc6cc382f9a1a1a0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "208202400",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Kinetics-TPS"
      ],
      "dataset_details": [
        {
          "dataset_name": "Kinetics-TPS",
          "dataset_description": "Used to provide over 15M part-level annotations for detailed human action understanding, enhancing multi-modal knowledge graph reasoning with fine-grained action data.",
          "citing_paper_id": "252782967",
          "cited_paper_id": 208202400,
          "context_text": "Different from the existing data sets in action recognition [26, 31, 34, 36, 54], Kinetics-TPS provides over 15M part-level annotations for detailed human action understanding, including 7.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Kinetics-TPS' as a dataset providing part-level annotations for detailed human action understanding, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3503161.3548257",
          "cited_paper_doi": "10.1609/AAAI.V34I07.6836",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8db962faf0ec4708f0e14410eabd23521d244619",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fa3473f1f5d4f19ec6d561e2700a4d88dad4ccf8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Kinetics-TPS",
          "dataset_description": "Used to build a visual knowledge graph, focusing on action parsing to enhance multi-modal reasoning capabilities. | Used as a benchmark for constructing a visual knowledge graph for human action understanding in videos, focusing on action parsing. | Used to parse human actions by compositional learning of body part movements, focusing on the specific research question of action recognition through multi-modal reasoning.",
          "citing_paper_id": "252782967",
          "cited_paper_id": null,
          "context_text": "Specifically, we investigate our paradigm on the recent Kinetics-TPS benchmark [25], since this dataset aims at parsing human actions by compositional learning of body part movements.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the Kinetics-TPS benchmark, which is a specific dataset used for parsing human actions through the compositional learning of body part movements.",
          "citing_paper_doi": "10.1145/3503161.3548257",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8db962faf0ec4708f0e14410eabd23521d244619",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "229339845",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "hasPart KB"
      ],
      "dataset_details": [
        {
          "dataset_name": "hasPart KB",
          "dataset_description": "Freebase is employed as an encyclopedia knowledge graph, containing structured information from various sources. | BabelNet is used for lexical knowledge, integrating WordNet and Wikipedia to provide a multilingual lexicalized semantic network. | Used to provide part-whole relationships for VQA, enhancing the system's ability to understand compositional structures in images. | WikiData is used as a collaborative, multilingual knowledge base, providing structured data for a wide range of topics. | Used to incorporate commonsense triples, enriching the knowledge graph with everyday reasoning and conceptual relationships. | GeoNames is utilized for geographic knowledge, providing a comprehensive database of place names and geographic features. | Used to provide has-Part triples, contributing to the construction of a knowledge graph with part-whole relationships. | Used to provide categorical knowledge for Visual Question Answering (VQA), enhancing the system's ability to understand and answer questions about images. | Probase is used for taxonomic knowledge, offering a large-scale taxonomy of concepts and their relationships. | Cyc is used as a source of common sense knowledge, providing a structured representation of everyday concepts and relationships. | ConceptNet is utilized for commonsense reasoning, offering a large, multilingual knowledge graph of semantic relationships. | YAGO is utilized for encyclopedia knowledge, providing a large, multilingual knowledge graph derived from Wikipedia and other sources. | Used to supply location triples of visual objects, integrating spatial information into the knowledge graph. | Used to extract hasPart/isA triples, enhancing the knowledge graph with hierarchical and part-whole relationships. | DBpedia is used to extract structured information from Wikipedia, serving as a comprehensive knowledge base for encyclopedia knowledge. | CN-DBpedia is employed for Chinese-specific encyclopedia knowledge, providing a structured representation of information from Chinese Wikipedia. | WordNet serves as a lexical knowledge base, providing a structured vocabulary and semantic relations between words. | Used to provide commonsense knowledge for VQA, enhancing the system's ability to reason about everyday concepts and situations.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 266028051,
          "context_text": "For example, the explicit knowledge in [51] has four sources: has-Part triples from hasPart KB [162], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used as sources of explicit knowledge, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1023/B:BTTJ.0000047600.45421.6D",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b3fea597033a46d5ae282464a8f16d6715187e70",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        },
        {
          "dataset_name": "hasPart KB",
          "dataset_description": "Used to provide commonsense triples, enriching the knowledge graph with everyday knowledge. | Used to provide location triples of visual objects, integrating visual and textual information in the knowledge graph. | Used to provide hasPart triples, contributing to the construction of a knowledge graph for open-domain VQA. | Used to provide hasPart and isA triples, enhancing the knowledge graph with structured information.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 229339845,
          "context_text": "For example, the explicit knowledge in [51] has four sources: hasPart triples from hasPart KB [163], hasPart/isA triples from DBpedia [6], commonsense triples from ConceptNet [2], and location triples of visual objects from Visual Genome [53].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used as sources of explicit knowledge for a knowledge graph. These datasets are clearly identified and are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01389",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "237453242",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "OK-VQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "OK-VQA",
          "dataset_description": "Used to evaluate models that incorporate functional knowledge to answer questions about images, focusing on the functional aspects of visual elements. | Used to assess the performance of models that combine knowledge graphs with visual question answering, focusing on the integration of structured knowledge. | Used to test models that integrate knowledge bases with visual question answering, enhancing the accuracy and depth of responses. | Used to test visual reasoning capabilities requiring external knowledge, specifically designed for evaluating multi-modal understanding. | Used to benchmark models that require external knowledge to answer visual questions, enhancing the depth of understanding in multi-modal reasoning. | Used to assess the need for external knowledge in visual question answering, emphasizing the integration of visual and textual information. | Used to train and evaluate models that answer questions about images, focusing on the integration of visual and textual information. | Used to assess the ability of models to reason about complex visual scenes and answer detailed questions, emphasizing compositional and logical reasoning. | Used to test the reasoning capability of VQA models by providing questions that require external knowledge, focusing on the model's ability to integrate visual and textual information. | Used to evaluate visual reasoning combined with external knowledge, focusing on complex question answering in the VQA domain.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 173991173,
          "context_text": "For example, the OK-VQA dataset [49], which contains only questions that require external knowledge to answer, is built to test the reasoning capability of VQA models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the OK-VQA dataset, which is a specific, verifiable resource used to test the reasoning capability of VQA models requiring external knowledge.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/CVPR.2019.00331",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/28ad018c39d1578bea84e7cedf94459e3dbe1e70",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "OK-VQA",
          "dataset_description": "Used as a textual knowledge base for knowledge-based question answering, specifically to provide external knowledge for the Vis-DPR model.",
          "citing_paper_id": "257576049",
          "cited_paper_id": 237453242,
          "context_text": "Additionally, we utilize the external knowledge collection constructed by Vis-DPR [19] for the OK-VQA dataset as the textual knowledge base, designated as P .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'OK-VQA dataset' as a textual knowledge base used in the research. It is a specific, verifiable dataset used for knowledge-based question answering.",
          "citing_paper_doi": "10.3390/electronics12061390",
          "cited_paper_doi": "10.18653/v1/2021.emnlp-main.517",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef7fc71297fd06c7acf89e12c8acaeceaf9375ed",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4e92fec0a61972ae076707d0630d1333affccdfc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "3365209",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Pascal-VOC"
      ],
      "dataset_details": [
        {
          "dataset_name": "Pascal-VOC",
          "dataset_description": "Used to pre-train the first-stage object detector in the QRG model, focusing on object detection and proposal generation for phrase grounding.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 3365209,
          "context_text": "Model Comparison We compare our models with the following baseline models: (1) QRG (Chen, Kovvuri, and Nevatia 2017) employs query-guided regression network by reinforcement learning for proposal generation and policy learning; we had its ﬁrst-stage object detector pre-trained on Pascal-VOC…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Pascal-VOC' as a dataset used for pre-training the first-stage object detector in the QRG model. No other datasets are explicitly mentioned.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1007/s13735-017-0139-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cb0047121fdb983e8189dd69006ddce89fe3bc3a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Pascal-VOC",
          "dataset_description": "Used to pre-train the first-stage object detector in the QRG model, focusing on object detection and proposal generation for phrase grounding.",
          "citing_paper_id": "250301328",
          "cited_paper_id": 12856358,
          "context_text": "Model Comparison We compare our models with the following baseline models: (1) QRG (Chen, Kovvuri, and Nevatia 2017) employs query-guided regression network by reinforcement learning for proposal generation and policy learning; we had its ﬁrst-stage object detector pre-trained on Pascal-VOC…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Pascal-VOC' as a dataset used for pre-training the first-stage object detector in the QRG model. No other datasets are explicitly mentioned.",
          "citing_paper_doi": "10.1609/aaai.v36i2.20123",
          "cited_paper_doi": "10.1109/ICCV.2017.95",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9693584f21834535592dfad01abcbf206ddd1bb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ff4b351dccb970f13a345adf0647ffe8c2021f1f",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "258509157",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MKG-W"
      ],
      "dataset_details": [
        {
          "dataset_name": "MKG-W",
          "dataset_description": "Used to assess the performance of multi-modal knowledge graph models, specifically in linking entities across different modalities. | Used to compare trends in multi-modal knowledge graph reasoning, focusing on the consistency of results across different datasets. | Used for ablation studies in multi-modal knowledge graph reasoning, evaluating the effectiveness of various model configurations. | Used to evaluate cross-lingual entity alignment in multi-modal knowledge graphs, focusing on linking entities across different languages. | Used for ablation studies in multi-modal knowledge graph reasoning, focusing on the impact of different components on performance. | Used to evaluate multi-modal knowledge graph reasoning models, focusing on link prediction performance metrics such as MRR and Hit@1. | Used to evaluate the performance of M O M O K, focusing on Hit@1 metric to assess the accuracy of entity linking in multi-modal knowledge graphs. | Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs. | Used to evaluate knowledge graph completion tasks in multi-modal settings, focusing on predicting missing links using both textual and visual data.",
          "citing_paper_id": "270064299",
          "cited_paper_id": 76663467,
          "context_text": "We conduct our experiments on four public MMKG benchmarks: MKG-W (Xu et al., 2022), MKG-Y (Xu et al., 2022), DB15K (Liu et al., 2019), and KVC16K (Zhang et al., 2024a).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific benchmarks used for experiments in multi-modal knowledge graphs. These benchmarks are clearly identified and are relevant to the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "MKG-W",
          "dataset_description": "Used to evaluate multi-modal knowledge graph reasoning, focusing on the integration of textual and visual information in knowledge graphs. | Used to evaluate knowledge graph completion tasks in multi-modal settings, focusing on predicting missing links using both textual and visual data. | Used to evaluate cross-lingual entity alignment in multi-modal knowledge graphs, focusing on linking entities across different languages.",
          "citing_paper_id": "270064299",
          "cited_paper_id": 258509157,
          "context_text": "We conduct our experiments on four public MMKG benchmarks: MKG-W (Xu et al., 2022), MKG-Y (Xu et al., 2022), DB15K (Liu et al., 2019), and KVC16K (Zhang et al., 2024a).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions four specific benchmarks used for experiments in multi-modal knowledge graphs. These benchmarks are clearly identified and are relevant to the research topic.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d9188b30d72c15fee4733c0602aaf3cc2b1b6f7a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6cd64d6558e2a7105b1f128e49d76e608507bfeb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "259203574",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "UUKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "UUKG",
          "dataset_description": "Used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities, focusing on integrating diverse urban data sources.",
          "citing_paper_id": "273643557",
          "cited_paper_id": 259203574,
          "context_text": "Urban knowledge graphs, organizing urban entities into a complex graph, have become crucial in modern smart cities [17].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Urban knowledge graphs' but does not specify a dataset. However, the cited paper title suggests a specific dataset.",
          "citing_paper_doi": "10.1145/3664647.3681705",
          "cited_paper_doi": "10.48550/arXiv.2306.11443",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9614386ac7823b17761b22ef4ba748359b2bbb49",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b3525e7911510e8b1adbb4290aefdaef7c0bd455",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "UUKG",
          "dataset_description": "Used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities, focusing on integrating diverse urban data sources.",
          "citing_paper_id": "273643557",
          "cited_paper_id": null,
          "context_text": "Urban knowledge graphs, organizing urban entities into a complex graph, have become crucial in modern smart cities [17].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Urban knowledge graphs' but does not specify a dataset. However, the cited paper title suggests a specific dataset.",
          "citing_paper_doi": "10.1145/3664647.3681705",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9614386ac7823b17761b22ef4ba748359b2bbb49",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "201881176",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Scene Graph"
      ],
      "dataset_details": [
        {
          "dataset_name": "Scene Graph",
          "dataset_description": "Used to retrieve images based on scene graphs, focusing on action and spatial relations within images. | Used to refocus visually-relevant relationships in images, enhancing the representation of spatial and relational information.",
          "citing_paper_id": "268697475",
          "cited_paper_id": 16414666,
          "context_text": "In addition, there are many datasets with images that embody both action and spatial relations, such as Scene Graph (Johnson et al. 2015) and VrR-VG (Liang et al. 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, 'Scene Graph' and 'VrR-VG', which are relevant to multi-modal knowledge graph reasoning involving images and spatial relations.",
          "citing_paper_doi": "10.1609/aaai.v38i17.29828",
          "cited_paper_doi": "10.1109/CVPR.2015.7298990",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80ef8e5aa7df5c96fd0f0e9d847e0617be743c14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/85ae705ef4353c6854f5be4a4664269d6317c66b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "Scene Graph",
          "dataset_description": "Used to retrieve images based on scene graphs, focusing on action and spatial relations within images. | Used to refocus visually-relevant relationships in images, enhancing the representation of spatial and relational information.",
          "citing_paper_id": "268697475",
          "cited_paper_id": 201881176,
          "context_text": "In addition, there are many datasets with images that embody both action and spatial relations, such as Scene Graph (Johnson et al. 2015) and VrR-VG (Liang et al. 2019).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, 'Scene Graph' and 'VrR-VG', which are relevant to multi-modal knowledge graph reasoning involving images and spatial relations.",
          "citing_paper_doi": "10.1609/aaai.v38i17.29828",
          "cited_paper_doi": "10.1109/ICCV.2019.01050",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80ef8e5aa7df5c96fd0f0e9d847e0617be743c14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MMKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "MMKG",
          "dataset_description": "Used to provide images for entities in multi-modal reasoning experiments, specifically integrating visual information into knowledge graph relations.",
          "citing_paper_id": "268697475",
          "cited_paper_id": 76663467,
          "context_text": "…the input of experiment A 2 is h and r , while there are five kinds of inputs for experiment B 2 : 1) h , r , and the image p ′ h of h from MMKG (Liu et al. 2019), 2) h , r , and the image p ∗ r 2 of ( h 1 , r, t 1 ) generated by VisualChatGPT (VCG), where h 1 ̸ = h and t 1 ̸ = t , 3) h , r ,…",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MMKG' as a source of images for the experiments, which is a multi-modal knowledge graph. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.1609/aaai.v38i17.29828",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80ef8e5aa7df5c96fd0f0e9d847e0617be743c14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "MMKG",
          "dataset_description": "Used to incorporate both numerical features and image links for all entities in multi-modal knowledge graphs, enhancing the representation of entity information.",
          "citing_paper_id": "260735729",
          "cited_paper_id": 76663467,
          "context_text": "(3) MMKG [11] is a collection of three knowledge graphs that incorporate both numerical features and image links for all entities.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'MMKG' as a collection of knowledge graphs, which fits the criteria for a dataset. It is used to incorporate both numerical features and image links for all entities.",
          "citing_paper_doi": "10.1145/3583780.3614782",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0356abf345662110b6d6a818304c886ce45b6f04",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "2887257",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB15K-YAGO15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15K-YAGO15K",
          "dataset_description": "Used to evaluate the performance of entity alignment on multi-modal knowledge graphs, focusing on the impact of seed entities or pre-aligned entities.",
          "citing_paper_id": "271961581",
          "cited_paper_id": 2887257,
          "context_text": "The applications of KGs are diverse, including question answering [10, 11, 37], recommender systems [51, 53] H @ 1 FB15K-YAGO15K MMEAMCLEA Figure 1: The percentage of seed entities, or pre-aligned entities, is important to the performance of the entity alignment on multi-modal knowledge graph.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FB15K-YAGO15K' which appears to be a dataset used in the context of multi-modal knowledge graph reasoning. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1145/3637528.3671769",
          "cited_paper_doi": "10.18653/v1/P16-1076",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d4d60a5c303cec0f8df9138eb2dd7a535f970294",
          "cited_paper_url": "https://www.semanticscholar.org/paper/76d28a1f4c52b2fbb798501e479023c4075b4803",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FB15K-YAGO15K",
          "dataset_description": "Used to evaluate the performance of entity alignment on multi-modal knowledge graphs, focusing on the impact of seed entities or pre-aligned entities.",
          "citing_paper_id": "271961581",
          "cited_paper_id": 30164212,
          "context_text": "The applications of KGs are diverse, including question answering [10, 11, 37], recommender systems [51, 53] H @ 1 FB15K-YAGO15K MMEAMCLEA Figure 1: The percentage of seed entities, or pre-aligned entities, is important to the performance of the entity alignment on multi-modal knowledge graph.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FB15K-YAGO15K' which appears to be a dataset used in the context of multi-modal knowledge graph reasoning. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.1145/3637528.3671769",
          "cited_paper_doi": "10.18653/v1/N18-2047",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d4d60a5c303cec0f8df9138eb2dd7a535f970294",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1154add9af6c19c8b264331308ec99776583c52d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "246411402",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Karpathy"
      ],
      "dataset_details": [
        {
          "dataset_name": "Karpathy",
          "dataset_description": "Used for training and evaluating image captioning models, focusing on aligning visual and semantic information in images and text. | Used for training and evaluating multi-modal models, specifically focusing on aligning visual and semantic information in image captioning tasks. | Used to train and evaluate image captioning models, focusing on aligning visual and textual information in the context of multi-modal reasoning.",
          "citing_paper_id": "261100976",
          "cited_paper_id": 8517067,
          "context_text": "2022), we utilize the Karpathy (Karpathy and Fei-Fei 2017) split for training and evaluation.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used for training and evaluation in multi-modal tasks, particularly in image captioning.",
          "citing_paper_doi": "10.1609/aaai.v38i3.28017",
          "cited_paper_doi": "10.1109/CVPR.2015.7298932",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b5b753aa23be12f24b4592429514df6e53289bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "Karpathy",
          "dataset_description": "Used to pre-train a multi-modal mixture of the encoder-decoder model, focusing on injecting diverse synthetic captions and removing noisy captions to improve model robustness. | Used to train and evaluate image captioning models, focusing on aligning visual and textual information in the context of multi-modal reasoning. | Used to evaluate structured representations in vision-language tasks, focusing on relationship prediction and reasoning. | Used for training and evaluating multi-modal models, specifically focusing on aligning visual and semantic information in image captioning tasks. | Used to evaluate structured representations in vision-language tasks, focusing on attribute prediction and reasoning.",
          "citing_paper_id": "261100976",
          "cited_paper_id": 246411402,
          "context_text": "Consistent with prior work (Li et al. 2022), we utilize the Karpathy (Karpathy and Fei-Fei 2017) split for training and evaluation.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Karpathy' split, which is a specific dataset split used for training and evaluation in multi-modal tasks involving images and text.",
          "citing_paper_doi": "10.1609/aaai.v38i3.28017",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b5b753aa23be12f24b4592429514df6e53289bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "16273722",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PropBank"
      ],
      "dataset_details": [
        {
          "dataset_name": "PropBank",
          "dataset_description": "Used to extend the size of the event knowledge graph by providing a large annotated corpus of semantic roles, enhancing the representation of verb-based events. | Used to extend the size of the event knowledge graph by providing an annotated corpus of nominal predicate argument structures, enhancing the representation of noun-based events.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 2486369,
          "context_text": "PropBank[31] and NomBank[27] further extend the size of event KG, with a number of 112,917",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions PropBank and NomBank, which are annotated corpora used for semantic role labeling and nominal predicate argument structure, respectively. These are specific, verifiable resources.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": "10.1162/0891201053630264",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99d2dcdcf4cf05facaa101a48c7e31d140b4736d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2005
        },
        {
          "dataset_name": "PropBank",
          "dataset_description": "Used to extend the size of the event knowledge graph by providing a large annotated corpus of semantic roles, enhancing the representation of verb-based events. | Used to extend the size of the event knowledge graph by providing an annotated corpus of nominal predicate argument structures, enhancing the representation of noun-based events. | Used to extend the size of an event knowledge graph, contributing 114,576 events to the dataset.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 16273722,
          "context_text": "PropBank[31] and NomBank[27] further extend the size of event KG, with a number of 112,917",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions PropBank and NomBank, which are annotated corpora used for semantic role labeling and nominal predicate argument structure, respectively. These are specific, verifiable resources.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/255d6867cb5c57810c909d5e488c9ae86e0d6d3e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "1103216",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "VGGFace2"
      ],
      "dataset_details": [
        {
          "dataset_name": "VGGFace2",
          "dataset_description": "This dataset 'FER2013' was mentioned in the citation context but no detailed description was generated. | Used to train a model for extracting facial identity features from detected faces in the context of multi-modal reasoning.",
          "citing_paper_id": "256698501",
          "cited_paper_id": 1103216,
          "context_text": "For facial component of visual features, we use FaceNet pretrained on VGGFace2 [4] to extract facial identity features and one pretrained on FER2013 [17] to extract facial emotion features, respectively, from the detected faces in each frame.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, VGGFace2 and FER2013, which are used to train models for extracting facial identity and emotion features, respectively.",
          "citing_paper_doi": "10.1145/3583690",
          "cited_paper_doi": "10.1016/j.neunet.2014.09.005",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b406f10ccd2d4c98af2849a941d897e8e111829",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db8c3cfaae04a14c1209d62953029b6fa53e23c7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "VGGFace2",
          "dataset_description": "This dataset 'FER2013' was mentioned in the citation context but no detailed description was generated. | Used to train a model for extracting facial identity features from detected faces in the context of multi-modal reasoning.",
          "citing_paper_id": "256698501",
          "cited_paper_id": 206592766,
          "context_text": "For facial component of visual features, we use FaceNet pretrained on VGGFace2 [4] to extract facial identity features and one pretrained on FER2013 [17] to extract facial emotion features, respectively, from the detected faces in each frame.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, VGGFace2 and FER2013, which are used to train models for extracting facial identity and emotion features, respectively.",
          "citing_paper_doi": "10.1145/3583690",
          "cited_paper_doi": "10.1109/CVPR.2015.7298682",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b406f10ccd2d4c98af2849a941d897e8e111829",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5aa26299435bdf7db874ef1640a6c3b5a4a2c394",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "247794106",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "WN9-IMG-TXT"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN9-IMG-TXT",
          "dataset_description": "Used to enhance the data diversity of multi-modal knowledge graphs by adding textual descriptions and images to entities, aiming to improve the richness of the graph.",
          "citing_paper_id": "259203022",
          "cited_paper_id": 247794106,
          "context_text": "To expand the auxiliary data with one modality, WN9-IMG-TXT and FB-IMG-TXT simultaneously add a number of textual descriptions and images to each entity, aiming to further enhance the data diversity of the MKGs [34], [48].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WN9-IMG-TXT and FB-IMG-TXT as datasets used to enhance the data diversity of multi-modal knowledge graphs (MKGs). These names are specific and plausible, fitting the criteria for inclusion.",
          "citing_paper_doi": "10.1109/TKDE.2025.3546686",
          "cited_paper_doi": "10.1109/TKDE.2023.3282907",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3f1473dd6c0f159a255b16f44998663e605b4878",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e6654520d8831f1721d4ec2dd1089b5d27f460f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "WN9-IMG-TXT",
          "dataset_description": "Used to verify reasoning performance in a multi-modal knowledge graph under the transductive setting, focusing on image and text integration.",
          "citing_paper_id": "259203022",
          "cited_paper_id": 252089825,
          "context_text": "Following MMKGR [34], [53], we use WN9-IMG-TXT and FB-IMG-TXT to verify the reasoning performance under the transductive setting.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for verifying reasoning performance in a multi-modal knowledge graph setting.",
          "citing_paper_doi": "10.1109/TKDE.2025.3546686",
          "cited_paper_doi": "10.1109/ICDE55515.2023.00015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3f1473dd6c0f159a255b16f44998663e605b4878",
          "cited_paper_url": "https://www.semanticscholar.org/paper/79801d46d6c495f1db8deb4eda461706ae4820d1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "195441339",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "LVIS"
      ],
      "dataset_details": [
        {
          "dataset_name": "LVIS",
          "dataset_description": "Used to pretrain the ResNet152 model for obtaining frame features, focusing on the multi-modal reasoning aspect, employing the dataset to address the research question of integrating visual and environmental components.",
          "citing_paper_id": "256698501",
          "cited_paper_id": 195441339,
          "context_text": "Also, for environment component, the ResNet152 model is pretrained on LVIS dataset [20] to obtain the frame features.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The LVIS dataset is mentioned as the source for pretraining the ResNet152 model to obtain frame features, which is directly relevant to the multi-modal knowledge graph reasoning context.",
          "citing_paper_doi": "10.1145/3583690",
          "cited_paper_doi": "10.1109/CVPR.2019.00550",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b406f10ccd2d4c98af2849a941d897e8e111829",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f902a64f7d08aaa6bfca7463e8729952ddc6134e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "LVIS",
          "dataset_description": "Used to detect objects and extract features, contributing to the object component in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "256698501",
          "cited_paper_id": null,
          "context_text": "For object component, we use the Detectron2 [64] detector pretrained on LVIS dataset to detect objects and obtain features.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the LVIS dataset for object detection and feature extraction, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3583690",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1b406f10ccd2d4c98af2849a941d897e8e111829",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "207167677",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB-IMG",
          "dataset_description": "Used to obtain textual and visual representations for multi-modal reasoning, combining pre-trained word2vec for text and VGG-m-128CNN embeddings for images.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 14124313,
          "context_text": "For the FB-IMG dataset, textual representations can be obtained by pre-trained word2vec and visual representations can be obtained by using embeddings of the VGG-m-128CNN [6] model.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the FB-IMG dataset, which is used to obtain textual and visual representations using pre-trained models. The dataset is clearly identified and used for multi-modal reasoning.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
          "citing_paper_year": 2021,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "FB-IMG",
          "dataset_description": "Mentioned as a representative knowledge graph, but no specific usage or research context is provided. | Used to construct a subset of FB15K, focusing on triples extracted from Freebase for multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "244222941",
          "cited_paper_id": 207167677,
          "context_text": "FB-IMG This dataset constructed by Hatem et al. [25] is the subset of FB15K [5], which consists of triples extracted from Freebase [3].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FB-IMG', which is a specific dataset derived from FB15K, a well-known knowledge graph dataset. The dataset is described as a subset of FB15K, containing triples extracted from Freebase.",
          "citing_paper_doi": "10.1007/s10489-021-02693-9",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd4aaccf5ca9e9cc0851ee4fbad77121952eabda",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2021,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "207163173",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "YAGO"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO",
          "dataset_description": "Employed to support multi-modal reasoning tasks, particularly in linking entities and enriching knowledge graph representations with diverse data types. | Used to explore multi-modal reasoning in knowledge graphs, focusing on entity linking and type inference across different modalities. | Utilized to enhance multi-modal reasoning by integrating structured data from Wikipedia, specifically for entity disambiguation and relation extraction.",
          "citing_paper_id": "246938066",
          "cited_paper_id": 2858079,
          "context_text": "Examples of classic general domain knowledge graphs include YAGO [11], DBpedia [12], Wikidata [13], etc.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions YAGO, DBpedia, and Wikidata as examples of classic general domain knowledge graphs. These are specific, verifiable resources that can be traced to their sources.",
          "citing_paper_doi": "10.3390/info13020091",
          "cited_paper_doi": "10.1145/2487788.2487935",
          "citing_paper_url": "https://www.semanticscholar.org/paper/cf7eb1e29a3d6eed2ec739e9cd5c8c96f8027f1b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec59845da9ee7efd33787b23f2f7c4be532da00e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "YAGO",
          "dataset_description": "Used as a lightweight and extensible ontology built from Wikidata and unified with WordNet, focusing on semantic knowledge integration. | Used to enhance temporal Knowledge Graph Reasoning by incorporating extra time information, specifically focusing on the integration of temporal data into the graph structure.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 207163173,
          "context_text": "• YAGO [233], [234], as a lightweight and extensible ontology, is built from Wikidata and uniﬁed with WordNet .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "YAGO is mentioned as a lightweight and extensible ontology built from Wikidata and unified with WordNet. It is a specific, verifiable resource.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/1242572.1242667",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "267802767",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "TinyImage"
      ],
      "dataset_details": [
        {
          "dataset_name": "TinyImage",
          "dataset_description": "Used to evaluate the accuracy of a multi-modal knowledge graph built on WordNet, focusing on the elimination of abstract concepts and its impact on hierarchical taxonomy. | Used as a baseline for comparison in constructing a Multi-Modal Knowledge Graph (MMKG), focusing on the distinction between visible and non-visible concepts. | Used as a multi-modal knowledge graph containing 75k noun concepts with an average of 1,052 images per concept, constructed based on WordNet for nonparametric object and scene recognition.",
          "citing_paper_id": "248435608",
          "cited_paper_id": 267802767,
          "context_text": "TinyImage [13] is a MMKG constructed based on WordNet and contains 75k noun concepts with 1,052 images per concept in average.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'TinyImage' as a multi-modal knowledge graph (MMKG) constructed based on WordNet, which is relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1007/978-3-031-00123-9_14",
          "cited_paper_doi": "10.1109/TPAMI.2008.128",
          "citing_paper_url": "https://www.semanticscholar.org/paper/adac9bfa73b47973c7b132963e7a4a5e612e6e3b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1d186975e456e7f3a26f35caaf3d983e49df08f8",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "TinyImage",
          "dataset_description": "Used to evaluate the accuracy of a multi-modal knowledge graph built on WordNet, focusing on the elimination of abstract concepts and its impact on hierarchical taxonomy.",
          "citing_paper_id": "248435608",
          "cited_paper_id": null,
          "context_text": "For example, TinyImage [13] (which is a MMKG built on WordNet [4]) simply eliminated all abstract concepts in the hierarchical taxonomy of WordNet while it is rough and inaccurate according to our experiments in Section 3.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "TinyImage is mentioned as a multi-modal knowledge graph (MMKG) built on WordNet, but it is described as rough and inaccurate based on the authors' experiments. WordNet is referenced but not as a dataset.",
          "citing_paper_doi": "10.1007/978-3-031-00123-9_14",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/adac9bfa73b47973c7b132963e7a4a5e612e6e3b",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "19139252",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB13"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB13",
          "dataset_description": "Used to train and evaluate knowledge graph completion models, focusing on a filtered subset of FB15k with 237 relations to reduce redundancy. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 24k entities. | Used to train and evaluate knowledge graph completion models, combining a subset of Freebase with New York Times articles. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 122 relations. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 15k entities and 1,345 relations. | Used to train and evaluate knowledge graph completion models, focusing on a smaller subset of Freebase with 13 relations. | Used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase with 5 million entities. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 20k entities.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 11223539,
          "context_text": "According to the entity set size, several subsets generated from it, including FB13 [201], FB122 [202], FB15k [203], FB20k [198], FB24k [204], FB5M [18], FB15k-237 [205], FB60k-NYT10 [206].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several subsets of Freebase, which are commonly used datasets in knowledge graph research. These datasets are used for training and evaluating knowledge graph completion models.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/01a858189394940d94ee00ee4285f3e84bff6f29",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "FB13",
          "dataset_description": "Used to train and evaluate knowledge graph completion models, focusing on a filtered subset of FB15k with 237 relations to reduce redundancy. | Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets for efficient testing and validation. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 24k entities. | Used to train and evaluate knowledge graph completion models, combining a subset of Freebase with New York Times articles. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 122 relations. | Used to assess the scalability and performance of knowledge graph embedding models on medium-sized entity sets. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 15k entities and 1,345 relations. | Used to train and evaluate knowledge graph completion models, focusing on a smaller subset of Freebase with 13 relations. | Used to train and evaluate knowledge graph completion models, focusing on a larger subset of Freebase with 5 million entities. | Used to test the robustness and generalization capabilities of knowledge graph reasoning algorithms on larger datasets. | Used to train and evaluate knowledge graph completion models, focusing on a subset of Freebase with 20k entities.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 19139252,
          "context_text": "According to the entity set size, several subsets generated from it, including FB13 [201], FB122 [202], FB15k [203], FB20k [198], FB24k [204], FB5M [18], FB15k-237 [205], FB60k-NYT10 [206].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several subsets of Freebase, which are commonly used datasets in knowledge graph research. These datasets are used for training and evaluating knowledge graph completion models.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1609/aaai.v32i1.11535",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3a5830800f7b471dafee2a1e6e070f45d9b3f7c7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "44145776",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "FB-IMG-TXT"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB-IMG-TXT",
          "dataset_description": "Used to enhance multimodal knowledge graphs by adding textual descriptions and images to entities, increasing data diversity. | Used to crawl 100 images per entity for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information. | Used to crawl 10 images per entity for multi-modal knowledge graph representation learning, focusing on integrating visual and textual information. | Used to set the embedding dimension for image features to 128, focusing on multi-modal knowledge graph representation learning. | Used to set the embedding dimension for image features to 4096, focusing on multi-modal knowledge graph representation learning. | Used as a benchmark to compare against FB-IMG-TXT, demonstrating differences in sparsity and complexity in multimodal knowledge graph representation learning. | Used to compare sparsity and complexity in multimodal knowledge graph representation learning, highlighting challenges in handling sparse and complex data.",
          "citing_paper_id": "252089825",
          "cited_paper_id": 44145776,
          "context_text": "The embedding dimension d s of entity, relation and history is set to 200, the embedding dimension d i of image feature is set to 128 and 4096 on FB-IMG-TXT and WN9-IMG-TXT respectively, and the embedding dimension d t of textual feature is 1000 [45].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FB-IMG-TXT and WN9-IMG-TXT, which are used to set the embedding dimensions for image features. These datasets are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.00015",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79801d46d6c495f1db8deb4eda461706ae4820d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "FB-IMG-TXT",
          "dataset_description": "Used to combine textual descriptions and images in a knowledge graph, specifically for multimodal translation-based representation learning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 44145776,
          "context_text": "• FB-IMG-TXT [173] is the KG combined with textual descriptions and images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FB-IMG-TXT' as a knowledge graph combined with textual descriptions and images, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/S18-2027",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be91946bedbf65d543a7eb9dd1e033e7aaf78c3c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "19187663",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "DBpedia50"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBpedia50",
          "dataset_description": "Used to assess the scalability and performance of knowledge graph embedding models on medium-sized entity sets. | Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets for efficient testing and validation. | Used to test the robustness and generalization capabilities of knowledge graph reasoning algorithms on larger datasets.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 19187663,
          "context_text": "According to the entity set size, we can derive several subsets from it, i.e., DBpedia50 [198], DBpedia500 [198] and DB100K [199].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific subsets derived from a larger entity set, which are likely datasets used in knowledge graph research.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/P18-1011",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/322aa32b2a409d2e135dbb14736d9aeb497f1c52",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DBpedia50",
          "dataset_description": "Used for knowledge graph completion, focusing on entity linking and relation prediction in a smaller subset of DBpedia. | Used for knowledge graph completion, focusing on entity linking and relation prediction in a larger subset of DBpedia. | Used for knowledge graph embedding, focusing on large-scale entity and relation representation learning.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 19187663,
          "context_text": ", DBpedia50 [210], DBpedia500 [210] and DB100K [211].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for knowledge graph completion and embedding, which are directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.18653/v1/P18-1011",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/322aa32b2a409d2e135dbb14736d9aeb497f1c52",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "53199920",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Visual7W-KB"
      ],
      "dataset_details": [
        {
          "dataset_name": "Visual7W-KB",
          "dataset_description": "Used to test the performance of the Out of the Box method, focusing on factual visual question answering using graph convolutional networks. | Used to test the performance of the Out of the Box method, focusing on factual visual question answering using graph convolution networks.",
          "citing_paper_id": "219708313",
          "cited_paper_id": 53199920,
          "context_text": "We also test the performance of Out of the Box (OB) [Narasimhan et al., 2018] on Visual7W-KB and report the results in Table 5.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Visual7W-KB' as a dataset used to test the performance of a method. The dataset is specific and relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.24963/ijcai.2020/153",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6b13065b4050800e30bb74e010b8aaba3355525d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ad08da5951437c117551a63c2f8b943bee2029ce",
          "citing_paper_year": 2020,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "252905085",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GDELT"
      ],
      "dataset_details": [
        {
          "dataset_name": "GDELT",
          "dataset_description": "Used as a dense knowledge graph derived from global events, language, and tone data, focusing on temporal knowledge graph completion and embedding.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 252905085,
          "context_text": "• GDELT [130] is a dense KG derived from the Global Database of Events, Language, and Tone.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "GDELT is identified as a dataset derived from the Global Database of Events, Language, and Tone, which is a dense KG. It is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/3511808.3557233",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "2424223",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SituNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "SituNet",
          "dataset_description": "Used to define visual event schemas, contributing to the development of situation recognition systems that understand image content through visual semantic role labeling. | Used to train models in situation recognition tasks, focusing on visual semantic role labeling for image understanding.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 2424223,
          "context_text": "Schemas deﬁned in datasets of situation recognition tasks, such as SituNet [101] and SWiG [102], can be used to train models in this task.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, SituNet and SWiG, which are used for training models in situation recognition tasks. These datasets are relevant to multi-modal knowledge graph reasoning as they involve visual and semantic information.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/CVPR.2016.597",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b65faba7088864e134e7eb3b68c8e2f18cc5b4f6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "209376177",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Action Genome"
      ],
      "dataset_details": [
        {
          "dataset_name": "Action Genome",
          "dataset_description": "Used to construct scene graphs about whole human instances, focusing on spatio-temporal relationships and compositions of actions.",
          "citing_paper_id": "252782967",
          "cited_paper_id": 209376177,
          "context_text": "something-else [38] mainly works on human hands, while action genome [23] constructs scene graph about the whole human instances.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'action genome' as a resource that constructs scene graphs about whole human instances, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3503161.3548257",
          "cited_paper_doi": "10.1109/cvpr42600.2020.01025",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8db962faf0ec4708f0e14410eabd23521d244619",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1242ba8fdb994b82a0575dc92f30f7b26a75707",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "245218982",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "QuALITY"
      ],
      "dataset_details": [
        {
          "dataset_name": "QuALITY",
          "dataset_description": "Used to compare the complexity of questions requiring deep reasoning and fact tracking across multiple segments, highlighting the need for advanced multi-modal reasoning.",
          "citing_paper_id": "279243397",
          "cited_paper_id": 245218982,
          "context_text": "…7 ) is much higher than existing biomedical QA datasets such as PubMedQA [46] and even datasets designed specifically for long-context QA such as QuALITY [88] (66 v/s 14 and 12.5 respectively) indicating a need for deep reasoning and tracking of facts across multiple segments for our questions.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for comparison in the research, indicating their use in evaluating the complexity and requirements of the current dataset.",
          "citing_paper_doi": "10.48550/arXiv.2506.05766",
          "cited_paper_doi": "10.18653/v1/2022.naacl-main.391",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e004fda8588f4ad18afffbf31f278bd65f761ead",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3c209e0703ffff26231b1145268c935df494631a",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "270357492",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "STaRK"
      ],
      "dataset_details": [
        {
          "dataset_name": "STaRK",
          "dataset_description": "Used to support RAG models, focusing on specific aspects of retrieval-augmented reasoning and knowledge graph integration. | Used to benchmark RAG models, focusing on comprehensive evaluation of retrieval-augmented generation systems.",
          "citing_paper_id": "279243397",
          "cited_paper_id": 270357492,
          "context_text": "That said, we do find two new datasets that directly support RAG, viz., CRAG [133] and STaRK [130].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, CRAG and STaRK, which are used to support RAG. The title 'CRAG - Comprehensive RAG Benchmark' confirms CRAG is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2506.05766",
          "cited_paper_doi": "10.48550/arXiv.2406.04744",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e004fda8588f4ad18afffbf31f278bd65f761ead",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec1bec009e68a4df478aaf11e3615e5587768990",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "271161780",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ProMQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "ProMQA",
          "dataset_description": "Utilized for multimodal question answering on scientific papers, combining text and images to improve understanding and accuracy. | Used for multimodal procedural activity understanding, focusing on question answering that integrates visual and textual information. | Applied to multimodal question answering, integrating various modalities to enhance reasoning and answer generation.",
          "citing_paper_id": "279243397",
          "cited_paper_id": 271161780,
          "context_text": "Multi-Modal QA Apart from STaRK, we find related datasets for multi-modal QA/RAG including ProMQA [31], MultiModalQA [111] and SPIQA [92].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets specifically designed for multi-modal QA/RAG, which are directly relevant to the research topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.05766",
          "cited_paper_doi": "10.48550/arXiv.2407.09413",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e004fda8588f4ad18afffbf31f278bd65f761ead",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e8c31cdb4b8d2cd27a2cf2b1e59ff0b3457d51e5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "273662191",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MultiModalQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MultiModalQA",
          "dataset_description": "Utilized for multimodal question answering on scientific papers, combining text and images to improve understanding and accuracy. | Applied to multimodal question answering, integrating various modalities to enhance reasoning and answer generation. | Used for multimodal procedural activity understanding, focusing on question answering that integrates visual and textual information.",
          "citing_paper_id": "279243397",
          "cited_paper_id": 273662191,
          "context_text": "Multi-Modal QA Apart from STaRK, we find related datasets for multi-modal QA/RAG including ProMQA [31], MultiModalQA [111] and SPIQA [92].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets specifically designed for multi-modal QA/RAG, which are directly relevant to the research topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.05766",
          "cited_paper_doi": "10.48550/arXiv.2410.22211",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e004fda8588f4ad18afffbf31f278bd65f761ead",
          "cited_paper_url": "https://www.semanticscholar.org/paper/660a8560d2dec107738a842f93a51784fa0dca05",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PubMed"
      ],
      "dataset_details": [
        {
          "dataset_name": "PubMed",
          "dataset_description": "Used to source abstracts for multi-modal reasoning, focusing on integrating textual information with other data types.",
          "citing_paper_id": "279243397",
          "cited_paper_id": null,
          "context_text": "We also consider sourcing texts (abstracts) from the PubMed database [128].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the PubMed database as a source of abstracts, which is a specific, verifiable dataset.",
          "citing_paper_doi": "10.48550/arXiv.2506.05766",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e004fda8588f4ad18afffbf31f278bd65f761ead",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "5583509",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Pascal Sentence"
      ],
      "dataset_details": [
        {
          "dataset_name": "Pascal Sentence",
          "dataset_description": "Used to visualize the distribution of generated features using t-SNE, focusing on the effectiveness of novel designs in multi-modal knowledge graph reasoning.",
          "citing_paper_id": "239011700",
          "cited_paper_id": 5583509,
          "context_text": "To further study the effectiveness of our novel designs, we use t-SNE tool to visualize the distribution of the generated features on Pascal Sentence [27].",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Pascal Sentence' which appears to be a specific dataset. However, the cited paper title does not help in confirming this as a dataset. The context is minimal and does not provide details on usage.",
          "citing_paper_doi": "10.1145/3474085.3475567",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/50ee08270ff4204b5d348c6d654d0aa57b29fe66",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146",
          "citing_paper_year": 2021,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "14040310",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NUS-WIDE"
      ],
      "dataset_details": [
        {
          "dataset_name": "NUS-WIDE",
          "dataset_description": "Used to disambiguate concepts and relate them better to images, enhancing multi-modal reasoning in image tagging tasks.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 14040310,
          "context_text": "Image Tagging NUS-WIDE [123] MIRFlickr [124] help disambiguation the concept and relate them better to images",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two image tagging datasets, NUS-WIDE and MIRFlickr, which are used to disambiguate concepts and relate them better to images.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/1460096.1460104",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f79131806747fce087d0fe73d0867cc621547b2a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "201871273",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VTKB"
      ],
      "dataset_details": [
        {
          "dataset_name": "VTKB",
          "dataset_description": "Used to construct a visio-textual knowledge base linking concepts to images and images by embedding similarities, enhancing image tagging quality through hierarchical concept organization.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 201871273,
          "context_text": "For example, [172] constructs an MMKG called VTKB containing hierarchical concepts, linking concepts of original tags to images and linking images by the similarities of embed-dings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the construction of a visio-textual knowledge base (VTKB) that links concepts to images and images by embedding similarities, which is directly relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/TMM.2019.2937181",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/77f5755926a0691efebf51c3b48fc71f306d70a9",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "224291855",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "academic MMKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "academic MMKG",
          "dataset_description": "Used to offer retrieval on the implementation level for deep learning papers and code, enhancing the linking between academic publications and their corresponding implementations.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 224291855,
          "context_text": "[138] uses an academic MMKG about papers and codes to offer retrieval on the implementation level.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions an academic MMKG, which is a specific multimodal knowledge graph. The title confirms it is about papers and codes, making it a relevant dataset for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1145/3340531.3417439",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/061c1a06d47170fa30f62018eee441a90f6967ce",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "235503675",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "unearthed oracle bones’ photos"
      ],
      "dataset_details": [
        {
          "dataset_name": "unearthed oracle bones’ photos",
          "dataset_description": "Used to construct a multi-modal knowledge graph for oracle bone recognition, focusing on integrating visual and textual data to enhance information processing.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 235503675,
          "context_text": "If the multi-modal data are treated as first-class citizens in some scenarios, the multi-modal data labeling way is more preferred to construct the MMKG, such as unearthed oracle bones’ photos in oracle bones recognition system [136], teachers’ class audios in educa-",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'unearthed oracle bones’ photos' which is a specific type of data used in the construction of a multi-modal knowledge graph for oracle bone recognition.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1016/J.COMPELECENG.2021.107173",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/70f304372e171c5f6ee61566d7943c46f1ed333d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "236428934",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Wikipedia articles and images"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikipedia articles and images",
          "dataset_description": "Used to pre-train a cross-modal entity matching module, aligning textual and visual scene graphs extracted from input articles and images.",
          "citing_paper_id": "246823061",
          "cited_paper_id": 236428934,
          "context_text": "In [18] the textual scene graph and visual scene graph extracted from the input article and images are aligned by the cross-modal entity matching module pre-trained on Wikipedia articles and images.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of Wikipedia articles and images for pre-training a cross-modal entity matching module, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TKDE.2022.3224228",
          "cited_paper_doi": "10.1109/TMM.2023.3301279",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c5e6720fa4c3cd73fd915bc71dea5e78184b262",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "220280200",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DocVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "DocVQA",
          "dataset_description": "Used to evaluate visual question answering on document images, focusing on the integration of textual and visual information for document understanding.",
          "citing_paper_id": "252782478",
          "cited_paper_id": 220280200,
          "context_text": "DocVQA The DocVQA [29] dataset represents the document understanding task.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DocVQA' as a dataset representing the document understanding task, which is relevant to multi-modal knowledge graph reasoning involving text and image data.",
          "citing_paper_doi": "10.1145/3503161.3548321",
          "cited_paper_doi": "10.1109/WACV48630.2021.00225",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3959ad38bea60eaeaaf415977030b11d385f49d8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b40bfcf339de3f0dba08fabb2b58b9368ff4c51a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "229923949",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CS-DVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CS-DVQA",
          "dataset_description": "Used to perform a similar experiment to DocVQA, evaluating the effectiveness of the add-on module in reducing errors in visually-rich document understanding. | Used to evaluate the performance of pre-trained Bert on visually-rich document understanding tasks, comparing it with LayoutLMv2 BASE.",
          "citing_paper_id": "252782478",
          "cited_paper_id": 229923949,
          "context_text": "For pre-trained Bert, through the combination of three mechanisms, the model’s performance on the CS-DVQA dataset has surpassed the multi-modal pre-trained model, LayoutLMv2 BASE .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the CS-DVQA dataset, which is used to evaluate the performance of the pre-trained Bert model against LayoutLMv2 BASE. The dataset is specific and relevant to the research context.",
          "citing_paper_doi": "10.1145/3503161.3548321",
          "cited_paper_doi": "10.18653/v1/2021.acl-long.201",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3959ad38bea60eaeaaf415977030b11d385f49d8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d42f77ee45e267cd0aa66103bbec8abf7f6d4767",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "53235839",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SpatialVOC2K"
      ],
      "dataset_details": [
        {
          "dataset_name": "SpatialVOC2K",
          "dataset_description": "Applied to analyze multilingual spatial relations in images, emphasizing manual annotations and features to explore cross-linguistic variations in spatial understanding.",
          "citing_paper_id": "268697475",
          "cited_paper_id": 53235839,
          "context_text": "For spatial relations, datasets such as SpatialSense (Yang, Rus-sakovsky, and Deng 2019) and SpatialVOC2K (Belz et al. 2018) are created using manual-based methods.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, SpatialSense and SpatialVOC2K, which are used for studying spatial relations between objects. Both datasets are created using manual-based methods.",
          "citing_paper_doi": "10.1609/aaai.v38i17.29828",
          "cited_paper_doi": "10.18653/v1/W18-6516",
          "citing_paper_url": "https://www.semanticscholar.org/paper/80ef8e5aa7df5c96fd0f0e9d847e0617be743c14",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d0fe63de22729bcecf12a84554cdfbccdb44c391",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "262466164",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CI-FAR"
      ],
      "dataset_details": [
        {
          "dataset_name": "CI-FAR",
          "dataset_description": "Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object classification and segmentation in visual scenes. | Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on object detection and annotation in diverse images. | Used to extract bounding boxes of objects for constructing Vi-sionKG, focusing on small-scale image recognition and object localization.",
          "citing_paper_id": "271962901",
          "cited_paper_id": 262466164,
          "context_text": "Vi-sionKG [29] is an MMKG containing bounding boxes of objects extracted from various image datasets such as MS-COCO [16], CI-FAR [13], and PASCAL VOC [9].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used to create Vi-sionKG, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3627673.3679175",
          "cited_paper_doi": "10.48550/arXiv.2309.13610",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3472db260af3cc01602db3ae2d6ef02a3ee937c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d33ef83d63c44462f6eaa8746d924af27f360e09",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "252280329",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KVC16K"
      ],
      "dataset_details": [
        {
          "dataset_name": "KVC16K",
          "dataset_description": "Used to evaluate knowledge graph completion methods, focusing on multi-modal reasoning and performance metrics such as MRR and Hit@1.",
          "citing_paper_id": "270711106",
          "cited_paper_id": 252280329,
          "context_text": "We report the MRR and Hit@1 results on the KVC16K/DB15K datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for reporting MRR and Hit@1 results, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3626772.3657800",
          "cited_paper_doi": "10.48550/arXiv.2209.07084",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6667a975df5b0928c0b88582af9321922b91d402",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0bc258895dcd06c224d770139e872249c25374fd",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB15K-DB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB15K-DB15K",
          "dataset_description": "Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and DBpedia subsets. | Subset of Freebase used for cross-KG reasoning experiments, focusing on entity and relation alignment. | Subset of DBpedia used for cross-KG reasoning experiments, focusing on entity and relation alignment. | Used to evaluate cross-KG reasoning, focusing on alignment between Freebase and YAGO subsets. | Subset of YAGO used for cross-KG reasoning experiments, focusing on entity and relation alignment.",
          "citing_paper_id": "271961581",
          "cited_paper_id": 76663467,
          "context_text": "The cross-KG datasets include two pairs of KGs: FB15K-DB15K and FB15K-YAGO15K [30], where FB15K, DB15K, and YAGO15K are subsets of Freebase [2], DBpedia [1], and YAGO [33], respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for cross-KG experiments, which are subsets of larger knowledge graphs.",
          "citing_paper_doi": "10.1145/3637528.3671769",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d4d60a5c303cec0f8df9138eb2dd7a535f970294",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "212737039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Multi-OpenEA"
      ],
      "dataset_details": [
        {
          "dataset_name": "Multi-OpenEA",
          "dataset_description": "Used to construct large-scale multimodal entity alignment benchmarks with a high ratio of image-equipped entities and multiple images per entity, enhancing the evaluation of multimodal knowledge graph reasoning methods.",
          "citing_paper_id": "257020057",
          "cited_paper_id": 212737039,
          "context_text": "In this work we first constructed a series of large-scale multimodal EA benchmarks named Multi-OpenEA with a high ratio of image equipped entities and multiple images per entity, based on the OpenEA benchmarks [4].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the construction of a new benchmark named Multi-OpenEA, which is based on the existing OpenEA benchmarks. However, OpenEA itself is not a dataset but a benchmark suite, so it is excluded.",
          "citing_paper_doi": "10.1109/ICASSP49357.2023.10094863",
          "cited_paper_doi": "10.14778/3407790.3407828",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d686c4c33feb37d64e40e7e3f4c1f6f5adc79d62",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65baa67a7cdb3b4b948d126ac5b41ca9c98b1f3b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "202770936",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DBP15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBP15K",
          "dataset_description": "Used to evaluate entity alignment models, specifically comparing performance with and without surface forms, demonstrating robustness and efficiency.",
          "citing_paper_id": "271961581",
          "cited_paper_id": 202770936,
          "context_text": "DBP15K 𝑍𝐻 − 𝐸𝑁 DBP15K − DBP15K − Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR w / o SF KECG [25] . the baselines even without surface forms, showing its robustness and efficiency.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DBP15K' which appears to be a dataset used for entity alignment experiments. The citation indicates that the dataset is used to evaluate the performance of a model without surface forms.",
          "citing_paper_doi": "10.1145/3637528.3671769",
          "cited_paper_doi": "10.18653/v1/D19-1274",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d4d60a5c303cec0f8df9138eb2dd7a535f970294",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cc9f702abca7d2c164e2a4207a835f27098fa63b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "208006241",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WD-singer"
      ],
      "dataset_details": [
        {
          "dataset_name": "WD-singer",
          "dataset_description": "Mentioned as a dataset, but no specific usage details are provided in the context. It is likely a large-scale knowledge graph dataset. | Used to derive a larger subset of Wikidata for evaluating knowledge graph completion tasks, covering a broader range of entities and relations. | Used to derive a subset of Wikidata for evaluating knowledge graph completion tasks, focusing on entities and relations within a specific scope.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 208006241,
          "context_text": "WD-singer [210] and wikidata5m [216] are derived from it according to different scopes.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets, WD-singer and wikidata5m, which are derived from a larger dataset. These are specific, named datasets relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1162/tacl_a_00360",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "246823061",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "N-MMKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "N-MMKG",
          "dataset_description": "Mentioned as a type of multi-modal knowledge graph, but specific usage and research context are not provided in the citation.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 246823061,
          "context_text": "Typically, there are two multi-modal KG [21], i.e., N-MMKG and A-MMKG (See Fig.",
          "confidence_score": 0.3,
          "citation_intent": [
            "e",
            "n",
            "o"
          ],
          "resource_type": [
            "e",
            "n",
            "o"
          ],
          "reasoning": "The citation mentions two multi-modal KGs, N-MMKG and A-MMKG, but does not provide enough context to determine their specific usage or characteristics. The names are plausible and specific, but the citation intent and resource type are unclear.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1109/TKDE.2022.3224228",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fa350b1089db1f8ab97bb72287b37ed4748c89cf",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "207167677",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WN18"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN18",
          "dataset_description": "Used to extend FB15k-237 with images for each entity, enhancing multi-modal reasoning in knowledge graphs. | Used to extend entities in FB15k-237 with images for multi-modal knowledge graph representation learning, enhancing the visual aspect of entity relationships. | Used to extend WN18 with images for each entity, enhancing multi-modal reasoning in knowledge graphs. | Used to extend entities in WN18 with images for multi-modal knowledge graph representation learning, integrating visual information into lexical semantic relationships.",
          "citing_paper_id": "259375740",
          "cited_paper_id": 207167677,
          "context_text": "Specifically, both FB15k-237-IMG [38] and WN18 [38] datasets are constructed by extending ten images for each entity based on FB15k-237 and WN18, which are the subset of the large-scale knowledge graph Freebase [4] and WordNet [37], separately.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, FB15k-237-IMG and WN18, which are extensions of FB15k-237 and WN18, respectively. These datasets are used for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.03591",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bd1271b05230267f114502eccdc84b1e15a4313b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2023,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "224271811",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "EAL dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "EAL dataset",
          "dataset_description": "Used to provide aspect content for calculating similarity between context and aspect, focusing on entity aspect linking in the research. | Used to collect entities and their aspect labels by treating section names in Wikipedia pages as aspect labels, focusing on entity representation and aspect labeling. | Used to evaluate the AspectMMKG model, demonstrating state-of-the-art performance in entity aspect linking by comparing against the original EAL model. | Used in experiments for entity aspect linking, focusing on the performance of the proposed method in linking entities to their aspects. | Used to define aspects for entity instances, enhancing the representation of entities in a knowledge graph by providing detailed attributes and relationships.",
          "citing_paper_id": "260735729",
          "cited_paper_id": 224271811,
          "context_text": "Following Ramsdell et al. [16], there are two aspect features can be used to calculate the similarity between the context and aspect: (a) only the aspect name; (b) the aspect content which is a description text containing aspect name provided by the EAL dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'EAL dataset' which is a specific, verifiable dataset used for entity aspect linking. The dataset is used to provide aspect content for calculating similarity.",
          "citing_paper_doi": "10.1145/3583780.3614782",
          "cited_paper_doi": "10.1145/3340531.3412875",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0356abf345662110b6d6a818304c886ce45b6f04",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f9c42de88c498b066b5f0963996f3ad46689cc1b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "266469822",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KnowSite"
      ],
      "dataset_details": [
        {
          "dataset_name": "KnowSite",
          "dataset_description": "Used to model site selection criteria for brands and stores, leveraging an urban knowledge graph to provide explainable site decisions based on relation path logic.",
          "citing_paper_id": "275054439",
          "cited_paper_id": 266469822,
          "context_text": "Besides, the relation path-based decoder is proposed to model site selection criteria for brands and stores so that the KnowSite [19] can not only achieve state-of-the-art (SOTA) performance but also provide explainable site decisions based on the relation path logic.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'KnowSite' in the context of a knowledge graph for site selection, which appears to be a specific dataset or knowledge base. However, the term 'KnowSite' could also refer to a method or tool. Given the context, it is more likely a dataset or knowledge base used for site selection.",
          "citing_paper_doi": "10.1109/TMM.2024.3521742",
          "cited_paper_doi": "10.1145/3589132.3625640",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79d7dbb3afe6fb46f77607dd40b5ad89007ee61b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86682e30d86f4d1d3a8bf2aaf482783803a40efc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "256631072",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "PKG",
          "dataset_description": "Used as a multi-modal knowledge graph for classical Chinese poetry, integrating textual and visual data to enhance understanding and reasoning about poetic works.",
          "citing_paper_id": "260735729",
          "cited_paper_id": 256631072,
          "context_text": "(6) PKG [10] is an MMKG for classical Chinese poetry.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions PKG as a multi-modal knowledge graph for classical Chinese poetry, which is directly relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1145/3583780.3614782",
          "cited_paper_doi": "10.18653/v1/2022.findings-emnlp.171",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0356abf345662110b6d6a818304c886ce45b6f04",
          "cited_paper_url": "https://www.semanticscholar.org/paper/94acf4682bce44810c377be075c20dd6de96b8e5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Freebase-15k Multi-Modal (FB15k)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Freebase-15k Multi-Modal (FB15k)",
          "dataset_description": "Used for evaluating multi-modal knowledge graph reasoning, particularly for cross-lingual entity alignment and relation prediction. | This dataset 'Freebase-15k (237) Multi-Modal (FB15k-(237))' was mentioned in the citation context but no detailed description was generated. | Used for evaluating multi-modal knowledge graph reasoning, focusing on entity and relation prediction tasks. | Used to extend the YAGO dataset with multi-modal information, focusing on integrating visual and textual data for multi-modal knowledge graph reasoning. | Used for evaluating multi-modal knowledge graph reasoning, emphasizing entity and relation prediction in a large-scale setting. | Used to extend the original FB15-III dataset for multi-modal reasoning, focusing on multi-modal reasoning and evaluation in multi-modal knowledge graphs. | Used to extend the original DBpedia dataset with images, enhancing multi-modal knowledge graph reasoning by incorporating visual information into entity representations. | Used to evaluate multi-modal reasoning methods, focusing on entity linking and relation prediction tasks.",
          "citing_paper_id": "271915694",
          "cited_paper_id": 76663467,
          "context_text": "■ Freebase-15k Multi-Modal (FB15k) [31] is a multi-modal extension of the original FB15k (NMM) dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Freebase-15k Multi-Modal (FB15k)' as a multi-modal extension of the original FB15k dataset, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2408.11526",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1534872453944130329d5d6b4cf3ced007bfb31e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "15928602",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NTU-RGB+D 60"
      ],
      "dataset_details": [
        {
          "dataset_name": "NTU-RGB+D 60",
          "dataset_description": "Used for skeleton-based action recognition, providing a large-scale dataset for 3D human activity data, supporting multi-modal reasoning tasks.",
          "citing_paper_id": "251846584",
          "cited_paper_id": 15928602,
          "context_text": "NTU-RGB+D 60 [11] is the most widely used dataset for skeleton-based action recognition.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'NTU-RGB+D 60' as a dataset used for skeleton-based action recognition, which is relevant to multi-modal knowledge graph reasoning involving human activity data.",
          "citing_paper_doi": "10.1109/ICME52920.2022.9859787",
          "cited_paper_doi": "10.1109/CVPR.2016.115",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f8e4f1d35fa533c3fd222b8bb2e5352745b4db41",
          "cited_paper_url": "https://www.semanticscholar.org/paper/091e4d3c85dc0a8212afea875cd3b162d273d46b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "141406559",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ASER"
      ],
      "dataset_details": [
        {
          "dataset_name": "ASER",
          "dataset_description": "ASER is used as a large-scale eventuality knowledge graph, providing a rich resource for semantic relations and eventualities in multi-modal reasoning contexts. | ASER is used as a large-scale eventuality knowledge graph, providing a rich resource for multi-modal knowledge graph reasoning tasks.",
          "citing_paper_id": "254097121",
          "cited_paper_id": 141406559,
          "context_text": "Up to now, the largest event KG ASER [51] has reached the scale of 194 million events.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions ASER as a large-scale eventuality knowledge graph, which is relevant to multi-modal knowledge graph reasoning. However, it does not specify usage details.",
          "citing_paper_doi": "10.1145/3573201",
          "cited_paper_doi": "10.1145/3366423.3380107",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a4e1ca08748933b1ec71470edd7982d8f3a995df",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c5e77461f796ab460ae5daa97c1323d2e25c893",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "233004700",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "UAV-Human"
      ],
      "dataset_details": [
        {
          "dataset_name": "UAV-Human",
          "dataset_description": "Used to evaluate human behavior understanding with unmanned aerial vehicles, focusing on multi-modal reasoning and multi-modal learning. | Used for human behavior understanding with unmanned aerial vehicles, specifically analyzing multi-modal video sequences captured from three sensors.",
          "citing_paper_id": "251846584",
          "cited_paper_id": 233004700,
          "context_text": "UAV-Human [6] is a large benchmark for human behavior understanding with unmanned aerial vehicles, including 22,476×3 multi-modal video sequences captured from three sensors.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions a specific benchmark dataset, UAV-Human, which is used for human behavior understanding with unmanned aerial vehicles. The dataset is described as large and multi-modal, containing video sequences from three sensors.",
          "citing_paper_doi": "10.1109/ICME52920.2022.9859787",
          "cited_paper_doi": "10.1109/CVPR46437.2021.01600",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f8e4f1d35fa533c3fd222b8bb2e5352745b4db41",
          "cited_paper_url": "https://www.semanticscholar.org/paper/45b98ccc785f9ef7e6b7c98b88ec770b49828dd2",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "53037206",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MIMIC-III"
      ],
      "dataset_details": [
        {
          "dataset_name": "MIMIC-III",
          "dataset_description": "Used to train models for multi-modal reasoning in critical care settings, specifically for training and evaluation in ICU environments.",
          "citing_paper_id": "259501302",
          "cited_paper_id": 53037206,
          "context_text": "…2016), HiRID (Faltys et al., 2021), and eICU (Pollard et al., 2018) enable modeling of disease progressions within a single hospital visit, for example in Intensive Care Units (ICU) (Harutyunyan et al., 2019; Y`eche et al., 2021), or progressions across multiple patient visits (Choi et al., 2018).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions three specific datasets used for modeling disease progressions in healthcare settings, which are relevant to multi-modal knowledge graph reasoning in medical contexts.",
          "citing_paper_doi": "10.48550/arXiv.2307.04461",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/05cdfa832e154559bc19101a5dc8623beeff2308",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dfa2fe00291a69d5fe982a6d980f35a8e58b0de6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "1055111",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NYTimes800k"
      ],
      "dataset_details": [
        {
          "dataset_name": "NYTimes800k",
          "dataset_description": "Used to collect images, captions, and news articles from the New York Times, focusing on the annotation of images with ground-truth captions.",
          "citing_paper_id": "236428934",
          "cited_paper_id": 1055111,
          "context_text": "The images, captions and news articles in the GoodNews dataset and the NYTimes800 k dataset are collected from the New York Times, and each image is annotated with one ground-truth caption.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two specific datasets, GoodNews and NYTimes800k, both of which are used for collecting images, captions, and news articles from the New York Times.",
          "citing_paper_doi": "10.1109/TMM.2023.3301279",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7c5e6720fa4c3cd73fd915bc71dea5e78184b262",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd",
          "citing_paper_year": 2021,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "205228801",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Unified Medical Language System (UMLS)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Unified Medical Language System (UMLS)",
          "dataset_description": "Used to integrate biomedical terminology, providing a comprehensive knowledge base for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "259501302",
          "cited_paper_id": 205228801,
          "context_text": "Other modalities of medical data exist outside of in-hospital datasets, where a vast amount of prior medical knowledge is stored in static form in databases such as the Unified Medical Language System (UMLS) (Bodenreider, 2004).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The UMLS is mentioned as a static database storing prior medical knowledge, which is relevant for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.04461",
          "cited_paper_doi": "10.1093/nar/gkh061",
          "citing_paper_url": "https://www.semanticscholar.org/paper/05cdfa832e154559bc19101a5dc8623beeff2308",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1f1eaf19e38b541eec8a02f099e3090536a4c936",
          "citing_paper_year": 2023,
          "cited_paper_year": 2004
        }
      ]
    },
    {
      "cited_paper_id": "235324796",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Semantic MEDLINE"
      ],
      "dataset_details": [
        {
          "dataset_name": "Semantic MEDLINE",
          "dataset_description": "Used to extract personalized graphs for enhancing EHR representation learning, focusing on multi-modal reasoning and knowledge integration.",
          "citing_paper_id": "259501302",
          "cited_paper_id": 235324796,
          "context_text": "With MedPath Ye et al. (2021) propose to enhance the performance of existing EHR representation learning architectures by incorporating a personalized graph extracted using knowledge from Semantic MEDLINE (Rindflesch et al., 2011).",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Semantic MEDLINE' as a knowledge source used to extract a personalized graph, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2307.04461",
          "cited_paper_doi": "10.1145/3442381.3449860",
          "citing_paper_url": "https://www.semanticscholar.org/paper/05cdfa832e154559bc19101a5dc8623beeff2308",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b2d336b5a222c4962ac4f5e36b955d60873fc177",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "235376996",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ICD/ATC"
      ],
      "dataset_details": [
        {
          "dataset_name": "ICD/ATC",
          "dataset_description": "This dataset 'ICD/ATC' was mentioned in the citation context but no detailed description was generated. | Extended with co-occurrence information to enhance the predictive power of models, specifically for temporal health event prediction.",
          "citing_paper_id": "259501302",
          "cited_paper_id": 235376996,
          "context_text": "First the tuple of the ICD and ATC hierarchy ICD/ATC (Shang et al., 2019) and second an extension thereof including co-occurrence information ICD/ATC − CO (e.g. Lu et al. (2021a)).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific resources, ICD/ATC and ICD/ATC − CO, which are used as hierarchical and co-occurrence data for health event prediction.",
          "citing_paper_doi": "10.48550/arXiv.2307.04461",
          "cited_paper_doi": "10.1109/TCYB.2021.3109881",
          "citing_paper_url": "https://www.semanticscholar.org/paper/05cdfa832e154559bc19101a5dc8623beeff2308",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dfa756c419a19f0503fa1cbfeea9774ce6b410ec",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "7483388",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FVQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "FVQA",
          "dataset_description": "Used to integrate background knowledge in visual question answering, representing instances as tuples of image, question, answer, and supporting fact subgraph.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 7483388,
          "context_text": "To address this limitation, Wang et al. [407] introduce the dataset named FVQA, which represents each instance as an tuple “image–question–answer–supporting fact subgraph”, requiring the integration of background knowledge.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context explicitly mentions the creation and use of the FVQA dataset, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1109/TPAMI.2017.2754246",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b60630911d7746fba06de7c34abe98c9a61c6bcc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "261065787",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "COCO"
      ],
      "dataset_details": [
        {
          "dataset_name": "COCO",
          "dataset_description": "Used for image captioning tasks, providing a large-scale dataset of images and corresponding captions to train and evaluate models. | Used to evaluate models' multi-modal reasoning abilities through a diverse range of questions, focusing on perception and reasoning tasks.",
          "citing_paper_id": "267211941",
          "cited_paper_id": 261065787,
          "context_text": "There are already many datasets for evaluating the multi-modal capabilities of models. and Manning, 2019] are used in visual question answering tasks, while COCO [Lin et al. , 2014], NoCaps [Agrawal et al. , 2019], and Flickr30K [Plummer et al. , 2015] are employed in image captioning tasks.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating multi-modal capabilities, specifically in visual question answering and image captioning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2401.14011",
          "cited_paper_doi": "10.48550/arXiv.2308.04813",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4d5262260022d8baff65242c6eb879d184447d5f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/692902404c282de936249aef68ce9f974815128c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "208006241",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DBPedia50k"
      ],
      "dataset_details": [
        {
          "dataset_name": "DBPedia50k",
          "dataset_description": "Used for inductive reasoning in knowledge graphs, focusing on the inductive version to enhance model performance.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 208006241,
          "context_text": "Besides, both DBPedia50k [345] and Wikidata5M [364] offer inductive as well as transductive versions, though only the inductive versions are considered here.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, DBPedia50k and Wikidata5M, which are used for inductive reasoning in the context of knowledge graphs.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1162/tacl_a_00360",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "218470438",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "HLVU"
      ],
      "dataset_details": [
        {
          "dataset_name": "HLVU",
          "dataset_description": "Used to test deep understanding of movies, focusing on high-level video understanding with 10 Creative Commons licensed movies totaling 681 minutes.",
          "citing_paper_id": "222278396",
          "cited_paper_id": 218470438,
          "context_text": "The challenge uses the recently introduced High Level Video Understanding (HLVU) dataset [5] which consists of 10 movies released under creative commons licenses with a total combined duration of 681 minutes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the HLVU dataset, which is a specific, verifiable resource used in the context of testing deep understanding of movies.",
          "citing_paper_doi": "10.1145/3394171.3416292",
          "cited_paper_doi": "10.1145/3372278.3390742",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a33d18b6168d7a50c475691bce0eb0f991dc1e3e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7105d91c324e6118dbc5ec119ec93af6471638c6",
          "citing_paper_year": 2020,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "201317624",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VCR"
      ],
      "dataset_details": [
        {
          "dataset_name": "VCR",
          "dataset_description": "Fine-tuned the BERT model on the VCR dataset to learn cross-modal alignment, focusing on visual-linguistic reasoning tasks.",
          "citing_paper_id": "258902254",
          "cited_paper_id": 201317624,
          "context_text": "[19]fedcom-binationsofvisualandlinguisticdatafromseveralcross-modal datasetsintoBERTmodeltolearncross-modalalignment,then ﬁne-tunedthemodelontheVCRdataset[8].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'cross-modal datasets' and 'VCR dataset'. VCR is a specific dataset used for fine-tuning the model, while 'cross-modal datasets' is too generic.",
          "citing_paper_doi": "10.1109/TMM.2023.3279691",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/385682ed410ff47de5898884f6c1d1ddc1fa9d58",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "227239576",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "electronic medical records"
      ],
      "dataset_details": [
        {
          "dataset_name": "electronic medical records",
          "dataset_description": "Used to integrate with a medical knowledge graph for safe medication recommendation, focusing on the application of knowledge graph embedding techniques.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 227239576,
          "context_text": "Gong et al. [426] integrate electronic medical records with a medical KG, using KGE techniques to recommend safe medications.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the integration of electronic medical records with a medical knowledge graph for safe medication recommendation, which aligns with multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1016/j.bdr.2020.100174",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2930168f3be575781939a57f4bb92e6b29c33b08",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "229377193",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Global Database of Events, Language, and Tone (GDELT)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Global Database of Events, Language, and Tone (GDELT)",
          "dataset_description": "Used for dynamic knowledge graph reasoning tasks, providing event data and linguistic tone analysis for global events. | Used for dynamic knowledge graph reasoning tasks, offering structured data on political events and crisis situations for predictive modeling.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 229377193,
          "context_text": "The most frequently employed datasets for dynamic KGR tasks are derived from two open-source databases: the Global Database of Events, Language, and Tone (GDELT) [357] and the Integrated Crisis Early Warning System (ICEWS) [358].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific databases used for dynamic KGR tasks, which are relevant to the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1145/2187980.2188242",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1858073b92df5a796123d2e592670219d840c625",
          "citing_paper_year": 2025,
          "cited_paper_year": 2012
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MM-FB15K"
      ],
      "dataset_details": [
        {
          "dataset_name": "MM-FB15K",
          "dataset_description": "Used to construct a multi-modal knowledge graph, integrating DBpedia with visual and textual information to support multi-modal reasoning tasks. | Used to construct a multi-modal knowledge graph, extending FB15K with visual and textual modalities to enhance reasoning capabilities.",
          "citing_paper_id": "252518772",
          "cited_paper_id": 76663467,
          "context_text": "Our constructed multi-modal datasets MM-FB15K and MM-DBpedia are based on FB15K [2,1] and DBpedia [14,11,23].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two constructed multi-modal datasets, MM-FB15K and MM-DBpedia, which are based on existing datasets FB15K and DBpedia. These are specific, verifiable datasets used in the research.",
          "citing_paper_doi": "10.1007/978-3-031-26390-3_11",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d0ba984394512bbf32d7135716f8bf2e8984a996",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "235614395",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Wikidata"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikidata",
          "dataset_description": "Used to construct dynamic KGR datasets by incorporating temporal information, enhancing the reasoning capabilities over time.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 235614395,
          "context_text": "Besides, the well-known KG Wikidata [239] can be leveraged for constructing dynamic KGR datasets when temporal information is incorporated.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Wikidata is mentioned as a knowledge graph that can be used to construct dynamic KGR datasets, particularly when temporal information is added.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/08ede1cbadd5c631f93c0f952ac7ca99605d8a21",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "257220329",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ICEWS"
      ],
      "dataset_details": [
        {
          "dataset_name": "ICEWS",
          "dataset_description": "Used to provide event data with daily temporal granularity, focusing on military and political events for multi-modal knowledge graph reasoning.",
          "citing_paper_id": "279392166",
          "cited_paper_id": 257220329,
          "context_text": "In contrast, ICEWS focuses on military and political events, providing event data with a daily temporal granularity [33].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ICEWS as a dataset providing event data with daily temporal granularity, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.11012",
          "cited_paper_doi": "10.1109/TPAMI.2024.3417451",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5fa9bcf38bcf71d9b0dd27b7c84023f1aa8b0f7e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "21715202",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Babel-Net"
      ],
      "dataset_details": [
        {
          "dataset_name": "Babel-Net",
          "dataset_description": "Used to build a multilingual knowledge graph, providing a large-scale lexicalized semantic network. | Used to generate a metallic materials knowledge graph, integrating structured information from DBpedia and Wikipedia. | Used to construct a knowledge graph by automatically labeling terms defined by natural language, focusing on interpretable text entailment recognition. | Used to construct a knowledge graph based on WordNet, focusing on lexical and semantic relationships.",
          "citing_paper_id": "201066287",
          "cited_paper_id": 21715202,
          "context_text": "One is to build knowledge graph based on structured or semi-structured information in Wikipedia or other existing knowledge base (e.g., MMKG [11], Babel-Net [38], and WordNetGraph [39]).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions MMKG, Babel-Net, and WordNetGraph as examples of knowledge bases used to build knowledge graphs. These are specific, verifiable resources.",
          "citing_paper_doi": "10.1109/ACCESS.2019.2933370",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/25cd421e0f999ef8151b02fe6c7660e059a898ce",
          "citing_paper_year": 2019,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "205692",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NELL23k"
      ],
      "dataset_details": [
        {
          "dataset_name": "NELL23k",
          "dataset_description": "Used to evaluate knowledge graph embedding models, focusing on a smaller, curated subset of NELL knowledge base. | Used to train and evaluate knowledge graph embedding models, containing a large subset of NELL knowledge base.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 205692,
          "context_text": "According to the different scopes, there are several subsets of it, e.g., Location [208], sports [208], NELL23k [210], NELL-995 [211].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Location', 'sports', 'NELL23k', and 'NELL-995' as subsets of a larger dataset. These are specific, identifiable datasets used in the context of knowledge graph embedding.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.3115/v1/P15-1009",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/69418ff5d4eac106c72130e152b807004e2b979c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "3226443",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "electronic medical database"
      ],
      "dataset_details": [
        {
          "dataset_name": "electronic medical database",
          "dataset_description": "Used to construct a health knowledge graph, focusing on reasoning tasks using patient data from electronic medical records.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 3226443,
          "context_text": "For example, [246] and [247] both perform reasoning on the KG constructed from the electronic medical database.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions an electronic medical database used for constructing a knowledge graph, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1038/s41598-017-05778-z",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8b0de2074bb42a55395efd7e95503bd74736cdd7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "4437414",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Hetionet"
      ],
      "dataset_details": [
        {
          "dataset_name": "Hetionet",
          "dataset_description": "Used as a knowledge graph derived from public biomedical resources to prioritize drugs for repurposing, integrating diverse types of biomedical data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 4437414,
          "context_text": "• Hetionet [207] is a knowledge graph derived from biomedical studies based on public resources.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Hetionet is mentioned as a knowledge graph derived from public resources, which aligns with the topic of multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.7554/eLife.26726",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1758eeac1b000571ee1eaf425abea72fc1ec6e9b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "6911541",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FAMILY"
      ],
      "dataset_details": [
        {
          "dataset_name": "FAMILY",
          "dataset_description": "Used to represent and reason about relations among family members, focusing on multi-modal knowledge graph construction and inference. | Used to represent relations among nations, specifically for reasoning about multi-modal knowledge graphs and their interconnected entities.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 6911541,
          "context_text": "• FAMILY [200] consists of relations among family members.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'FAMILY' as a dataset consisting of relations among family members, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/1273496.1273551",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b6ce4ec0d28c050b99ec647a16e47116c939473c",
          "citing_paper_year": 2022,
          "cited_paper_year": 2007
        }
      ]
    },
    {
      "cited_paper_id": "13955854",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Countries"
      ],
      "dataset_details": [
        {
          "dataset_name": "Countries",
          "dataset_description": "Used to represent and reason about geographical relations among countries, focusing on low-rank vector spaces for approximate reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 13955854,
          "context_text": "• Countries [194] consists of relations among countries based on public geographical data.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Countries' as a dataset consisting of relations among countries based on public geographical data, which is relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03dfada96b88c741bb26bd4ce7b5ae4232157d37",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB-Img-Few"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB-Img-Few",
          "dataset_description": "Used to provide image embeddings for multi-modal knowledge graph reasoning, focusing on few-shot learning scenarios.",
          "citing_paper_id": "271405981",
          "cited_paper_id": 76663467,
          "context_text": "In addition, the image embeddings of FB-Img-Few and DB-Img-Few are provided from [17] and [55], respectively.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'FB-Img-Few' and 'DB-Img-Few' as sources of image embeddings, which are likely datasets used in the context of multi-modal knowledge graphs.",
          "citing_paper_doi": "10.1109/ICDE60146.2024.00061",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6fcb7f9fb1dab34affe2a3129c9c58536e1c01c7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "53080423",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YAGO3-10"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO3-10",
          "dataset_description": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 53080423,
          "context_text": "According to the scopes of relations, YAGO3-10 [217], YAGO37 [218] and YAGO39k [219] are the subsets of it.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions YAGO3-10, YAGO37, and YAGO39k as subsets of a larger dataset, which are likely knowledge graphs used in multi-modal reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/D18-1222",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "52160797",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WN-9"
      ],
      "dataset_details": [
        {
          "dataset_name": "WN-9",
          "dataset_description": "Used to expand the knowledge graph by adding images to entities, enhancing multimodal reasoning capabilities.",
          "citing_paper_id": "252089825",
          "cited_paper_id": 52160797,
          "context_text": "Similar studies [28] [41] expand the existing KGs WN-9 and FB15K respectively, only adding images for each entity to further explain them.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions expanding existing KGs WN-9 and FB15K by adding images, which aligns with multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.1109/ICDE55515.2023.00015",
          "cited_paper_doi": "10.18653/v1/D18-1359",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79801d46d6c495f1db8deb4eda461706ae4820d1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bca4a782116e663dfd0119b6176a3c228c651bda",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "8423494",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NELL"
      ],
      "dataset_details": [
        {
          "dataset_name": "NELL",
          "dataset_description": "Used as a knowledge base for multi-modal reasoning, focusing on integrating textual and structured data to enhance knowledge graph construction.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 8423494,
          "context_text": "• NELL [229] is the knowledge base built based on NeverEnding Language Learner, which attempts to learn to read the web over time.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "NELL is mentioned as a knowledge base, but it is not a dataset in the traditional sense. It is a continuously evolving knowledge base built by a machine learning system.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1609/aaai.v24i1.7519",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ddf0f2226cc837750eb1eb57c43d8192ef0fc2b3",
          "citing_paper_year": 2022,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YAGO37"
      ],
      "dataset_details": [
        {
          "dataset_name": "YAGO37",
          "dataset_description": "Used as a subset of a larger knowledge graph to evaluate relation prediction tasks, focusing on the scope of relations within the dataset.",
          "citing_paper_id": "254564635",
          "cited_paper_id": null,
          "context_text": "According to the scopes of relations, YAGO3-10 [217], YAGO37 [218] and YAGO39k [219] are the subsets of it.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions YAGO3-10, YAGO37, and YAGO39k as subsets of a larger dataset, which are likely knowledge graphs used in multi-modal reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "19139252",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FB122"
      ],
      "dataset_details": [
        {
          "dataset_name": "FB122",
          "dataset_description": "Used to evaluate knowledge graph completion methods, addressing issues in FB15k by filtering out inverse relations and improving dataset quality. | Used for knowledge graph completion, focusing on entity linking and relation prediction in a larger subset of DBpedia. | Used to evaluate knowledge graph completion models, offering a larger entity set to challenge the scalability of algorithms. | Used to assess knowledge graph reasoning algorithms, emphasizing a moderate-sized entity set with diverse relations. | Used to benchmark knowledge graph embedding models, focusing on a widely-used subset of Freebase with 15,000 entities. | Used to test knowledge graph reasoning systems, providing a slightly larger entity set than FB15k for more comprehensive evaluation. | Used for knowledge graph completion, focusing on entity linking and relation prediction in a smaller subset of DBpedia. | Used for knowledge graph embedding, focusing on large-scale entity and relation representation learning. | Used to assess large-scale knowledge graph reasoning, focusing on a significantly larger entity set to test system robustness. | Used to evaluate knowledge graph completion methods, focusing on smaller-scale entity sets and relation types. | Used to evaluate multi-modal knowledge graph reasoning, combining textual data from New York Times articles with Freebase entities.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 19139252,
          "context_text": "According to the entity set size, we can derive several subsets from it, including FB13 [213], FB122 [214], FB15k [215], FB20k [210], FB24k [216], FB5M [19], FB15k-237 [217], FB60k-NYT10 [218].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions multiple subsets derived from a larger entity set, all of which are specific datasets used in knowledge graph research.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.1609/aaai.v32i1.11535",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3a5830800f7b471dafee2a1e6e070f45d9b3f7c7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "53082197",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "YOGA11k"
      ],
      "dataset_details": [
        {
          "dataset_name": "YOGA11k",
          "dataset_description": "Used to generate temporal knowledge graph embeddings, focusing on different periods for multi-modal reasoning.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 53082197,
          "context_text": "YOGA11k/YOGA [157], YOGA15k [222], YOGA-3SP [220] and YOGA1830 [116] are generated from it according to different periods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets (YOGA11k, YOGA15k, YOGA-3SP, YOGA1830) that are generated from a common source, likely for multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.18653/v1/D18-1225",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/83d58bc46b7adb92d8750da52313f060b10f201d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "76663467",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MMKG-FB15k-IMG"
      ],
      "dataset_details": [
        {
          "dataset_name": "MMKG-FB15k-IMG",
          "dataset_description": "Used to integrate YAGO knowledge graph with images and text, facilitating comprehensive multi-modal reasoning. | Used to integrate Freebase knowledge graph with images, enhancing multi-modal reasoning capabilities. | Used to integrate DBpedia knowledge graph with numeric literals, supporting multi-modal reasoning with structured data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 76663467,
          "context_text": "• MMKG [224] provides three subsets, including MMKG-FB15k-IMG [224], MMKG-DB15k [224] and Yago15k-IMG-TXT [224], which integrates the speciﬁc KGs with numeric literals and images.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used in the research, which are relevant to multi-modal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-030-21348-0_30",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d593a5830a7e7d84443473c3912b59165056d45a",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "211123242",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Richpedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "Richpedia",
          "dataset_description": "Used as a comprehensive multi-modal knowledge graph, integrating triplets, textual descriptions, and images to support reasoning tasks.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 211123242,
          "context_text": "• Richpedia [225] is the KGs, composed of the triplets, textual descriptions, and images.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Richpedia is identified as a multi-modal knowledge graph, which aligns with the research topic of multi-modal knowledge graph reasoning. It is used as a reusable resource.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1007/978-3-030-41407-8_9",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8a29f76fd7214431c84607d06d6257ff5def6a35",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "212737039",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MKG-Wikipedia"
      ],
      "dataset_details": [
        {
          "dataset_name": "MKG-Wikipedia",
          "dataset_description": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using Wikipedia data. | Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using YAGO data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 212737039,
          "context_text": "• MKG [181], [241] consists of two subsets, i.e., MKG-Wikipedia and MKG-YAGO .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MKG-Wikipedia' and 'MKG-YAGO' as subsets of MKG, which are specific datasets used in multimodal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.14778/3407790.3407828",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/65baa67a7cdb3b4b948d126ac5b41ca9c98b1f3b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "252783084",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MKG-YAGO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MKG-YAGO",
          "dataset_description": "Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using Wikipedia data. | Used to evaluate multimodal knowledge graph completion methods, focusing on entity alignment and relation prediction using YAGO data.",
          "citing_paper_id": "254564635",
          "cited_paper_id": 252783084,
          "context_text": "• MKG [181], [241] consists of two subsets, i.e., MKG-Wikipedia and MKG-YAGO .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MKG-Wikipedia' and 'MKG-YAGO' as subsets of MKG, which are specific datasets used in multimodal knowledge graph reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2212.05767",
          "cited_paper_doi": "10.1145/3503161.3548388",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3a811a0dc9bbdca3c69026da97f6ceb1916f69a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4ba9aab31d0a5e2af0147bd18e3381bdcfd15cd1",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "60254729",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Nation"
      ],
      "dataset_details": [
        {
          "dataset_name": "Nation",
          "dataset_description": "Used to represent relations among nations, focusing on the structure and connections within the dataset. The specific research question involves analyzing the dimensionality of these relations.",
          "citing_paper_id": "257220329",
          "cited_paper_id": 60254729,
          "context_text": "• Nation [212] contains relations among nations [221].",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Nation' as containing relations among nations, which could be a dataset. However, without more specific details or a clear identifier, it is ambiguous.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3417451",
          "cited_paper_doi": "10.21236/ad0759998",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e451cd1f8645589f71848eb97948052e07047748",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cc889a3ac654a1ad2226a4f3f0aca9a8c40a6983",
          "citing_paper_year": 2022,
          "cited_paper_year": 1968
        }
      ]
    },
    {
      "cited_paper_id": "231591445",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Fashion-MMKG"
      ],
      "dataset_details": [
        {
          "dataset_name": "Fashion-MMKG",
          "dataset_description": "Used to train a CLIP-style model for e-commerce image-text retrieval, incorporating cross-modal fashion knowledge to enhance the model's performance.",
          "citing_paper_id": "259370889",
          "cited_paper_id": 231591445,
          "context_text": "The Fashion-MMKG is later incorporated as the prior cross-modal fashion knowledge in training a CLIP-style model to support e-commerce image-text retrieval.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Fashion-MMKG' as a specific dataset used for training a CLIP-style model for e-commerce image-text retrieval. The dataset is clearly identified and its usage is described.",
          "citing_paper_doi": "10.18653/v1/2023.acl-industry.16",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a69e19a94028ae7d949b169929352bd02d370c81",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    }
  ],
  "citation_count_distribution": {
    "2768038": 11,
    "4328400": 9,
    "5458500": 11,
    "6628106": 4,
    "15150247": 6,
    "152282269": 3,
    "199453025": 3,
    "199528533": 7,
    "201103729": 4,
    "216080982": 4,
    "220496043": 1,
    "235436185": 4,
    "236957384": 1,
    "259075619": 2,
    "259129314": 2,
    "259145427": 4,
    "267334765": 1,
    "267547866": 5,
    "272655172": 1,
    "1219941": 1,
    "2840197": 1,
    "5471519": 1,
    "8849206": 2,
    "9909815": 12,
    "13694791": 1,
    "13756489": 6,
    "14941970": 14,
    "18347865": 1,
    "44145776": 10,
    "52143467": 2,
    "52160797": 7,
    "73425227": 2,
    "201070367": 2,
    "201698166": 2,
    "210695009": 2,
    "211010433": 7,
    "211137418": 2,
    "213318844": 1,
    "218563659": 1,
    "226283804": 1,
    "233407536": 1,
    "235412627": 1,
    "235503675": 2,
    "260537889": 1,
    "262380035": 1,
    "1915014": 2,
    "3875633": 1,
    "3879949": 1,
    "3982237": 1,
    "4090850": 2,
    "4502993": 1,
    "4669223": 3,
    "6233645": 1,
    "6911541": 2,
    "8040343": 2,
    "8423494": 3,
    "8492900": 1,
    "10811631": 2,
    "10838787": 1,
    "18367155": 1,
    "18938726": 1,
    "19139252": 2,
    "19187663": 2,
    "20667722": 3,
    "26996000": 1,
    "38485677": 2,
    "54434537": 1,
    "56517517": 1,
    "59516071": 1,
    "60254729": 1,
    "67474824": 1,
    "67855617": 10,
    "67856459": 2,
    "85558018": 1,
    "117107657": 1,
    "118675981": 1,
    "168633605": 2,
    "201317624": 4,
    "202775885": 3,
    "202777324": 2,
    "203605587": 9,
    "203626690": 1,
    "206477883": 1,
    "206710454": 1,
    "207116476": 1,
    "207167677": 7,
    "207228784": 2,
    "208006241": 5,
    "211082667": 3,
    "211096730": 4,
    "214390104": 1,
    "214693100": 1,
    "214803074": 1,
    "218581196": 1,
    "218900866": 3,
    "218971783": 2,
    "219531264": 1,
    "221819250": 1,
    "222124934": 2,
    "222205878": 2,
    "226281660": 4,
    "231925372": 3,
    "232126110": 1,
    "232222958": 2,
    "233295959": 2,
    "233486383": 2,
    "235262529": 2,
    "235266233": 1,
    "235271284": 2,
    "235606453": 1,
    "236772282": 3,
    "237100866": 1,
    "238583240": 1,
    "239011538": 7,
    "239016536": 3,
    "240230810": 2,
    "245124050": 1,
    "245591477": 2,
    "245634466": 1,
    "246210481": 3,
    "246823061": 6,
    "246867470": 2,
    "248435922": 2,
    "248496482": 1,
    "248834330": 2,
    "249210042": 2,
    "249626322": 2,
    "250340389": 1,
    "250629390": 1,
    "252199918": 1,
    "252782076": 2,
    "252905025": 1,
    "252918783": 4,
    "252918814": 1,
    "253862877": 1,
    "254926959": 1,
    "257129854": 1,
    "257631615": 1,
    "257912512": 2,
    "257921533": 1,
    "258987428": 1,
    "259949958": 1,
    "260171695": 1,
    "261030303": 1,
    "263886314": 2,
    "263896114": 1,
    "268042282": 1,
    "269634664": 1,
    "261138": 3,
    "27263492": 2,
    "31606602": 5,
    "49644765": 1,
    "195351633": 1,
    "201058752": 3,
    "208201975": 3,
    "220730157": 1,
    "221376974": 2,
    "225039882": 6,
    "252089825": 4,
    "252783084": 4,
    "259165563": 2,
    "227151278": 1,
    "244521828": 1,
    "264057354": 1,
    "264325404": 1,
    "1033682": 2,
    "1157792": 2,
    "2949428": 3,
    "6071257": 4,
    "145832359": 1,
    "202539519": 3,
    "207775322": 1,
    "231591445": 5,
    "240241582": 1,
    "249440365": 1,
    "250118042": 2,
    "258509157": 4,
    "260887576": 2,
    "266939061": 1,
    "267841518": 1,
    "269684581": 1,
    "8429835": 2,
    "15027084": 5,
    "125343003": 2,
    "135465817": 1,
    "252917745": 2,
    "252968266": 1,
    "257168851": 2,
    "1671874": 5,
    "1926319": 2,
    "3656231": 1,
    "4703853": 1,
    "4852047": 1,
    "258220916": 1,
    "219708313": 1,
    "252780775": 1,
    "253309006": 1,
    "258352810": 1,
    "261562315": 1,
    "108300309": 1,
    "117223439": 1,
    "228861368": 1,
    "233863555": 1,
    "234826966": 1,
    "257118958": 1,
    "264439270": 1,
    "563473": 1,
    "2486369": 1,
    "3016223": 2,
    "3508727": 1,
    "5959482": 6,
    "12983389": 1,
    "15206880": 2,
    "15483870": 1,
    "16273722": 1,
    "17011026": 1,
    "21723549": 1,
    "27494872": 2,
    "38934160": 1,
    "49313245": 3,
    "54206179": 1,
    "54559476": 1,
    "69481030": 1,
    "141406559": 2,
    "159042183": 2,
    "204972196": 1,
    "206594692": 9,
    "213104909": 1,
    "215717250": 1,
    "219615799": 1,
    "236428227": 1,
    "218486837": 2,
    "233219869": 6,
    "3117929": 5,
    "7204540": 1,
    "16153365": 1,
    "16771371": 1,
    "57189444": 1,
    "210839653": 1,
    "219437602": 1,
    "224814368": 1,
    "225115084": 2,
    "231626386": 1,
    "234357689": 1,
    "236477903": 1,
    "245119728": 2,
    "247794106": 1,
    "249017794": 1,
    "258879983": 1,
    "205692": 2,
    "982761": 1,
    "1181640": 5,
    "1336493": 1,
    "1619841": 1,
    "1969092": 2,
    "3544741": 1,
    "3696627": 1,
    "3766110": 1,
    "3806582": 1,
    "3814153": 1,
    "3882054": 3,
    "4557963": 1,
    "4755450": 3,
    "6719686": 1,
    "7431082": 1,
    "7483388": 3,
    "7958862": 1,
    "10717843": 1,
    "12643399": 1,
    "13614891": 1,
    "13846713": 1,
    "19131678": 2,
    "19140125": 1,
    "19441281": 1,
    "25418227": 1,
    "25753806": 2,
    "37591724": 1,
    "51989311": 1,
    "52042083": 1,
    "52110037": 2,
    "53082197": 2,
    "53775725": 1,
    "54444869": 1,
    "56472583": 1,
    "73729352": 1,
    "86631164": 1,
    "88523916": 2,
    "128363050": 1,
    "195833229": 1,
    "196173551": 1,
    "196187271": 1,
    "196204964": 1,
    "201142785": 1,
    "202540096": 2,
    "202541491": 1,
    "202749877": 1,
    "202776155": 1,
    "203158029": 1,
    "203658320": 1,
    "204076452": 1,
    "204576077": 1,
    "204788875": 1,
    "207163173": 3,
    "207852450": 2,
    "207880490": 1,
    "208291464": 2,
    "209318312": 1,
    "210064217": 1,
    "212848342": 1,
    "214063723": 1,
    "214076868": 1,
    "214612503": 1,
    "214802539": 1,
    "218589276": 1,
    "218862816": 1,
    "219295273": 1,
    "220047862": 1,
    "220886470": 1,
    "221090697": 1,
    "221192811": 1,
    "221280346": 1,
    "221819493": 1,
    "221839825": 1,
    "221954184": 1,
    "222133165": 1,
    "222208985": 1,
    "224769806": 1,
    "226203042": 1,
    "227231162": 1,
    "227239576": 2,
    "229180723": 2,
    "229213013": 2,
    "229339845": 2,
    "229377193": 1,
    "229703490": 1,
    "233224480": 1,
    "233303189": 1,
    "233347189": 2,
    "233481294": 3,
    "235306387": 2,
    "235324797": 1,
    "235422273": 2,
    "235614395": 1,
    "235703270": 1,
    "235792443": 1,
    "236279449": 1,
    "236349787": 1,
    "236979995": 1,
    "237454564": 1,
    "237571654": 1,
    "237785556": 1,
    "240417169": 1,
    "243696417": 1,
    "244119616": 1,
    "244119769": 1,
    "244222941": 2,
    "244894903": 1,
    "244946393": 1,
    "245144534": 1,
    "245313643": 1,
    "245395720": 1,
    "245648744": 1,
    "246026190": 1,
    "246063616": 1,
    "246608097": 1,
    "246671483": 1,
    "246863638": 1,
    "247244896": 1,
    "248524814": 6,
    "248545885": 1,
    "248729692": 1,
    "248798765": 1,
    "248986385": 1,
    "249109773": 1,
    "250289207": 1,
    "250511662": 1,
    "250562885": 1,
    "250635180": 1,
    "251086467": 1,
    "251294990": 1,
    "251594533": 1,
    "251708766": 1,
    "251718718": 1,
    "251719495": 1,
    "251779418": 1,
    "252015770": 1,
    "252070677": 1,
    "252090252": 1,
    "252292167": 1,
    "252364875": 1,
    "252564311": 1,
    "252625996": 1,
    "252716986": 1,
    "252819373": 1,
    "252846570": 1,
    "253117165": 1,
    "253157669": 1,
    "253384318": 1,
    "253386566": 1,
    "253448309": 1,
    "253761315": 1,
    "254096435": 1,
    "254205278": 1,
    "256597814": 1,
    "256598084": 1,
    "256610018": 1,
    "257220329": 3,
    "257771370": 1,
    "258219651": 1,
    "258240832": 1,
    "258264587": 2,
    "258281574": 1,
    "258333655": 2,
    "258363396": 1,
    "258423071": 1,
    "258436828": 1,
    "258440205": 1,
    "258440388": 1,
    "258457252": 1,
    "258686648": 1,
    "258714753": 1,
    "258762793": 1,
    "259341618": 1,
    "259630548": 1,
    "259858848": 1,
    "259936842": 1,
    "260611397": 1,
    "261214582": 1,
    "261431541": 1,
    "262690390": 3,
    "263147330": 1,
    "263807455": 1,
    "263830580": 1,
    "264172465": 1,
    "264350156": 1,
    "266149723": 1,
    "266166905": 2,
    "266201568": 1,
    "266231035": 1,
    "266550719": 1,
    "266551134": 1,
    "266681126": 1,
    "266755771": 1,
    "266933295": 1,
    "267180466": 1,
    "267627414": 1,
    "267751000": 1,
    "267751414": 1,
    "268032370": 1,
    "268157585": 1,
    "268717837": 1,
    "268856632": 1,
    "268860898": 1,
    "269100293": 1,
    "269157470": 1,
    "269580299": 1,
    "270711106": 2,
    "270990971": 1,
    "271003412": 1,
    "271961547": 1,
    "272884970": 1,
    "273037266": 1,
    "10328909": 2,
    "52158102": 1,
    "53199920": 3,
    "221397171": 2,
    "237418303": 1,
    "237453242": 1,
    "263895473": 2,
    "577805": 2,
    "3226443": 1,
    "4437414": 1,
    "5958691": 1,
    "7278297": 3,
    "9095914": 1,
    "11223539": 1,
    "13955854": 1,
    "21883095": 1,
    "52056218": 1,
    "53080423": 1,
    "56657926": 1,
    "56915438": 1,
    "59316623": 2,
    "59821525": 1,
    "76663467": 9,
    "108296188": 1,
    "207847719": 3,
    "211003696": 1,
    "211123242": 1,
    "212737039": 3,
    "214198198": 1,
    "219573823": 1,
    "221082536": 1,
    "221083274": 1,
    "222177069": 1,
    "232135385": 1,
    "232269660": 1,
    "233324265": 1,
    "235845598": 2,
    "245502155": 1,
    "246828738": 1,
    "247451243": 1,
    "250630286": 1,
    "250631397": 1,
    "252683295": 1,
    "252904804": 1,
    "252905085": 1,
    "252992488": 1,
    "738850": 1,
    "3753452": 2,
    "9665943": 1,
    "14521054": 1,
    "14843884": 3,
    "29150617": 1,
    "46935302": 1,
    "57375753": 1,
    "67856593": 1,
    "208138178": 1,
    "2237901": 1,
    "7197134": 2,
    "8701238": 1,
    "26517743": 1,
    "40114756": 1,
    "51880810": 1,
    "57573854": 1,
    "102350405": 1,
    "204960442": 1,
    "206596979": 1,
    "208202400": 1,
    "209376177": 1,
    "214775220": 1,
    "216080778": 1,
    "231741093": 1,
    "233387838": 1,
    "235694438": 1,
    "246240170": 1,
    "1957433": 4,
    "2332513": 1,
    "5276660": 1,
    "5575601": 1,
    "11117517": 1,
    "16414666": 5,
    "40027675": 1,
    "54458106": 1,
    "108296442": 1,
    "195847902": 1,
    "209532101": 1,
    "215754564": 1,
    "2622646": 1,
    "26419490": 1,
    "127986044": 1,
    "215737187": 1,
    "220919723": 1,
    "224722163": 1,
    "225883893": 1,
    "244772950": 1,
    "245218982": 1,
    "247450969": 1,
    "249284566": 1,
    "251371732": 1,
    "251765079": 1,
    "253479599": 1,
    "254877499": 1,
    "258615731": 1,
    "258822888": 1,
    "259204112": 1,
    "259360665": 1,
    "262138540": 1,
    "263310448": 1,
    "263671998": 1,
    "265067168": 1,
    "265455405": 1,
    "265610070": 1,
    "267335111": 1,
    "267412232": 1,
    "267626929": 1,
    "268531479": 1,
    "269005493": 1,
    "269293311": 1,
    "269740933": 1,
    "270357492": 1,
    "271161780": 1,
    "271218596": 1,
    "273662191": 1,
    "274234014": 1,
    "276317958": 1,
    "276742400": 1,
    "278166534": 1,
    "220599074": 1,
    "224888856": 1,
    "2209131": 1,
    "3144218": 5,
    "3180429": 3,
    "3994012": 1,
    "7748515": 2,
    "10347107": 2,
    "21277943": 1,
    "52304560": 2,
    "3292002": 6,
    "3593775": 1,
    "9697423": 1,
    "12255087": 1,
    "26071662": 1,
    "52895589": 1,
    "95861012": 1,
    "133608068": 1,
    "169032532": 2,
    "201646309": 4,
    "212748233": 1,
    "221364718": 1,
    "231963511": 1,
    "231981018": 1,
    "235166820": 1,
    "247115730": 1,
    "247677346": 1,
    "254366618": 1,
    "255083414": 1,
    "256549502": 1,
    "257039063": 1,
    "257122091": 1,
    "257219404": 2,
    "257326550": 1,
    "257378479": 2,
    "257532815": 1,
    "258519932": 1,
    "258573462": 1,
    "259692010": 1,
    "260164663": 1,
    "260722930": 1,
    "263777406": 1,
    "264172683": 1,
    "265189794": 1,
    "265871676": 1,
    "266412890": 1,
    "267938234": 1,
    "271334161": 1,
    "271916060": 1,
    "274859421": 1,
    "1082740": 1,
    "1560943": 1,
    "4193919": 1,
    "5583509": 1,
    "8770925": 1,
    "9492646": 1,
    "9672033": 1,
    "14061443": 1,
    "15512280": 1,
    "27837890": 1,
    "122166830": 1,
    "212628625": 1,
    "213176935": 1,
    "219615944": 1,
    "222278217": 1,
    "233024902": 1,
    "255105361": 1,
    "2141740": 1,
    "3365209": 1,
    "3441497": 1,
    "4714433": 2,
    "6941275": 1,
    "12856358": 1,
    "14113767": 3,
    "49420146": 1,
    "53036732": 1,
    "201070217": 1,
    "266028051": 2,
    "14124313": 10,
    "52967399": 13,
    "249625869": 1,
    "253018586": 1,
    "264492337": 1,
    "265150374": 2,
    "267898006": 2,
    "270710718": 1,
    "275054439": 1,
    "9316331": 1,
    "52284222": 1,
    "237363704": 1,
    "246426909": 1,
    "254220898": 1,
    "257219550": 1,
    "259203574": 1,
    "261064713": 1,
    "266359151": 1,
    "267782436": 1,
    "268032947": 1,
    "4492210": 3,
    "85528899": 1,
    "158046772": 3,
    "206593880": 2,
    "207718082": 1,
    "219182395": 1,
    "235306349": 1,
    "238158425": 1,
    "248367577": 1,
    "268718050": 1,
    "272331125": 1,
    "2272015": 1,
    "14494942": 1,
    "14728290": 1,
    "52815006": 1,
    "52841398": 1,
    "54465873": 1,
    "85528598": 1,
    "173991173": 2,
    "189898023": 1,
    "202558968": 1,
    "209515395": 1,
    "211572655": 1,
    "218487288": 1,
    "218900797": 1,
    "220280200": 1,
    "229923949": 1,
    "232335877": 1,
    "235349192": 1,
    "235489690": 1,
    "235592814": 1,
    "758237": 1,
    "1399322": 1,
    "2924682": 1,
    "10363459": 1,
    "22177955": 1,
    "195347831": 1,
    "195908774": 1,
    "206594383": 1,
    "207061473": 1,
    "207178704": 1,
    "208248243": 1,
    "231572861": 1,
    "261076072": 1,
    "263605944": 1,
    "263610099": 1,
    "1430801": 1,
    "2210455": 1,
    "17682909": 1,
    "54460890": 2,
    "202719040": 1,
    "202734445": 1,
    "226096901": 3,
    "235097195": 1,
    "235692795": 3,
    "33389032": 1,
    "35467721": 1,
    "40176582": 1,
    "59291975": 1,
    "102352093": 3,
    "207997778": 1,
    "232061871": 1,
    "263873151": 1,
    "195657814": 1,
    "203626972": 1,
    "204960716": 3,
    "219182397": 1,
    "230435739": 1,
    "231573431": 1,
    "235417196": 1,
    "236635565": 1,
    "237416585": 1,
    "239016062": 1,
    "240420063": 1,
    "247618722": 1,
    "256503897": 1,
    "256868484": 1,
    "259095695": 1,
    "260435365": 1,
    "261016305": 1,
    "263227026": 1,
    "270764307": 1,
    "201666793": 1,
    "202573071": 1,
    "216641856": 1,
    "256436290": 1,
    "256827430": 1,
    "259309148": 1,
    "291713": 1,
    "11319376": 1,
    "20480879": 1,
    "21688777": 1,
    "21698461": 1,
    "212414835": 1,
    "1998416": 1,
    "10073982": 1,
    "31319559": 1,
    "52919654": 1,
    "54458229": 1,
    "623013": 1,
    "2239473": 1,
    "11202498": 4,
    "19135805": 2,
    "238419331": 1,
    "248525030": 1,
    "252280329": 1,
    "6706414": 1,
    "9059612": 1,
    "224281034": 3,
    "258298672": 2,
    "3104920": 2,
    "8355505": 1,
    "13145195": 1,
    "13618178": 1,
    "19153102": 1,
    "202888986": 1,
    "214743601": 1,
    "231879586": 1,
    "233444011": 1,
    "235306301": 1,
    "236428622": 1,
    "247011309": 1,
    "715463": 2,
    "2315434": 1,
    "2424223": 2,
    "2493017": 1,
    "6425394": 1,
    "6483070": 1,
    "7062707": 3,
    "7352553": 1,
    "7732372": 1,
    "7771402": 1,
    "8505367": 1,
    "10282227": 1,
    "13296639": 1,
    "13755946": 1,
    "14040310": 1,
    "14217450": 1,
    "14775471": 2,
    "15397918": 1,
    "16619709": 2,
    "38582742": 1,
    "49867191": 1,
    "91184120": 2,
    "201066287": 1,
    "201871273": 1,
    "202565460": 1,
    "202782699": 1,
    "204402762": 1,
    "206751218": 1,
    "208547698": 1,
    "211532586": 1,
    "213755659": 1,
    "215746363": 1,
    "215754208": 1,
    "218551030": 2,
    "220265934": 3,
    "221703022": 1,
    "222291117": 1,
    "224291855": 2,
    "229924402": 3,
    "233296711": 1,
    "233375020": 1,
    "233444273": 1,
    "234685749": 1,
    "235270502": 1,
    "236428934": 1,
    "239011558": 1,
    "244117525": 2,
    "244920947": 1,
    "246634906": 1,
    "246938066": 1,
    "248779998": 2,
    "254054622": 1,
    "3608725": 1,
    "53235839": 1,
    "201881176": 1,
    "230433941": 1,
    "236273668": 1,
    "265051720": 1,
    "202121966": 2,
    "221193809": 7,
    "221995513": 5,
    "245904709": 2,
    "251117920": 1,
    "251518434": 1,
    "251684399": 1,
    "255340818": 1,
    "260334664": 2,
    "262464639": 1,
    "652286": 1,
    "7164502": 1,
    "18268744": 1,
    "245634781": 2,
    "262466164": 1,
    "12161567": 1,
    "219530451": 1,
    "221655683": 1,
    "221970271": 1,
    "225040610": 1,
    "260124505": 1,
    "2887257": 1,
    "4929980": 1,
    "30164212": 1,
    "174802832": 1,
    "196205749": 1,
    "198354047": 1,
    "202583325": 2,
    "202770936": 1,
    "237420821": 1,
    "252819516": 1,
    "252907242": 1,
    "235358423": 1,
    "261329076": 1,
    "263830084": 1,
    "264492372": 1,
    "2407601": 2,
    "53082628": 1,
    "129946212": 1,
    "196172975": 1,
    "202712648": 1,
    "214714259": 1,
    "222177127": 1,
    "232404011": 1,
    "248227575": 1,
    "352650": 1,
    "198953378": 2,
    "226246289": 2,
    "233296292": 1,
    "236459932": 1,
    "252351757": 1,
    "259949770": 1,
    "259949942": 1,
    "264492774": 1,
    "265213194": 1,
    "268692718": 1,
    "269457193": 1,
    "273164062": 1,
    "276117175": 1,
    "271104595": 1,
    "221191635": 1,
    "237332868": 1,
    "259165073": 1,
    "261158362": 1,
    "264584799": 1,
    "4336741": 1,
    "16407324": 2,
    "204838007": 1,
    "237353134": 1,
    "253790338": 1,
    "5625629": 2,
    "20401422": 1,
    "49298988": 1,
    "51972201": 1,
    "189761996": 1,
    "201701022": 1,
    "207168823": 1,
    "207178741": 1,
    "211043589": 1,
    "220730030": 1,
    "235829175": 1,
    "245502839": 1,
    "246015673": 1,
    "251518175": 1,
    "252070586": 3,
    "252519575": 1,
    "258714901": 1,
    "3401524": 1,
    "84843937": 1,
    "255393926": 1,
    "12544231": 1,
    "14136367": 1,
    "15810061": 1,
    "211010432": 1,
    "224271811": 1,
    "244527692": 1,
    "256631072": 1,
    "10319744": 3,
    "14083350": 1,
    "51935625": 1,
    "216078090": 1,
    "220425719": 1,
    "231592822": 1,
    "54459095": 1,
    "214714330": 1,
    "216056360": 1,
    "116132630": 1,
    "211204736": 1,
    "213801097": 1,
    "243865362": 1,
    "249062918": 1,
    "15912887": 2,
    "57246310": 2,
    "69930495": 1,
    "86510052": 1,
    "8592977": 1,
    "11198605": 1,
    "62841652": 1,
    "86505256": 1,
    "174797737": 2,
    "222278475": 2,
    "5378837": 1,
    "11091552": 1,
    "19370455": 1,
    "44113572": 1,
    "215828509": 1,
    "226601508": 1,
    "232417873": 1,
    "238244608": 1,
    "245788482": 1,
    "1055111": 2,
    "2724321": 1,
    "3120635": 1,
    "8454173": 1,
    "8517067": 3,
    "10585115": 2,
    "11080756": 1,
    "14068874": 1,
    "108294843": 1,
    "206592766": 2,
    "215814392": 1,
    "222278716": 1,
    "231632752": 1,
    "233689007": 1,
    "237431500": 1,
    "252782756": 1,
    "67700681": 1,
    "85566873": 1,
    "203704895": 1,
    "213399202": 1,
    "216535344": 1,
    "219071361": 1,
    "219330527": 1,
    "220543075": 1,
    "225287688": 1,
    "228971082": 1,
    "232023070": 1,
    "179895": 1,
    "7079167": 1,
    "44075854": 1,
    "195779694": 1,
    "1900911": 1,
    "267802767": 1,
    "144167": 1,
    "4951598": 1,
    "4973991": 1,
    "7284112": 1,
    "8350875": 1,
    "14928728": 1,
    "36593804": 1,
    "46990556": 1,
    "52164698": 1,
    "53597419": 1,
    "57880229": 1,
    "60600755": 1,
    "159041743": 1,
    "195347125": 1,
    "195441316": 1,
    "195697699": 1,
    "201698324": 1,
    "202786261": 1,
    "203598995": 1,
    "204837691": 1,
    "207756951": 1,
    "209415079": 1,
    "210873091": 1,
    "245934348": 1,
    "216144439": 2,
    "2633248": 1,
    "3389583": 1,
    "3919301": 1,
    "9815689": 1,
    "13838003": 1,
    "14298338": 1,
    "29944424": 1,
    "49901898": 1,
    "52183483": 1,
    "67748132": 1,
    "69990369": 1,
    "70038990": 1,
    "146121289": 1,
    "198952485": 1,
    "203056539": 1,
    "204766590": 1,
    "211252408": 1,
    "211572518": 1,
    "214802049": 1,
    "215746373": 1,
    "216553528": 1,
    "216553689": 1,
    "218487793": 1,
    "218528456": 1,
    "218599990": 1,
    "219471191": 1,
    "220541874": 1,
    "221355139": 1,
    "227191214": 1,
    "235689895": 1,
    "706860": 1,
    "1023605": 2,
    "9505704": 1,
    "16447573": 1,
    "206592218": 1,
    "218470438": 1,
    "202785103": 1,
    "207758781": 1,
    "209376561": 1,
    "218889832": 1,
    "218923747": 1,
    "393948": 1,
    "588863": 1,
    "3582486": 1,
    "7547770": 1,
    "8722811": 1,
    "10442573": 1,
    "10910955": 1,
    "21715202": 1,
    "73728465": 1,
    "103802785": 1,
    "28913990": 1,
    "49299019": 1,
    "208176414": 1,
    "225076220": 1,
    "234950071": 2,
    "248108992": 1,
    "254564635": 1,
    "257697222": 1,
    "2127100": 2,
    "67413369": 2,
    "207930212": 2,
    "212827903": 1,
    "240070688": 1,
    "249538526": 1,
    "252518772": 1,
    "254221022": 1,
    "258333674": 1,
    "258333851": 1,
    "926364": 1,
    "6095318": 1,
    "6334682": 1,
    "51876975": 2,
    "202889175": 1,
    "219573512": 1,
    "232417264": 1,
    "237386166": 2,
    "246823486": 1,
    "250627337": 1,
    "252668458": 1,
    "13716346": 1,
    "18553623": 1,
    "28641153": 1,
    "36279912": 1,
    "57755729": 1,
    "202144421": 1,
    "214466795": 1,
    "220128101": 1,
    "221385483": 1,
    "226415469": 1,
    "229653947": 1,
    "235125956": 1,
    "238687946": 1,
    "247362647": 1,
    "248505867": 1,
    "252450639": 1,
    "252839708": 1,
    "253251385": 1,
    "253354759": 1,
    "253555095": 1,
    "253901615": 1,
    "254034416": 1,
    "254154806": 1,
    "256780457": 1,
    "258007950": 1,
    "207178809": 1,
    "231846408": 1,
    "250340301": 1,
    "2495502": 1,
    "7040223": 1,
    "7409058": 1,
    "12476041": 1,
    "13919896": 1,
    "18117752": 1,
    "247857460": 1,
    "251007195": 1,
    "265094990": 1,
    "199466173": 1,
    "210971227": 1,
    "218571035": 1,
    "189900": 1,
    "1012652": 1,
    "6392154": 1,
    "6655663": 1,
    "7847519": 1,
    "12726540": 1,
    "15152621": 1,
    "30698343": 1,
    "52901171": 1,
    "59291937": 1,
    "73489935": 1,
    "153313270": 1,
    "195892710": 1,
    "207190913": 1,
    "207198269": 1,
    "215827489": 1,
    "218870205": 1,
    "230770066": 1,
    "237494860": 1,
    "1780254": 1,
    "4379400": 1,
    "4593810": 1,
    "119284150": 1,
    "231802355": 1,
    "235593404": 1,
    "237291550": 1,
    "246411402": 1,
    "248512473": 1,
    "250264976": 1,
    "251710138": 1,
    "251719655": 1,
    "661123": 1,
    "1128753": 1,
    "15091093": 1,
    "70289226": 1,
    "102352623": 1,
    "203163196": 1,
    "232335647": 1,
    "232380138": 1,
    "234951345": 1,
    "235829068": 1,
    "236361803": 1,
    "249191733": 1,
    "252782215": 1,
    "253801768": 1,
    "256868818": 1,
    "257729173": 1,
    "258756463": 1,
    "258762390": 1,
    "258967414": 1,
    "266469822": 1,
    "268063320": 1,
    "268357641": 1,
    "1629541": 1,
    "1644335": 1,
    "3225424": 1,
    "3333648": 1,
    "10137425": 1,
    "12350611": 1,
    "17265929": 1,
    "17719760": 1,
    "33753227": 1,
    "44131945": 1,
    "53845347": 1,
    "53957733": 2,
    "56895382": 1,
    "196202217": 1,
    "206594738": 1,
    "207169186": 1,
    "209097551": 1,
    "221150486": 1,
    "225419066": 1,
    "235421836": 1,
    "236293464": 1,
    "237513894": 1,
    "239771829": 1,
    "52171640": 1,
    "201124533": 1,
    "225040315": 1,
    "232417534": 1,
    "248965170": 1,
    "257767382": 1,
    "258733774": 1,
    "7668308": 1,
    "10832728": 1,
    "23672393": 1,
    "30459525": 1,
    "218806820": 1,
    "221492015": 1,
    "222278304": 1,
    "222278652": 1,
    "235792531": 1,
    "246926460": 1,
    "950292": 2,
    "3864050": 1,
    "6659365": 1,
    "52115700": 1,
    "141321709": 1,
    "227238996": 1,
    "235235583": 1,
    "239054406": 1,
    "249192060": 1,
    "254853727": 1,
    "15928602": 1,
    "195440283": 1,
    "214728271": 1,
    "219964813": 1,
    "233004700": 1,
    "236234758": 1,
    "1103216": 1,
    "3896491": 1,
    "5083989": 1,
    "5205529": 1,
    "7900381": 1,
    "8281592": 1,
    "51608183": 1,
    "53172956": 1,
    "54447939": 1,
    "195441339": 1,
    "204837049": 1,
    "212725353": 1,
    "216080787": 1,
    "226202653": 1,
    "232153047": 1,
    "238419264": 1,
    "254097121": 1,
    "254097189": 1,
    "261514205": 1,
    "948039": 1,
    "11336213": 1,
    "50766964": 1,
    "53037206": 1,
    "54527549": 1,
    "59379420": 1,
    "205228801": 1,
    "220977071": 1,
    "235324796": 1,
    "235376996": 1,
    "237441141": 1,
    "247595191": 1,
    "253098669": 1,
    "2158023": 1,
    "2858079": 1,
    "3632923": 1,
    "8310135": 1,
    "12740621": 1,
    "15795805": 1,
    "16119010": 1,
    "102485964": 1,
    "206618067": 1,
    "208176418": 1,
    "215415914": 1,
    "340063": 1,
    "57814228": 1,
    "216562330": 1,
    "222310337": 1,
    "222341612": 1,
    "235248430": 1,
    "247084444": 1,
    "216009": 1,
    "1017389": 1,
    "3603249": 1,
    "6839244": 1,
    "102483628": 1,
    "145050804": 1,
    "233296845": 1,
    "239011786": 1,
    "236567496": 1,
    "259165040": 1,
    "259837088": 1,
    "261065787": 1,
    "261101015": 1,
    "265656168": 1,
    "309759": 1
  },
  "merged_dataset_groups": [
    {
      "display_name": "Freebase",
      "normalized_name": "freebase",
      "name_variants": [
        "FreeBASE",
        "FreeBase",
        "Freebase"
      ],
      "mention_count": 15,
      "cited_papers_count": 12,
      "topic_summary": "Freebase is a collaboratively created graph database used to structure human knowledge, integrating data from diverse sources like Wikipedia and NNDB. It serves as a general and domain-specific knowledge graph, enhancing semantic understanding and reasoning capabilities. Freebase is utilized to improve downstream tasks such as semantic search, question answering, and entity recognition by providing comprehensive structured data. It also supports concept-based search and recommendation systems, and is used to enhance entity representations through multi-modal data."
    },
    {
      "display_name": "MSCOCO",
      "normalized_name": "mscoco",
      "name_variants": [
        "MS-COCO",
        "MSCOCO"
      ],
      "mention_count": 9,
      "cited_papers_count": 7,
      "topic_summary": "The MSCOCO dataset is extensively used for evaluating and enhancing multi-modal reasoning and cross-modal tasks, such as image captioning, image tagging, and cross-modal hashing retrieval. It is leveraged to train and evaluate models on diverse labeled images and annotations, improving the accuracy and context of generated captions and enhancing visual object extraction and recognition. The dataset supports the development of methods for fine-grained multimedia knowledge extraction, object detection, and annotation, often using metrics like Mean Average Precision (MAP) and R@K for performance assessment."
    },
    {
      "display_name": "ImageNet",
      "normalized_name": "imagenet",
      "name_variants": [
        "ImageNet"
      ],
      "mention_count": 7,
      "cited_papers_count": 6,
      "topic_summary": "ImageNet is primarily used for pre-training deep learning models such as VGG16, ResNet-50, and ResNet-152 to enhance feature extraction capabilities in image encoding. It is utilized in multi-modal learning setups to improve visual content recognition and reasoning. The dataset also supports zero-shot learning experiments by leveraging attributes and class labels, and it aids in constructing a semantic tree aligned with WordNet synonyms, using high-resolution images. Additionally, ImageNet is used to pretrain models for extracting 2D visual features from video frames, enhancing object and scene recognition."
    },
    {
      "display_name": "IMGpedia",
      "normalized_name": "imgpedia",
      "name_variants": [
        "IMGpedia"
      ],
      "mention_count": 7,
      "cited_papers_count": 3,
      "topic_summary": "IMGpedia is used to enhance multi-modal knowledge graph reasoning by integrating content-based analysis of Wikimedia images with structured data. It grounds images into DBpedia entities, creating a linked dataset that supports reasoning across various data types and modalities. The dataset facilitates visuo-semantic queries and content-based analysis, enabling researchers to explore image-to-image and image-to-text relationships in large-scale, comprehensive multi-modal knowledge graphs."
    },
    {
      "display_name": "WN9-IMG",
      "normalized_name": "wn9img",
      "name_variants": [
        "WN9-IMG"
      ],
      "mention_count": 7,
      "cited_papers_count": 5,
      "topic_summary": "The WN9-IMG dataset is used to evaluate and train multi-modal knowledge graph representation models, focusing on integrating visual and textual information. It employs image-embodied knowledge representation and translation-based approaches, often using the VGG19 model for visual embeddings. Research primarily assesses model performance on metrics like raw Hits@10, emphasizing multi-modal reasoning tasks involving image and word pairs."
    },
    {
      "display_name": "Visual Genome",
      "normalized_name": "visualgenome",
      "name_variants": [
        "Visual Genome"
      ],
      "mention_count": 6,
      "cited_papers_count": 4,
      "topic_summary": "The Visual Genome dataset is extensively used in research to connect language and vision through dense image annotations, supporting complex multi-modal reasoning tasks. It is utilized for training and evaluating models in image-text alignment, visual relationship detection, and scene graph generation. The dataset provides rich caption annotations, diverse visual content, and hasPart/isA triples, enhancing knowledge graphs with part-whole and hierarchical relationships. It also supports zero-shot phrase grounding and pre-training of object detection models, facilitating advanced multi-modal reasoning capabilities."
    },
    {
      "display_name": "ConceptNet",
      "normalized_name": "conceptnet",
      "name_variants": [
        "ConceptNet"
      ],
      "mention_count": 6,
      "cited_papers_count": 5,
      "topic_summary": "ConceptNet is used to integrate structured and common-sense knowledge into multi-modal reasoning systems, enhancing the understanding of complex relationships and object interactions. It is leveraged to construct and enrich knowledge graphs by combining entities from various sources, such as document images and QA pairs, and to improve factual visual question answering. The dataset provides a rich, multilingual source of structured data, enabling the retrieval of subgraphs and the enhancement of scene graphs generated by models like Faster R-CNN, thereby improving multi-modal reasoning capabilities."
    },
    {
      "display_name": "Wikipedia",
      "normalized_name": "wikipedia",
      "name_variants": [
        "Wikipedia"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The Wikipedia dataset is used for cross-modal retrieval experiments, generating imagetext pairs to test models like GCR in handling diverse multimedia content, focusing on correlation and abstraction. It supports event classification and relation extraction for constructing multi-modal event knowledge graphs, using both textual and visual data. Additionally, it aids in entity representation, aspect labeling, and the construction of structured knowledge graphs, integrating multi-modal information from web sources."
    },
    {
      "display_name": "DBpedia",
      "normalized_name": "dbpedia",
      "name_variants": [
        "DBpedia"
      ],
      "mention_count": 5,
      "cited_papers_count": 4,
      "topic_summary": "DBpedia is used to extract raw triplet facts and provide structured information from Wikipedia, serving as a foundational knowledge graph. It enhances multi-modal knowledge graph reasoning by integrating textual descriptions and structured content, facilitating the construction and reasoning over complex, multi-modal data."
    },
    {
      "display_name": "WordNet",
      "normalized_name": "wordnet",
      "name_variants": [
        "WordNet"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "WordNet is used as a lexical database for English, providing a structured vocabulary for semantic relationships. It serves to build undirected graphs for commonsense knowledge, incorporating entity and relation nodes for images and queries. Additionally, it functions as a large lexical knowledge base for constructing and evaluating knowledge graphs, particularly for link prediction tasks, focusing on relational patterns and entity types."
    },
    {
      "display_name": "DB15K",
      "normalized_name": "db15k",
      "name_variants": [
        "DB15K"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "DB15K is used for multi-modal knowledge graph reasoning, enhancing representation learning by integrating textual and visual information. It serves as a subset of DBpedia and is utilized for entity alignment tasks, particularly to align entities between FB15K and DB15K or YAGO15K. The dataset is split into training and testing sets in various proportions (2:8, 5:5, 8:2) to evaluate model performance in cross-modal entity alignment and reasoning."
    },
    {
      "display_name": "FB15K",
      "normalized_name": "fb15k",
      "name_variants": [
        "FB15K",
        "FB15k"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "FB15K is primarily used for evaluating and training models on relational reasoning tasks, particularly link prediction in knowledge graphs. It is extended by ImageGraph to incorporate images and additional relation types, enhancing its utility for multi-modal reasoning tasks such as visual-relational queries and entity prediction. The dataset supports research in diverse areas including healthcare, where it aids in disease diagnosis and treatment recommendations, and in pandemic studies, where it facilitates information extraction and synthesis from COVID-19 research articles."
    },
    {
      "display_name": "FB15k-237",
      "normalized_name": "fb15k237",
      "name_variants": [
        "FB15k-237"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "FB15k-237 is primarily used to evaluate and validate methods for multi-modal knowledge graph reasoning, focusing on tasks such as entity and relation prediction, link prediction, and cross-lingual knowledge graph alignment. The dataset is utilized to demonstrate state-of-the-art performance and improved convergence speed, often compared against other models like CNN-based ConvE and GNN-based WGCN. It is also used to integrate textual information for entities, enhancing multi-modal data integration in knowledge graph reasoning."
    },
    {
      "display_name": "GQA",
      "normalized_name": "gqa",
      "name_variants": [
        "GQA"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The GQA dataset is primarily used for developing and evaluating models in visual question answering (VQA), with a focus on real-world visual reasoning and compositional question answering. It addresses shortcomings in previous datasets by incorporating complex interactions between images and questions, integrating factual knowledge, and utilizing knowledge graphs to enhance reasoning capabilities. The dataset is employed to test models on fine-grained reasoning tasks, open-ended questions, and multi-step reasoning over structured data and images."
    },
    {
      "display_name": "Fakeddit",
      "normalized_name": "fakeddit",
      "name_variants": [
        "Fakeddit"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Fakeddit dataset is used for evaluating fine-grained fake news detection, particularly focusing on multimodal content and social context. It is also utilized to analyze millions of fauxtography posts from Reddit, examining visual and textual elements to understand the spread of manipulated images. This dataset enables researchers to study the nuances of fake news dissemination in a controlled environment, leveraging both visual and textual data."
    },
    {
      "display_name": "MSCOCO captions",
      "normalized_name": "mscococaptions",
      "name_variants": [
        "MSCOCO captions"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The MSCOCO captions dataset is primarily used for fine-tuning models on image captioning and visual question answering tasks. It provides a large set of images paired with descriptive captions, enhancing the model's ability to generate accurate captions and answer questions about images. The dataset is also used to develop and evaluate models that integrate visual and textual information, focusing on multi-modal reasoning. Its diverse image-caption pairs and balanced dataset characteristics support the enhancement of multi-modal knowledge graph reasoning capabilities."
    },
    {
      "display_name": "kgbench",
      "normalized_name": "kgbench",
      "name_variants": [
        "kgbench"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The kgbench dataset is used to evaluate relational and multimodal machine learning models by providing a collection of knowledge graph datasets for benchmarking. It enables researchers to assess model performance across various tasks, ensuring robust and comprehensive evaluation. The dataset's diverse and multimodal nature supports the development and testing of advanced machine learning algorithms designed to handle complex relational data."
    },
    {
      "display_name": "Recipe1M+",
      "normalized_name": "recipe1m",
      "name_variants": [
        "Recipe1M+"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Recipe1M+ dataset is used to learn cross-modal embeddings for cooking recipes and food images, integrating textual and visual data. It focuses on enhancing recipe recommendation systems by leveraging large-scale structured data to improve the integration of textual and visual information. This dataset enables researchers to develop models that can effectively recommend recipes based on both ingredient lists and food images."
    },
    {
      "display_name": "FB15k-237-IMG",
      "normalized_name": "fb15k237img",
      "name_variants": [
        "FB15K-237-IMG",
        "FB15k-237-IMG"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The FB15k-237-IMG dataset is used to integrate visual information into multi-modal knowledge graph representation learning, extending entities in FB15k-237 and WN18 with images. It enhances the visual aspect of entity relationships and tests models' effectiveness in multi-modal reasoning, focusing on visual context integration and representation learning. The dataset is crucial for evaluating models' performance in handling relational reasoning with visual data and extending the scope of triplets in multi-modal knowledge graph completion."
    },
    {
      "display_name": "MovieQA",
      "normalized_name": "movieqa",
      "name_variants": [
        "MovieQA"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The MovieQA dataset is used to explore multi-modal reasoning by integrating video and text data for question-answering tasks, specifically focusing on understanding movie stories. This involves methodologies that combine visual and textual information to enhance comprehension and answer complex questions about movie plots. The dataset's multi-modal nature enables researchers to develop and test models that can effectively process and reason over both video and text data."
    },
    {
      "display_name": "Kinetics-TPS",
      "normalized_name": "kineticstps",
      "name_variants": [
        "Kinetics-TPS"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Kinetics-TPS dataset is used to enhance multi-modal knowledge graph reasoning by providing over 15 million part-level annotations for detailed human action understanding. It is employed to build and benchmark visual knowledge graphs, focusing on action parsing and compositional learning of body part movements. This dataset specifically addresses the research question of action recognition through multi-modal reasoning, enabling detailed analysis and enhancement of human action understanding in videos."
    },
    {
      "display_name": "hasPart KB",
      "normalized_name": "haspartkb",
      "name_variants": [
        "hasPart KB"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The 'hasPart KB' dataset is primarily used to provide part-whole relationships and commonsense triples, enhancing knowledge graphs for Visual Question Answering (VQA). It contributes to the construction of knowledge graphs by supplying hasPart and isA triples, integrating categorical and spatial information. This enriches the system's ability to understand compositional structures, everyday concepts, and relationships in images, thereby improving VQA performance."
    },
    {
      "display_name": "OK-VQA",
      "normalized_name": "okvqa",
      "name_variants": [
        "OK-VQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The OK-VQA dataset is primarily used to evaluate and train models that integrate visual and textual information to answer complex questions about images. It focuses on the need for external knowledge and the integration of structured knowledge, such as knowledge graphs, to enhance the accuracy and depth of responses. The dataset is used to test visual reasoning capabilities, compositional and logical reasoning, and the ability to understand functional aspects of visual elements. It benchmarks models that require external knowledge to answer visual questions, emphasizing multi-modal understanding and reasoning."
    },
    {
      "display_name": "Pascal-VOC",
      "normalized_name": "pascalvoc",
      "name_variants": [
        "Pascal-VOC"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Pascal-VOC dataset is primarily used for pre-training object detectors in models like QRG, focusing on tasks such as object detection and proposal generation. It enables researchers to ground phrases by accurately identifying and localizing objects within images, enhancing the performance of multi-modal systems in understanding and interacting with visual content."
    },
    {
      "display_name": "MKG-W",
      "normalized_name": "mkgw",
      "name_variants": [
        "MKG-W"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MKG-W dataset is primarily used to evaluate and compare the performance of multi-modal knowledge graph reasoning models. It supports research in linking entities across different modalities, including textual and visual information, and across languages. The dataset facilitates ablation studies to assess the impact of various model configurations and components on performance metrics such as MRR and Hit@1. It is also used for knowledge graph completion tasks, focusing on predicting missing links using multi-modal data."
    },
    {
      "display_name": "UUKG",
      "normalized_name": "uukg",
      "name_variants": [
        "UUKG"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The UUKG dataset is used to organize urban entities into a complex graph for spatiotemporal prediction in smart cities. It integrates diverse urban data sources, enabling researchers to analyze and predict urban dynamics by leveraging the structured representation of urban entities and their relationships. This approach supports the development of more efficient and responsive smart city systems."
    },
    {
      "display_name": "Scene Graph",
      "normalized_name": "scenegraph",
      "name_variants": [
        "Scene Graph"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Scene Graph dataset is used to enhance image retrieval and representation by focusing on action and spatial relations within images. It refocuses visually-relevant relationships, improving the depiction of spatial and relational information. This dataset enables researchers to retrieve images more accurately based on complex scene graph queries, thereby advancing the field of visual relationship understanding."
    },
    {
      "display_name": "MMKG",
      "normalized_name": "mmkg",
      "name_variants": [
        "MMKG"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The MMKG dataset is used to integrate visual and numerical information into multi-modal knowledge graphs, enhancing entity representation. It provides images and numerical features for entities, which are utilized in multi-modal reasoning experiments to incorporate visual data into knowledge graph relations. This integration enriches the representation and reasoning capabilities of the knowledge graphs."
    },
    {
      "display_name": "FB15K-YAGO15K",
      "normalized_name": "fb15kyago15k",
      "name_variants": [
        "FB15K-YAGO15K"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The FB15K-YAGO15K dataset is used to evaluate the performance of entity alignment in multi-modal knowledge graphs. Research focuses on the impact of seed entities or pre-aligned entities, employing methodologies that assess how these initial alignments influence overall alignment accuracy. This dataset enables researchers to test and refine algorithms for improving entity alignment, a critical aspect of integrating and reasoning across diverse knowledge sources."
    },
    {
      "display_name": "Karpathy",
      "normalized_name": "karpathy",
      "name_variants": [
        "Karpathy"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Karpathy dataset is primarily used for training and evaluating image captioning models, focusing on aligning visual and textual information. It supports multi-modal reasoning by pre-training models with diverse synthetic captions and evaluating structured representations in vision-language tasks, such as relationship and attribute prediction. This dataset enhances model robustness and reasoning capabilities in image captioning and related vision-language tasks."
    },
    {
      "display_name": "PropBank",
      "normalized_name": "propbank",
      "name_variants": [
        "PropBank"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "PropBank is used to enhance the representation of events in knowledge graphs by providing a large annotated corpus of semantic roles and nominal predicate argument structures. This dataset contributes to extending the size of event knowledge graphs, specifically adding over 114,576 events, thereby enriching the representation of both verb-based and noun-based events."
    },
    {
      "display_name": "VGGFace2",
      "normalized_name": "vggface2",
      "name_variants": [
        "VGGFace2"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The VGGFace2 dataset is used to train models for extracting facial identity features from detected faces. It is employed in the context of multi-modal reasoning, enhancing the ability to recognize and differentiate individuals in various applications. The dataset's large scale and diverse set of facial images enable robust training, improving model performance in facial recognition tasks."
    },
    {
      "display_name": "WN9-IMG-TXT",
      "normalized_name": "wn9imgtxt",
      "name_variants": [
        "WN9-IMG-TXT"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The WN9-IMG-TXT dataset is used to enhance the data diversity of multi-modal knowledge graphs by integrating textual descriptions and images into entities, thereby enriching the graph's content. It is also employed to verify reasoning performance in a multi-modal knowledge graph under the transductive setting, specifically focusing on the integration of image and text data. This dataset facilitates research by providing a richer, more diverse set of modalities for evaluating and improving multi-modal reasoning systems."
    },
    {
      "display_name": "LVIS",
      "normalized_name": "lvis",
      "name_variants": [
        "LVIS"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The LVIS dataset is used to pretrain the ResNet152 model for extracting frame features, focusing on integrating visual and environmental components in multi-modal reasoning. It is also employed for object detection and feature extraction, contributing to the object component in multi-modal knowledge graph reasoning. These applications enhance the integration of visual data with other modalities, enabling more comprehensive multi-modal analysis."
    },
    {
      "display_name": "FB-IMG",
      "normalized_name": "fbimg",
      "name_variants": [
        "FB-IMG"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The FB-IMG dataset is used to obtain textual and visual representations for multi-modal reasoning, integrating pre-trained word2vec for text and VGG-m-128CNN embeddings for images. It is also utilized to construct a subset of FB15K, focusing on triples extracted from Freebase for multi-modal knowledge graph reasoning tasks. This dataset enables the combination of textual and visual data to enhance reasoning capabilities in multi-modal contexts."
    },
    {
      "display_name": "YAGO",
      "normalized_name": "yago",
      "name_variants": [
        "YAGO"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The YAGO dataset is primarily used to support multi-modal reasoning tasks, focusing on entity linking, type inference, and relation extraction across different data types. It integrates structured data from Wikipedia and Wikidata, enhancing knowledge graph representations. Additionally, YAGO is utilized as a lightweight ontology for semantic knowledge integration and to enhance temporal knowledge graph reasoning by incorporating time information into the graph structure."
    },
    {
      "display_name": "TinyImage",
      "normalized_name": "tinyimage",
      "name_variants": [
        "TinyImage"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The TinyImage dataset is used to evaluate the accuracy of multi-modal knowledge graphs built on WordNet, particularly focusing on the elimination of abstract concepts and its impact on hierarchical taxonomies. It serves as a baseline for constructing and comparing MMKGs, distinguishing between visible and non-visible concepts. With 75k noun concepts and an average of 1,052 images per concept, it supports nonparametric object and scene recognition research."
    },
    {
      "display_name": "FB13",
      "normalized_name": "fb13",
      "name_variants": [
        "FB13"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The FB13 dataset is primarily used to train and evaluate knowledge graph completion models, focusing on various subsets of Freebase with differing numbers of entities and relations. It is utilized to assess model performance, scalability, and robustness on medium to large datasets, often combined with other data sources like New York Times articles. The dataset's filtered subsets help reduce redundancy and enable efficient testing and validation."
    },
    {
      "display_name": "FB-IMG-TXT",
      "normalized_name": "fbimgtxt",
      "name_variants": [
        "FB-IMG-TXT"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The FB-IMG-TXT dataset is used to enhance multimodal knowledge graphs by integrating textual descriptions and images into entity representations. It is employed in multimodal knowledge graph representation learning, where it helps in crawling images (ranging from 10 to 100 per entity) and setting image feature embedding dimensions (from 128 to 4096). The dataset is also used as a benchmark to compare sparsity and complexity in multimodal knowledge graph representation learning, addressing challenges in handling sparse and complex data."
    },
    {
      "display_name": "DBpedia50",
      "normalized_name": "dbpedia50",
      "name_variants": [
        "DBpedia50"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The DBpedia50 dataset is used to evaluate and assess the performance, scalability, and robustness of knowledge graph embedding and completion methods. It focuses on entity linking, relation prediction, and large-scale entity and relation representation learning, particularly in both smaller and larger subsets of DBpedia. This dataset enables researchers to efficiently test and validate algorithms on medium-sized entity sets, ensuring generalization and efficiency."
    },
    {
      "display_name": "Visual7W-KB",
      "normalized_name": "visual7wkb",
      "name_variants": [
        "Visual7W-KB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Visual7W-KB dataset is used to test the performance of the Out of the Box method, specifically for factual visual question answering tasks. This involves employing graph convolutional networks to reason over multi-modal data, integrating visual and textual information to answer questions accurately. The dataset's focus on factual questions and its structure supporting graph-based reasoning make it suitable for evaluating models in this domain."
    },
    {
      "display_name": "GDELT",
      "normalized_name": "gdelt",
      "name_variants": [
        "GDELT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GDELT dataset is used as a dense knowledge graph derived from global events, language, and tone data. It focuses on temporal knowledge graph completion and embedding, enabling researchers to analyze and predict event sequences and their impacts over time. This dataset's comprehensive temporal and relational data supports methodologies aimed at enhancing understanding of dynamic global phenomena."
    },
    {
      "display_name": "SituNet",
      "normalized_name": "situnet",
      "name_variants": [
        "SituNet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "SituNet is used to develop and train models for situation recognition tasks, particularly focusing on visual semantic role labeling to enhance image understanding. It contributes to defining visual event schemas, enabling systems to interpret and recognize situations depicted in images. This dataset facilitates the advancement of visual semantic role labeling methodologies, crucial for improving the accuracy and depth of image content analysis."
    },
    {
      "display_name": "Action Genome",
      "normalized_name": "actiongenome",
      "name_variants": [
        "Action Genome"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Action Genome dataset is used to construct scene graphs that capture spatio-temporal relationships and compositions of actions involving whole human instances. This dataset enables researchers to analyze and model complex human activities by providing detailed annotations of actions and their interactions within scenes. The focus on spatio-temporal relationships facilitates the development of algorithms that can understand and predict human behavior in dynamic environments."
    },
    {
      "display_name": "QuALITY",
      "normalized_name": "quality",
      "name_variants": [
        "QuALITY"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The QuALITY dataset is used to evaluate and compare the complexity of questions that require deep reasoning and fact tracking across multiple segments. It highlights the need for advanced multi-modal reasoning, enabling researchers to assess and develop models capable of handling intricate, multi-faceted queries."
    },
    {
      "display_name": "STaRK",
      "normalized_name": "stark",
      "name_variants": [
        "STaRK"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The STaRK dataset is used to support and benchmark RAG models, focusing on retrieval-augmented reasoning and knowledge graph integration. It enables comprehensive evaluation of retrieval-augmented generation systems, ensuring they effectively integrate and reason over multi-modal data. This dataset facilitates the assessment of specific aspects of these models, enhancing their performance and reliability in complex reasoning tasks."
    },
    {
      "display_name": "ProMQA",
      "normalized_name": "promqa",
      "name_variants": [
        "ProMQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ProMQA dataset is utilized for multimodal question answering, integrating text and images to enhance understanding and accuracy in scientific papers. It is also applied to procedural activity understanding, focusing on the integration of visual and textual information to improve reasoning and answer generation. This dataset enables researchers to develop and test models that can effectively combine multiple modalities, thereby advancing the field of multimodal question answering."
    },
    {
      "display_name": "MultiModalQA",
      "normalized_name": "multimodalqa",
      "name_variants": [
        "MultiModalQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MultiModalQA dataset is utilized for multimodal question answering, integrating text and images to enhance understanding and accuracy in scientific papers. It is also applied to multimodal procedural activity understanding, focusing on combining visual and textual information to improve reasoning and answer generation. This dataset enables researchers to develop and test models that can effectively process and integrate multiple data types, thereby advancing the field of multimodal information processing."
    },
    {
      "display_name": "PubMed",
      "normalized_name": "pubmed",
      "name_variants": [
        "PubMed"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PubMed dataset is used to source abstracts for multi-modal reasoning, integrating textual information with other data types. This integration supports research in combining diverse data sources, enhancing the ability to reason across multiple modalities. The dataset's extensive collection of biomedical abstracts facilitates this by providing rich textual content for analysis and integration."
    },
    {
      "display_name": "Pascal Sentence",
      "normalized_name": "pascalsentence",
      "name_variants": [
        "Pascal Sentence"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Pascal Sentence dataset is used to visualize the distribution of generated features using t-SNE, specifically to evaluate the effectiveness of novel designs in multi-modal knowledge graph reasoning. This visualization helps researchers understand how different features cluster and interact, providing insights into the performance and improvements of multi-modal reasoning models."
    },
    {
      "display_name": "NUS-WIDE",
      "normalized_name": "nuswide",
      "name_variants": [
        "NUS-WIDE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NUS-WIDE dataset is used to enhance multi-modal reasoning in image tagging tasks by disambiguating concepts and relating them more effectively to images. This involves methodologies that integrate textual and visual data to improve the accuracy and relevance of image tags. The dataset's large-scale and diverse collection of images and associated tags enable researchers to develop and evaluate algorithms that better understand and annotate visual content."
    },
    {
      "display_name": "VTKB",
      "normalized_name": "vtkb",
      "name_variants": [
        "VTKB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VTKB dataset is used to construct a visio-textual knowledge base that links concepts to images and enhances image tagging quality through hierarchical concept organization. It employs embedding similarities to link images, improving the accuracy and relevance of image tags. This dataset enables research focused on integrating visual and textual data to create more robust and organized knowledge bases."
    },
    {
      "display_name": "academic MMKG",
      "normalized_name": "academicmmkg",
      "name_variants": [
        "academic MMKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The academic MMKG dataset is used to enhance the retrieval of deep learning paper implementations, facilitating the linkage between academic publications and their corresponding code. This supports researchers in efficiently accessing and understanding the practical aspects of deep learning research, improving the integration and application of theoretical findings in real-world scenarios."
    },
    {
      "display_name": "unearthed oracle bones’ photos",
      "normalized_name": "unearthedoraclebonesphotos",
      "name_variants": [
        "unearthed oracle bones’ photos"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'unearthed oracle bones’ photos' dataset is used to construct a multi-modal knowledge graph for oracle bone recognition. This involves integrating visual and textual data to enhance information processing. The dataset enables researchers to develop and refine methods for recognizing and interpreting oracle bone inscriptions, leveraging both image and text data to improve accuracy and depth of analysis."
    },
    {
      "display_name": "Wikipedia articles and images",
      "normalized_name": "wikipediaarticlesandimages",
      "name_variants": [
        "Wikipedia articles and images"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Wikipedia articles and images dataset is used to pre-train a cross-modal entity matching module, which aligns textual and visual scene graphs extracted from the input articles and images. This methodology supports research in cross-modal alignment, enhancing the integration of textual and visual information in multi-modal systems."
    },
    {
      "display_name": "DocVQA",
      "normalized_name": "docvqa",
      "name_variants": [
        "DocVQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DocVQA dataset is used to evaluate visual question answering (VQA) on document images, integrating textual and visual information for document understanding. Researchers employ this dataset to assess models' ability to comprehend and answer questions based on both the text and visual elements within documents. This enables the development and testing of multi-modal VQA systems tailored for document analysis."
    },
    {
      "display_name": "CS-DVQA",
      "normalized_name": "csdvqa",
      "name_variants": [
        "CS-DVQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CS-DVQA dataset is used to evaluate the performance of models in visually-rich document understanding tasks. Specifically, it assesses the effectiveness of an add-on module in reducing errors and compares pre-trained BERT with LayoutLMv2 BASE. This dataset enables researchers to test and improve models' ability to understand complex visual documents."
    },
    {
      "display_name": "SpatialVOC2K",
      "normalized_name": "spatialvoc2k",
      "name_variants": [
        "SpatialVOC2K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SpatialVOC2K dataset is used to analyze multilingual spatial relations in images, focusing on manual annotations and features. It explores cross-linguistic variations in spatial understanding, employing a methodology that emphasizes detailed annotations to address specific research questions related to how different languages represent spatial concepts in visual contexts."
    },
    {
      "display_name": "CI-FAR",
      "normalized_name": "cifar",
      "name_variants": [
        "CI-FAR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CI-FAR dataset is used to extract bounding boxes of objects for constructing VisionKG, focusing on object classification, segmentation, detection, and annotation in diverse visual scenes. It supports small-scale image recognition and object localization, enabling research in visual knowledge graph construction and enhancing the accuracy of object-related tasks in images."
    },
    {
      "display_name": "KVC16K",
      "normalized_name": "kvc16k",
      "name_variants": [
        "KVC16K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KVC16K dataset is used to evaluate knowledge graph completion methods, particularly focusing on multi-modal reasoning. Researchers employ performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1 to assess the effectiveness of these methods. This dataset enables the examination of how well models can integrate and reason over multiple data modalities, enhancing the accuracy and robustness of knowledge graph completion tasks."
    },
    {
      "display_name": "FB15K-DB15K",
      "normalized_name": "fb15kdb15k",
      "name_variants": [
        "FB15K-DB15K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB15K-DB15K dataset is primarily used to evaluate cross-KG reasoning, focusing on the alignment between entities and relations in different knowledge graphs such as Freebase, DBpedia, and YAGO. Researchers employ this dataset to conduct experiments on entity and relation alignment, enhancing the interoperability and integration of diverse knowledge bases. This dataset facilitates the development and testing of methods for aligning and reasoning across multiple knowledge graphs, supporting research in cross-KG alignment and integration."
    },
    {
      "display_name": "Multi-OpenEA",
      "normalized_name": "multiopenea",
      "name_variants": [
        "Multi-OpenEA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Multi-OpenEA dataset is used to construct large-scale multimodal entity alignment benchmarks, featuring a high ratio of image-equipped entities and multiple images per entity. This enhances the evaluation of multimodal knowledge graph reasoning methods, specifically focusing on the alignment of entities across different modalities. The dataset's rich multimodal content supports the development and assessment of advanced reasoning techniques in this domain."
    },
    {
      "display_name": "DBP15K",
      "normalized_name": "dbp15k",
      "name_variants": [
        "DBP15K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DBP15K dataset is primarily used to evaluate entity alignment models, focusing on the comparison of model performance with and without surface forms. This evaluation helps demonstrate the robustness and efficiency of these models. The dataset's utility lies in its ability to provide a standardized benchmark for assessing and improving entity alignment techniques in knowledge graphs."
    },
    {
      "display_name": "WD-singer",
      "normalized_name": "wdsinger",
      "name_variants": [
        "WD-singer"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WD-singer dataset is used to derive subsets of Wikidata for evaluating knowledge graph completion tasks. These subsets cover a broad range of entities and relations, enabling researchers to assess the performance of knowledge graph completion models across various scopes. The dataset's large scale and diverse content facilitate comprehensive evaluation and improvement of these models."
    },
    {
      "display_name": "N-MMKG",
      "normalized_name": "nmmkg",
      "name_variants": [
        "N-MMKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The N-MMKG dataset is mentioned as a type of multi-modal knowledge graph, but the specific usage, research context, methodology, and research questions or applications are not provided in the available citations. Therefore, there is insufficient evidence to detail its actual use in research beyond its classification as a multi-modal knowledge graph."
    },
    {
      "display_name": "WN18",
      "normalized_name": "wn18",
      "name_variants": [
        "WN18"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WN18 dataset is used to enhance multi-modal knowledge graph reasoning by extending entities with images, integrating visual information into lexical semantic relationships. This approach supports representation learning that combines textual and visual data, improving the understanding of entity relationships in knowledge graphs. The dataset's extension with images enables more comprehensive multi-modal reasoning, particularly in the context of visual and semantic integration."
    },
    {
      "display_name": "EAL dataset",
      "normalized_name": "eal",
      "name_variants": [
        "EAL dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The EAL dataset is primarily used for entity aspect linking and representation in research. It provides aspect content and labels derived from Wikipedia section names, enabling the calculation of similarity between context and aspects. The dataset is employed to evaluate models like AspectMMKG, focusing on enhancing entity representation in knowledge graphs by defining detailed attributes and relationships. Experiments using this dataset aim to improve the performance of entity aspect linking methods."
    },
    {
      "display_name": "KnowSite",
      "normalized_name": "knowsite",
      "name_variants": [
        "KnowSite"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KnowSite dataset is used to model site selection criteria for brands and stores by leveraging an urban knowledge graph. It employs relation path logic to provide explainable site decisions, enabling researchers to understand and justify the selection process through structured data relationships. This approach supports decision-making in retail and urban planning contexts."
    },
    {
      "display_name": "PKG",
      "normalized_name": "pkg",
      "name_variants": [
        "PKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PKG dataset is used as a multi-modal knowledge graph for classical Chinese poetry, integrating textual and visual data. It enhances understanding and reasoning about poetic works by combining these modalities. Researchers use this dataset to explore the interplay between text and imagery in classical poetry, employing methods that leverage both types of data to deepen insights into the cultural and artistic context of the poems."
    },
    {
      "display_name": "Freebase-15k Multi-Modal (FB15k)",
      "normalized_name": "freebase15kmultimodalfb15k",
      "name_variants": [
        "Freebase-15k Multi-Modal (FB15k)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Freebase-15k Multi-Modal (FB15k) dataset is primarily used for evaluating multi-modal knowledge graph reasoning, with a focus on entity and relation prediction tasks. It is employed to enhance and extend existing datasets like YAGO and DBpedia by integrating visual and textual data, enabling more robust multi-modal reasoning and evaluation in large-scale settings. The dataset supports research in cross-lingual entity alignment, entity linking, and relation prediction, leveraging its multi-modal features to improve the accuracy and comprehensiveness of knowledge graph models."
    },
    {
      "display_name": "NTU-RGB+D 60",
      "normalized_name": "nturgbd60",
      "name_variants": [
        "NTU-RGB+D 60"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NTU-RGB+D 60 dataset is primarily used for skeleton-based action recognition, offering a large-scale collection of 3D human activity data. It supports multi-modal reasoning tasks by providing rich, multi-view data, enabling researchers to develop and test algorithms for recognizing and understanding human actions in various contexts."
    },
    {
      "display_name": "ASER",
      "normalized_name": "aser",
      "name_variants": [
        "ASER"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "ASER is utilized as a large-scale eventuality knowledge graph, offering a rich resource for semantic relations and eventualities. It is specifically employed in multi-modal knowledge graph reasoning tasks, enhancing the ability to reason across different data types and modalities. This dataset enables researchers to explore complex relationships and patterns in multi-modal contexts, supporting advanced reasoning applications."
    },
    {
      "display_name": "UAV-Human",
      "normalized_name": "uavhuman",
      "name_variants": [
        "UAV-Human"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The UAV-Human dataset is used for evaluating human behavior understanding with unmanned aerial vehicles, focusing on multi-modal reasoning and learning. It involves analyzing multi-modal video sequences captured from three sensors, enabling researchers to study complex human activities and interactions in diverse environments. This dataset supports the development and testing of algorithms that integrate data from multiple sources to enhance the accuracy and robustness of behavior analysis."
    },
    {
      "display_name": "MIMIC-III",
      "normalized_name": "mimiciii",
      "name_variants": [
        "MIMIC-III"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MIMIC-III dataset is used to train models for multi-modal reasoning in critical care settings, particularly for ICU environments. It enables researchers to develop and evaluate models that can integrate and reason across various data types, enhancing decision-making and patient care in intensive care units."
    },
    {
      "display_name": "NYTimes800k",
      "normalized_name": "nytimes800k",
      "name_variants": [
        "NYTimes800k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NYTimes800k dataset is used to collect and annotate images, captions, and news articles from the New York Times. It focuses on providing ground-truth captions for images, enabling research in image captioning and multimodal content analysis. This dataset supports methodologies that require accurate image-text pairing, facilitating studies on natural language processing and computer vision."
    },
    {
      "display_name": "Unified Medical Language System (UMLS)",
      "normalized_name": "unifiedmedicallanguagesystemumls",
      "name_variants": [
        "Unified Medical Language System (UMLS)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Unified Medical Language System (UMLS) is used to integrate biomedical terminology, serving as a comprehensive knowledge base for multi-modal knowledge graph reasoning. It facilitates the alignment and integration of diverse medical terminologies, enhancing the coherence and interoperability of biomedical data. This integration supports advanced reasoning tasks and improves the accuracy and comprehensiveness of knowledge graphs in biomedical research."
    },
    {
      "display_name": "Semantic MEDLINE",
      "normalized_name": "semanticmedline",
      "name_variants": [
        "Semantic MEDLINE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Semantic MEDLINE dataset is used to extract personalized graphs to enhance Electronic Health Record (EHR) representation learning. It focuses on multi-modal reasoning and knowledge integration, enabling researchers to improve the accuracy and comprehensiveness of EHR data through advanced graph-based methods. This approach supports more sophisticated and contextually rich patient data analysis."
    },
    {
      "display_name": "ICD/ATC",
      "normalized_name": "icdatc",
      "name_variants": [
        "ICD/ATC"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ICD/ATC dataset is extended with co-occurrence information to enhance the predictive power of models, specifically for temporal health event prediction. It is used to improve the accuracy of predicting future health events by leveraging the relationships between International Classification of Diseases (ICD) codes and Anatomical Therapeutic Chemical (ATC) classification system codes."
    },
    {
      "display_name": "FVQA",
      "normalized_name": "fvqa",
      "name_variants": [
        "FVQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FVQA dataset is used to integrate background knowledge in visual question answering by representing instances as tuples of image, question, answer, and supporting fact subgraph. This approach enhances the reasoning capabilities of models by incorporating external knowledge, enabling more accurate and contextually informed answers to complex visual questions."
    },
    {
      "display_name": "COCO",
      "normalized_name": "coco",
      "name_variants": [
        "COCO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The COCO dataset is primarily used for image captioning tasks, offering a large-scale collection of images paired with descriptive captions to train and evaluate models. It also serves to assess models' multi-modal reasoning abilities through a variety of perception and reasoning tasks, leveraging its diverse and richly annotated content."
    },
    {
      "display_name": "DBPedia50k",
      "normalized_name": "dbpedia50k",
      "name_variants": [
        "DBPedia50k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DBPedia50k dataset is used for inductive reasoning in knowledge graphs, specifically to enhance model performance in the inductive setting. This involves training models to reason about entities and relationships not seen during training, leveraging the dataset's structured information to improve generalization and predictive accuracy."
    },
    {
      "display_name": "HLVU",
      "normalized_name": "hlvu",
      "name_variants": [
        "HLVU"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The HLVU dataset is used to test deep understanding of movies, specifically focusing on high-level video understanding. It consists of 10 Creative Commons licensed movies totaling 681 minutes. Researchers employ this dataset to evaluate models' ability to comprehend complex visual and narrative elements, addressing questions related to advanced video comprehension and analysis."
    },
    {
      "display_name": "VCR",
      "normalized_name": "vcr",
      "name_variants": [
        "VCR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VCR dataset is used for fine-tuning the BERT model to learn cross-modal alignment, specifically focusing on visual-linguistic reasoning tasks. This involves integrating visual and textual data to improve the model's ability to reason about complex scenes and their descriptions. The dataset enables researchers to enhance multi-modal understanding and reasoning capabilities in AI models."
    },
    {
      "display_name": "electronic medical records",
      "normalized_name": "electronicmedicalrecords",
      "name_variants": [
        "electronic medical records"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The electronic medical records dataset is used to integrate with a medical knowledge graph for safe medication recommendation. Researchers employ knowledge graph embedding techniques to enhance the accuracy and safety of medication suggestions, leveraging the structured data within the medical records to inform and refine these recommendations."
    },
    {
      "display_name": "Global Database of Events, Language, and Tone (GDELT)",
      "normalized_name": "globaldatabaseofeventslanguageandtonegdelt",
      "name_variants": [
        "Global Database of Events, Language, and Tone (GDELT)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Global Database of Events, Language, and Tone (GDELT) is used for dynamic knowledge graph reasoning tasks, providing structured data on global events, political activities, and crisis situations. It supports predictive modeling by integrating event data and linguistic tone analysis, enabling researchers to analyze and forecast complex social and political dynamics."
    },
    {
      "display_name": "MM-FB15K",
      "normalized_name": "mmfb15k",
      "name_variants": [
        "MM-FB15K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MM-FB15K dataset is used to construct multi-modal knowledge graphs by integrating DBpedia with visual and textual information. It extends the FB15K dataset with additional modalities to enhance reasoning capabilities. This dataset supports multi-modal reasoning tasks, enabling researchers to explore how visual and textual data can improve knowledge graph reasoning."
    },
    {
      "display_name": "Wikidata",
      "normalized_name": "wikidata",
      "name_variants": [
        "Wikidata"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Wikidata is used to construct dynamic knowledge graph reasoning (KGR) datasets by incorporating temporal information. This enhances the reasoning capabilities over time, allowing researchers to analyze and model evolving relationships within the knowledge graph. The dataset's temporal features enable more sophisticated and contextually relevant research questions and applications."
    },
    {
      "display_name": "ICEWS",
      "normalized_name": "icews",
      "name_variants": [
        "ICEWS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ICEWS dataset is used to provide event data with daily temporal granularity, specifically focusing on military and political events. It is employed in multi-modal knowledge graph reasoning research, enabling the analysis and integration of diverse data sources to understand complex event dynamics and relationships."
    },
    {
      "display_name": "Babel-Net",
      "normalized_name": "babelnet",
      "name_variants": [
        "Babel-Net"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Babel-Net is used to construct and enrich multilingual knowledge graphs by providing a large-scale lexicalized semantic network. It integrates structured information from sources like DBpedia and Wikipedia, and is employed to label terms defined by natural language, enhancing text entailment recognition. The dataset focuses on lexical and semantic relationships, enabling the creation of comprehensive, interpretable knowledge graphs."
    },
    {
      "display_name": "NELL23k",
      "normalized_name": "nell23k",
      "name_variants": [
        "NELL23k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NELL23k dataset is used to train and evaluate knowledge graph embedding models, focusing on both a smaller, curated subset and a larger subset of the NELL knowledge base. It enables researchers to assess the performance of these models in tasks such as link prediction and entity classification, contributing to advancements in knowledge graph representation learning."
    },
    {
      "display_name": "electronic medical database",
      "normalized_name": "electronicmedicaldatabase",
      "name_variants": [
        "electronic medical database"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The electronic medical database is used to construct a health knowledge graph, specifically for reasoning tasks that involve patient data from electronic medical records. This dataset enables researchers to develop and test methods for integrating and analyzing complex medical information, enhancing the accuracy and utility of health-related reasoning systems."
    },
    {
      "display_name": "Hetionet",
      "normalized_name": "hetionet",
      "name_variants": [
        "Hetionet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Hetionet is utilized as a knowledge graph integrating diverse biomedical data from public resources. It is primarily employed to prioritize drugs for repurposing by leveraging its structured representation of complex relationships between drugs, diseases, and biological entities. This enables researchers to identify potential therapeutic applications of existing drugs through systematic analysis and reasoning over the integrated data."
    },
    {
      "display_name": "FAMILY",
      "normalized_name": "family",
      "name_variants": [
        "FAMILY"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FAMILY dataset is used for constructing and reasoning about multi-modal knowledge graphs, specifically to represent and infer relationships among family members and nations. It enables researchers to explore interconnected entities and their relations, facilitating complex inference tasks and knowledge representation in these domains."
    },
    {
      "display_name": "Countries",
      "normalized_name": "countries",
      "name_variants": [
        "Countries"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Countries' dataset is used to represent and reason about geographical relations among countries, employing low-rank vector spaces for approximate reasoning. This approach facilitates the analysis of spatial relationships and supports research focused on understanding and modeling geographical data in a computationally efficient manner."
    },
    {
      "display_name": "FB-Img-Few",
      "normalized_name": "fbimgfew",
      "name_variants": [
        "FB-Img-Few"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB-Img-Few dataset is used to provide image embeddings for multi-modal knowledge graph reasoning, specifically in few-shot learning scenarios. This dataset enables researchers to integrate visual information into knowledge graphs, enhancing reasoning capabilities with limited data. The focus is on developing models that can effectively learn from a small number of examples, making it particularly useful for scenarios where labeled data is scarce."
    },
    {
      "display_name": "YAGO3-10",
      "normalized_name": "yago310",
      "name_variants": [
        "YAGO3-10"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YAGO3-10 dataset is used as a subset of a larger knowledge graph to evaluate relation prediction tasks. Researchers focus on the scope of relations within the dataset, employing it to assess the performance of models in predicting relationships between entities. This enables the evaluation of knowledge graph completion methods, ensuring they can accurately infer missing links within the graph."
    },
    {
      "display_name": "WN-9",
      "normalized_name": "wn9",
      "name_variants": [
        "WN-9"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WN-9 dataset is used to enhance multimodal reasoning capabilities by expanding knowledge graphs through the addition of images to entities. This approach integrates visual data with existing textual information, enabling more comprehensive and contextually rich representations. The dataset facilitates research focused on improving the multimodal reasoning abilities of knowledge graphs, specifically by incorporating visual elements to enrich entity descriptions and relationships."
    },
    {
      "display_name": "NELL",
      "normalized_name": "nell",
      "name_variants": [
        "NELL"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NELL dataset is used as a knowledge base for multi-modal reasoning, specifically for integrating textual and structured data. This integration enhances the construction of knowledge graphs by combining diverse data types, improving the depth and accuracy of the resulting graphs. The dataset's structured nature and textual content enable researchers to explore and develop methods for more robust knowledge graph construction and reasoning."
    },
    {
      "display_name": "YAGO37",
      "normalized_name": "yago37",
      "name_variants": [
        "YAGO37"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YAGO37 dataset is used as a subset of a larger knowledge graph to evaluate relation prediction tasks. Researchers focus on the scope of relations within the dataset, employing it to assess the performance of models in predicting relationships. This usage highlights the dataset's relevance in enhancing the accuracy and reliability of relation prediction in knowledge graphs."
    },
    {
      "display_name": "FB122",
      "normalized_name": "fb122",
      "name_variants": [
        "FB122"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FB122 dataset is primarily used to evaluate and benchmark knowledge graph completion methods, focusing on entity linking, relation prediction, and knowledge graph embedding. It addresses issues in FB15k by filtering out inverse relations and improving dataset quality. The dataset supports both small and large-scale evaluations, offering a moderate to large entity set with diverse relations, enhancing the scalability and robustness testing of algorithms. Additionally, it is used to assess multi-modal knowledge graph reasoning, integrating textual data from New York Times articles with Freebase entities."
    },
    {
      "display_name": "YOGA11k",
      "normalized_name": "yoga11k",
      "name_variants": [
        "YOGA11k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The YOGA11k dataset is used to generate temporal knowledge graph embeddings, specifically focusing on different periods for multi-modal reasoning. This approach enables researchers to analyze and reason about multi-modal data over time, enhancing the understanding of dynamic relationships within the dataset. The temporal aspect of the embeddings is a key feature, facilitating research into how multi-modal information evolves and interacts over various periods."
    },
    {
      "display_name": "MMKG-FB15k-IMG",
      "normalized_name": "mmkgfb15kimg",
      "name_variants": [
        "MMKG-FB15k-IMG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MMKG-FB15k-IMG dataset is used to integrate various knowledge graphs (YAGO, Freebase, DBpedia) with images and other data types, enhancing multi-modal reasoning capabilities. It facilitates comprehensive reasoning by combining textual, visual, and structured data, enabling researchers to explore complex relationships and interactions within multi-modal datasets. This integration supports more robust and diverse multi-modal reasoning applications."
    },
    {
      "display_name": "Richpedia",
      "normalized_name": "richpedia",
      "name_variants": [
        "Richpedia"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Richpedia dataset is used as a comprehensive multi-modal knowledge graph that integrates triplets, textual descriptions, and images to support reasoning tasks. It enables researchers to explore complex relationships and entities through a rich, integrated data structure, facilitating advanced reasoning and analysis in multi-modal contexts."
    },
    {
      "display_name": "MKG-Wikipedia",
      "normalized_name": "mkgwikipedia",
      "name_variants": [
        "MKG-Wikipedia"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MKG-Wikipedia dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. It leverages Wikipedia data to assess the performance of these methods, enabling researchers to improve the accuracy and robustness of knowledge graph reasoning systems."
    },
    {
      "display_name": "MKG-YAGO",
      "normalized_name": "mkgyago",
      "name_variants": [
        "MKG-YAGO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MKG-YAGO dataset is used to evaluate multimodal knowledge graph completion methods, specifically focusing on entity alignment and relation prediction. It leverages Wikipedia and YAGO data to assess the performance of these methods, enabling researchers to enhance the accuracy and robustness of knowledge graph reasoning systems."
    },
    {
      "display_name": "Nation",
      "normalized_name": "nation",
      "name_variants": [
        "Nation"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Nation' dataset is used to represent and analyze relations among nations, focusing on the structural connections within the dataset. Researchers employ this dataset to explore the dimensionality of these relations, using it to address specific research questions related to the complexity and structure of international relationships. The dataset's focus on relational structures enables detailed analysis of how nations interconnect."
    },
    {
      "display_name": "Fashion-MMKG",
      "normalized_name": "fashionmmkg",
      "name_variants": [
        "Fashion-MMKG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Fashion-MMKG dataset is used to train a CLIP-style model for e-commerce image-text retrieval, leveraging cross-modal fashion knowledge to improve model performance. This dataset incorporates both textual and visual data, enabling researchers to enhance the accuracy and relevance of search results in e-commerce applications."
    }
  ]
}