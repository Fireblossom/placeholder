Name (extracted)	Citing Article	Citied Article	Features
GSM8K	https://doi.org/10.48550/arXiv.2310.13227 (2023), https://doi.org/10.48550/arXiv.2307.12856 (2023), https://doi.org/10.48550/arXiv.2302.06706 (2023), https://doi.org/10.48550/arXiv.2502.14565 (2025), https://doi.org/10.48550/arXiv.2301.13379 (2023), https://doi.org/10.48550/arXiv.2305.04091 (2023), https://doi.org/10.48550/arXiv.2304.11657 (2023), https://doi.org/10.48550/arXiv.2411.15382 (2024), https://doi.org/10.48550/arXiv.2310.01798 (2023), https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c (2024), https://doi.org/10.48550/arXiv.2406.14283 (2024), https://doi.org/10.48550/arXiv.2302.00618 (2023), https://doi.org/10.48550/arXiv.2410.20749 (2024), https://doi.org/10.48550/arXiv.2308.00436 (2023), https://doi.org/10.48550/arXiv.2309.10691 (2023), https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691 (2022)	https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea (2021)	The GSM8K dataset is primarily used to train and evaluate large language models (LLMs) on solving and explaining algebraic word problems, emphasizing multi-step reasoning, logical deduction, and the generation of natural language rationales. It is employed to assess the models' accuracy, robustness, and generalization across diverse arithmetic problems, including fractions, decimals, and complex operations. The dataset also supports few-shot learning experiments, evaluates commonsense reasoning, and tests strategic thinking and symbolic reasoning tasks. Additionally, it is used to train process reward models that provide intermediate signals for reasoning steps, enhancing the models' planning capabilities.
HotpotQA	https://doi.org/10.48550/arXiv.2402.15506 (2024), https://doi.org/10.48550/arXiv.2502.06589 (2025), https://doi.org/10.48550/arXiv.2309.10691 (2023), https://doi.org/10.48550/arXiv.2311.05657 (2023), https://doi.org/10.48550/arXiv.2503.05592 (2025), https://doi.org/10.48550/arXiv.2310.04406 (2023), https://doi.org/10.48550/arXiv.2310.01798 (2023), https://doi.org/10.48550/arXiv.2312.10003 (2023), https://doi.org/10.48550/arXiv.2310.05915 (2023), https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d (2022), https://doi.org/10.1145/3704435 (2023)	https://doi.org/10.18653/v1/D18-1259 (2018)	The HotpotQA dataset is primarily used to evaluate multi-hop question answering capabilities, focusing on diverse and explainable reasoning processes. It is employed to assess models' ability to integrate information from multiple documents, solve complex mathematical problems, and perform logical reasoning tasks. The dataset supports research on planning capabilities of LLMs, particularly in complex agent environments and instruction-following tasks. It emphasizes fair comparisons and the challenges of random test example selection.
MATH	https://doi.org/10.48550/arXiv.2405.19425 (2024), https://doi.org/10.48550/arXiv.2504.16129 (2025), https://doi.org/10.48550/arXiv.2502.14565 (2025), https://doi.org/10.48550/arXiv.2309.10691 (2023), https://www.semanticscholar.org/paper/9ea0757c750ab1222a7442d3485a74d1c526b04c (2023), https://doi.org/10.48550/arXiv.2308.00436 (2023), https://www.semanticscholar.org/paper/f32bcc2155997110a7905da050df4c8404867b24 (2024)	https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5 (2021)	The MATH dataset is extensively used to evaluate and enhance the mathematical reasoning and problem-solving capabilities of large language models (LLMs). It focuses on multi-step reasoning, theorem proving, and logical challenges, covering a wide range of math topics from grade school to college level. The dataset is utilized to benchmark performance, assess accuracy, and measure improvements in solving complex mathematical problems, including multi-hop question answering and evidence combination. It also tests the robustness and scalability of models, highlighting both strengths and limitations in handling diverse and challenging tasks.
Common Crawl	https://doi.org/10.48550/arXiv.2206.08853 (2022), https://doi.org/10.48550/arXiv.2307.12856 (2023)	https://doi.org/10.18653/v1/2022.findings-naacl.55 (2021)	The Common Crawl dataset is primarily used for pre-training large language models, providing a vast and diverse collection of web-scraped text data. It is utilized to improve long-span denoising objectives, enhancing the models' efficiency in handling long sequences. Additionally, it is applied for training multimodal models, focusing on the understanding of instructional videos and their corresponding transcripts. This dataset enables researchers to evaluate and enhance the task planning capabilities of models like HTML-T5, comparing their performance against other models on offline task planning tasks.
ALFRED	https://doi.org/10.24963/ijcai.2024/15 (2024), https://doi.org/10.1109/ICCV51070.2023.00280 (2022), https://doi.org/10.48550/arXiv.2410.05669 (2024), https://doi.org/10.1609/aaai.v37i11.26549 (2022), https://doi.org/10.48550/arXiv.2310.10021 (2023)	https://doi.org/10.1007/978-3-030-24337-1_3 (2018)	The ALFRED dataset is primarily used to develop and evaluate AI agents capable of understanding and executing grounded instructions in embodied environments, focusing on vision-language tasks. It supports the creation of agents that can interact in complex scenarios using both text and visuals. The dataset is also used to evaluate hierarchical planning models, generate questions based on PDDL tasks, and study planning capabilities in multi-turn interactions and simulated 3D environments. It provides goals and plans for the Alfworld and AI2THOR environments, enabling researchers to assess the planning and performance of LLMs and other AI models in various tasks.
MMLU	https://doi.org/10.48550/arXiv.2501.12948 (2025), https://doi.org/10.48550/arXiv.2502.06589 (2025)	https://doi.org/10.18653/v1/2023.emnlp-main.187 (2023)	The MMLU dataset is primarily used to evaluate the performance of large language models (LLMs) across a wide range of tasks, including advanced language understanding, multitask language understanding, interactive and feedback-based tasks, frame-based reasoning, software engineering benchmarks, and live coding. It is also used to assess planning capabilities, particularly in task-oriented dialogues and complex environments. The dataset supports scaling law experiments by providing a benchmark for evaluating model performance and generalization. Pre-training on MMLU enhances core agentic capabilities and improves model performance on diverse and complex tasks.
WebShop	https://doi.org/10.48550/arXiv.2403.02502 (2024), https://doi.org/10.48550/arXiv.2402.15506 (2024), https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d (2022), https://doi.org/10.48550/arXiv.2310.12823 (2023), https://doi.org/10.48550/arXiv.2311.05657 (2023)	https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb (2020)	The WebShop dataset is primarily used to evaluate and train models in various interactive and embodied environments, focusing on tasks such as web navigation, question answering, and embodied household activities. It assesses models' planning capabilities, logical reasoning, and ability to follow instructions in both web and virtual home settings. The dataset supports out-of-distribution testing, multi-hop reasoning, and fact verification, often comparing different models and strategies like ReAct and Act. It is also used to train agents for tasks requiring structured knowledge and to align text with actions in virtual worlds.
CLUTRR	https://doi.org/10.48550/arXiv.2307.07696 (2023), https://doi.org/10.48550/arXiv.2301.13379 (2023)	https://doi.org/10.18653/v1/D19-1458 (2019)	The CLUTRR dataset is used to evaluate the inductive and systematic reasoning capabilities of models in natural language understanding, focusing on robustness and generalization. It assesses models' ability to handle logical rules, unseen combinations, and noisy descriptions, particularly in relational reasoning tasks. The dataset supports research on various models, including ASP, MAC, BiLSTM-attention, and GSM, across clean, supporting, irrelevant, and disconnected data instances.
iGibson	https://doi.org/10.48550/arXiv.2205.10712 (2022), https://doi.org/10.1109/LRA.2024.3441495 (2024)	https://doi.org/10.1109/ICAR.2015.7251504 (2015)	The iGibson dataset is used to create and evaluate datasets for object rearrangement preferences in 3D scenes, focusing on diverse object categories and detailed room annotations. It provides 3D environments for object placement and curation, enabling the development and testing of LLM-based approaches in both training and unseen apartment scenes.
R2R	https://doi.org/10.48550/arXiv.2305.16986 (2023), https://doi.org/10.1109/ICCV51070.2023.00280 (2022)	https://doi.org/10.1109/CVPR.2018.00387 (2017)	The R2R dataset is used to train and evaluate vision-and-language navigation models, focusing on generating action sequences for navigation tasks in visually-grounded environments. It supports cross-modality matching, such as identifying landmarks from observations, and is particularly effective for end-to-end performance evaluation with Transformer models. This dataset enables researchers to assess model capabilities in interpreting and executing visually-grounded instructions in real-world settings.
DS-1000	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269 (2021)	Dataset DS-1000 is used to evaluate various capabilities of large language models (LLMs), including code generation, complex question answering, theorem-driven reasoning, and decision-making in simulated environments. It measures the reliability, naturalness, and correctness of LLM outputs, particularly in multi-turn interactions, multi-hop reasoning, and task completion. The dataset includes subsets like HumanEval and MBPP, which focus on human-like code evaluation and benchmarking performance, respectively. It also assesses LLMs in generating SQL and Bash scripts, and interacting with robot APIs.
ALF-World	https://doi.org/10.18653/v1/2024.findings-emnlp.879 (2024), https://doi.org/10.48550/arXiv.2309.10691 (2023), https://doi.org/10.48550/arXiv.2308.03688 (2023)	https://doi.org/10.48550/arXiv.2207.01206 (2022)	The ALF-World dataset is primarily used to evaluate and train agents in interactive learning and decision-making tasks within embodied environments. It focuses on aligning text and physical actions, enhancing multi-hop question answering, and improving formal logic and proof construction. The dataset is also utilized to assess the performance of grounded language agents in web interaction tasks, including complex instruction following and navigation, as well as household navigation and manipulation tasks to enhance planning capabilities.
NL2Bash	https://doi.org/10.18653/v1/2024.acl-long.670 (2023)	https://doi.org/10.1109/CVPR.2019.00688 (2018)	The NL2Bash dataset is primarily used for interactive coding, where it tests and evaluates models' abilities to translate natural language instructions into executable bash commands. It is also applied in embodied AI tasks, assessing models' performance in navigating and interacting with simulated environments using textual and visual inputs. The dataset highlights the need for new actions in embodied environments, such as opening, taking, moving, and travel-related actions, which are often missing from existing annotations. This enables research on interactive coding, command synthesis, and embodied task planning, focusing on the alignment of textual instructions with actions in both coding and simulated environments.
HumanEval	https://doi.org/10.48550/arXiv.2310.04406 (2023), https://doi.org/10.48550/arXiv.2405.11106 (2024), https://doi.org/10.48550/arXiv.2405.11403 (2024)	https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df (2021)	The HumanEval dataset, comprising 974 short Python functions, is primarily used to evaluate the performance of large language models in generating correct Python code. It focuses on program synthesis and coding challenges, assessing the effectiveness of models like GPT-4 with and without self-reflection. The dataset also highlights the importance of external observations for complex reasoning tasks, enabling researchers to compare different approaches and baselines in solving programming problems.
SCAN	https://www.semanticscholar.org/paper/4bc2bb6584774b0d8ad0b4f5215dc2075487c192 (2020), https://doi.org/10.48550/arXiv.2205.10625 (2022)	https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347 (2021)	The SCAN dataset is primarily used to evaluate compositional generalization in sequence-to-sequence models, focusing on tasks such as navigation command execution, numerical reasoning, and multi-step operations. It assesses models' abilities to generalize to unseen command-action pairs and handle novel compositions, emphasizing systematic and rule-based generalization. This dataset enables researchers to test and improve models' capabilities in understanding and executing complex instructions, particularly in linguistic and navigational contexts.
2Wiki	https://doi.org/10.48550/arXiv.2411.19443 (2024)	https://doi.org/10.18653/v1/d13-1160 (2013)	The 2Wiki dataset is primarily used to evaluate and assess the multi-hop question answering (QA) capabilities of Auto-RAG, focusing on the system's ability to reason across multiple documents or sentences. It emphasizes complex question answering tasks that require integrating information from multiple Wikipedia articles. The dataset helps researchers test the effectiveness and robustness of Auto-RAG in generating accurate answers, particularly in scenarios where multi-hop reasoning and evidence combination are necessary.
ASDIV	https://doi.org/10.48550/arXiv.2211.10435 (2022), https://doi.org/10.48550/arXiv.2307.12856 (2023)	https://doi.org/10.18653/v1/2020.acl-main.92 (2020)	The ASDIV dataset is primarily used to assess the symbolic reasoning and problem-solving abilities of language models, particularly in solving arithmetic word problems. It evaluates models on a wide range of tasks, including single-variable arithmetic, multi-step reasoning, and complex problem-solving scenarios. The dataset emphasizes the complexity and variability of problems, serving as a benchmark for advanced cognitive and mathematical reasoning skills. It is also used to test models like Flan-LongT5 in few-shot and zero-shot settings, focusing on chain-of-thought reasoning and generalization.
nuScenes	https://doi.org/10.48550/arXiv.2310.01415 (2023), https://doi.org/10.48550/arXiv.2311.10813 (2023)	https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5 (2022)	The nuScenes dataset is used to evaluate and fine-tune large language models (LLMs) like GPT-Driver in the context of autonomous driving. It focuses on assessing the model's effectiveness, generalization ability, and interpretability in real-world scenarios. The dataset is also used to enhance motion planning capabilities by fine-tuning the LLM with human driving trajectories.
SmartPlay	https://doi.org/10.48550/arXiv.2310.01557 (2023)	https://doi.org/10.1007/s13218-010-0082-7 (2011)	The SmartPlay dataset is used to evaluate agent capabilities across various tasks, including Two-armed Bandits, Rock Paper Scissors, Messenger, Crafter, and Minecraft creative navigation tasks. It simplifies LLMs to control a hand-coded agent, focusing on creative tasks such as finding specific biomes in Minecraft. This dataset enables researchers to assess and enhance the performance of agents in diverse and complex environments.
ACPBench	https://doi.org/10.48550/arXiv.2503.18971 (2025), https://doi.org/10.48550/arXiv.2410.05669 (2024), https://doi.org/10.48550/arXiv.2503.24378 (2025)	https://www.semanticscholar.org/paper/b0886f8c52f4fdae95ca8cb29a9200598713c966 (2024)	ACPBench is used to standardize evaluation tasks and metrics for assessing the reasoning and planning capabilities of large language models across 13 domains and 22 state-of-the-art models. It generates a challenging set of questions to evaluate the reliability of these models in producing components for automated planners, particularly in complex PDDL domains. The dataset focuses on improving model performance over time and addressing poor performance in planning and reasoning tasks.
MINT-MBPP	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://doi.org/10.48550/arXiv.2305.17390 (2023)	The MINT-MBPP dataset is used to evaluate the multi-turn interaction and code generation capabilities of large language models (LLMs). It focuses on various subsets, including HumanEval and MBPP, to assess human-like code evaluation, interaction with robot APIs, and the generation of SQL and Bash scripts. The dataset tests LLMs' ability to perform complex tasks such as theorem-driven question answering, multi-hop reasoning, and solving mathematical problems using Python. It also evaluates the impact of feedback, particularly from GPT-4, on accuracy and efficiency in multi-turn interactions and tool utilization.
MiniWoB++	https://doi.org/10.48550/arXiv.2407.01476 (2024), https://doi.org/10.1145/3704435 (2023), https://doi.org/10.48550/arXiv.2307.12856 (2023)	https://doi.org/10.48550/arXiv.2403.08978 (2024)	MiniWoB++ is used to evaluate and compare the performance of large language models and other systems in executing web-based tasks. It serves as a benchmark for assessing task completion, planning capabilities, and generalization to diverse web interfaces. The dataset is employed to fine-tune models like HTML-T5, compare them against reinforcement learning methods, and evaluate their adaptability across 56 web automation tasks.
SeqMultiWoz	https://doi.org/10.48550/arXiv.2402.15491 (2024)	https://www.semanticscholar.org/paper/15c10b24ef645d83ff4059affd86945c33e00328 (2018)	The SeqMultiWoz dataset is primarily used to build, train, and evaluate multi-domain conversational agents and models, focusing on schema-guided dialogue management and natural language understanding. It is employed to assess model performance on complex, multi-turn conversations and top-level dialogue tasks, emphasizing scalability and multi-domain capabilities. The dataset supports research into spoken language understanding, particularly in converting mixed-intent utterances into API sequences and handling specific topics. It contains 12 APIs, which aids in studying the impact of API count on model performance.
Ego4D	https://doi.org/10.48550/arXiv.2206.08853 (2022)	https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500 (2021)	The Ego4D dataset is used to train a general-purpose visual encoder for robot perception, leveraging first-person video data to enhance representation learning. This dataset enables researchers to focus on improving the visual understanding capabilities of robots by providing extensive, real-world first-person perspective videos, which are crucial for developing robust and context-aware robotic systems.
AddSub	https://doi.org/10.48550/arXiv.2303.17491 (2023)	https://doi.org/10.3115/v1/D14-1058 (2014)	The AddSub dataset is used to assess and evaluate the performance of language models in solving arithmetic word problems, particularly focusing on addition and subtraction. It emphasizes step-by-step reasoning, numerical computation, and complex problem-solving strategies. The dataset is utilized to compare zero-shot and RCI (Retrieval-Augmented Chain-of-Thought) prompting methods, examining performance improvements over standard prompting techniques. It includes a variety of problem types, from simple to multi-step, to study diverse solution methods and mathematical reasoning.
DialogStudio	https://doi.org/10.48550/arXiv.2409.03215 (2024)	https://doi.org/10.48550/arXiv.2307.10172 (2023)	The DialogStudio dataset is used to enhance the instruction capability of xLAM by integrating diverse instruction-tuning datasets, specifically focusing on conversational AI. This integration improves the model's ability to handle and respond to complex instructions in a conversational context, thereby advancing the development of more sophisticated conversational agents.
E2ENLG	https://doi.org/10.18653/v1/2022.naacl-main.57 (2021)	https://doi.org/10.18653/v1/W18-6539 (2018)	The E2ENLG dataset is used for few-shot learning experiments, where models are fine-tuned with varying percentages (0.1%, 0.5%, 1%, and 5%) of training instances. This approach helps researchers evaluate the effectiveness of small amounts of data in improving model performance, focusing on the impact of limited training samples on model accuracy and generalization.
PrOntoQA	https://doi.org/10.48550/arXiv.2305.12295 (2023), https://doi.org/10.18653/v1/2024.acl-long.531 (2023)	https://doi.org/10.48550/arXiv.2210.01240 (2022)	PrOntoQA is used to evaluate the reasoning capabilities of language models, particularly in formal logic and ontology-based questions. It assesses the ability of models to generate and verify logical proofs, handle first-order logic inference tasks, and perform synthetic question answering. The dataset emphasizes step-by-step and chain-of-thought reasoning, enabling researchers to systematically analyze deductive reasoning and logical inference in language models.
MultiArith	https://doi.org/10.48550/arXiv.2305.04091 (2023), https://doi.org/10.48550/arXiv.2401.04925 (2024)	https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea (2021)	The MultiArith dataset is primarily used to evaluate and assess various aspects of mathematical and logical reasoning, including commonsense reasoning, strategic thinking, and multi-step problem-solving. It focuses on solving complex arithmetic and algebraic word problems, emphasizing step-by-step reasoning, solution accuracy, and the ability to generate rationales. The dataset is employed to test both single-equation and multi-equation problem-solving skills, as well as the comprehension and logical deduction required for story-based math problems. This enables researchers to assess models' capabilities in handling real-world, context-rich mathematical challenges.
TDW-MAP	https://doi.org/10.48550/arXiv.2405.11106 (2024)	https://www.semanticscholar.org/paper/c562477737cc35e08d5a84aef01163ee4652d796 (2020)	The TDW-MAP dataset is used to evaluate multi-agent planning capabilities, focusing on decision-making, communication, and memory in complex and dynamic environments. It is applied in simulated settings to assess how agents make decisions, communicate, and retain information, enabling researchers to analyze and improve multi-agent systems in intricate scenarios.
StrategyQA	https://doi.org/10.48550/arXiv.2310.05915 (2023), https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c (2024)	https://doi.org/10.1162/tacl_a_00370 (2021)	The StrategyQA dataset is used to evaluate models' implicit reasoning capabilities, particularly in multi-step reasoning processes. It assesses models on various symbolic reasoning tasks, including memory, sequence reconstruction, probability, logical operations, and string manipulation. The dataset also tests models on math word problems and strategic questions, emphasizing commonsense knowledge and diverse solution methods. This enables researchers to comprehensively analyze the reasoning and problem-solving abilities of language models.
ToolAlpaca corpus	https://doi.org/10.48550/arXiv.2306.05301 (2023), https://doi.org/10.18653/v1/2024.findings-acl.928 (2024)	https://doi.org/10.48550/arXiv.2306.05301 (2023)	The ToolAlpaca corpus is primarily used to evaluate and enhance the tool usage and reasoning capabilities of large language models (LLMs). It contains a diverse set of cases in English, ranging from 120 to 12,657, which are used to assess LLMs' performance in interacting with APIs, following instructions, and executing task-oriented dialogues. The dataset supports both training and evaluation, focusing on accuracy, generalized tool learning, and planning capabilities.
Room-to-Room (R2R)	https://doi.org/10.1109/IROS58592.2024.10801822 (2024)	https://doi.org/10.1109/CVPR.2018.00387 (2017)	The Room-to-Room (R2R) dataset is used to develop and evaluate vision-and-language navigation models, specifically focusing on interpreting visually-grounded instructions in real indoor environments. It serves as a benchmark for Vision-and-Language Navigation (VLN), enabling researchers to test and improve models' ability to follow navigation instructions in complex, real-world settings.
Freebase	https://doi.org/10.48550/arXiv.2310.01061 (2023)	https://doi.org/10.1145/3437963.3441753 (2021)	The Freebase dataset is used to construct subgraphs for multi-hop question answering, focusing on extracting relevant triples within the maximum reasoning hops of question entities in WebQSP and CWQ. It is employed to improve the performance of models on multi-hop question answering tasks, specifically enhancing Hits@1 and F1 scores compared to state-of-the-art models like UniKGQA.
CWQ	https://doi.org/10.48550/arXiv.2310.01061 (2023)	https://doi.org/10.1145/1376616.1376746 (2008)	The CWQ dataset is used to evaluate and enhance complex question answering capabilities, particularly focusing on reasoning over Freebase knowledge graphs and handling complex, compositional web questions. It is employed to fine-tune models like LLaMA2-Chat-7B, improving their ability to perform multi-hop reasoning and answer intricate queries from web sources. This dataset enables researchers to assess and refine the reasoning and compositional skills of language models in real-world contexts.
API-Bank	https://doi.org/10.18653/v1/2024.findings-acl.928 (2024), https://doi.org/10.48550/arXiv.2402.15491 (2024)	https://doi.org/10.48550/arXiv.2311.16867 (2023)	The API-Bank dataset is used to evaluate large language models (LLMs) on their tool usage and reasoning capabilities, particularly in interacting with APIs. It contains a large number of cases (ranging from 120 to 12,657) in English, which are used to assess generalized tool learning, accuracy, and performance on API-related tasks. The dataset is employed to compare different LLMs, such as FLAN-T5-XXL, MPT-30B, Falcon-40B, and StarCoder-15B, focusing on their effectiveness in handling tool-related questions and API interactions.
Open Assistant crowdsourced annotated dialogue corpus	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://doi.org/10.18653/v1/2022.emnlp-main.340 (2022)	The Open Assistant crowdsourced annotated dialogue corpus is used to enhance various capabilities in the Lemur-Chat model, including complex coding tasks, chain of thought reasoning, and handling diverse conversational scenarios. It incorporates human-written tasks, logical problem-solving, and real user interactions, improving code generation, dialogue management, and overall conversational proficiency. The dataset's annotated dialogues are crucial for both training and evaluation, enabling the model to better understand and respond to realistic user inputs.
VirtualHome dataset	https://doi.org/10.48550/arXiv.2409.01806 (2024), https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8 (2022)	https://doi.org/10.1109/CVPR.2018.00886 (2018)	The VirtualHome dataset is used to simulate household activities through programs in eight different household scenes, primarily to study the planning capabilities of large language models (LLMs). It compares human-annotated action plans with those generated by models like GPT-3, focusing on task execution and planning accuracy. This dataset enables researchers to evaluate and enhance the planning and execution skills of LLMs in realistic domestic settings.
CoScript	https://doi.org/10.48550/arXiv.2409.01806 (2024), https://doi.org/10.48550/arXiv.2305.05252 (2023)	https://doi.org/10.48550/arXiv.2305.05252 (2023)	The CoScript dataset, comprising 55,000 scripts, is used to train and evaluate models for constrained language planning, particularly focusing on goal-oriented scripts. It enables researchers to study script knowledge in large language models and demonstrates the effectiveness of smaller models like T5, which can outperform larger models in specific tasks. This dataset facilitates the assessment of model performance in generating coherent and contextually appropriate scripts.
bAbI datasets	https://doi.org/10.48550/arXiv.2305.10626 (2023), https://doi.org/10.48550/arXiv.2307.07696 (2023)	https://www.semanticscholar.org/paper/abb33d75dc297993fcc3fb75e0f4498f413eb4f6 (2015)	The bAbI datasets are used to evaluate and compare models on various cognitive tasks, including embodied knowledge, logic reasoning, and linguistic knowledge, with a focus on assessing planning capabilities. These datasets enable researchers to test and benchmark model performance, particularly in question answering and reasoning tasks, against baselines like GPT-3 and other state-of-the-art models.
PROOF WRITER	https://doi.org/10.48550/arXiv.2210.01240 (2022)	https://doi.org/10.18653/v1/2021.findings-acl.317 (2020)	The PROOF WRITER dataset is used to test reasoning abilities in QA systems, focusing on natural language reasoning with first-order logic. It generates examples from an ontology with unique proofs, including natural language implications and abductive statements. This dataset enables researchers to evaluate the reasoning and planning capabilities of LLMs by generating and testing complex logical statements and implications.
ActivityPrograms dataset	https://doi.org/10.48550/arXiv.2304.08587 (2023), https://doi.org/10.1007/s10514-023-10133-5 (2023)	https://doi.org/10.1109/CVPR.2018.00886 (2018)	The ActivityPrograms dataset is used to simulate and evaluate household activities, particularly dining tasks, in virtual environments. It provides a structured set of 12 everyday activities to assess the planning capabilities of models. The dataset enhances the realism of simulations, enabling systematic evaluations of model performance in executing complex, real-world tasks.
ProScript	https://doi.org/10.48550/arXiv.2402.02805 (2024)	https://doi.org/10.18653/V1/2020.EMNLP-MAIN.374 (2020)	The ProScript dataset is used to collect and enhance human-annotated partial-order plans, serving as a base for deriving planning tasks focused on goal-oriented steps and temporal ordering. It supplements WikiHow data, providing rich, step-by-step instructions for training and evaluating LLMs on complex planning tasks. The dataset supports research on LLM planning capabilities by combining human and LLM annotations, assessing step dependencies, and generating natural language prompts. It includes asynchronous instances with complete time annotations, contributing to a collection of 1.6K instances for analysis.
CommonsenseQA	https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c (2024), https://doi.org/10.48550/arXiv.2303.17491 (2023)	https://doi.org/10.18653/v1/N19-1421 (2019)	The CommonsenseQA dataset is primarily used to evaluate the commonsense reasoning capabilities of language models, focusing on questions that require understanding of everyday situations and knowledge. It is also utilized to assess models' abilities in solving few-hop math word problems, emphasizing multi-step reasoning, basic arithmetic, algebraic reasoning, and procedural extension capabilities. The dataset enables researchers to test both symbolic reasoning and strategic question-solving, highlighting the models' proficiency in handling complex, real-world scenarios.
P3	https://doi.org/10.18653/v1/2024.acl-srw.15 (2023)	https://www.semanticscholar.org/paper/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca (2021)	Dataset P3 is used as an NLP task instruction dataset, reformatted for a wide range of downstream tasks using diverse human-written templates. It is employed to assess the model's capability to handle general-purpose instructions and evaluate its performance on various linguistic tasks. This dataset enables researchers to test and improve the model's versatility in handling different instructional and linguistic challenges.
Language-Table	https://doi.org/10.48550/arXiv.2303.03378 (2023)	https://doi.org/10.48550/arXiv.2210.06407 (2022)	The Language-Table dataset is primarily used to study multi-object tabletop pushing environments and robotic manipulation tasks, integrating language and physical actions. It is employed to simulate and evaluate complex dynamics and large language cardinality in robotic interactions. Additionally, it serves as a textual corpus to train language models, enhancing their coherence and contextual relevance. In some studies, it contributes 3.1% of the data for training or evaluation, particularly for robotic manipulation tasks.
AQuA	https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691 (2022)	https://doi.org/10.18653/v1/P17-1015 (2017)	The AQuA dataset is primarily used to evaluate models on algebraic word problems, focusing on complex reasoning, problem-solving, and explanation capabilities. It serves as a testbed for few-shot learning, particularly with 8 shots, and is also used to assess performance in hybrid tabular and textual content, especially in financial contexts. The dataset emphasizes the integration of diverse data types and the handling of complex queries, enabling researchers to test and improve models' reasoning and calculation abilities.
NetHack	https://doi.org/10.48550/arXiv.2307.11922 (2023)	https://www.semanticscholar.org/paper/822bdb6e8c39e272ebfee127666e032bd3aa0107 (2020)	The NetHack dataset is used to evaluate the success rate of reinforcement learning algorithms, such as BLINDER and other baselines, in generalizing to unseen tasks within the NetHack environment. This evaluation focuses on the algorithms' ability to adapt and perform in complex, dynamic scenarios, providing insights into their generalization capabilities and effectiveness in reinforcement learning settings.
PlanBench	https://doi.org/10.48550/arXiv.2305.19165 (2023)	https://doi.org/10.48550/arXiv.2210.13312 (2022)	The PlanBench dataset is used to evaluate large language models (LLMs) on their planning and reasoning capabilities, particularly focusing on the brittleness and unreliability of these models in handling changes. This dataset enables researchers to assess how well LLMs can reason about dynamic scenarios and plan effectively, providing insights into the limitations and robustness of current LLMs in these tasks.
GrailQA	https://doi.org/10.48550/arXiv.2308.03688 (2023)	https://doi.org/10.18653/v1/D16-1054 (2016)	The GrailQA dataset is used to evaluate the long-term planning capabilities of large language models (LLMs), specifically focusing on complex question answering tasks sourced from Freebase. Researchers compile this dataset to assess how effectively LLMs can handle intricate queries, emphasizing the models' ability to plan and execute multi-step reasoning processes. This dataset enables the examination of LLMs' performance in retrieving and integrating information from structured knowledge bases.
WikiSQL	https://doi.org/10.48550/arXiv.2308.03688 (2023)	https://doi.org/10.18653/v1/P17-1167 (2017)	The WikiSQL dataset is used to acquire source queries and databases, enhancing the diversity of instructions and data for training models. It contributes to the development of natural language processing models by providing a rich set of SQL queries and corresponding database entries, which helps in improving model performance on query generation and understanding tasks.
TheoremQA	https://doi.org/10.48550/arXiv.2309.10691 (2023)	https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea (2021)	TheoremQA is used to evaluate the planning capabilities and reasoning skills of language models, particularly in solving complex mathematical problems and theorem proving. It focuses on logical reasoning, step-by-step solutions, and formal logic. The dataset is also employed for multi-hop question answering, interactive learning in embodied environments, and assessing knowledge problem-solving across various domains. It emphasizes diverse and explainable reasoning processes, aligning text with actions, and integrating information from multiple sources.
SayCan dataset	https://doi.org/10.48550/arXiv.2301.13379 (2023)	https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc (2022)	The SayCan dataset is used to ground language in robotic affordances, focusing on how robots interpret and execute commands in physical environments. It enables researchers to investigate the interaction between natural language processing and robotic action, specifically addressing the challenges in command interpretation and execution. This dataset facilitates the development and evaluation of models that enhance the ability of robots to understand and perform tasks based on human instructions.
ToolACE	https://doi.org/10.48550/arXiv.2502.06589 (2025)	https://doi.org/10.48550/arXiv.2409.00920 (2024)	The ToolACE dataset is used to evaluate and enhance the planning capabilities of LLMs in complex agent environments, focusing on instruction-following and task completion. It employs single-tool conversations and function-calling tasks to improve the model's ability to execute instructions effectively, particularly in diverse and complex scenarios. This dataset supports research aimed at refining LLMs' function-calling and instruction-completion abilities, enabling more efficient and accurate task execution in agent-based settings.
LAION-400M	https://doi.org/10.48550/arXiv.2305.15021 (2023)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The LAION-400M dataset is used for pre-training and evaluating models in generating accurate image captions. It involves filtering and re-captioning 491 thousand image-text pairs using BLIP-2, which enhances the model's performance in caption generation. This dataset supports research focused on improving and evaluating the planning capabilities of language models through image caption tasks, ensuring diverse and accurate captions.
Meta-World	https://doi.org/10.48550/arXiv.2305.15021 (2023)	https://www.semanticscholar.org/paper/8c54e8575e7c17a4097838305915e6e7b00fd4af (2019)	The Meta-World dataset is used to evaluate embodied control tasks, particularly focusing on the success rate of long-horizon tasks. Researchers employ imitation and reinforcement learning methodologies to assess performance. This dataset enables the study of complex, sequential decision-making processes in robotic control, providing a benchmark for comparing different algorithms and approaches.
GPT-4 generated 52K English instruction-following dataset	https://doi.org/10.48550/arXiv.2305.15021 (2023)	https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75 (2023)	The GPT-4 generated 52K English instruction-following dataset is used to fine-tune pre-trained LLaMA-7B models, specifically to enhance their instruction-following capabilities. This large, synthetically generated dataset improves the model's performance on English instruction-following tasks by providing additional training data. The dataset's extensive size and synthetic nature enable researchers to effectively augment the model's ability to follow complex instructions.
EgoCOT	https://doi.org/10.48550/arXiv.2305.15021 (2023)	https://doi.org/10.48550/arXiv.2303.03378 (2023)	The EgoCOT dataset is used to build a scalable dataset for embodied multimodal language models, specifically focusing on part-level interactions and sub-goal sequences in robotic tasks. This dataset enables researchers to develop and evaluate models that can understand and execute complex, multi-step actions in physical environments, enhancing the capabilities of robotic systems through detailed interaction data.
DROP	https://doi.org/10.48550/arXiv.2205.10625 (2022)	https://www.semanticscholar.org/paper/b35b0a19425129432eefc21c3a9a1825f328c4b1 (2020)	The DROP dataset is used to assess models' discrete reasoning capabilities over paragraphs, particularly in question answering tasks that involve numerical reasoning and multi-step operations. It also evaluates compositional generalization in sequence-to-sequence models, testing their ability to handle unseen command-action pairs in navigation tasks. This dataset enables researchers to measure and improve model performance in complex reasoning and generalization scenarios.
AI2 Reasoning Challenge	https://doi.org/10.48550/arXiv.2411.19443 (2024)	https://www.semanticscholar.org/paper/88bb0a28bb58d847183ec505dda89b63771bb495 (2018)	The AI2 Reasoning Challenge dataset is used to evaluate the performance of Auto-RAG on general reasoning tasks, particularly focusing on complex question answering and problem-solving capabilities. This dataset enables researchers to assess how well models can handle intricate reasoning problems, providing insights into their effectiveness in real-world applications.
Natural Questions	https://doi.org/10.48550/arXiv.2411.19443 (2024)	https://doi.org/10.1162/tacl_a_00276 (2019)	The Natural Questions dataset is used to synthesize reasoning-based instructions for training models like Auto-RAG, specifically focusing on multi-hop question answering tasks. It assesses the model's ability to reason across multiple documents to answer complex questions, emphasizing the need for advanced reasoning capabilities in natural language processing.
RobotHow	https://www.semanticscholar.org/paper/418085c9726669bf53f3d66e0018f2b08ffc4ce6 (2022)	https://doi.org/10.1109/CVPR.2018.00886 (2018)	The RobotHow dataset is used to conduct experiments on completing daily household goals, focusing on human-generated instructions and planning capabilities. It serves as a knowledge base for common household tasks, enabling zero-shot experiments on procedural information. The dataset enhances the planning capabilities of AI models by simulating activities in the VirtualHome environment and testing the ability to understand and generate instructions without prior training.
ConceptNet	https://www.semanticscholar.org/paper/418085c9726669bf53f3d66e0018f2b08ffc4ce6 (2022)	https://doi.org/10.1609/aaai.v31i1.11164 (2016)	ConceptNet is used as a graph-based knowledge base, representing concepts and commonsense relations. It supports the development of planning capabilities in language models by providing structured external knowledge. Researchers sample local subgraphs for specific entities, enhancing the model's understanding through concept nodes and their relationships. This dataset enables the integration of commonsense reasoning into AI systems, improving their ability to plan and reason.
L-Eval	https://doi.org/10.48550/arXiv.2311.04939 (2023)	https://doi.org/10.48550/arXiv.2307.11088 (2023)	The L-Eval dataset is used for the standardized evaluation of long-context language models. It involves re-annotated data and instructions from similar public datasets to ensure high-quality evaluations. This dataset enables researchers to assess the performance and capabilities of language models in handling extended contexts, providing a benchmark for comparing different models.
CHALET	https://doi.org/10.48550/arXiv.2305.10626 (2023)	https://www.semanticscholar.org/paper/b5a30dc9bb96f686da830b93033569475688424d (2019)	The CHALET dataset is used to train and evaluate AI agents in navigation and interaction tasks within 3D virtual indoor environments, such as houses and kitchens. It provides a platform for benchmarking task-oriented learning, emphasizing the development of planning and navigation skills in realistic, interactive scenarios. This dataset enables researchers to focus on the training and evaluation of household agents, enhancing their ability to perform complex tasks in simulated home settings.
VRKitchen	https://doi.org/10.48550/arXiv.2305.10626 (2023)	https://www.semanticscholar.org/paper/b58e80ad8c6e6844c41535080ccbdef06bce3b6e (2018)	The VRKitchen dataset is used to train and evaluate AI agents in navigation and interaction tasks within 3D virtual kitchen and household environments. It provides a platform for benchmarking and developing planning and navigation skills, focusing on task-oriented learning and realistic scenarios. This dataset enables researchers to assess agent performance in complex, interactive settings, emphasizing the development of robust planning capabilities in domestic contexts.
Pile	https://doi.org/10.48550/arXiv.2305.10626 (2023)	https://www.semanticscholar.org/paper/db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e (2020)	The Pile dataset is used to evaluate the performance of language models, particularly GPT-Neo and GPT-J, by measuring perplexity on a diverse text corpus. Researchers sample subsets of the test set, typically 5000 examples, to assess language modeling capabilities and ensure the preservation of generality and improved performance on downstream tasks. The dataset's diversity supports comprehensive evaluation of model performance.
P bw 2	https://doi.org/10.1609/icaps.v30i1.6754 (2019)	https://doi.org/10.1016/S0004-3702(00)00079-5 (2001)	The 'P bw 2' dataset is used to evaluate the STRIPS-HGN network on test problems within the Blocksworld domain. It focuses on assessing planning capabilities and performance in solving specific planning tasks. This dataset enables researchers to measure the effectiveness of the STRIPS-HGN network in handling complex planning scenarios, providing insights into its strengths and limitations.
MineDojo	https://doi.org/10.48550/arXiv.2305.17144 (2023)	https://doi.org/10.48550/arXiv.2206.08853 (2022)	The MineDojo dataset is used to build open-ended embodied agents with internet-scale knowledge, focusing on decomposing and extracting structured actions. It specifically enhances agent capabilities by teaching them crafting and smelting recipes. This dataset enables researchers to develop agents that can perform complex tasks in dynamic environments, leveraging large-scale data to improve their decision-making and action execution.
PEB	https://doi.org/10.1007/s11704-024-40231-1 (2023)	https://doi.org/10.48550/arXiv.2308.06782 (2023)	The PEB dataset is used to assess the planning and execution capabilities of LLM-based agents in penetration testing scenarios. It evaluates these agents across diverse targets from leading platforms, focusing on their ability to plan and execute complex tasks. This dataset enables researchers to systematically analyze and compare the performance of LLMs in cybersecurity contexts, providing insights into their effectiveness and limitations.
RoboTHOR	https://doi.org/10.1109/LRA.2023.3346800 (2023)	https://doi.org/10.1109/CVPR42600.2020.00323 (2020)	The RoboTHOR dataset is used as a simulation environment to test and validate the planning capabilities of large language models (LLMs) in embodied AI tasks. Researchers employ the validation set to conduct experiments, assessing how effectively LLMs can plan and execute actions in simulated environments. This dataset enables the evaluation of LLMs' performance in complex, interactive settings, providing insights into their ability to handle real-world tasks.
SingleEq	https://doi.org/10.48550/arXiv.2305.04091 (2023)	https://doi.org/10.1162/tacl_a_00160 (2015)	The SingleEq dataset is used to assess models on single-equation grade-school algebra word problems, emphasizing multi-step reasoning, symbolic manipulation, and numerical accuracy. It is employed in few-shot learning experiments, testing the model's ability to generalize from limited examples. The dataset evaluates various reasoning skills, including strategic thinking, pattern recognition, and commonsense inference, using a mix of arithmetic problems, algebraic word problems, and letter-based puzzles. It focuses on tasks that require parsing, solving equations, and generating natural language rationales, often involving real-world scenarios and multiple math operations over nonnegative rational numbers.
MAWPS	https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c (2024)	https://doi.org/10.1162/tacl_a_00370 (2021)	The MAWPS dataset is used to evaluate Chain-of-Thought (CoT) reasoning in various types of math word problems, ranging from basic arithmetic to complex multi-step calculations. It assesses the ability to solve problems requiring logical reasoning, multi-step calculations, and everyday knowledge. The dataset supports research in strategic problem-solving and commonsense reasoning, focusing on the accuracy and coherence of reasoning processes in both simple and complex scenarios.
Habitat-Matterport 3D	https://doi.org/10.48550/arXiv.2405.11106 (2024)	https://doi.org/10.1109/ICRA57147.2024.10610855 (2023)	The Habitat-Matterport 3D dataset is used to evaluate dialectic multi-robot collaboration and semantic navigation tasks with large language models. It provides realistic 3D environments to test motion planning and decision-making capabilities, enabling researchers to assess how LLMs can effectively navigate and collaborate in complex, real-world settings.
TDW-MAT	https://doi.org/10.48550/arXiv.2405.11106 (2024)	https://doi.org/10.18653/v1/2023.emnlp-main.13 (2023)	The TDW-MAT dataset is used to evaluate multi-agent planning capabilities, particularly focusing on decision-making and theory of mind in complex environments. It assesses how agents make decisions and communicate in collaborative tasks, enabling researchers to analyze and improve multi-agent systems' performance in intricate scenarios.
MineRL	https://doi.org/10.1109/TPAMI.2024.3511593 (2023)	https://doi.org/10.24963/ijcai.2019/339 (2019)	The MineRL dataset is used to set up the Minecraft environment, providing a large-scale dataset of Minecraft demonstrations for training and evaluation. It serves as a benchmark for reinforcement learning algorithms, enabling researchers to evaluate agent performance in complex, interactive tasks within the Minecraft environment.
G-PlanET	https://doi.org/10.3233/FAIA240916 (2024)	https://doi.org/10.1609/aaai.v37i11.26549 (2022)	The G-PlanET dataset is used to represent structured inputs for language models (LLMs) in task planning, particularly for grounding these models in 3D environments. It is utilized to enhance and evaluate the planning capabilities of LLMs in embodied tasks. The dataset includes simulation environments in a tabular format, enabling researchers to compare different methods and assess their performance in grounded planning scenarios.
3D Scene Graphs	https://doi.org/10.3233/FAIA240916 (2024)	https://doi.org/10.48550/arXiv.2307.06135 (2023)	The 3D Scene Graphs dataset is used to represent structured inputs for language models (LLMs) in task planning, specifically to ground these models in 3D environments. This enhances the planning capabilities of LLMs by providing them with context-rich, structured data that reflects real-world scenarios. The dataset enables researchers to explore how LLMs can better understand and interact with complex 3D scenes, improving their ability to plan and execute tasks in these environments.
Vicuna	https://doi.org/10.48550/arXiv.2306.02707 (2023)	https://doi.org/10.48550/arXiv.2304.06364 (2023)	The Vicuna dataset is used to evaluate Orca's performance in natural language understanding, generation, and reasoning. It assesses response quality, coherence, and problem-solving skills across diverse prompts, scenarios, and challenging academic tasks. This dataset enables researchers to comprehensively test and analyze Orca's capabilities in handling complex linguistic and cognitive challenges.
WizardLM	https://doi.org/10.48550/arXiv.2306.02707 (2023)	https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881 (2022)	The WizardLM dataset is used to evaluate Orca's performance across various tasks, focusing on reasoning, problem-solving, response quality, and coherence. It assesses Orca's natural language understanding and generation through diverse prompts and scenarios, emphasizing academic tasks, puzzles, and generative abilities. This dataset enables researchers to comprehensively test and analyze the model's capabilities in complex linguistic and cognitive tasks.
EPIC-KITCHENS	https://doi.org/10.48550/arXiv.2406.09455 (2024)	https://www.semanticscholar.org/paper/fc50c9392fd23b6c88915177c6ae904a498aacea (2018)	The EPIC-KITCHENS dataset is used to collect video-action pairs focusing on egocentric kitchen activities. It enhances the understanding of complex, sequential actions through detailed visual data. Researchers employ this dataset to analyze and model human behavior in kitchen environments, leveraging its rich, first-person perspective videos to improve action recognition and understanding in real-world settings.
LoCoMo	https://doi.org/10.48550/arXiv.2410.10813 (2024)	https://doi.org/10.48550/arXiv.2402.17753 (2024)	The LoCoMo dataset is used to evaluate long-term conversational memory in LLM agents, specifically focusing on reasoning types such as single-hop, multi-hop, temporal, commonsense, world knowledge, and adversarial reasoning. This dataset enables researchers to assess the agents' ability to maintain and utilize context over extended conversations, providing insights into their reasoning capabilities and memory retention.
4nums.com	https://doi.org/10.48550/arXiv.2308.10379 (2023)	https://doi.org/10.48550/arXiv.2305.10601 (2023)	The 4nums.com dataset is used to evaluate the problem-solving capabilities of large language models by sourcing games ranked by relative difficulty. Specifically, it focuses on a subset of 100 games (indices 901-1000) from a larger set of 1362 games. This subset is selected to assess the models' performance on more challenging problems, enabling researchers to analyze their planning and reasoning abilities.
TravelPlanner	https://doi.org/10.48550/arXiv.2404.11891 (2024)	https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04 (2023)	The TravelPlanner dataset is used to evaluate the planning capabilities of large language models (LLMs) in U.S. domestic travel scenarios. It demonstrates the effectiveness and limitations of LLMs like GPT-4 in generating travel plans, highlighting their low success rates without pre-collected information. This dataset enables researchers to assess and improve the practical planning abilities of LLMs in real-world contexts.
MetaQA-3hop	https://doi.org/10.48550/arXiv.2310.01061 (2023)	https://doi.org/10.1609/aaai.v32i1.12057 (2017)	The MetaQA-3hop dataset is used to evaluate multi-hop reasoning capabilities in question answering systems, particularly focusing on complex queries over the Wiki-Movies knowledge graph. It enables researchers to assess how effectively these systems can integrate information from multiple sources to answer intricate questions, thereby advancing the field of natural language processing and knowledge retrieval.
Live-CodeBench	https://doi.org/10.48550/arXiv.2504.16129 (2025)	https://doi.org/10.48550/arXiv.2403.07974 (2024)	Live-CodeBench is used to evaluate large language models in coding tasks, ensuring a contamination-free and holistic assessment of model performance. The dataset supports research by providing a robust framework to measure the effectiveness of these models in coding scenarios, focusing on their ability to generate accurate and functional code without prior exposure to similar problems.
CSQA	https://doi.org/10.48550/arXiv.2304.11657 (2023)	https://doi.org/10.3115/v1/D14-1058 (2014)	The CSQA dataset is primarily used to assess the problem-solving and arithmetic reasoning capabilities of models, particularly in solving algebraic and arithmetic word problems. It emphasizes logical and mathematical reasoning, the role of contextual information, and the effectiveness of exemplars in improving model performance. The dataset is often used alongside GSM8K to evaluate model robustness and generalization across diverse and complex arithmetic tasks, including single-equation, addition, subtraction, and story problems.
ELI5	https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d (2022)	https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22 (2021)	The ELI5 dataset is used to train models to answer complex questions in simple terms, leveraging natural language processing techniques to infer answers from web pages. This focuses on enhancing the model's ability to understand and simplify complex information, making it accessible to a broader audience. The dataset's emphasis on simplification and inference supports research in improving the clarity and comprehensibility of AI-generated responses.
Household	https://doi.org/10.48550/arXiv.2503.18971 (2025)	https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb (2020)	The 'Household' dataset is used to simulate and evaluate planning capabilities in a typical household environment. It focuses on generating textual observations using PDDL semantics, enabling researchers to assess the effectiveness of planning algorithms in realistic domestic settings. This dataset supports the development and testing of automated planning systems by providing a structured representation of household tasks and environments.
SciBench	https://doi.org/10.48550/arXiv.2405.19425 (2024)	https://doi.org/10.1145/3383458 (2020)	The SciBench dataset is used to evaluate the scientific and mathematical problem-solving abilities of large language models, particularly in college-level physics and chemistry. It measures performance across a variety of math problems, assessing the models' capabilities in these domains. This dataset enables researchers to systematically test and compare the problem-solving skills of different models, providing insights into their strengths and limitations in scientific contexts.
MiniWobChat	https://www.semanticscholar.org/paper/9ea0757c750ab1222a7442d3485a74d1c526b04c (2023)	https://doi.org/10.48550/arXiv.2303.17491 (2023)	The MiniWobChat dataset is used to evaluate the planning capabilities of language models across various tasks. It provides a comprehensive analysis of model performance and task difficulty, enabling researchers to assess how well language models can plan and execute complex sequences of actions. This dataset supports the examination of specific planning challenges and helps identify areas where models excel or struggle.
ScienceQA	https://doi.org/10.48550/arXiv.2304.09842 (2023)	https://doi.org/10.48550/arXiv.2209.14610 (2022)	The ScienceQA dataset is used to evaluate the performance of language models, particularly GPT-4 and Chameleon, on mathematical reasoning and science-related questions. It focuses on the models' abilities to understand, reason, and solve problems presented in various formats, including tabular contexts and semi-structured tasks. The dataset emphasizes logical reasoning, numerical computation, and adaptability in handling complex scientific and mathematical problems, enabling researchers to measure and compare model performance on these specific tasks.
MNLI	https://doi.org/10.48550/arXiv.2310.13227 (2023)	https://doi.org/10.18653/v1/N18-1101 (2017)	The MNLI dataset is used to fine-tune DeBERTa-large models for natural language inference tasks, specifically to determine semantic entailment between generated actions. This involves training models to understand and infer relationships between textual statements, enhancing their ability to process and analyze complex linguistic data.
MINT-TheoremQA	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://doi.org/10.48550/arXiv.2305.12524 (2023)	The MINT-TheoremQA dataset is used to evaluate large language models (LLMs) in theorem-driven question answering, focusing on multi-step reasoning, logical proofs, and mathematical problem-solving. It assesses LLMs' abilities in multi-turn interactions, including knowledge searches and Python calculations, and tests their reasoning capabilities across various subjects, including multi-hop reasoning over multiple paragraphs. The dataset also evaluates decision-making and planning in text-based games.
ToolBench	https://doi.org/10.48550/arXiv.2310.13227 (2023)	https://doi.org/10.48550/arXiv.2303.17580 (2023)	The ToolBench dataset is used to evaluate models' tool-use capabilities in simulated environments and to assess their reasoning abilities, particularly in solving complex arithmetic problems. It focuses on the model's interaction with tools and its performance on specific tasks, providing insights into the effectiveness of these interactions and reasoning processes.
ToolQA	https://doi.org/10.48550/arXiv.2310.13227 (2023)		The ToolQA dataset is used to evaluate the problem-solving capabilities of language models, particularly focusing on their performance on challenging questions. Researchers employ the dataset to conduct experiments on both the full set of questions and subsets of hard questions, specifically to assess the model's planning capabilities in tackling complex problems. This dataset enables detailed analysis of model performance in handling intricate tasks, providing insights into the effectiveness of language models in problem-solving scenarios.
Movie Recommendation	https://doi.org/10.48550/arXiv.2312.04511 (2023)	https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881 (2022)	The Movie Recommendation dataset is used to evaluate LLMCompiler's performance on embarrassingly parallel patterns, specifically focusing on achieving speedup and reducing computational costs compared to the ReAct method. This dataset enables researchers to benchmark and optimize large language model compilers for efficiency in parallel processing tasks.
AITW	https://doi.org/10.1145/3637528.3671650 (2024)	https://doi.org/10.48550/arXiv.2307.10088 (2023)	The AITW dataset is used to provide human demonstrations of device interactions through static images, supporting research into the planning capabilities of LLMs. This dataset enables researchers to evaluate how LLMs can understand and predict human interaction sequences with devices, enhancing their ability to generate coherent and contextually appropriate plans.
The Stack	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://doi.org/10.48550/arXiv.2211.15533 (2022)	The Stack is used to construct a code-centric corpus with 90B tokens and a 10:1 code-to-text ratio, which enhances coding ability while maintaining natural language performance. It serves as the foundation for the code component in research, providing permissively licensed source codes from GitHub to develop and evaluate models. This dataset supports the creation of large-scale, high-quality training data for improving model performance in both coding and natural language tasks.
InterCode-CTF	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://doi.org/10.48550/arXiv.2307.13854 (2023)	The InterCode-CTF dataset is used to evaluate the planning capabilities of large language models (LLMs) in various environments. It assesses LLM performance in realistic web environments, physical environments, and digital environments, focusing on tasks such as autonomous agent building, navigation, embodied agent interactions, and complex interactive tasks. The dataset emphasizes fast thinking processes and real-world interaction challenges, enabling researchers to test and improve LLMs' planning and execution skills in diverse scenarios.
Orca data	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://doi.org/10.48550/arXiv.2306.02707 (2023)	The Orca data dataset is used to enhance the training of language models, particularly in understanding real-world dialogue patterns through real user interactions and ChatGPT history records. It also improves code generation capabilities via progressive learning with evolving code samples and explanations. Additionally, the dataset trains models in chain-of-thought reasoning for complex human-written tasks, leveraging detailed explanation traces generated by GPT-4. These methodologies focus on refining the model's ability to handle nuanced and evolving tasks, thereby improving its performance in dialogue, code generation, and complex reasoning tasks.
Evol-Instruct-Code-80k-v1	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://www.semanticscholar.org/paper/454c8fef2957aa2fb13eb2c7a454393a2ee83805 (2023)	The Evol-Instruct-Code-80k-v1 dataset is used to train and evaluate large language models on complex coding tasks and real user interactions with ChatGPT. It focuses on enhancing code generation, instruction-following, and conversational response quality, particularly for Python programming tasks. The dataset employs evolutionary instruction tuning to improve model performance in generating correct, efficient, and contextually relevant code solutions and responses.
MINT-Reasoning	https://doi.org/10.48550/arXiv.2310.06830 (2023)		The MINT-Reasoning dataset is used to evaluate models' reasoning and code generation skills in multi-turn interactions, particularly focusing on the impact of GPT-4 feedback on accuracy, efficiency, and overall performance. This dataset enables researchers to assess how language feedback influences model behavior, providing insights into the effectiveness of interactive learning and feedback mechanisms.
TaskLAMA	https://doi.org/10.48550/arXiv.2409.01806 (2024)	https://doi.org/10.18653/V1/2021.NAACL-MAIN.217 (2021)	The TaskLAMA dataset is used for developing and evaluating models on complex task decomposition and organization. It comprises 1,612 annotated tasks, with a subset of 711 complex tasks specifically designed for model training and evaluation. This dataset enables researchers to assess how models handle intricate tasks, focusing on their ability to break down and organize steps effectively.
TO QA	https://doi.org/10.48550/arXiv.2409.01806 (2024)	https://doi.org/10.48550/arXiv.2210.01240 (2022)	The TO QA dataset is used to assess the reasoning capabilities of large language models (LLMs) by generating synthetic questions. It focuses on evaluating the LLMs' ability to perform chain-of-thought reasoning, providing a structured approach to test and analyze their cognitive processes. This dataset enables researchers to systematically evaluate and compare the reasoning skills of different LLMs.
MINT	https://doi.org/10.48550/arXiv.2410.05669 (2024)	https://doi.org/10.48550/arXiv.2309.10691 (2023)	The MINT dataset is used to evaluate the planning capabilities of large language models (LLMs) in multi-turn interactions involving tools and language feedback. It focuses on assessing LLM performance in complex scenarios, providing a structured environment to analyze how these models plan and execute tasks over multiple steps. This dataset enables researchers to test and improve the sequential reasoning and adaptive behavior of LLMs in interactive settings.
ByteSized32	https://doi.org/10.48550/arXiv.2409.01806 (2024)	https://doi.org/10.48550/arXiv.2305.14879 (2023)	The ByteSized32 dataset is used to generate task-specific world models expressed as text games, focusing on reasoning capabilities within a controlled environment. It involves 20k lines of Python code, enabling researchers to evaluate and enhance the reasoning abilities of language models in complex, text-based scenarios.
natural language based question answering style dataset	https://doi.org/10.48550/arXiv.2410.05669 (2024)	https://doi.org/10.18653/v1/2023.acl-long.255 (2023)	The natural language based question answering style dataset is used to evaluate large language models (LLMs) on reasoning tasks, specifically focusing on their planning capabilities. It assesses LLMs in areas such as projection, execution, planning, and goal recognition. This dataset enables researchers to systematically test and analyze the reasoning and planning abilities of LLMs through structured question-answer interactions.
PartNet	https://doi.org/10.1109/IROS58592.2024.10802746 (2023)	https://doi.org/10.1109/CVPR.2019.00100 (2018)	The PartNet dataset is used to generate data for microwave assets, enabling fine-grained and hierarchical part-level 3D object understanding. This supports research in the planning capabilities of large language models (LLMs), specifically by providing detailed 3D object data necessary for enhancing LLMs' ability to reason about and plan interactions with complex objects.
MQA-2H	https://doi.org/10.48550/arXiv.2403.08593 (2024)	https://doi.org/10.1609/aaai.v32i1.12057 (2017)	The MQA-2H dataset is used to evaluate and test complex multi-hop questions in the MetaQA dataset, focusing on advanced reasoning and query structures within knowledge graphs. It specifically addresses two-hop and three-hop questions, enabling researchers to assess the ability of models to handle more intricate relationships and multi-relational reasoning tasks.
WebQSP	https://doi.org/10.48550/arXiv.2403.08593 (2024)	https://doi.org/10.1145/1376616.1376746 (2008)	The WebQSP dataset is used to evaluate and enhance the performance of language models and knowledge graph question answering systems. It supports multi-hop reasoning and multi-constrained questions on Freebase, as well as single-constrained 2-hop reasoning paths. Researchers use it to retrieve similar relations for LLM-generated content, compare against structured knowledge bases, and assess complex reasoning capabilities.
competition-level mathematics problem set	https://doi.org/10.48550/arXiv.2402.02658 (2024)	https://doi.org/10.48550/arXiv.2305.20050 (2023)	The competition-level mathematics problem set is used to evaluate verifiers trained on annotated intermediate solutions, specifically assessing performance improvements over verifiers trained on final solutions. This dataset enables researchers to test and enhance the accuracy and reliability of verifiers in complex mathematical contexts, focusing on the effectiveness of intermediate solution annotations.
Geolife	https://doi.org/10.48550/arXiv.2308.15197 (2023)	https://www.semanticscholar.org/paper/27faf0300710a345ff878e3a09f3803e36a0a064 (2010)	The Geolife dataset is used to analyze the reasoning ability and interpretability of LLM-Mob, focusing on user, location, and trajectory data. Researchers employ this dataset to evaluate the model's performance and decision-making processes, leveraging its rich spatiotemporal information to understand how the model processes and reasons about mobility patterns.
CLEVR	https://www.semanticscholar.org/paper/c2c8482c713b94073f3d59895b373db4398ddfbb (2019)	https://doi.org/10.1109/CVPR.2017.215 (2016)	The CLEVR dataset is used to generate language statements for compositional language and elementary visual reasoning, and to inspire benchmark environments that integrate language understanding and control. It supports research on planning capabilities in LLMs by providing a foundation for tasks requiring both linguistic and visual reasoning.
static dataset of 600 three to five block problems	https://doi.org/10.48550/arXiv.2409.13373 (2024)	https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc (2022)	The static dataset of 600 three to five block problems is used to evaluate large language models on planning and reasoning tasks, specifically focusing on block manipulation. It assesses the models' ability to reason about changes in block configurations, providing a benchmark for their planning capabilities. The dataset's structured nature allows researchers to systematically test and compare model performance on these specific reasoning challenges.
Pororo-SV	https://doi.org/10.48550/arXiv.2309.15091 (2023)	https://doi.org/10.1109/ICCV.2017.83 (2017)	The Pororo-SV dataset is used to generate multi-scene videos from various inputs, including coreference-based scene prompts and lists of sentences describing events. It focuses on enhancing narrative coherence, coreference resolution, and coherent scene transitions. The dataset supports research in video generation, particularly in event captioning and scene generation, by providing structured data that facilitates the creation of narratively consistent videos.
MSR-VTT	https://doi.org/10.48550/arXiv.2309.15091 (2023)	https://www.semanticscholar.org/paper/d4dbdeb772105a7ee780543483c6142743b20298 (2019)	The MSR-VTT dataset is used to evaluate video generation quality and video-text alignment, focusing on visual quality and alignment scores. It is also employed to assess the performance of GPT models on video captioning tasks, specifically measuring FID, FVD, and CLIPSIM metrics to evaluate in-context learning skills and layout generation quality.
ActionBench-Direction	https://doi.org/10.48550/arXiv.2309.15091 (2023)	https://doi.org/10.1109/CVPR.2016.571 (2016)	The ActionBench-Direction dataset is utilized to evaluate open-domain video generation, focusing on a wide range of action categories and object dynamics through skill-based prompts. It enhances the assessment of directional actions in videos and bridges video and language by generating captions for diverse video content. This dataset enables comprehensive evaluation and improvement of video generation models.
AI2-THOR	https://doi.org/10.1109/IROS58592.2024.10802322 (2023)	https://www.semanticscholar.org/paper/89c8aad71433f7638d2e2c009e1ea20e039f832d (2017)	The AI2-THOR dataset is used to evaluate multi-agent task planning systems in a 3D simulation environment. It focuses on assessing the planning capabilities of large language models (LLMs) across a spectrum of tasks, from simple to complex. This dataset enables researchers to test and refine algorithms that enhance the coordination and decision-making processes of multiple agents in dynamic environments.
AVALONBENCH	https://doi.org/10.48550/arXiv.2402.01680 (2024)		The AVALONBENCH dataset is used to develop and evaluate advanced LLMs and multi-agent frameworks in the context of playing Resistance Avalon. It focuses on strategic decision-making and interaction, enabling researchers to assess the planning capabilities and collaborative performance of AI agents in complex, social-deduction games.
FanLang-9	https://doi.org/10.48550/arXiv.2402.02330 (2024)	https://doi.org/10.18653/v1/2022.acl-long.26 (2021)	The FanLang-9 dataset is used to fine-tune the ChatGLM-6B model, specifically to enhance its action prediction accuracy. This is achieved by training the model on a set of 100 test games, which helps in evaluating and improving the model's performance in predicting actions within game environments. The dataset's focus on game scenarios makes it particularly useful for refining the model's decision-making capabilities in interactive contexts.
48 game logs	https://doi.org/10.48550/arXiv.2402.02330 (2024)	https://doi.org/10.48550/arXiv.2302.10646 (2023)	The '48 game logs' dataset is used to fine-tune a RoBERTa-like pretrained model for constructing a value network in the Werewolf game. This involves enhancing the model's language understanding and action prediction capabilities. The dataset enables researchers to improve the model's ability to predict optimal actions based on game logs, focusing on the integration of linguistic and strategic elements.
contrastive trajectory dataset	https://doi.org/10.48550/arXiv.2406.11176 (2024)	https://doi.org/10.48550/arXiv.2305.18290 (2023)	The contrastive trajectory dataset is used to compute the outcome-DPO loss, providing process-level supervision in training agents. This enables agents to learn effectively from incorrect trajectories, enhancing their ability to navigate and make decisions in complex environments. The dataset's focus on incorrect trajectories is crucial for improving the robustness and adaptability of the trained agents.
agent trajectory data	https://doi.org/10.48550/arXiv.2406.11176 (2024)	https://doi.org/10.48550/arXiv.2311.05657 (2023)	The 'agent trajectory data' dataset is used to train and fine-tune open-source language models (LLMs) for specific agent abilities, such as reasoning. This is achieved by constructing trajectories from more advanced teacher agents like GPT-4. The dataset enables researchers to enhance the reasoning capabilities of LLMs by leveraging expert-generated sequences of actions and decisions.
CommonGen	https://doi.org/10.1609/aaai.v37i11.26549 (2022)	https://doi.org/10.1109/ICCV.2019.00468 (2019)	The CommonGen dataset is used to evaluate CIDEr and SPICE metrics in generating natural scenarios, focusing on common sense reasoning and contextualized outputs. It enables researchers to assess the performance of these metrics in creating coherent and contextually appropriate scenarios, emphasizing the importance of common sense reasoning in natural language generation tasks.
Mind2Web	https://doi.org/10.48550/arXiv.2402.15057 (2024)	https://www.semanticscholar.org/paper/d365978adf0a5c9c6028820857e015617856256b (2021)	The Mind2Web dataset is used to build the MT-Mind2Web dataset, which contributes to the development of conversational agents for task-oriented dialogues and provides a foundation for web interaction and navigation tasks. This dataset enables researchers to enhance the capabilities of conversational agents in performing specific web-related tasks through improved dialogue management and interaction.
WOMD	https://doi.org/10.1109/ICCV51070.2023.00361 (2023)	https://www.semanticscholar.org/paper/b88b38ec61a4881173ab94647d1e97500f4af15b (2021)	The WOMD dataset is used for training and testing models in planning tasks, particularly for evaluating autonomous vehicle planning capabilities. It provides a comprehensive benchmark and focuses on selected interactive scenarios to assess model performance. This dataset enables researchers to systematically evaluate and compare different planning algorithms in complex, real-world driving situations.
Wikitext	https://doi.org/10.48550/arXiv.2210.07229 (2022)	https://www.semanticscholar.org/paper/996445d847f06e99b0bd259345408a0cf1bce87e (2022)	The Wikitext dataset is used to collect covariance statistics for optimizing factual associations in GPT models. Research focuses on the impact of sample size and precision on model performance, employing statistical methods to enhance the accuracy and reliability of language models. This dataset enables researchers to analyze and improve the factual consistency of large language models.
MathQA	https://doi.org/10.1145/3704435 (2023)	https://doi.org/10.18653/v1/N19-1245 (2019)	The MathQA dataset is used to evaluate the accuracy of large language models (LLMs) in solving math word problems. Researchers focus on operation-based formalisms to enhance the interpretability of the models' solutions. This dataset enables the assessment of LLMs' ability to understand and solve complex mathematical problems, providing insights into their reasoning capabilities.
MixATIS	https://doi.org/10.48550/arXiv.2402.15491 (2024)		The MixATIS dataset is primarily used to evaluate the out-of-distribution performance of API models in tool-related contexts, such as sequential question-answering. It is also utilized to build and evaluate multi-intent models, extending the capabilities of the original ATIS and SNIPS datasets for more complex natural language processing tasks. This dataset enables researchers to measure and improve the generalizability and multi-intent handling capabilities of API models.
ToolLLM	https://doi.org/10.48550/arXiv.2402.15491 (2024)	https://www.semanticscholar.org/paper/3e4085e5869f1b7959707a1e1d7d273b6057eb4e (2023)	The ToolLLM dataset is used to evaluate and compare the performance of large language models (LLMs) such as FLAN-T5-XXL, MPT-30B, Falcon-40B, and StarCoder-15B, focusing on tool-related and API-based tasks. It benchmarks these models by assessing their effectiveness in handling specific tasks, providing insights into their capabilities and limitations in practical applications.
Kinetics Human Action Video Dataset	https://doi.org/10.48550/arXiv.2206.08853 (2022)	https://www.semanticscholar.org/paper/86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6 (2017)	The Kinetics Human Action Video Dataset is used to train and evaluate models on recognizing human actions in videos. It focuses on enhancing the planning capabilities of large language models to understand and predict complex human behaviors, leveraging the dataset's extensive video content to improve model performance in these tasks.
M INE CLIP	https://doi.org/10.48550/arXiv.2206.08853 (2022)	https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b (2017)	The M INE CLIP dataset is used to train policies with Proximal Policy Optimization (PPO) algorithms, specifically focusing on reward shaping to enhance planning capabilities in large language models. This dataset enables researchers to improve the strategic decision-making and planning performance of LLMs through optimized reward structures.
Go-rilla	https://doi.org/10.48550/arXiv.2402.15491 (2024)	https://doi.org/10.48550/arXiv.2307.16789 (2023)	The Go-rilla dataset utilizes synthetic, single-sequence API data generated from language models, particularly from Deep Learning libraries' APIs. It is used to compare with multi-sequence REST-API data from GPT-4, focusing on the differences and similarities in the structure and functionality of these API sequences. This comparison helps researchers understand the nuances and capabilities of different API data types in the context of language model outputs.
Unnatural Instructions	https://doi.org/10.48550/arXiv.2304.03277 (2023)	https://doi.org/10.48550/arXiv.2212.09689 (2022)	The 'Unnatural Instructions' dataset is used to evaluate the performance and behavior of large language models like GPT-4 and text-davinci-002 under non-standard conditions. It focuses on decoding model answers and assessing the quality and diversity of 68,478 synthesized samples using 3-shot in-context learning. This dataset enables researchers to test and analyze model responses to synthetic and unconventional instructions, providing insights into model robustness and adaptability.
wikiHow	https://doi.org/10.48550/arXiv.2305.05252 (2023)	https://www.semanticscholar.org/paper/c0199a7a37c22797c899571e51dba9690e606fa2 (2018)	The wikiHow dataset is used to provide examples of instructional articles on various topics, focusing on the content and structure of how-to guides. It serves as an initial dataset for analyzing and understanding the format and language of instructional texts, enabling research into the creation and improvement of instructional materials.
Matterport3D	https://doi.org/10.1109/ICRA48891.2023.10160969 (2022)	https://doi.org/10.1109/ICCV.2019.00943 (2019)	The Matterport3D dataset is used to evaluate multi-object and spatial goal navigation tasks within the Habitat simulator, focusing on embodied AI research. It supports methodologies that test agents' ability to navigate complex 3D environments, addressing research questions related to spatial understanding and goal-directed navigation. The dataset's rich 3D reconstructions enable realistic simulation scenarios, enhancing the robustness of embodied AI models.
gSCAN test dataset	https://doi.org/10.48550/arXiv.2307.07696 (2023)	https://doi.org/10.18653/v1/2021.emnlp-main.166 (2021)	The gSCAN test dataset is used to evaluate the systematic generalization capabilities of models, particularly in language and vision tasks. It is employed across eight different splits to assess how well models can generalize to unseen data, focusing on their ability to understand and apply learned concepts in new contexts. This dataset enables researchers to rigorously test and compare model performance in these specific areas.
StepGame	https://doi.org/10.48550/arXiv.2307.07696 (2023)	https://doi.org/10.48550/arXiv.2204.08292 (2022)	The StepGame dataset is used to evaluate and enhance robust multi-hop spatial reasoning in texts, specifically focusing on the planning capabilities of language models. It is employed in evaluating models through complex reasoning tasks and in pretraining SynSup to improve understanding of intricate spatial relationships.
OlympiadBench	https://doi.org/10.48550/arXiv.2505.20196 (2025)	https://doi.org/10.1007/978-0-387-39940-9_2108 (1988)	OlympiadBench is used to evaluate the performance of large language models (LLMs) on complex problem-solving tasks, particularly focusing on logical reasoning, mathematical skills, and quantitative analysis. The dataset assesses the consistency and accuracy of LLMs' solutions across different training checkpoints, enabling researchers to analyze the models' reasoning capabilities and general problem-solving abilities.
DeepscaleR-40k	https://doi.org/10.48550/arXiv.2505.20196 (2025)		The DeepscaleR-40k dataset is used to train and fine-tune models, specifically focusing on enhancing the planning capabilities of large language models (LLMs). It involves using 4k randomly selected samples to train a model and fine-tuning the Qwen-7B-Base model with specific training settings to improve planning performance. This dataset enables researchers to evaluate and enhance the strategic and planning abilities of LLMs through targeted training methodologies.
Wikipedia text	https://doi.org/10.48550/arXiv.2303.03378 (2023)	https://doi.org/10.18653/v1/P18-1238 (2018)	The Wikipedia text dataset is used in mixtures for training or evaluation, contributing 0.5% to 3.1% of the data. It is primarily employed for language modeling and text generation tasks, and also for robotic manipulation tasks. This dataset enables researchers to enhance model performance in generating coherent text and improving robotic control through diverse textual inputs.
GQA	https://doi.org/10.48550/arXiv.2312.14150 (2023)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The GQA dataset is used to train DriveLM-Agent by providing questions and answers about images, enhancing the model's ability to reason about visual content. This involves using the dataset's rich visual and textual data to improve the model's understanding and reasoning capabilities in visual contexts.
DriveLM-Data	https://doi.org/10.48550/arXiv.2312.14150 (2023)	https://doi.org/10.48550/arXiv.2301.12597 (2023)	The DriveLM-Data dataset is used to fine-tune the BLIP-2 model for motion tasks, employing a trajectory tokenization scheme. This approach specifically enhances the model's ability to process and generate trajectories, addressing research questions related to improving motion prediction and planning capabilities in language models. The dataset's trajectory data is crucial for training and evaluating these fine-tuned models.
OpenLane-V2	https://doi.org/10.48550/arXiv.2312.14150 (2023)	https://www.semanticscholar.org/paper/bc8bbd71148f7f170bbe52e955f4babc9324e61b (2023)	The OpenLane-V2 dataset is used to generate Perception QAs, providing ground truth data for topology reasoning and unified 3D HD mapping. It leverages ground truth annotations to create questions about observational facets of objects within a scene, which are then used to enhance the planning capabilities of LLMs by focusing on topology reasoning and 3D mapping.
DealOrNotDeal	https://doi.org/10.48550/arXiv.2406.04784 (2024)	https://doi.org/10.18653/v1/D17-1259 (2017)	The 'DealOrNotDeal' dataset is used to implement and study bargaining over multiple issues, with a focus on end-to-end learning of negotiation dialogues. Researchers employ this dataset to develop and evaluate models that can engage in complex negotiations, enhancing the understanding of automated dialogue systems in multi-issue bargaining scenarios.
GAMA-Bench	https://doi.org/10.48550/arXiv.2406.04784 (2024)	https://doi.org/10.48550/arXiv.2403.11807 (2024)	The GAMA-Bench dataset is used as an implemented environment for the Public Goods Game to evaluate the decision-making and gaming abilities of LLMs in multi-agent settings. It provides a structured framework to assess how LLMs interact and make decisions in complex, multi-agent scenarios, focusing on their strategic behavior and cooperation.
FOLIO	https://doi.org/10.18653/v1/2024.acl-long.531 (2023)		The FOLIO dataset is used to assess and enhance LLMs' logical reasoning capabilities, particularly in handling complex logical structures such as first-order logic, quantifiers, and logical connectives. It employs a range of methodologies, including multiple-choice questions, formal proofs, and progressively challenging logical inference problems, to measure and improve LLMs' deductive, inductive, and abductive reasoning skills. This dataset is crucial for evaluating and training models to perform structured reasoning and logical deduction, thereby enhancing their planning capabilities.
TP-RAG	https://doi.org/10.48550/arXiv.2504.08694 (2025)		The TP-RAG dataset is used to create a travel planning benchmark for retrieval-augmented and spatiotemporal-aware travel planning. Constructed using approximately 1 billion GPT-4 tokens, it enables researchers to evaluate and enhance the capabilities of language models in generating detailed and contextually relevant travel plans. This dataset supports the development and testing of advanced travel planning systems by providing a large-scale, high-quality resource for benchmarking and improving model performance in this specific domain.
ActivityPrograms knowledge base	https://doi.org/10.48550/arXiv.2305.11554 (2023)	https://doi.org/10.1109/CVPR.2018.00886 (2018)	The ActivityPrograms knowledge base is used to generate executable plans for household tasks within the VirtualHome simulation platform. It focuses on enhancing the planning capabilities of systems by providing structured data that enables the simulation of complex household activities. This dataset supports research into automated task planning and execution in virtual environments, specifically addressing how systems can effectively plan and execute sequences of actions to complete household tasks.
KAMEL	https://doi.org/10.48550/arXiv.2305.11554 (2023)	https://www.semanticscholar.org/paper/ceab663aada792b409891c96847ee6e8e18998ed (2022)	The KAMEL dataset is used to analyze the planning capabilities of language models by leveraging structured knowledge from Wikidata. Researchers employ relation-based question templates to assess how these models utilize and reason with structured data, focusing on their ability to plan and execute complex tasks based on relational information. This approach helps evaluate the models' understanding and application of structured knowledge in planning scenarios.
Abstraction and Reasoning Corpus (ARC)	https://doi.org/10.48550/arXiv.2411.07279 (2024)	https://www.semanticscholar.org/paper/f48eed915cbb9c6592cdb9df80c1edaeb46959af (2012)	The Abstraction and Reasoning Corpus (ARC) is used to evaluate methods in few-shot visual reasoning, specifically focusing on the ability to solve complex, abstract problems with minimal examples. This dataset enables researchers to assess and improve algorithms' reasoning capabilities by providing a set of tasks that require understanding and generalization from limited data.
NaturalCodeBench (NCB)	https://doi.org/10.48550/arXiv.2406.12793 (2024)	https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269 (2021)	The NaturalCodeBench (NCB) dataset is used to measure models' capacities to solve practical programming tasks, particularly focusing on real-world coding challenges. It evaluates programming problems in languages beyond Python, assessing the model's ability to generate correct code solutions. This dataset enables researchers to test and compare the performance of language models in diverse coding scenarios, providing insights into their practical utility and robustness.
IFEval	https://doi.org/10.48550/arXiv.2406.12793 (2024)	https://doi.org/10.48550/arXiv.2311.07911 (2023)	The IFEval dataset is used to assess the instruction-following capabilities of large language models, such as GLM-4, by evaluating their proficiency in executing complex tasks and understanding diverse instructions. This dataset enables researchers to measure and compare the performance of different models in adhering to specific instructions, thereby providing insights into their ability to follow and execute complex commands accurately.
HumanEval-X	https://doi.org/10.48550/arXiv.2406.12793 (2024)	https://doi.org/10.48550/arXiv.2401.18058 (2024)	The HumanEval-X dataset is used to evaluate the performance of large language models (LLMs) in solving programming problems across multiple languages. It focuses on assessing LLMs' capabilities in tackling coding challenges, providing a benchmark to measure their effectiveness and identify areas for improvement. This dataset enables researchers to systematically analyze and compare the programming skills of different LLMs.
VirtualHome-Env	https://doi.org/10.48550/arXiv.2308.12682 (2023)	https://doi.org/10.1109/CVPR.2019.00645 (2019)	The VirtualHome-Env dataset is used to synthesize environment-aware activities in household settings, focusing on daily activities across 7 scenes. These activities are gathered through crowdsourcing, enabling researchers to study and model natural human behaviors in realistic home environments. The dataset facilitates the development and evaluation of algorithms that can understand and predict daily activities in various household contexts.
PRM12K	https://doi.org/10.48550/arXiv.2412.21187 (2024)	https://doi.org/10.48550/arXiv.2305.20050 (2023)	The PRM12K dataset is used to train large language models (LLMs) to enhance their planning capabilities. Specifically, it generates self-training data that focuses on improving the models' ability to perform consistent step-by-step verification. This dataset enables researchers to refine LLMs' planning processes, ensuring more reliable and structured output.
LiLA	https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691 (2022)	https://doi.org/10.48550/arXiv.2210.17517 (2022)	The LiLA dataset is used to unify various mathematical datasets, enabling the evaluation and enhancement of planning capabilities in large language models (LLMs) for mathematical reasoning tasks. This unification facilitates a more comprehensive assessment of LLMs' ability to plan and reason mathematically, addressing specific research questions related to improving these models' performance in complex mathematical problem-solving.
MWP datasets	https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691 (2022)	https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04 (2023)	The MWP datasets are used to evaluate the performance of models like PoT+SC on math word problems, specifically focusing on their accuracy and effectiveness in solving complex problems. These datasets enable researchers to assess how well these models can interpret and solve mathematical problems presented in natural language, providing insights into their problem-solving capabilities.
BIRD	https://doi.org/10.48550/arXiv.2310.12823 (2023)		The BIRD dataset is used to evaluate and enhance the planning capabilities of large language models (LLMs), particularly in interacting with databases. It constructs SELECT-only query instructions and collects interaction trajectories of GPT-4 with databases. The dataset generates correct trajectories for training and evaluating LLMs, focusing on their ability to plan and execute structured data interactions effectively.
12K-episode dataset	https://doi.org/10.48550/arXiv.2307.12856 (2023)	https://doi.org/10.48550/arXiv.2210.03945 (2022)	The 12K-episode dataset is used to fine-tune the HTML-T5 model for web navigation tasks. It specifically supports the comparison of model performance across 56 MiniWoB++ tasks, enabling researchers to evaluate and enhance the model's capabilities in navigating and interacting with web pages. This dataset facilitates the development and assessment of web navigation skills in machine learning models.
MINT-HotpotQA	https://doi.org/10.48550/arXiv.2310.06830 (2023)	https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5 (2021)	The MINT-HotpotQA dataset is used to evaluate models' ability to answer complex questions that require multi-hop reasoning, emphasizing natural language understanding. It focuses on testing the model's capacity to integrate information from multiple sources to derive answers, thereby assessing advanced reasoning skills in natural language processing tasks.
ObstructedSuite	https://doi.org/10.48550/arXiv.2405.01534 (2024)	https://www.semanticscholar.org/paper/64d16d0bef64c9a36ef91877c3687e260430534e (2020)	The ObstructedSuite dataset is used to evaluate agents' planning, movement, and interaction capabilities in obstacle-filled environments. It focuses on reinforcement learning and motion planning, enabling researchers to assess how effectively agents navigate and interact within complex, obstructed settings. This dataset supports the development and testing of algorithms designed to enhance these specific capabilities.
GSM240K	https://doi.org/10.48550/arXiv.2502.14565 (2025)	https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0 (2020)	The GSM240K dataset is used to train and enhance the performance of the SFT baseline model, specifically examining how additional training data impacts model performance. This dataset enables researchers to evaluate and optimize model improvements through extensive training, focusing on empirical performance gains rather than theoretical planning capabilities.
MBPP	https://doi.org/10.48550/arXiv.2502.14565 (2025)	https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df (2021)	The MBPP dataset is used to evaluate the effectiveness of ReVISE in enhancing reasoning capabilities within the coding domain, specifically focusing on program synthesis tasks. This involves assessing the model's ability to generate correct code solutions, thereby enabling research into improving the reasoning and synthesis capabilities of language models in programming contexts.
Blocksworld	https://doi.org/10.48550/arXiv.2410.01696 (2024)	https://doi.org/10.1609/AAAI.V34I05.6319 (2019)	The Blocksworld dataset is used to evaluate various cognitive and computational tasks, including question answering, logical reasoning, and problem-solving. It assesses the ability to compose sentences, solve complex arithmetic problems, and apply recursive algorithms. Specifically, it tests planning and sequential decision-making in block manipulation environments and the Tower of Hanoi puzzle, emphasizing step-by-step reasoning and strategic thinking.
Letter Concatenation	https://doi.org/10.48550/arXiv.2304.11657 (2023)	https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5 (2022)	The 'Letter Concatenation' dataset is used to evaluate symbolic reasoning capabilities in large language models, specifically focusing on the ability to concatenate letters logically. It also assesses the effectiveness of revised examples in enhancing date understanding tasks, thereby improving the reasoning capabilities of these models. This dataset enables researchers to test and refine the logical and sequential reasoning skills of language models through targeted evaluation tasks.
Musique	https://doi.org/10.48550/arXiv.2311.05657 (2023)	https://doi.org/10.1162/tacl_a_00370 (2021)	The Musique dataset is primarily used to evaluate and improve models' abilities in complex question answering, focusing on multi-hop reasoning, implicit reasoning strategies, and step-by-step problem-solving. It assesses models on tasks such as answering strategy-based questions, solving math problems, and executing complex web interactions, using decomposed queries, supporting facts, and Wikipedia paragraphs. This dataset emphasizes the logical and inferential reasoning capabilities of models, particularly in handling implicit and multi-step reasoning processes.
PRM800K	https://doi.org/10.48550/arXiv.2311.05657 (2023)	https://doi.org/10.1162/tacl_a_00475 (2021)	The PRM800K dataset is used to train and evaluate large language models (LLMs) on tasks requiring complex reasoning, including solving math problems with step-by-step solutions, answering strategy-based questions with supporting facts, and executing complex web interactions. It emphasizes multi-hop reasoning, decomposing complex questions into sub-questions, and using supporting evidence. The dataset includes natural language solution steps, formulas, and Wikipedia paragraph indices, enabling researchers to assess models' logical reasoning and problem-solving capabilities.
Mystery Blocksworld	https://doi.org/10.48550/arXiv.2402.08115 (2024)	https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc (2022)	The Mystery Blocksworld dataset is used to evaluate large language models on planning and reasoning tasks, particularly focusing on the ability to reason about change. It consists of 100 instances drawn from a domain similar to Blocksworld, enabling researchers to assess the models' planning capabilities through specific reasoning challenges.
data scraped from 4nums.com	https://doi.org/10.48550/arXiv.2402.08115 (2024)	https://doi.org/10.48550/arXiv.2305.10601 (2023)	The dataset scraped from 4nums.com is used to evaluate the problem-solving capabilities of large language models, particularly in numerical reasoning tasks and algorithmic challenges. Researchers employ this dataset to assess how well these models can handle complex mathematical problems, providing insights into their reasoning abilities and identifying areas for improvement.
Tell Me More	https://doi.org/10.48550/arXiv.2406.12639 (2024)	https://doi.org/10.48550/arXiv.2306.06070 (2023)	The 'Tell Me More' dataset is used to evaluate language agents' capabilities in asking clarification and follow-up questions, enhancing user interaction by improving the understanding of ambiguous instructions. It focuses on testing agents' planning capabilities in travel-related and web-based tasks, assuming clear and explicit user instructions to assess performance without additional clarifications. This dataset enables researchers to refine language agents' interaction strategies, making them more effective and user-friendly.
Clamber	https://doi.org/10.48550/arXiv.2406.12639 (2024)	https://doi.org/10.48550/arXiv.2405.19425 (2024)	The Clamber dataset is used to evaluate language agents' abilities in performing web-based tasks, particularly in travel-related scenarios, by assessing their planning capabilities and need for clarification. It focuses on the agents' ability to ask follow-up questions to enhance user interaction and improve the clarity and effectiveness of responses to ambiguous instructions.
Ask-before-Plan	https://doi.org/10.48550/arXiv.2406.12639 (2024)		The Ask-before-Plan dataset is used to evaluate the planning capabilities of language models, particularly in interactive travel planning scenarios. It serves as a foundational resource, providing travel planning data for both training and evaluating these models. The dataset enables researchers to assess how well language models can handle complex, real-world planning tasks through interactive dialogues.
