{
  "summary": {
    "total_unique_datasets": 174,
    "total_dataset_mentions": 326,
    "unique_dataset_names": 174,
    "extraction_successful": 770,
    "extraction_failed": 10163,
    "unique_contexts_processed": 8078,
    "total_citation_instances": 10933,
    "total_processing_time": 479.57410073280334
  },
  "datasets_sorted_by_citation_count": [
    {
      "cited_paper_id": "53296520",
      "citation_count": 0,
      "total_dataset_mentions": 29,
      "unique_datasets": [
        "GSM8K"
      ],
      "dataset_details": [
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to generate mathematical expressions for algebraic word problems, focusing on solving and explaining problems through rationale generation. | Used to train and evaluate models on solving and explaining algebraic word problems, focusing on the generation of rationales to enhance interpretability and planning capabilities. | Applied to assess the ability of LLMs to solve single-variable arithmetic problems, emphasizing accuracy and robustness. | Utilized to test the capability of LLMs in solving multi-arithmetic problems, specifically targeting complex mathematical operations. | Used to measure the performance of LLMs on algebraic word problems, focusing on logical reasoning and equation formulation. | Used to assess Boolean answer accuracy, ensuring exact matches between predicted and ground truth Boolean values. | Used to evaluate the correctness of generated plans, considering a plan correct if it matches any of the ground truth plans. | Employed to evaluate the generalization of LLMs across diverse arithmetic problems, including fractions and decimals. | Used to solve and explain algebraic word problems through program induction, focusing on generating rationales for problem-solving steps. | Used to evaluate the performance of PAL/PoT in solving and explaining algebraic word problems, focusing on the model's ability to generate correct solutions and rationales. | Used to evaluate the performance of LLMs on grade school math problems, focusing on multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "256416127",
          "cited_paper_id": 12777818,
          "context_text": "We follow Wei et al. (2022) and consider the same five MWP benchmarks: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), ASDiv (Miao et al., 2020), and AQuA (Ling et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions five specific benchmarks used for evaluating math word problem solvers. These benchmarks are clearly identified and are relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2301.13379",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Applied to assess the ability of LLMs to solve single-variable arithmetic problems, emphasizing accuracy and robustness. | Utilized to test the capability of LLMs in solving multi-arithmetic problems, specifically targeting complex mathematical operations. | Used to measure the performance of LLMs on algebraic word problems, focusing on logical reasoning and equation formulation. | Used to evaluate the performance of Faithful CoT with GPT-4, focusing on solving math word problems and comparing against few-shot SOTA methods. | Used to identify and clean annotation issues in math word problems, enhancing the dataset's reliability for evaluating problem-solving capabilities. | Employed to evaluate the generalization of LLMs across diverse arithmetic problems, including fractions and decimals. | Used to address annotation errors in date-related questions, improving the dataset's quality for assessing temporal reasoning in language models. | Used to clean up annotation errors in command-following tasks, enhancing the dataset's utility for assessing action understanding in language models. | Used to evaluate the performance of LLMs on grade school math problems, focusing on multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "256416127",
          "cited_paper_id": 220047831,
          "context_text": "We follow Wei et al. (2022) and consider the same five MWP benchmarks: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), ASDiv (Miao et al., 2020), and AQuA (Ling et al., 2017).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions five specific benchmarks used for evaluating math word problem solvers. These benchmarks are clearly identified and are relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2301.13379",
          "cited_paper_doi": "10.18653/v1/2020.acl-main.92",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f13e41d24e5d0a68ca662c1b49de398a6fb68251",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to integrate format instructions into prompts for precise automatic evaluation of math word problems. | Used to integrate format instructions into prompts for precise automatic evaluation of common sense reasoning questions.",
          "citing_paper_id": "263609132",
          "cited_paper_id": 239998651,
          "context_text": "For GSM8K and CommonSenseQA, we integrate format instructions into the prompts of Kim et al. (2023) to facilitate a more precise automatic evaluation (detailed prompts can be found in Appendix A).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions GSM8K and CommonSenseQA, which are known datasets, but does not specify their usage beyond integrating format instructions into prompts.",
          "citing_paper_doi": "10.48550/arXiv.2310.01798",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6d4bacb69923e1e94fb4de468b939ce6db32fb51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to assess strategic thinking and reasoning, focusing on complex problem-solving and logical deduction. | Used to test symbolic reasoning, focusing on tasks that involve manipulating sequences of characters. | Used to test multi-arithmetic problem-solving skills, focusing on complex arithmetic operations and reasoning. | Used to train and evaluate models on solving algebraic word problems with natural language rationales, emphasizing the generation of step-by-step explanations. | Used to assess problem-solving skills in complex mathematical scenarios, emphasizing step-by-step reasoning. | Used to evaluate arithmetic problem-solving, emphasizing multi-step reasoning and real-world scenarios. | Used to evaluate multi-step reasoning with questions that require strategic thinking, but the reasoning steps are not provided. | Used to evaluate commonsense reasoning, focusing on questions that require everyday knowledge and logical inference. | Used to assess complex arithmetic problem-solving skills, emphasizing multi-step reasoning and accurate computation. | Used to train and evaluate models on solving single equation algebraic word problems, focusing on the ability to generate and understand natural language rationales. | Used to evaluate commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Used to assess the ability to solve complex math word problems, emphasizing step-by-step reasoning and verification. | Used to evaluate algebraic word problem solving, focusing on generating rationales and solutions using prompting techniques. | Used to evaluate the proposed prompting method for solving algebraic word problems, focusing on the model's ability to generate correct solutions and explanations. | Used to test math word problems requiring multiple reasoning steps and operations, focusing on complex problem-solving skills. | Used to test single-equation problem-solving, focusing on the ability to form and solve linear equations. | Used to test multi-step arithmetic problem-solving skills, focusing on the accuracy of intermediate calculations. | Used to evaluate strategic thinking and reasoning, focusing on complex questions that require multi-step logical deduction. | Used to assess strategic thinking and reasoning, focusing on questions that require planning and multi-step problem-solving. | Used to evaluate algebraic word problem solving, focusing on generating rationales and explanations for solutions. | Used to assess single equation problem-solving, focusing on simple algebraic reasoning and equation solving. | Used to test single equation algebraic word problems, focusing on reasoning and solution accuracy. | Used to evaluate arithmetic reasoning with high-quality, linguistically diverse grade school math word problems created by human problem writers. | Used to evaluate story-based math problems, focusing on contextual understanding and arithmetic reasoning. | Used to assess algebraic word problems with natural language rationales, emphasizing problem-solving and explanation generation. | Used to train and evaluate models on solving basic arithmetic word problems, focusing on understanding and generating correct solutions. | Used to assess the ability to solve grade school math problems, emphasizing step-by-step reasoning and solution generation. | Used to assess one-unknown arithmetic word problems for up-to-4 grade level students, derived from another dataset with simple modifications. | Used to evaluate the model's ability to solve story problems involving arithmetic operations, emphasizing comprehension and reasoning. | Used to assess the effectiveness of the proposed prompting technique in solving grade school math word problems, emphasizing the model's reasoning and problem-solving skills.",
          "citing_paper_id": "258558102",
          "cited_paper_id": 12777818,
          "context_text": "The proposed method is evaluated on the ten benchmark datasets from three categories of reasoning problems: Arithmetic Reasoning: (1) the GSM8K (Cobbe et al., 2021) dataset of high quality linguistically diverse grade school math word problems created by human problem writers, (2) the SVAMP (Patel et al., 2021) benchmark of oneunknown arithmetic word problems for up-to-4 grade level students by making simple changes to a set of problems from another existing dataset, (3) the MultiArith (Roy and Roth, 2016) dataset of math word problems requiring multiple reasoning steps and operations, (4) the AddSub (Hosseini et al., 2014) dataset of addition and subtraction arithmetic word problems, (5) the AQUA (Ling et al., 2017) dataset of algebraic word problems with natural language rationales, and (6) the SingleEq (Koncel-Kedziorski et al., 2015) dataset of single-equation grade-school algebra word problems with multiple math operations over nonnegative rational numbers and one variable; Commonsense Reasoning: (7) the CSQA (Talmor et al., 2019) benchmark dataset of multiple-choice questions that require different types of commonsense knowledge to obtain the correct answers; and (8) the StrategyQA (Geva et al., 2021) benchmark dataset with questions requiring multi-step reasoning but the reasoning steps are not given.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating the proposed method across different reasoning problems. Each dataset is described with specific characteristics and usage.",
          "citing_paper_doi": "10.48550/arXiv.2305.04091",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to assess the LLM's capability to solve arithmetic problems involving multiple operations, emphasizing step-by-step reasoning. | Used to evaluate the ability of LLMs to solve grade school math problems, focusing on multi-step reasoning and arithmetic operations. | Used for few-shot learning experiments, containing multiple-choice questions to assess the model's reasoning and comprehension. | Applied to evaluate commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Used to evaluate commonsense reasoning capabilities in language models through multiple-choice questions requiring various types of commonsense knowledge. | Used to assess models' commonsense reasoning capabilities through multiple-choice questions requiring various types of commonsense knowledge to answer correctly. | Used to evaluate symbolic reasoning, focusing on pattern recognition and sequence completion tasks. | Used to evaluate multi-step reasoning with questions that require strategic thinking, but the reasoning steps are not provided. | Used to assess strategic reasoning abilities, specifically in tasks that require multi-step logical thinking and problem-solving. | Used for few-shot learning experiments, providing simple and varied arithmetic problems to evaluate the model's generalization. | Used for few-shot learning experiments, focusing on strategic questions to evaluate the model's higher-order reasoning and planning. | Used to assess the ability to solve complex questions that require strategic reasoning and multiple steps of inference. | Used to evaluate arithmetic reasoning skills in LLMs, focusing on multi-step problem-solving and numerical accuracy. | Used to test math word problems requiring multiple reasoning steps and operations, focusing on complex problem-solving skills. | Used to evaluate commonsense reasoning capabilities, focusing on questions that require everyday knowledge and reasoning skills. | Used to assess arithmetic reasoning in story problems, emphasizing the integration of language and numerical skills. | Used for few-shot learning experiments, providing algebraic word problems to test the model's ability to parse and solve equations. | Used for few-shot learning experiments, focusing on letter-based puzzles to test the model's pattern recognition and logical reasoning. | Used to assess the LLM's ability to answer questions that require strategic thinking and multi-step reasoning, focusing on complex problem-solving. | Used to evaluate arithmetic reasoning with high-quality, linguistically diverse grade school math word problems created by human problem writers. | Used to evaluate the LLM's commonsense reasoning abilities, focusing on answering questions that require everyday knowledge and logical inference. | Used for few-shot learning experiments, featuring single-equation problems to test the model's equation-solving abilities. | Used to assess one-unknown arithmetic word problems for up-to-4 grade level students, derived from another dataset with simple modifications. | Used for few-shot learning experiments, targeting commonsense knowledge and reasoning through multiple-choice questions. | Used to evaluate commonsense reasoning capabilities, focusing on questions that require common knowledge and reasoning skills. | Utilized to test strategic reasoning, specifically the ability to plan and execute multi-step solutions to complex problems.",
          "citing_paper_id": "258558102",
          "cited_paper_id": 53296520,
          "context_text": "The proposed method is evaluated on the ten benchmark datasets from three categories of reasoning problems: Arithmetic Reasoning: (1) the GSM8K (Cobbe et al., 2021) dataset of high quality linguistically diverse grade school math word problems created by human problem writers, (2) the SVAMP (Patel et al., 2021) benchmark of oneunknown arithmetic word problems for up-to-4 grade level students by making simple changes to a set of problems from another existing dataset, (3) the MultiArith (Roy and Roth, 2016) dataset of math word problems requiring multiple reasoning steps and operations, (4) the AddSub (Hosseini et al., 2014) dataset of addition and subtraction arithmetic word problems, (5) the AQUA (Ling et al., 2017) dataset of algebraic word problems with natural language rationales, and (6) the SingleEq (Koncel-Kedziorski et al., 2015) dataset of single-equation grade-school algebra word problems with multiple math operations over nonnegative rational numbers and one variable; Commonsense Reasoning: (7) the CSQA (Talmor et al., 2019) benchmark dataset of multiple-choice questions that require different types of commonsense knowledge to obtain the correct answers; and (8) the StrategyQA (Geva et al., 2021) benchmark dataset with questions requiring multi-step reasoning but the reasoning steps are not given.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating the proposed method across different reasoning problems. Each dataset is described with specific characteristics and usage.",
          "citing_paper_doi": "10.48550/arXiv.2305.04091",
          "cited_paper_doi": "10.18653/v1/N19-1421",
          "citing_paper_url": "https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate models on symbolic reasoning tasks involving memory and sequence reconstruction. | Used to evaluate LLMs' ability to solve math word problems, focusing on scaling instances to larger problems and assessing procedural extension capabilities. | Used to evaluate the step-by-step problem-solving capabilities of models, specifically analyzing the complexity and number of steps required to solve math word problems. | Used to evaluate models on symbolic reasoning tasks involving probability and logical operations. | Used to assess models' ability to answer strategic questions, emphasizing multi-step reasoning and commonsense knowledge. | Used to assess LLMs' commonsense reasoning, specifically evaluating their ability to handle questions requiring everyday knowledge and understanding. | Used to evaluate models on math word problems, focusing on diverse problem types and solution methods. | Used to assess models on symbolic reasoning tasks requiring string manipulation and pattern recognition.",
          "citing_paper_id": "269626390",
          "cited_paper_id": 239998651,
          "context_text": "GSM8k was designed partly so that its problems would \"require more steps to solve\", but its problems only range 2 to 8 steps[10], and, in fact, previous analyses have found that only 10% of those problems require more than five steps–the majority is 2 , 3 , or 4 .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions GSM8k, which is a specific dataset used for evaluating the step-by-step problem-solving capabilities of models. The dataset is used to analyze the complexity of math word problems.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate Flan-LongT5's ability to solve grade school math problems in few-shot/zero-shot settings, focusing on chain-of-thought reasoning. | Used to assess Flan-LongT5's capability to answer questions requiring strategic reasoning, emphasizing implicit reasoning strategies. | Used to assess Flan-LongT5's commonsense reasoning abilities, focusing on answering questions that require everyday knowledge. | Used to evaluate chain-of-thought reasoning capabilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Used to evaluate Flan-LongT5's performance on a wide range of tasks across multiple domains, assessing its general knowledge and reasoning skills. | Used to challenge Flan-LongT5 with difficult tasks from the BIG-Bench suite, testing its general reasoning and problem-solving capabilities. | Applied to assess the ability to answer strategic questions requiring multi-step reasoning, emphasizing logical and critical thinking. | Used to evaluate Flan-LongT5's ability to solve arithmetic word problems, particularly those involving diverse and complex scenarios. | Employed to evaluate mathematical reasoning and problem-solving abilities, particularly in diverse and challenging arithmetic tasks. | Applied to evaluate multi-modal learning and reasoning capabilities, specifically in tasks that require integrating information from various sources. | Used to assess commonsense reasoning and knowledge, focusing on questions that require everyday understanding and practical intelligence. | Utilized to test arithmetic and algebraic problem-solving skills, specifically focusing on complex word problems and numerical reasoning. | Used to test Flan-LongT5's performance on solving arithmetic word problems, focusing on multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 234790100,
          "context_text": "As a sanity check of instruction-tuning, we evaluate Flan-LongT5 with few-shot/zero-shot settings on CoT benchmark (GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), SVAMP (Patel et al., 2021), Asdiv (Miao et al., 2021), CommonsenseQA (Talmor et al., 2019)), BigBench-Hard (BBH) (Suzgun et al., 2022), and MMLU (Hendrycks et al., 2021b) as tested in Longpre et al. (2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and datasets used to evaluate Flan-LongT5 in few-shot/zero-shot settings. These are specific datasets with clear identifiers and are used for evaluating the model's performance.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1ccd031f28dccfb226f6c0c588c93a97a50bf95f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate Flan-LongT5's ability to solve grade school math problems in few-shot/zero-shot settings, focusing on chain-of-thought reasoning. | Used to assess Flan-LongT5's capability to answer questions requiring strategic reasoning, emphasizing implicit reasoning strategies. | Used to assess Flan-LongT5's commonsense reasoning abilities, focusing on answering questions that require everyday knowledge. | Used to evaluate chain-of-thought reasoning capabilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Applied to evaluate multi-modal learning and reasoning capabilities, specifically in tasks that require integrating information from various sources. | Used to evaluate Flan-LongT5's performance on a wide range of tasks across multiple domains, assessing its general knowledge and reasoning skills. | Used to challenge Flan-LongT5 with difficult tasks from the BIG-Bench suite, testing its general reasoning and problem-solving capabilities. | Applied to assess the ability to answer strategic questions requiring multi-step reasoning, emphasizing logical and critical thinking. | Used to evaluate Flan-LongT5's ability to solve arithmetic word problems, particularly those involving diverse and complex scenarios. | Employed to evaluate mathematical reasoning and problem-solving abilities, particularly in diverse and challenging arithmetic tasks. | Used to assess commonsense reasoning and knowledge, focusing on questions that require everyday understanding and practical intelligence. | Utilized to test arithmetic and algebraic problem-solving skills, specifically focusing on complex word problems and numerical reasoning. | Used to test Flan-LongT5's performance on solving arithmetic word problems, focusing on multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 252917648,
          "context_text": "As a sanity check of instruction-tuning, we evaluate Flan-LongT5 with few-shot/zero-shot settings on CoT benchmark (GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), SVAMP (Patel et al., 2021), Asdiv (Miao et al., 2021), CommonsenseQA (Talmor et al., 2019)), BigBench-Hard (BBH) (Suzgun et al., 2022), and MMLU (Hendrycks et al., 2021b) as tested in Longpre et al. (2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and datasets used to evaluate Flan-LongT5 in few-shot/zero-shot settings. These are specific datasets with clear identifiers and are used for evaluating the model's performance.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": "10.48550/arXiv.2210.09261",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate Flan-LongT5's ability to solve grade school math problems in few-shot/zero-shot settings, focusing on chain-of-thought reasoning. | Used to assess Flan-LongT5's capability to answer questions requiring strategic reasoning, emphasizing implicit reasoning strategies. | Used to assess Flan-LongT5's commonsense reasoning abilities, focusing on answering questions that require everyday knowledge. | Used to evaluate chain-of-thought reasoning capabilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Applied to evaluate multi-modal learning and reasoning capabilities, specifically in tasks that require integrating information from various sources. | Used to evaluate Flan-LongT5's performance on a wide range of tasks across multiple domains, assessing its general knowledge and reasoning skills. | Used to challenge Flan-LongT5 with difficult tasks from the BIG-Bench suite, testing its general reasoning and problem-solving capabilities. | Applied to assess the ability to answer strategic questions requiring multi-step reasoning, emphasizing logical and critical thinking. | Used to evaluate Flan-LongT5's ability to solve arithmetic word problems, particularly those involving diverse and complex scenarios. | Employed to evaluate mathematical reasoning and problem-solving abilities, particularly in diverse and challenging arithmetic tasks. | Used to assess commonsense reasoning and knowledge, focusing on questions that require everyday understanding and practical intelligence. | Utilized to test arithmetic and algebraic problem-solving skills, specifically focusing on complex word problems and numerical reasoning. | Used to test Flan-LongT5's performance on solving arithmetic word problems, focusing on multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 53296520,
          "context_text": "As a sanity check of instruction-tuning, we evaluate Flan-LongT5 with few-shot/zero-shot settings on CoT benchmark (GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), SVAMP (Patel et al., 2021), Asdiv (Miao et al., 2021), CommonsenseQA (Talmor et al., 2019)), BigBench-Hard (BBH) (Suzgun et al., 2022), and MMLU (Hendrycks et al., 2021b) as tested in Longpre et al. (2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and datasets used to evaluate Flan-LongT5 in few-shot/zero-shot settings. These are specific datasets with clear identifiers and are used for evaluating the model's performance.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": "10.18653/v1/N19-1421",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps. | Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Used to evaluate implicit reasoning strategies in question answering, focusing on the ability to infer and apply logical reasoning. | Employed to test common sense reasoning, particularly in understanding and answering questions that require everyday knowledge.",
          "citing_paper_id": "256846992",
          "cited_paper_id": 12777818,
          "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2302.06706",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps. | Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Used to evaluate implicit reasoning strategies in question answering, focusing on the ability to infer and apply logical reasoning. | Employed to test common sense reasoning, particularly in understanding and answering questions that require everyday knowledge.",
          "citing_paper_id": "256846992",
          "cited_paper_id": 230799347,
          "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2302.06706",
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps. | Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Used to evaluate implicit reasoning strategies in question answering, focusing on the ability to infer and apply logical reasoning. | Employed to test common sense reasoning, particularly in understanding and answering questions that require everyday knowledge.",
          "citing_paper_id": "256846992",
          "cited_paper_id": 232223322,
          "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2302.06706",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.168",
          "citing_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to train a process reward model with intermediate signals for each reasoning step, employing the min aggregation function. | Used to evaluate the model with a fairness setting where the goal function g(s) is set to 0 for all states, ensuring unbiased evaluation. | Used to train the process reward model for GSM8K, providing intermediate signals for reasoning steps. | Used to train a process reward model (PRM) for providing intermediate signals in reasoning steps, enhancing the planning capabilities of the model.",
          "citing_paper_id": "270620269",
          "cited_paper_id": 258987659,
          "context_text": "For GSM8K dataset, we adopt a process reward model R P trained on PRM800K [22] to provide intermediate signal for each reasoning step, and use min as the aggregation function; For MATH dataset, we set g ( s ) = 0 for all state s for fairness because PRM800K contains samples constructed from MATH…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluation in the context of planning capabilities of LLMs. GSM8K and MATH are clearly identified and used for different purposes.",
          "citing_paper_doi": "10.48550/arXiv.2406.14283",
          "cited_paper_doi": "10.48550/arXiv.2305.20050",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d269ad2a38bcbfc533303ce0f9be2537ba7b71c2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Applied to evaluate the robustness of models on a diverse set of arithmetic word problems, using shared exemplars with GSM8K. | Utilized to evaluate the ability of models to solve complex arithmetic and algebraic word problems, emphasizing the role of contextual information. | Used to solve arithmetic word problems, focusing on the effectiveness of exemplars in improving model performance. | Used to evaluate the effectiveness of LLMs in solving simple single-equation word problems, assessing basic mathematical understanding. | Applied to solve arithmetic word problems, emphasizing multi-step reasoning and equation generation to assess model performance. | Used to test the capability of models to solve single-equation word problems, leveraging shared exemplars with GSM8K. | Used to assess performance on arithmetic word problems, particularly those involving simple and complex operations. | Utilized to assess the generalization of models across different types of arithmetic word problems, using shared exemplars with GSM8K. | Employed to assess the performance of models on addition and subtraction word problems, using shared exemplars with GSM8K. | Used to assess arithmetic reasoning in story problems, emphasizing the complexity and variety of problem types. | Used to test the capability of LLMs to handle single-variable arithmetic word problems, focusing on accuracy and robustness.",
          "citing_paper_id": "258297976",
          "cited_paper_id": 4894130,
          "context_text": "In Table 1, for GSM8K, CSQA, Letter Concate-nation and other datasets sharing the same exem-plars with GSM8K (AddSub, SingleEq, SVAMP, and ASDiv), we utilize the best exemplars in this section.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to solve arithmetic and algebraic word problems. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2304.11657",
          "cited_paper_doi": "10.1162/tacl_a_00160",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/17230f5b3956188055a48c5f4f61d131cce0662f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Applied to evaluate the robustness of models on a diverse set of arithmetic word problems, using shared exemplars with GSM8K. | Utilized to evaluate the ability of models to solve complex arithmetic and algebraic word problems, emphasizing the role of contextual information. | Used to solve arithmetic word problems, focusing on the effectiveness of exemplars in improving model performance. | Used to evaluate the effectiveness of LLMs in solving simple single-equation word problems, assessing basic mathematical understanding. | Applied to assess the ability of NLP models to solve algebraic word problems, emphasizing multi-step reasoning and equation formulation. | Used to test the capability of models to solve single-equation word problems, leveraging shared exemplars with GSM8K. | Used to assess performance on arithmetic word problems, particularly those involving simple and complex operations. | Utilized to evaluate single-equation problem-solving abilities of NLP models, focusing on translating simple word problems into equations. | Utilized to assess the generalization of models across different types of arithmetic word problems, using shared exemplars with GSM8K. | Employed to assess the performance of models on addition and subtraction word problems, using shared exemplars with GSM8K.",
          "citing_paper_id": "258297976",
          "cited_paper_id": 232223322,
          "context_text": "In Table 1, for GSM8K, CSQA, Letter Concate-nation and other datasets sharing the same exem-plars with GSM8K (AddSub, SingleEq, SVAMP, and ASDiv), we utilize the best exemplars in this section.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to solve arithmetic and algebraic word problems. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2304.11657",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.168",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate reasoning capabilities of large language models, specifically comparing ToolChain with state-of-the-art approaches like Chain-of-Thought and Self-Consistency.",
          "citing_paper_id": "264405734",
          "cited_paper_id": 246411621,
          "context_text": "…reasoning tasks in GSM8K, we compare ToolChain ∗ with the state-of-the-art reasoning approaches, including GPT (OpenAI, 2023), Chain-of-Thought (Wei et al., 2022), Self-Consistency (Wang et al., 2022b), ReAct (Yao et al., 2023b), Tree-of-Thoughts (Yao et al., 2023a), and MCTS (Hao et al.,…",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions GSM8K as a dataset used for reasoning tasks, but does not provide details on other potential datasets. The context focuses on comparing various reasoning approaches.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate reasoning tasks in large language models, comparing various approaches including ToolChain, Chain-of-Thought, Self-Consistency, ReAct, Tree-of-Thoughts, and MCTS.",
          "citing_paper_id": "264405734",
          "cited_paper_id": 247595263,
          "context_text": "…reasoning tasks in GSM8K, we compare ToolChain ∗ with the state-of-the-art reasoning approaches, including GPT (OpenAI, 2023), Chain-of-Thought (Wei et al., 2022), Self-Consistency (Wang et al., 2022b), ReAct (Yao et al., 2023b), Tree-of-Thoughts (Yao et al., 2023a), and MCTS (Hao et al., 2023a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions GSM8K as a dataset used for reasoning tasks, which is relevant to the topic of planning capabilities of LLMs. No other datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": "10.48550/arXiv.2203.11171",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate reasoning tasks in large language models, comparing various approaches including ToolChain, Chain-of-Thought, Self-Consistency, ReAct, Tree-of-Thoughts, and MCTS.",
          "citing_paper_id": "264405734",
          "cited_paper_id": 252762395,
          "context_text": "…reasoning tasks in GSM8K, we compare ToolChain ∗ with the state-of-the-art reasoning approaches, including GPT (OpenAI, 2023), Chain-of-Thought (Wei et al., 2022), Self-Consistency (Wang et al., 2022b), ReAct (Yao et al., 2023b), Tree-of-Thoughts (Yao et al., 2023a), and MCTS (Hao et al., 2023a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions GSM8K as a dataset used for reasoning tasks, which is relevant to the topic of planning capabilities of LLMs. No other datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate reasoning tasks in large language models, comparing various approaches including ToolChain, Chain-of-Thought, Self-Consistency, ReAct, Tree-of-Thoughts, and MCTS.",
          "citing_paper_id": "264405734",
          "cited_paper_id": 258762525,
          "context_text": "…reasoning tasks in GSM8K, we compare ToolChain ∗ with the state-of-the-art reasoning approaches, including GPT (OpenAI, 2023), Chain-of-Thought (Wei et al., 2022), Self-Consistency (Wang et al., 2022b), ReAct (Yao et al., 2023b), Tree-of-Thoughts (Yao et al., 2023a), and MCTS (Hao et al., 2023a).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions GSM8K as a dataset used for reasoning tasks, which is relevant to the topic of planning capabilities of LLMs. No other datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": "10.48550/arXiv.2305.10601",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to provide seed examples for training and evaluating large language models in numerical reasoning tasks, focusing on chain-of-thought prompting.",
          "citing_paper_id": "256459681",
          "cited_paper_id": 246411621,
          "context_text": "All numerical reasoning datasets share one set of seed examples either randomly sampled from GSM8K (when the number of seeds is 2 or 4) or from Wei et al. (2022b) (when the number of seeds is 8).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions GSM8K as a source of seed examples for numerical reasoning datasets. The dataset is used for training and evaluation in the context of prompting large language models to perform reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2302.00618",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/69619a2a47faee7a29ec596db13172e2a42ff921",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to sample code-based solutions for solving mathematical problems, focusing on the effectiveness of training verifiers with gpt-3. | Used for training a model to solve math word problems, focusing on high-level planning capabilities. | Used to assess planning and reasoning skills in a text-based environment, emphasizing navigation and task completion in a simulated world. | Used to train and evaluate LLaMA on solving math word problems, focusing on generating breakdowns and planning solutions for 7,473 problems. | Used to evaluate reasoning and planning capabilities in solving math word problems, focusing on step-by-step problem-solving and logical reasoning. | Used for the planning task, specifically to assess the model's ability to navigate and interact in a simulated environment. | Used for evaluating the model's performance on more challenging math word problems, assessing its planning and problem-solving skills. | Used to evaluate the planning capabilities of black-box LLMs, focusing on task completion in interactive environments.",
          "citing_paper_id": "273654757",
          "cited_paper_id": 239998651,
          "context_text": "Subsequently, LLaMA functions as the planner, generating breakdowns and planning solutions for each of the 7,473 problems in the GSM8K training set.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GSM8K training set, which is a specific dataset used for training and evaluating models on math word problems.",
          "citing_paper_doi": "10.48550/arXiv.2410.20749",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2bc4fb0d2f185f0e24fa783c8187557583510ca1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to fine-tune GPT-3 for predicting the correctness of solutions, focusing on mathematical problem-solving and reasoning capabilities. | Used to finetune GPT-4 for predicting step-wise correctness in solutions, enhancing the model's ability to provide structured feedback on intermediate reasoning steps. | Used to train a GPT-4 model to predict step-wise correctness in solutions, enhancing the model's ability to provide structured feedback on intermediate reasoning steps. | Used to fine-tune GPT-3 for predicting the correctness of solutions, focusing on the entire solution and individual steps. | Used to evaluate step-wise correctness in reasoning tasks, providing labels for each step through crowdsourcing. This dataset supports the development of models that can provide structured feedback on intermediate reasoning steps. | Used to finetune GPT-4 for predicting step-wise correctness in problem-solving tasks, enhancing the model's ability to provide structured feedback on intermediate reasoning steps.",
          "citing_paper_id": "260350986",
          "cited_paper_id": 247951931,
          "context_text": "For example, Cobbe et al. (2021) finetune a GPT-3 on GSM8K to predict the correctness of a solution as a whole.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'GSM8K' as a dataset used for fine-tuning GPT-3 to predict the correctness of solutions. No other datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2308.00436",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to assess multi-hop question answering capabilities, focusing on diverse and explainable reasoning processes. | Used for multi-hop question answering, emphasizing diverse and explainable reasoning processes. | This dataset 'GSM8K' was mentioned in the citation context but no detailed description was generated. | Used to evaluate the model's ability to prove mathematical theorems, focusing on formal logic and proof construction. | Used to test theorem proving and logical reasoning, focusing on advanced mathematical and logical challenges. | Used for interactive learning in embodied environments, focusing on aligning text and actions in a virtual world. | Used to evaluate interactive learning and decision-making in embodied environments, focusing on aligning text and physical actions.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 222208810,
          "context_text": "(Austin et al., 2021) 500 91 Decision Making ALFWorld (Shridhar et al., 2020) 134 134 Reasoning GSM8K (Cobbe et al., 2021) 1319 48 HotpotQA (Yang et al., 2018) 7,405 43 MATH (Hendrycks et al., 2021) 5,000 100 MMLU (Hendrycks et al., 2020) 13,985 76 TheoremQA (Chen et al., 2023a) 800 49 Total 29,307…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating decision-making and reasoning capabilities of LLMs. Each dataset is associated with a specific task or domain, such as multi-hop question answering, math problems, and theorem proving.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to fine-tune a model for answering complex questions requiring multi-step reasoning and comprehension of context. | Used to fine-tune Llama models, examining the effect on generating faithful chain-of-thoughts (CoTs) in problem-solving tasks. | Used to evaluate early termination CoT Pred Match values, focusing on solving math word problems and assessing the reasoning process of LLMs. | Used to evaluate zero-shot accuracy of LLMs fine-tuned on math word problems, focusing on problem-solving capabilities without chain-of-thought reasoning. | Used to evaluate the LLM's ability to solve math word problems from diverse grade school levels, focusing on numerical reasoning and problem-solving skills. | Used to evaluate the decline in CoT Pred Match values for LLMs as α decreases, focusing on math word problems. | Used to evaluate logical reasoning skills in LLMs, focusing on deductive and inductive reasoning tasks. | Used to fine-tune Llama models, investigating the impact on generating faithful chain-of-thoughts (CoTs) in problem-solving tasks. | Used to compare the decline in CoT Pred Match values for LLMs as α decreases, focusing on commonsense reasoning questions. | Used to assess the mathematical reasoning capabilities of LLMs, focusing on solving complex math word problems. | Used to evaluate the length and complexity of chain-of-thought (CoT) reasoning generated by GPT-4, focusing on the brevity of CoTs compared to other datasets. | Used to fine-tune Llama models, demonstrating better performance in generating faithful chain-of-thoughts (CoTs) compared to other datasets. | Used to fine-tune models and evaluate the faithfulness of chain-of-thought reasoning, showing a sharper decline in CoT Pred Match as α decreases. | Used to test commonsense reasoning through multiple-choice questions derived from reading comprehensions, focusing on the LLM's ability to understand and apply everyday knowledge. | Used to fine-tune a model for solving math word problems, focusing on step-by-step reasoning and problem-solving capabilities. | Used to assess zero-shot accuracy of LLMs fine-tuned on comprehension and reasoning tasks, specifically evaluating the ability to answer questions based on context. | Used to evaluate early termination CoT Pred Match values, focusing on comprehension and reasoning over context-rich passages, assessing the LLM's ability to understand and reason about complex scenarios. | Used to test common-sense reasoning in LLMs, focusing on understanding and inferring from everyday scenarios and contexts. | Used to evaluate the faithfulness of CoTs generated by fine-tuned models, specifically comparing the performance of GSM8K, MedQA, and CosmosQA models. | Used to fine-tune models and evaluate the faithfulness of chain-of-thought reasoning, showing a smaller drop in CoT Pred Match as α decreases. | Used to evaluate the performance of fine-tuned LLMs, specifically assessing their generalization capabilities after fine-tuning on other datasets.",
          "citing_paper_id": "274234789",
          "cited_paper_id": 239998651,
          "context_text": "Interestingly, for QLoRA ranks 8 and 32, we observe that the fine-tuned GSM8K model generates less faithful CoTs on the GSM8K dataset compared to the fine-tuned MedQA and CosmosQA models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GSM8K dataset, which is used to evaluate the performance of fine-tuned models. The dataset is clearly identified and used in the research context.",
          "citing_paper_doi": "10.48550/arXiv.2411.15382",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd75496350494cdea76c2fca1e90d754d03cbe2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to fine-tune and evaluate the faithfulness of CoTs generated by QLoRA configurations, focusing on commonsense reasoning questions. | Used to evaluate the faithfulness of CoTs generated by fine-tuned models, specifically comparing the performance of GSM8K, MedQA, and CosmosQA models.",
          "citing_paper_id": "274234789",
          "cited_paper_id": 258841328,
          "context_text": "Interestingly, for QLoRA ranks 8 and 32, we observe that the fine-tuned GSM8K model generates less faithful CoTs on the GSM8K dataset compared to the fine-tuned MedQA and CosmosQA models.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the GSM8K dataset, which is used to evaluate the performance of fine-tuned models. The dataset is clearly identified and used in the research context.",
          "citing_paper_doi": "10.48550/arXiv.2411.15382",
          "cited_paper_doi": "10.48550/arXiv.2305.14314",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd75496350494cdea76c2fca1e90d754d03cbe2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/32ac52069e562d4f900afee70bdca63f53461481",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to assess models on single-variable arithmetic word problems, testing numerical reasoning with 1000 test questions. | Used to test models on multi-step arithmetic word problems, focusing on the ability to handle complex numerical reasoning tasks. | Used to test models on table-based mathematical word problems, assessing reasoning over structured data. | Used to test models on table-based arithmetic word problems, assessing their ability to integrate tabular data with textual information. | Used to evaluate models on multiple-choice mathematical word problems, assessing reasoning and comprehension with 253 test questions. | Used to evaluate models on multi-step arithmetic word problems, focusing on the ability to perform sequential calculations. | Used to evaluate models on multiple-choice arithmetic word problems, assessing their ability to select correct answers from given options. | Used to assess models on single-variable arithmetic word problems, testing their capability to solve basic numerical reasoning tasks. | Used to evaluate models on multi-arithmetic word problems, focusing on complex numerical reasoning with 600 test questions. | Applied to evaluate question-answering performance, specifically in the context of logical reasoning over textual inputs.",
          "citing_paper_id": "253801709",
          "cited_paper_id": 235399966,
          "context_text": "Dataset Split Example Domain Input Output GSM8K Cobbe et al. (2021) Test 1318 MWP Question Number AQuA Ling et al. (2017) Test 253 MWP Question Option SVAMP Patel et al. (2021) Test 1000 MWP Question Number MultiArith Roy & Roth (2015) Test 600 MWP Question Number TabMWP Lu et al. (2022) Test 7861 MWP Table + Question Number + Text FinQA Chen et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating models on mathematical word problems and numerical reasoning tasks. Each dataset is clearly identified and used for testing specific aspects of model performance.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/2021.emnlp-main.300",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99053e3a708fc27709c9dab33110dc98b187c158",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate ReVISE and baselines on solving math word problems, focusing on step-by-step reasoning and verification. | Used to assess enhancements in mathematical problem-solving, focusing on accuracy improvements with Llama3 8B model. | Used to evaluate reasoning performance improvements, specifically measuring accuracy on math word problems with Llama3 1B model. | Used to train ReVISE with a 50k subset, an augmented version of MATH, focusing on mathematical problem-solving capabilities.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 232134851,
          "context_text": "We evaluate ReVISE and baselines on GSM8K (Cobbe et al., 2021) and MATH-500 (Hendrycks et al., 2021), a widely used evaluation benchmark subset of MATH.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GSM8K and MATH-500, which are used for evaluating models on mathematical problem-solving tasks.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to assess model capabilities in solving advanced math problems, emphasizing complex reasoning and mathematical understanding. | Used to evaluate ReVISE and baselines on solving math word problems, focusing on step-by-step reasoning and verification. | Used to assess enhancements in mathematical problem-solving, focusing on accuracy improvements with Llama3 8B model. | Used to evaluate reasoning performance improvements, specifically measuring accuracy on math word problems with Llama3 1B model.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 239998651,
          "context_text": "We evaluate ReVISE and baselines on GSM8K (Cobbe et al., 2021) and MATH-500 (Hendrycks et al., 2021), a widely used evaluation benchmark subset of MATH.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GSM8K and MATH-500, which are used for evaluating models on mathematical problem-solving tasks.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to assess model capabilities in solving advanced math problems, emphasizing complex reasoning and mathematical understanding. | Used to evaluate ReVISE and baselines on solving math word problems, focusing on step-by-step reasoning and verification. | Used for evaluating few-shot learning performance, focusing on step-by-step problem-solving in mathematical tasks. | Used to train and evaluate the ReVISE model on mathematical problem-solving tasks, focusing on step-by-step verification and reasoning processes.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 258987659,
          "context_text": "We evaluate ReVISE and baselines on GSM8K (Cobbe et al., 2021) and MATH-500 (Hendrycks et al., 2021), a widely used evaluation benchmark subset of MATH.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GSM8K and MATH-500, which are used for evaluating models on mathematical problem-solving tasks.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": "10.48550/arXiv.2305.20050",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "249017698",
      "citation_count": 0,
      "total_dataset_mentions": 23,
      "unique_datasets": [
        "HotpotQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to include mathematical reasoning examples, focusing on problem-solving tasks and theorem-based questions. | Used to include multi-hop QA-style reasoning examples, focusing on diverse and explainable question answering tasks. | Used to reserve the first 500 instances for diverse, explainable multi-hop question answering, focusing on the dataset's structure and content. | Used to evaluate the planning capabilities of LLMs in solving complex mathematical problems, focusing on theorem proving and logical reasoning. | Used to evaluate math reasoning skills, focusing on solving grade school math problems with step-by-step solutions. | Used for interactive learning in embodied environments, focusing on aligning text and actions in a virtual world. | Used to evaluate model performance on diverse, explainable multi-hop question answering tasks, focusing on the challenges of fair comparisons due to random test example selection. | Used to assess multi-hop question answering capabilities, focusing on diverse and explainable reasoning processes. | Used for multi-hop question answering, emphasizing diverse and explainable reasoning processes. | This dataset 'GSM8K' was mentioned in the citation context but no detailed description was generated. | Used to test knowledge problem-solving, evaluating the model's ability to answer questions across various subjects and domains. | Used to evaluate the model's ability to prove mathematical theorems, focusing on formal logic and proof construction. | Used to test theorem proving and logical reasoning, focusing on advanced mathematical and logical challenges. | Used to evaluate interactive learning and decision-making in embodied environments, focusing on aligning text and physical actions. | Used to assess multi-hop question answering, requiring the model to integrate information from multiple documents to answer questions. | Used to measure mathematical problem-solving abilities, covering a wide range of math topics and difficulty levels.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 52822214,
          "context_text": "We manually curate the in-context example for the reasoning subset to include the characteristic for both multi-hop QA-style reasoning (e.g., HotpotQA) and mathematical reasoning (e.g., MATH, TheoremQA).",
          "confidence_score": 1.0,
          "citation_intent": [
            "n",
            "b",
            "o",
            "r",
            "a",
            "i",
            " ",
            "f",
            "s",
            "d",
            "g",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of curated examples for reasoning subsets, including multi-hop QA and mathematical reasoning, which aligns with the cited papers' titles.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate model performance on diverse, explainable multi-hop question answering tasks, focusing on the challenges of fair comparisons due to random test example selection.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 249017698,
          "context_text": "For instance, HotpotQA (Yang et al., 2018) (Yao et al., 2022; Shinn et al., 2023) choose to evaluate on randomly drawn test examples, increasing the barriers to fair comparisons.",
          "confidence_score": 0.7,
          "citation_intent": "findings",
          "resource_type": "dataset",
          "reasoning": "HotpotQA is mentioned as a dataset used for evaluation, but the context does not specify its use in the current research. The citation is more about the evaluation methodology rather than the dataset itself.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": "10.48550/arXiv.2205.12255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/354bf043179e3e9f05df73e3f04517e53c326d1f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate model performance on diverse, explainable multi-hop question answering tasks, focusing on the challenges of fair comparisons due to random test example selection.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 257636839,
          "context_text": "For instance, HotpotQA (Yang et al., 2018) (Yao et al., 2022; Shinn et al., 2023) choose to evaluate on randomly drawn test examples, increasing the barriers to fair comparisons.",
          "confidence_score": 0.7,
          "citation_intent": "findings",
          "resource_type": "dataset",
          "reasoning": "HotpotQA is mentioned as a dataset used for evaluation, but the context does not specify its use in the current research. The citation is more about the evaluation methodology rather than the dataset itself.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": "10.48550/arXiv.2303.11366",
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/46299fee72ca833337b3882ae1d8316f44b32b3c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "This dataset 'ALFWorld' was mentioned in the citation context but no detailed description was generated. | Used to evaluate multi-step reasoning capabilities in question answering, focusing on diverse and explainable multi-hop questions. | Used for multi-step reasoning tasks, focusing on diverse, explainable multi-hop question answering to evaluate the planning capabilities of LLMs.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 52822214,
          "context_text": "…agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' as an example of a dataset used for multi-step reasoning tasks. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "This dataset 'ALFWorld' was mentioned in the citation context but no detailed description was generated. | Used to evaluate multi-step reasoning capabilities in language models, focusing on complex question answering that requires reasoning about entities and relations. | Used for multi-step reasoning tasks, focusing on diverse, explainable multi-hop question answering to evaluate the planning capabilities of LLMs.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 222208810,
          "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve interaction with and feedback from the environment.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used for multi-step reasoning tasks to evaluate the performance of agent frameworks in complex question-answering tasks, focusing on the planning capabilities of language models. | Used for sequential decision-making tasks to assess the ability of agent frameworks to navigate and interact with environments, emphasizing the planning capabilities of language models.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 252762395,
          "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively. These are clearly identifiable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs in complex agent environments, focusing on instruction-following and task completion. | Used to evaluate LLM agents in six distinct environments, focusing on multi-turn, open-ended generation tasks to assess planning capabilities. | Used to fine-tune models for enhanced instruction-following capabilities, focusing on the planning capabilities of large language models using a dataset of real-world conversations. | Used to enhance the function calling abilities of LLMs, specifically in the context of instruction fine-tuning for complex agent environments. | Used to improve the model's instruction-following capabilities in agent environments, focusing on complex and diverse instruction-completion tasks. | This dataset 'ShareGPT' was mentioned in the citation context but no detailed description was generated. | Used to improve the instruction-following capabilities of LLMs in agent-based scenarios, emphasizing high-quality instruction-completion tasks. | Used for multi-step reasoning tasks to evaluate the performance of agent frameworks in complex question-answering tasks, focusing on the planning capabilities of language models. | Used for sequential decision-making tasks to assess the ability of agent frameworks to navigate and interact with environments, emphasizing the planning capabilities of language models. | Used to enhance the model's function calling capabilities, specifically to win points in LLM function calling tasks.",
          "citing_paper_id": "276250494",
          "cited_paper_id": null,
          "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively. These are clearly identifiable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate multi-hop reasoning capabilities of large language models, specifically comparing different prompting methods and their effectiveness in improving performance.",
          "citing_paper_id": "263829963",
          "cited_paper_id": 258762525,
          "context_text": "HotpotQA (EM) ↑ Base LM 0.32 CoT (Wei et al., 2022) 0.34 CoT - SC (Wang et al., 2022) 0.38 ToT (Yao et al., 2023a) 0.55 RAP (Hao et al., 2023) 0.60 RAP ( n = 10 ) 0.60 LATS (CoT) 0.62 Table 2.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions HotpotQA, which is a dataset used for evaluating multi-hop reasoning in language models. No other datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2310.04406",
          "cited_paper_doi": "10.48550/arXiv.2305.10601",
          "citing_paper_url": "https://www.semanticscholar.org/paper/700bd9681f1b9e9e2212e10415d27b11c7e6836b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate multi-hop question answering capabilities, focusing on the ability to integrate information from multiple documents to answer complex questions.",
          "citing_paper_id": "263609132",
          "cited_paper_id": 52822214,
          "context_text": "• HotpotQA (Yang et al., 2018): HotpotQA is an open-domain multi-hop question answering dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "HotpotQA is explicitly mentioned as a dataset and is relevant to the topic of planning capabilities of LLMs, particularly in the context of multi-hop question answering.",
          "citing_paper_doi": "10.48550/arXiv.2310.01798",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6d4bacb69923e1e94fb4de468b939ce6db32fb51",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used for multi-hop question-answering tasks, requiring logical reasoning across Wikipedia passages via the Wikipedia API, focusing on diverse and explainable reasoning processes. | Used to evaluate multi-hop question answering capabilities, focusing on explainability and diverse reasoning tasks.",
          "citing_paper_id": "267897975",
          "cited_paper_id": 52822214,
          "context_text": "Webshop (Yao et al., 2022) creates an online shopping environment simulating product purchases, while HotpotQA (Yang et al., 2018) involves multi-hop question-answering tasks requiring logical reasoning across Wikipedia passages via the Wikipedia API.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two resources, Webshop and HotpotQA, which are used to simulate real-world web interactions and multi-hop question-answering tasks, respectively. However, neither is a traditional dataset but rather environments or datasets for specific tasks.",
          "citing_paper_doi": "10.48550/arXiv.2402.15506",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ae65bb64fcf6a7d114f5df8888c9cede8d6b57f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate multi-hop question answering capabilities, focusing on explainability and diverse reasoning tasks.",
          "citing_paper_id": "267897975",
          "cited_paper_id": 262053695,
          "context_text": "In the subsequent sections, we will present experimental evaluations conducted across five benchmarks: Webshop (Yao et al., 2022), HotpotQA (Yang et al., 2018), ToolEval (Qin et al., 2023), ToolQuery (Ma et al., 2024) and MINT-Bench (Wang et al., 2023a).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions five benchmarks, but they are primarily used for score comparison rather than as specific, downloadable datasets. However, 'HotpotQA' is a known dataset for multi-hop question answering.",
          "citing_paper_doi": "10.48550/arXiv.2402.15506",
          "cited_paper_doi": "10.48550/arXiv.2309.10691",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ae65bb64fcf6a7d114f5df8888c9cede8d6b57f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate multi-hop question answering capabilities, focusing on explainability and diverse reasoning tasks.",
          "citing_paper_id": "267897975",
          "cited_paper_id": 267199917,
          "context_text": "In the subsequent sections, we will present experimental evaluations conducted across five benchmarks: Webshop (Yao et al., 2022), HotpotQA (Yang et al., 2018), ToolEval (Qin et al., 2023), ToolQuery (Ma et al., 2024) and MINT-Bench (Wang et al., 2023a).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions five benchmarks, but they are primarily used for score comparison rather than as specific, downloadable datasets. However, 'HotpotQA' is a known dataset for multi-hop question answering.",
          "citing_paper_doi": "10.48550/arXiv.2402.15506",
          "cited_paper_doi": "10.48550/arXiv.2401.13178",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ae65bb64fcf6a7d114f5df8888c9cede8d6b57f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cf270bea2fba82bcff83f380c1f100d346b14ecf",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate question-answering capabilities of language models, specifically focusing on multi-hop reasoning and complex queries. | Used to evaluate the framework’s adaptability and practical utility in generating trajectories from complex question-answering environments.",
          "citing_paper_id": "267897975",
          "cited_paper_id": 259108190,
          "context_text": "Follow Action[ (A) HotpotQA (B) ToolAlpaca [{\" user query \": \"Which magazine was started ﬁrst Arthur's Magazine or First for Women?\"",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'HotpotQA' and 'ToolAlpaca', but 'ToolAlpaca' is a method/model, not a dataset. 'HotpotQA' is a dataset used for evaluating question-answering systems.",
          "citing_paper_doi": "10.48550/arXiv.2402.15506",
          "cited_paper_doi": "10.48550/arXiv.2306.05301",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ae65bb64fcf6a7d114f5df8888c9cede8d6b57f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/455866ca838f356b53a7e3e5b344834f9e93dbbc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate question-answering capabilities of language models, specifically focusing on multi-hop reasoning and complex queries. | Used to evaluate the framework’s adaptability and practical utility in generating trajectories from complex question-answering environments.",
          "citing_paper_id": "267897975",
          "cited_paper_id": null,
          "context_text": "Follow Action[ (A) HotpotQA (B) ToolAlpaca [{\" user query \": \"Which magazine was started ﬁrst Arthur's Magazine or First for Women?\"",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'HotpotQA' and 'ToolAlpaca', but 'ToolAlpaca' is a method/model, not a dataset. 'HotpotQA' is a dataset used for evaluating question-answering systems.",
          "citing_paper_doi": "10.48550/arXiv.2402.15506",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ae65bb64fcf6a7d114f5df8888c9cede8d6b57f",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used alongside HotpotQA to generate a pilot dataset with 500 trajectories, providing a variety of questions for the research. | Used to provide initial questions for Search Agent trajectories, focusing on multi-hop reasoning tasks requiring information from multiple documents. | Used to generate a pilot dataset with 500 trajectories, focusing on diverse, explainable multi-hop question answering.",
          "citing_paper_id": "266335848",
          "cited_paper_id": 52822214,
          "context_text": "3 Alongside the main self-improvement setup, described in Section 3, we also generate a simpler “pilot” data with 500 trajectories, where the initial questions are selected from HotpotQA and Eli5 datasets only (i.e., smaller and without Eli5-askH or Eli5-askS).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, HotpotQA and Eli5, which are used to generate a pilot dataset for the research. These datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2312.10003",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e35426fd81c78b044258cf419be6b7e5093b71c0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate multi-hop reasoning capabilities, focusing on diverse and explainable question answering. | Used to assess multi-hop reasoning, specifically designed for comprehensive evaluation of reasoning steps. | Used to train models for multi-hop question answering, providing a comprehensive evaluation of reasoning steps. | Used to train models for multi-hop question answering, focusing on diverse and explainable reasoning steps. | Used to test multi-hop reasoning, likely involving challenging and diverse question types. | Used to evaluate multi-hop reasoning, focusing on complex questions requiring multiple evidence pieces.",
          "citing_paper_id": "276884818",
          "cited_paper_id": 226236740,
          "context_text": "We evaluate using four multi-hop datasets: HotpotQA [5], 2WikiMulti-HopQA [6], Musique [22], and Bamboogle [9].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four multi-hop datasets used for evaluation, which are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2503.05592",
          "cited_paper_doi": "10.18653/V1/2020.COLING-MAIN.580",
          "citing_paper_url": "https://www.semanticscholar.org/paper/78f85a3a4e9d1b83ac33179c777e6eb2d756be82",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9001eb3c3d5a96ad3d804410c2437e6f60feade9",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate multi-hop reasoning capabilities, focusing on diverse and explainable question answering. | Used to assess multi-hop reasoning, specifically designed for comprehensive evaluation of reasoning steps. | Used to train models for multi-hop question answering, providing a comprehensive evaluation of reasoning steps. | Used to train models for multi-hop question answering, focusing on diverse and explainable reasoning steps. | Used to test multi-hop reasoning, likely involving challenging and diverse question types. | Used to evaluate multi-hop reasoning, focusing on complex questions requiring multiple evidence pieces.",
          "citing_paper_id": "276884818",
          "cited_paper_id": 52822214,
          "context_text": "We evaluate using four multi-hop datasets: HotpotQA [5], 2WikiMulti-HopQA [6], Musique [22], and Bamboogle [9].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions four multi-hop datasets used for evaluation, which are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2503.05592",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/78f85a3a4e9d1b83ac33179c777e6eb2d756be82",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used for multi-method fine-tuning with CoT and Reflexion, focusing on improving question-answering performance through reinforcement learning. | Used for multi-method fine-tuning with ReAct, focusing on improving question answering capabilities through verbal reinforcement learning.",
          "citing_paper_id": "263829338",
          "cited_paper_id": 258833055,
          "context_text": ", 2022b) and Reflexion (Shinn et al., 2023) with ReAct for multi-method fine-tuning on HotpotQA.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'HotpotQA' which is a specific dataset used for question answering tasks. It is used for multi-method fine-tuning in the context of the research.",
          "citing_paper_doi": "10.48550/arXiv.2310.05915",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/67daf8c4fe1958d20ebdf95c2a36dd490c73836f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate the performance of GPT-3 and PaLM-540B in question-answering tasks, focusing on multi-hop reasoning and complex queries.",
          "citing_paper_id": "252762395",
          "cited_paper_id": 218971783,
          "context_text": "As shown in Table 5, GPT-3 (text-davinci-002, greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly because it is finetuned with human instruction following.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions HotpotQA and ALFWorld, which are specific datasets or benchmarks. However, ALFWorld is more likely a benchmark or environment rather than a dataset. HotpotQA is a dataset used for evaluating question-answering systems.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate question answering capabilities, focusing on multi-hop reasoning and complex query resolution in the context of LLM planning.",
          "citing_paper_id": "258179336",
          "cited_paper_id": 3800579,
          "context_text": "We focus on HotpotQA (Yang et al., 2018b) for question answering.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "HotpotQA is mentioned as a specific dataset used for question answering, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.1145/3704435",
          "cited_paper_doi": "10.1126/scirobotics.aar7650",
          "citing_paper_url": "https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/536c764040b57bb33ef64b8e9b05686640533939",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to construct high-quality annotations for reading comprehension tasks, ensuring correctness through self-instruction.",
          "citing_paper_id": "270823634",
          "cited_paper_id": 26501419,
          "context_text": "These annotations, which are constructed based on the HotpotQA and Trivi-aQA (Joshi et al., 2017) datasets using the Self-Instruct method, only include those that lead to the Preprint correct answers to ensure quality.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of HotpotQA and TriviaQA datasets for constructing annotations. These datasets are used to ensure the quality of the annotations by including only those that lead to correct answers.",
          "citing_paper_doi": "10.48550/arXiv.2311.05657",
          "cited_paper_doi": "10.18653/v1/P17-1147",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f010affab57b5fcf1cd6be23df79d8ec98c7289c",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to evaluate LUMOS's performance in complex question answering tasks requiring implicit reasoning strategies. | Used to evaluate LUMOS's performance in solving grade school math word problems, emphasizing step-by-step reasoning. | Used to assess LUMOS's ability to handle multi-hop reasoning in complex question answering scenarios. | Used to test LUMOS's capabilities in web navigation and interaction tasks, focusing on complex user interactions. | Used to assess LUMOS's ability to solve math word problems, emphasizing cross-task generalization and performance relative to larger models. | Used to assess LUMOS's ability to solve simple math word problems, focusing on arithmetic and algebraic reasoning.",
          "citing_paper_id": "270823634",
          "cited_paper_id": 232223322,
          "context_text": "Our extensive evaluation affirms that L UMOS provides state-of-the-art or parallel performance across various complex interactive tasks, including intricate QA (e.g., StrategyQA and HotpotQA), web (e.g., Mind2Web), and mathematics tasks (e.g., GSM8K and SVAMP).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating LUMOS's performance in complex interactive tasks, including QA, web, and mathematics tasks.",
          "citing_paper_doi": "10.48550/arXiv.2311.05657",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.168",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "cited_paper_url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "HotpotQA",
          "dataset_description": "Used to provide execution results of mathematical computations in reasoning steps, enhancing the evaluation of LLMs' problem-solving capabilities. | Used to evaluate LUMOS's performance in complex question answering tasks requiring implicit reasoning strategies. | Used to evaluate LUMOS's performance in solving grade school math word problems, emphasizing step-by-step reasoning. | Used to provide HTML code results after web operations, aiding in the assessment of LLMs' interaction with web environments. | Used to assess LUMOS's ability to handle multi-hop reasoning in complex question answering scenarios. | Used to test LUMOS's capabilities in web navigation and interaction tasks, focusing on complex user interactions. | Used to assess LUMOS's ability to solve math word problems, emphasizing cross-task generalization and performance relative to larger models. | Used to assess LUMOS's ability to solve simple math word problems, focusing on arithmetic and algebraic reasoning.",
          "citing_paper_id": "270823634",
          "cited_paper_id": 239998651,
          "context_text": "Our extensive evaluation affirms that L UMOS provides state-of-the-art or parallel performance across various complex interactive tasks, including intricate QA (e.g., StrategyQA and HotpotQA), web (e.g., Mind2Web), and mathematics tasks (e.g., GSM8K and SVAMP).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating LUMOS's performance in complex interactive tasks, including QA, web, and mathematics tasks.",
          "citing_paper_doi": "10.48550/arXiv.2311.05657",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "265351664",
      "citation_count": 0,
      "total_dataset_mentions": 12,
      "unique_datasets": [
        "MATH"
      ],
      "dataset_details": [
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to include mathematical reasoning examples, focusing on problem-solving tasks and theorem-based questions. | Used to include multi-hop QA-style reasoning examples, focusing on diverse and explainable question answering tasks. | Utilized to test general knowledge and reasoning, including a variety of subjects and question types. | Used to evaluate the planning capabilities of LLMs in solving complex mathematical problems, focusing on theorem proving and logical reasoning. | Used to evaluate math reasoning skills, focusing on solving grade school math problems with step-by-step solutions. | Used to assess multi-hop question answering, requiring the model to integrate information from multiple documents to answer questions. | Used for multi-hop question answering, emphasizing diverse and explainable reasoning processes. | Used to test knowledge problem-solving, evaluating the model's ability to answer questions across various subjects and domains. | Used to generate reasoning trajectories for mathematical problem-solving tasks, focusing on the capabilities of gpt-3 in solving complex problems. | Used to test theorem proving and logical reasoning, focusing on advanced mathematical and logical challenges. | Applied to assess complex question answering, emphasizing multi-hop reasoning and evidence combination. | Used to evaluate theorem proving and logical reasoning, focusing on formal mathematical proofs and derivations. | Used for interactive learning in embodied environments, focusing on aligning text and actions in a virtual world. | Used to evaluate multi-modal logical understanding, covering a wide range of topics and question types. | Used to measure mathematical problem-solving abilities, covering a wide range of math topics and difficulty levels.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 232134851,
          "context_text": "We manually curate the in-context example for the reasoning subset to include the characteristic for both multi-hop QA-style reasoning (e.g., HotpotQA) and mathematical reasoning (e.g., MATH, TheoremQA).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of curated examples for reasoning subsets, including multi-hop QA and mathematical reasoning, which aligns with the cited papers' titles.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate multi-step mathematical reasoning in large language models, focusing on problem-solving tasks and benchmarking performance. | Used to expand experiments to more comprehensive mathematical problem-solving tasks, assessing the model's ability to handle diverse and challenging problems. | Used to evaluate mathematical problem-solving capabilities, focusing on multi-step reasoning and robustness of MARFT in solving complex problems. | Used to test the model's performance on advanced mathematical problem-solving tasks, particularly those requiring deep understanding and multi-step reasoning. | Applied to assess the ability of models to solve complex math word problems, emphasizing step-by-step reasoning and accuracy. | Utilized to test advanced mathematical problem-solving skills, particularly in algebra, geometry, and combinatorics, to measure model robustness.",
          "citing_paper_id": "277999502",
          "cited_paper_id": 232134851,
          "context_text": "Given the preliminary nature of our experiment on the only MATH (Hendrycks et al., 2021), we outline a series of future experiments to further explore the capabilities and robustness of MARFT: • Multi-step MARFT on mathematical benchmarks : Expand experiments to more comprehensive mathematical problem-solving tasks and benchmarks, including MATH (Hendrycks et al., 2021), GSM8k (Cobbe et al., 2021), and AIME (MAA, 2024).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating mathematical problem-solving capabilities of models. These datasets are directly relevant to the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2504.16129",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a6f3178221ee4bacbdddd7b8fc0d6861e39b25",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate multi-step mathematical reasoning in large language models, focusing on problem-solving tasks and benchmarking performance. | Used to expand experiments to more comprehensive mathematical problem-solving tasks, assessing the model's ability to handle diverse and challenging problems. | Used to evaluate mathematical problem-solving capabilities, focusing on multi-step reasoning and robustness of MARFT in solving complex problems. | Used to test the model's performance on advanced mathematical problem-solving tasks, particularly those requiring deep understanding and multi-step reasoning. | Applied to assess the ability of models to solve complex math word problems, emphasizing step-by-step reasoning and accuracy. | Utilized to test advanced mathematical problem-solving skills, particularly in algebra, geometry, and combinatorics, to measure model robustness.",
          "citing_paper_id": "277999502",
          "cited_paper_id": 239998651,
          "context_text": "Given the preliminary nature of our experiment on the only MATH (Hendrycks et al., 2021), we outline a series of future experiments to further explore the capabilities and robustness of MARFT: • Multi-step MARFT on mathematical benchmarks : Expand experiments to more comprehensive mathematical problem-solving tasks and benchmarks, including MATH (Hendrycks et al., 2021), GSM8k (Cobbe et al., 2021), and AIME (MAA, 2024).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating mathematical problem-solving capabilities of models. These datasets are directly relevant to the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2504.16129",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a6f3178221ee4bacbdddd7b8fc0d6861e39b25",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Evaluates college-level scientific problem-solving abilities in physics and chemistry, assessing the capabilities of large language models in these domains. | Used to measure mathematical problem-solving abilities, focusing on a variety of math problems to evaluate the performance of large language models.",
          "citing_paper_id": "270123654",
          "cited_paper_id": 232134851,
          "context_text": "…approaches for complex task solving and our adaptive build approach with Captain Agent on six real-world scenarios, including many mathematics problem-solving [45], data analysis [46], programming [47], scientific problem-solving [48] (Physics and Chemistry), and world-information retrieval [49].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several domains where the adaptive build approach with Captain Agent is applied, including mathematics problem-solving, data analysis, programming, scientific problem-solving, and world-information retrieval. However, only 'MATH' and 'SciBench' are specific datasets mentioned in the cited papers.",
          "citing_paper_doi": "10.48550/arXiv.2405.19425",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Evaluates college-level scientific problem-solving abilities in physics and chemistry, assessing the capabilities of large language models in these domains. | Used to measure mathematical problem-solving abilities, focusing on a variety of math problems to evaluate the performance of large language models.",
          "citing_paper_id": "270123654",
          "cited_paper_id": 259991511,
          "context_text": "…approaches for complex task solving and our adaptive build approach with Captain Agent on six real-world scenarios, including many mathematics problem-solving [45], data analysis [46], programming [47], scientific problem-solving [48] (Physics and Chemistry), and world-information retrieval [49].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several domains where the adaptive build approach with Captain Agent is applied, including mathematics problem-solving, data analysis, programming, scientific problem-solving, and world-information retrieval. However, only 'MATH' and 'SciBench' are specific datasets mentioned in the cited papers.",
          "citing_paper_doi": "10.48550/arXiv.2405.19425",
          "cited_paper_doi": "10.48550/arXiv.2307.10635",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Evaluates college-level scientific problem-solving abilities in physics and chemistry, assessing the capabilities of large language models in these domains. | Used to measure mathematical problem-solving abilities, focusing on a variety of math problems to evaluate the performance of large language models.",
          "citing_paper_id": "270123654",
          "cited_paper_id": 265351664,
          "context_text": "…approaches for complex task solving and our adaptive build approach with Captain Agent on six real-world scenarios, including many mathematics problem-solving [45], data analysis [46], programming [47], scientific problem-solving [48] (Physics and Chemistry), and world-information retrieval [49].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several domains where the adaptive build approach with Captain Agent is applied, including mathematics problem-solving, data analysis, programming, scientific problem-solving, and world-information retrieval. However, only 'MATH' and 'SciBench' are specific datasets mentioned in the cited papers.",
          "citing_paper_doi": "10.48550/arXiv.2405.19425",
          "cited_paper_doi": "10.48550/arXiv.2311.12983",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Evaluates college-level scientific problem-solving abilities in physics and chemistry, assessing the capabilities of large language models in these domains. | Used to measure mathematical problem-solving abilities, focusing on a variety of math problems to evaluate the performance of large language models.",
          "citing_paper_id": "270123654",
          "cited_paper_id": 266933185,
          "context_text": "…approaches for complex task solving and our adaptive build approach with Captain Agent on six real-world scenarios, including many mathematics problem-solving [45], data analysis [46], programming [47], scientific problem-solving [48] (Physics and Chemistry), and world-information retrieval [49].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several domains where the adaptive build approach with Captain Agent is applied, including mathematics problem-solving, data analysis, programming, scientific problem-solving, and world-information retrieval. However, only 'MATH' and 'SciBench' are specific datasets mentioned in the cited papers.",
          "citing_paper_doi": "10.48550/arXiv.2405.19425",
          "cited_paper_doi": "10.48550/arXiv.2401.05507",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ecd5d38b8590f6e6a504b29110d16ff717cd56ef",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate mathematical problem-solving capabilities of various language models, including open-source and commercial products, focusing on accuracy and reasoning skills.",
          "citing_paper_id": "263611068",
          "cited_paper_id": 232134851,
          "context_text": "…open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain Re-Act (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and ChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize the results in Figure 4a.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MATH dataset, which is a specific, verifiable dataset used for evaluating mathematical problem-solving capabilities of language models.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9ea0757c750ab1222a7442d3485a74d1c526b04c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate the accuracy of small open-source LLMs, highlighting their limitations on simple tasks. | Used to evaluate LLM performance on complex reasoning problems, highlighting the low accuracy of LLMs in solving multi-step mathematical problems. | Used to evaluate the performance of LLMs on complex reasoning problems, specifically focusing on the accuracy of multi-step reasoning processes. | Used to evaluate the performance of LLMs on complex reasoning problems, specifically measuring the accuracy of multi-step reasoning tasks.",
          "citing_paper_id": "260350986",
          "cited_paper_id": 218971783,
          "context_text": "From GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), Llama (Touvron et al., 2023) to GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023), the increasing model sizes and exploding amount of training data have empowered LLMs to achieve human-level performance on a large range of tasks, including summarization, translation, question answering and basic mathematical reasoning. However, the performance of even the largest LLMs on complex reasoning problems is still unsatisfactory. For example, GPT-4 only correctly answers 42.5% of problems in the MATH dataset, which is far below human level. Such problems require careful multi-step reasoning to solve, where LLMs are prone to make mistakes. Namely, to solve such complex reasoning problems, LLMs need to generate many reasoning steps. Even though their error rate on a single step is low, the probability of LLMs generating at least one erroneous step can be fairly high, which makes final predictions less accurate. It is thus necessary to check for errors in LLMs’ generated reasoning in order to filter out incorrect predictions and provide an estimation of confidence for a prediction. Recently, Cobbe et al. (2021) finetune GPT-3 to recognize errors in the whole reasoning process, and Li et al. (2022) train a BERT model (He et al., 2020) to check the correctness of each reasoning step. Ling et al. (2023) eliminate the need to finetune by performing few-shot checking on a special type of reasoning chains called Natural Program.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MATH dataset, which is used to evaluate the performance of LLMs on complex reasoning problems. The dataset is specifically used to highlight the limitations of LLMs in solving multi-step reasoning problems.",
          "citing_paper_doi": "10.48550/arXiv.2308.00436",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to assess enhancements in mathematical problem-solving, focusing on accuracy improvements with Llama3 8B model. | Used to evaluate reasoning performance improvements, specifically measuring accuracy on math word problems with Llama3 1B model.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 271571434,
          "context_text": "Notably, ReVISE enhances reasoning performance beyond prior methods, improving accuracy from 27.1 → 31.1% on GSM8K (Maj@3) (Cobbe et al., 2021) with Llama3 1B (Dubey et al., 2024) and from 33.2 → 36.0% on MATH (Maj@3) (Hendrycks et al., 2021) with Llama3 8B.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions GSM8K and MATH as datasets used to measure reasoning performance improvements. These are specific datasets used in the evaluation of mathematical problem-solving capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": "10.48550/arXiv.2407.21783",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to test the scalability and efficiency of STaR+ in handling a larger dataset of grade school math problems, with a focus on fewer completions per iteration. | Used to assess the effectiveness of STaR+ in solving advanced mathematical problems, specifically examining the influence of different sampling rates. | Used to evaluate the performance of STaR+ in solving grade school math problems, focusing on the impact of sampling multiple completions per iteration.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 260438790,
          "context_text": "We run STaR + for three iterations, sampling k completions per iteration (GSM8K: k = 10 , MATH: k = 4 , GSM240K: k = 1 ) with a temperature of 0.7 for both RFT and STaR + .",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions GSM8K, MATH, and GSM240K, which appear to be specific datasets used for evaluating the performance of the STaR+ method. These datasets are likely related to mathematical reasoning and problem-solving.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": "10.48550/arXiv.2308.01825",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate the performance of a distilled policy model on mathematical problem-solving tasks, achieving an accuracy of approximately 51%. | Used to evaluate LLM mathematical reasoning performance, specifically measuring success rates after applying the weighted self-consistency algorithm. The dataset assesses problem-solving capabilities and demonstrates significant performance improvements.",
          "citing_paper_id": "270379625",
          "cited_paper_id": 232134851,
          "context_text": "Similarly, we tune a pretrained Gemini Pro (Gemini Team et al., 2023) by distilling knowledge from Gemini Ultra on math instruction datasets, resulting in a policy model that achieves an accuracy of approximately 51% on the MATH test set.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MATH test set, which is a specific dataset used for evaluating mathematical problem-solving capabilities.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f32bcc2155997110a7905da050df4c8404867b24",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "241033103",
      "citation_count": 0,
      "total_dataset_mentions": 8,
      "unique_datasets": [
        "Common Crawl"
      ],
      "dataset_details": [
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Used to pre-train models with a large-scale HTML corpus, focusing on long-span denoising objectives to improve efficiency in handling long sequences. | Used to evaluate task planning capabilities of HTML-T5, comparing its performance against other models on offline task planning tasks.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 245144820,
          "context_text": "It is pre-trained using a mixture of long-span denoising objective (Tay et al., 2022) on a large-scale HTML corpus extracted from CommonCrawl.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'CommonCrawl' as a source of a large-scale HTML corpus used for pre-training. CommonCrawl is a well-known web crawl dataset.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": "10.18653/v1/2022.findings-naacl.55",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Used to pre-train models with a large-scale HTML corpus, focusing on long-span denoising objectives to improve efficiency in handling long sequences.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 252780443,
          "context_text": "It is pre-trained using a mixture of long-span denoising objective (Tay et al., 2022) on a large-scale HTML corpus extracted from CommonCrawl.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CommonCrawl' as a source of a large-scale HTML corpus used for pre-training. CommonCrawl is a well-known web crawl dataset.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b21670e8061a06ab97e7d6052c9345a326e84ff8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Applied for training models on instructional videos, focusing on multimodal understanding of video and text. | Utilized for training large language models, offering a diverse collection of text from various sources. | Used to train large pretrained language models, providing a vast amount of web-scraped text data. | Used to train large language models, providing a vast amount of web-scraped text data for pretraining. | Applied to train multimodal models, focusing on instructional videos and their corresponding transcripts.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 11241677,
          "context_text": "Big dataset such as Common Crawl [19], the Pile [30], LAION [85], YouTube-8M [2] and HowTo100M [67] have been fueling the success of large pretrained language models [21, 80, 11] and multimodal models [103, 6, 68, 127, 7, 4, 120].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several large datasets used for training large pretrained language and multimodal models. These datasets are explicitly named and are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Utilized for training large language models, offering a diverse collection of text from various sources. | Used to train large language models, providing a vast amount of web-scraped text data for pretraining. | Applied for training models on instructional videos, focusing on multimodal understanding of video and text.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 209370497,
          "context_text": "Big dataset such as Common Crawl [19], the Pile [30], LAION [85], YouTube-8M [2] and HowTo100M [67] have been fueling the success of large pretrained language models [21, 80, 11] and multimodal models [103, 6, 68, 127, 7, 4, 120].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several large datasets used for training large pretrained language and multimodal models. These datasets are explicitly named and are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": "10.1109/cvpr42600.2020.00990",
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9de403a58395a1b56bfceee6e009788c43db6d08",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Utilized for training large language models, offering a diverse collection of text from various sources. | Used to train large language models, providing a vast amount of web-scraped text data for pretraining. | Applied for training models on instructional videos, focusing on multimodal understanding of video and text.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 238215257,
          "context_text": "Big dataset such as Common Crawl [19], the Pile [30], LAION [85], YouTube-8M [2] and HowTo100M [67] have been fueling the success of large pretrained language models [21, 80, 11] and multimodal models [103, 6, 68, 127, 7, 4, 120].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several large datasets used for training large pretrained language and multimodal models. These datasets are explicitly named and are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": "10.18653/v1/2021.emnlp-main.544",
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Applied for training models on instructional videos, focusing on multimodal understanding of video and text. | Utilized for training large language models, offering a diverse collection of text from various sources. | Used to train large pretrained language models, providing a vast amount of web-scraped text data. | Used to train large language models, providing a vast amount of web-scraped text data for pretraining. | Applied to train multimodal models, focusing on instructional videos and their corresponding transcripts.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 241033103,
          "context_text": "Big dataset such as Common Crawl [19], the Pile [30], LAION [85], YouTube-8M [2] and HowTo100M [67] have been fueling the success of large pretrained language models [21, 80, 11] and multimodal models [103, 6, 68, 127, 7, 4, 120].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several large datasets used for training large pretrained language and multimodal models. These datasets are explicitly named and are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Utilized for training large language models, offering a diverse collection of text from various sources. | Used to train large pre-trained language models, providing a vast amount of web-scraped text data. | Applied to train multimodal models, focusing on instructional videos and their corresponding transcripts.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 52967399,
          "context_text": "Big dataset such as Common Crawl [24], the Pile [37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pre-trained language models [27, 91, 15] and multimodal models [118, 6, 78, 145, 7, 4, 136].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several large datasets used for training language and multimodal models. These datasets are clearly identified and are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": "10.18653/v1/N19-1423",
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "Common Crawl",
          "dataset_description": "Utilized for training large language models, offering a diverse collection of text from various sources. | Used to train large pre-trained language models, providing a vast amount of web-scraped text data. | Applied to train multimodal models, focusing on instructional videos and their corresponding transcripts.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 203594078,
          "context_text": "Big dataset such as Common Crawl [24], the Pile [37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pre-trained language models [27, 91, 15] and multimodal models [118, 6, 78, 145, 7, 4, 136].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several large datasets used for training language and multimodal models. These datasets are clearly identified and are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/025a0dc4a2a98742f1b410b6318a46de2c854b22",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "238857090",
      "citation_count": 0,
      "total_dataset_mentions": 7,
      "unique_datasets": [
        "ALFRED"
      ],
      "dataset_details": [
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Used to develop AI agents capable of understanding and executing grounded instructions in embodied environments, focusing on vision-language tasks.",
          "citing_paper_id": "271497434",
          "cited_paper_id": 49552345,
          "context_text": "ALF-World [Shridhar et al. , 2021] combines the interactive text-based game engine TextWorld [Cˆot´e et al. , 2019] with AL-FRED [Shridhar et al. , 2020], a dataset for vision-language tasks in embodied environments, creating a platform for developing AI agents capable of understanding and…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions ALFRED, which is identified as a dataset for vision-language tasks in embodied environments. It is used in combination with TextWorld to create a platform for developing AI agents.",
          "citing_paper_doi": "10.24963/ijcai.2024/15",
          "cited_paper_doi": "10.1007/978-3-030-24337-1_3",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4bf1d4946ec440a00f2521f861d14a29e81712a5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/89daae27e7df4a418b9610d307ce3df0e30fc8a2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Used to develop AI agents capable of understanding and executing grounded instructions in embodied environments, focusing on vision-language tasks. | Used to develop AI agents capable of understanding and interacting in complex scenarios using both text and visuals, focusing on vision-language tasks in embodied environments.",
          "citing_paper_id": "271497434",
          "cited_paper_id": 208617407,
          "context_text": "ALF-World [Shridhar et al. , 2021] combines the interactive text-based game engine TextWorld [Cˆot´e et al. , 2019] with AL-FRED [Shridhar et al. , 2020], a dataset for vision-language tasks in embodied environments, creating a platform for developing AI agents capable of understanding and…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions ALFRED, which is identified as a dataset for vision-language tasks in embodied environments. It is used in combination with TextWorld to create a platform for developing AI agents.",
          "citing_paper_doi": "10.24963/ijcai.2024/15",
          "cited_paper_doi": "10.1109/cvpr42600.2020.01075",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4bf1d4946ec440a00f2521f861d14a29e81712a5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4cf4246f3882aa6337e9c05d5675a3b8463a32e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Used to evaluate hierarchical planning models in complex VLN tasks, focusing on the effectiveness of separating high-level and low-level planning.",
          "citing_paper_id": "254408960",
          "cited_paper_id": 238857090,
          "context_text": "However, in more complex VLN, or embodied instruction following, datasets such as ALFRED [32], hierarchical planning models [3, 17, 24] that separate the high-level and low-level planning have proven to be most effective.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ALFRED' as a dataset used in complex VLN tasks, which aligns with the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00280",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bef63d4f7656393b7bceb2ec704e86577c286166",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Used to evaluate hierarchical planning models in complex VLN tasks, focusing on the effectiveness of separating high-level and low-level planning.",
          "citing_paper_id": "254408960",
          "cited_paper_id": 247318425,
          "context_text": "However, in more complex VLN, or embodied instruction following, datasets such as ALFRED [32], hierarchical planning models [3, 17, 24] that separate the high-level and low-level planning have proven to be most effective.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ALFRED' as a dataset used in complex VLN tasks, which aligns with the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00280",
          "cited_paper_doi": "10.48550/arXiv.2203.04637",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718",
          "cited_paper_url": "https://www.semanticscholar.org/paper/47e135ef718baf6a3a86a1167b67ed96f7932ca4",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Used to generate questions based on PDDL tasks, focusing on evaluating the planning capabilities of AI systems through a set of predefined planning problems. | Used to provide goals for the Alfworld environment, encoding the dynamics of the domain as PDDL, focusing on planning capabilities in a simulated environment. | Used to evaluate LLMs in multi-turn interactions with tools and language feedback, focusing on planning capabilities and performance in complex scenarios.",
          "citing_paper_id": "273228858",
          "cited_paper_id": 2730999,
          "context_text": "Alfworld uses goals from the Al-fred dataset (Shridhar et al. 2020) and encodes the dynamics of the domain as PDDL.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'Al-fred dataset' which is a specific, verifiable dataset used in the research. The dataset is used to provide goals for the Alfworld environment.",
          "citing_paper_doi": "10.48550/arXiv.2410.05669",
          "cited_paper_doi": "10.1609/aimag.v21i2.1506",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7bee11da995d968394477a66e78aaab9721ecdf7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8995b12a886a177e98b83e6e1ae2f1eda3df20ef",
          "citing_paper_year": 2024,
          "cited_paper_year": 2000
        },
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Reused for building a large-scale dataset to study the G-PlanET task, specifically using its goals and plans to align with the AI2THOR environment. | Used to support the evaluation of G-PlanET by converting the data into tables, focusing on the planning capabilities of the model. | Used to provide an interactive environment for agents with an ego-centric vision to perform actions, focusing on the planning capabilities of LLMs in a simulated 3D environment.",
          "citing_paper_id": "251979509",
          "cited_paper_id": 28328610,
          "context_text": "The ALFRED dataset uses the AI2THOR engine to provide an interactive environment for agents with an ego-centric vision to perform actions.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ALFRED dataset, which is used to provide an interactive environment for agents with an ego-centric vision to perform actions. The dataset is clearly identified and its usage is described.",
          "citing_paper_doi": "10.1609/aaai.v37i11.26549",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/59b71e2a248d67a2692bc7e35faa504ee2dbc98d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/89c8aad71433f7638d2e2c009e1ea20e039f832d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "ALFRED",
          "dataset_description": "Used to train and evaluate LLaMA-13B with 8-bit inference, focusing on the performance and efficiency of the model in a simulated environment over 3-5 days.",
          "citing_paper_id": "263708879",
          "cited_paper_id": 251564521,
          "context_text": "In practice, a single RTX 3090 is sufficient for our experiments using LLaMA-13B with 8-bit inference [65] on ALFRED, requiring around 3-5 days of training, mainly due to the speed of the underlying simulator used in ALFRED.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ALFRED' but does not specify if it is a dataset, method, or tool. Given the context of training and evaluation, it is likely a dataset or environment used for experiments.",
          "citing_paper_doi": "10.48550/arXiv.2310.10021",
          "cited_paper_doi": "10.48550/arXiv.2208.07339",
          "citing_paper_url": "https://www.semanticscholar.org/paper/90df9c970aa98aa49bcf79e252ab26b1bb6889c5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4be7d1524edb0137599a5cc95f72844b85a52fe1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "258865184",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "MMLU"
      ],
      "dataset_details": [
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of LLMs across various tasks and domains. | Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of large language models across various tasks.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 258179056,
          "context_text": "Following Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot demonstrations (Lu et al., 2024), environmental feedback (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2023), and tree-like reasoning procedures (Yao et al., 2024; Zhuang et al., 2024a).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks, but only 'MMLU' is a specific, verifiable dataset. The others are excluded as they are primarily used for score comparison or are not clearly identified as downloadable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": "10.18653/v1/2023.emnlp-main.187",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/19c222d1f18317d58cc85491f37479bc0dc49f41",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of large language models across various tasks. | Pre-trained on this corpus to enhance core agentic capabilities and improve generalization, focusing on diverse agent interactions and scenarios. | Used to evaluate Hephaestus's planning capabilities, specifically assessing long-term planning and generalization in complex environments. | Used to assess function-calling proficiencies of LLMs, focusing on evaluating planning capabilities through a rigorous framework of diverse tasks.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 258865184,
          "context_text": "…Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks, but only MMLU is a specific, downloadable dataset. The others are primarily used for score comparison and do not qualify as traditional datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d8905a1fd288068f12c8347caeabefd36d0dd6c",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to test advanced language understanding capabilities, focusing on complex and challenging tasks. | Used to evaluate models on a wide range of tasks, focusing on massive multitask language understanding across various domains. | Used to evaluate models on interactive and feedback-based tasks, assessing their ability to engage in dynamic dialogues. | Used to assess models on frame-based reasoning tasks, focusing on understanding and generating structured information. | Used to evaluate the planning capabilities of LLMs, focusing on task-oriented dialogue and interaction scenarios. | Used to assess software engineering benchmark tasks, focusing on verified and reliable performance metrics. | Used to evaluate live coding tasks, focusing on real-time code generation and execution. | Used to assess model performance on a revised version of MMLU, emphasizing updated and expanded task coverage.",
          "citing_paper_id": "275789950",
          "cited_paper_id": 259164635,
          "context_text": "Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1 , LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2 , Chinese National High School Mathematics Olympiad (CNMO 2024) 3 , and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024).",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several benchmarks and challenges, but most are excluded as they are primarily used for score comparison. Only those that refer to specific, downloadable datasets are included.",
          "citing_paper_doi": "10.48550/arXiv.2501.12948",
          "cited_paper_doi": "10.48550/arXiv.2306.09212",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34471a2fa18ea22efad5287cf4aeb18542c98a9b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bb9a44c94a89dbe00f0061d05c70a45064ff6ea6",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to test advanced language understanding capabilities, focusing on complex and challenging tasks. | Used to evaluate models on a wide range of tasks, focusing on massive multitask language understanding across various domains. | Used to evaluate models on interactive and feedback-based tasks, assessing their ability to engage in dynamic dialogues. | Used to assess models on frame-based reasoning tasks, focusing on understanding and generating structured information. | Used to assess software engineering benchmark tasks, focusing on verified and reliable performance metrics. | Used to evaluate distilled models on graduate-level, Google-proof questions, focusing on complex reasoning and knowledge integration. | Used to evaluate the performance of language models on graduate-level, Google-proof questions, focusing on complex reasoning and knowledge retrieval. | Used to evaluate live coding tasks, focusing on real-time code generation and execution. | Used to assess model performance on a revised version of MMLU, emphasizing updated and expanded task coverage.",
          "citing_paper_id": "275789950",
          "cited_paper_id": 265295009,
          "context_text": "Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1 , LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2 , Chinese National High School Mathematics Olympiad (CNMO 2024) 3 , and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024).",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several benchmarks and challenges, but most are excluded as they are primarily used for score comparison. Only those that refer to specific, downloadable datasets are included.",
          "citing_paper_doi": "10.48550/arXiv.2501.12948",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/34471a2fa18ea22efad5287cf4aeb18542c98a9b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/210b0a3d76e93079cc51b03c4115fde545eea966",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to test advanced language understanding capabilities, focusing on complex and challenging tasks. | Used to evaluate models on a wide range of tasks, focusing on massive multitask language understanding across various domains. | Used to evaluate models on interactive and feedback-based tasks, assessing their ability to engage in dynamic dialogues. | Used to assess models on frame-based reasoning tasks, focusing on understanding and generating structured information. | Used to assess software engineering benchmark tasks, focusing on verified and reliable performance metrics. | Used to evaluate live coding tasks, focusing on real-time code generation and execution. | Used to assess model performance on a revised version of MMLU, emphasizing updated and expanded task coverage.",
          "citing_paper_id": "275789950",
          "cited_paper_id": 272753636,
          "context_text": "Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1 , LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2 , Chinese National High School Mathematics Olympiad (CNMO 2024) 3 , and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024).",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several benchmarks and challenges, but most are excluded as they are primarily used for score comparison. Only those that refer to specific, downloadable datasets are included.",
          "citing_paper_doi": "10.48550/arXiv.2501.12948",
          "cited_paper_doi": "10.48550/arXiv.2409.12941",
          "citing_paper_url": "https://www.semanticscholar.org/paper/34471a2fa18ea22efad5287cf4aeb18542c98a9b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/46ff7e02fd4ff5fdfb9f85bc7071725b8089061f",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "208617407",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "WebShop"
      ],
      "dataset_details": [
        {
          "dataset_name": "WebShop",
          "dataset_description": "Utilized to assess embodied household tasks, examining the model's ability to interact with and manipulate objects in a virtual home setting. | Used to evaluate model performance on out-of-distribution test sets, focusing on unseen variations in embodied environments for interactive learning. | Used to assess web navigation capabilities, focusing on the ability to complete tasks in a simulated online shopping environment. | Applied to evaluate performance in simulated science experiments, testing the model's understanding and execution of scientific procedures. | Used for web navigation experiments to evaluate planning capabilities of LLMs in interactive web environments. | Used to evaluate the performance of ETO in an interactive learning environment, focusing on the first iteration's improvement in aligning text and embodied environments. | Used for embodied household tasks to test LLMs' planning and interaction skills in a home-like setting. | Used to evaluate model performance on out-of-distribution test sets, focusing on unseen variations in scientific environments. | Used for embodied science experiments to assess LLMs' ability to perform scientific tasks in a simulated environment.",
          "citing_paper_id": "268249221",
          "cited_paper_id": 222208810,
          "context_text": "Datasets We conduct experiments on three representative agent datasets, WebShop (Yao et al., 2022a) for web navigation, ScienceWorld (Wang et al., 2022) for embodied science experiments, and ALFWorld (Shridhar et al., 2021) for embodied house holding tasks.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for conducting experiments on planning capabilities of LLMs in different environments.",
          "citing_paper_doi": "10.48550/arXiv.2403.02502",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f95da5b7be2fac2381eb5dfe26dc7dc5bc2d9a90",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "WebShop",
          "dataset_description": "Used to evaluate grounded language agents, specifically serving 200 user queries as a test subset to assess performance in real-world web interaction tasks. | Used for multi-hop question-answering tasks, requiring logical reasoning across Wikipedia passages via the Wikipedia API, focusing on diverse and explainable reasoning processes. | Used to evaluate multi-hop question answering capabilities, focusing on explainability and diverse reasoning tasks.",
          "citing_paper_id": "267897975",
          "cited_paper_id": 250264533,
          "context_text": "For the Webshop benchmark, BOLAA comprises 900 user queries, of which we serve 200 as a test subset.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'Webshop benchmark' and 'BOLAA', but BOLAA is not a dataset; it is part of the Webshop benchmark. The Webshop benchmark is a specific, verifiable resource used for evaluating language agents.",
          "citing_paper_doi": "10.48550/arXiv.2402.15506",
          "cited_paper_doi": "10.48550/arXiv.2207.01206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3ae65bb64fcf6a7d114f5df8888c9cede8d6b57f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/23525374cfd3af714f3ffb7a203b1ef3253333fe",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "WebShop",
          "dataset_description": "Used to assess the effectiveness of ReAct in web navigation and task completion, comparing its performance to Act through a set of predefined challenges. | Used to assess fact verification performance, specifically evaluating the ability to verify claims using evidence from Wikipedia articles. | Used to evaluate the performance of ReAct in interpreting grounded instructions for everyday tasks, comparing it to Act using a series of benchmark tests. | Used for evaluating question answering capabilities, focusing on multi-hop reasoning and complex queries. | Used to evaluate question answering capabilities, focusing on multi-hop reasoning and complex queries in the context of empirical evaluations. | Used to train agents to interpret user instructions and perform tasks such as purchasing products, focusing on the variety of structured and unstructured texts from Amazon. | Used for fact verification, assessing the ability to determine the truthfulness of claims using evidence from Wikipedia. | Used to evaluate the performance of GPT-3 and PaLM-540B in question-answering tasks, focusing on multi-hop reasoning and complex queries.",
          "citing_paper_id": "252762395",
          "cited_paper_id": 208617407,
          "context_text": "Unlike ALFWorld, Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions, and options crawled from Amazon), and requires an agent to purchase a product based on a user instruction (e.g. “I am looking for a nightstand with drawers.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Webshop' as a dataset containing structured and unstructured texts from Amazon, used for training agents to perform tasks like purchasing products based on user instructions.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/cvpr42600.2020.01075",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4cf4246f3882aa6337e9c05d5675a3b8463a32e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "WebShop",
          "dataset_description": "Used to train models in an embodied environment for interactive learning, focusing on aligning text and actions in a virtual world. | Used to train models for web-based tasks, focusing on the ability to follow instructions and perform actions on web pages with high accuracy. | Used to train models on text and embodied environments, focusing on interactive learning tasks with a turns ratio of 13.52. | Used to train models for web navigation and interaction tasks, evaluating the effectiveness of different planning strategies in complex web environments. | Used to train models on web navigation and shopping tasks, with a turns ratio of 3.68 and 23.6% of the train split. | Used to train models for tasks requiring structured knowledge, focusing on the integration of textual and graph-based information for improved performance. | Used to train models on knowledge graph tasks, with a turns ratio of 6.04 and 13.0% of the train split.",
          "citing_paper_id": "264306101",
          "cited_paper_id": 222208810,
          "context_text": "Turns Ratio ALFWorld (Shridhar et al., 2020) Train split 954 336 13.52 35.2% WebShop (Yao et al., 2022) Train split 1,485 351 3.68 23.6% Mind2Web (Deng et al., 2023) Train split 23,378 122 1.00 1 0.52% Knowledge Graph (Liu et al., 2023) Train split 2,501 324 6.04 13.0% Operating System (Liu et al.,…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used in training splits for different environments or tasks. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2310.12823",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/46fe9ce789408b8a50fb4259e6bf0cc5855f4ed5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "WebShop",
          "dataset_description": "Used to evaluate the performance of LUMOS on an unseen task, specifically comparing it against larger LLM-based agents and domain-specific agents in an online shopping environment. | Used to evaluate the performance of LUMOS in an online shopping text game, comparing it against larger LLM-based agents and measuring reward points.",
          "citing_paper_id": "270823634",
          "cited_paper_id": 250264533,
          "context_text": "When evaluated on an unseen task, namely WebShop (Yao et al., 2022a), a text game for online shopping, L UMOS surpasses the performance of larger LLM-based agents (for instance, WizardLM-30B (Xu et al., 2023b)) by approximately 20 reward points and also delivers a notable 5-10 reward point improvement over domain-specific agents.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'WebShop' as a text game for online shopping used to evaluate the performance of LUMOS. It is a specific, verifiable resource used in the evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2311.05657",
          "cited_paper_doi": "10.48550/arXiv.2207.01206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "cited_paper_url": "https://www.semanticscholar.org/paper/23525374cfd3af714f3ffb7a203b1ef3253333fe",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "8528277",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "CLUTRR"
      ],
      "dataset_details": [
        {
          "dataset_name": "CLUTRR",
          "dataset_description": "Used to evaluate inductive reasoning capabilities of models from textual data, focusing on relational reasoning and generalization.",
          "citing_paper_id": "256416127",
          "cited_paper_id": 198184456,
          "context_text": "We use the CLUTRR (Sinha et al., 2019) benchmark described in Section 3.4.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "CLUTRR is mentioned as a benchmark, but since it is used for evaluating inductive reasoning from text, it is included as a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2301.13379",
          "cited_paper_doi": "10.18653/v1/D19-1458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5e35895fc4731858f0b286cb5a1613a819cc2367",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "CLUTRR",
          "dataset_description": "Used to evaluate systematic generalization and robust reasoning in logical stories, focusing on unseen combinations of rules and noisy descriptions.",
          "citing_paper_id": "259859069",
          "cited_paper_id": 220496138,
          "context_text": "CLUTRR consists of two subtasks, systematic generalization that evaluates stories containing un-seen combinations of logical rules (Minervini et al., 2020; Bergen et al., 2021) and robust reasoning that evaluates stories with noisy descriptions (Tian et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions CLUTRR, which is a dataset used for evaluating systematic generalization and robust reasoning in logical stories. The cited papers support the use of CLUTRR for these specific tasks.",
          "citing_paper_doi": "10.48550/arXiv.2307.07696",
          "cited_paper_doi": "10.3233/faia210359",
          "citing_paper_url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2e8c84fd61c91e067dddef52ced76b824beb7013",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "CLUTRR",
          "dataset_description": "Used to evaluate robust reasoning tasks in neural networks, focusing on the ability to handle logical rules and combinations. The dataset is employed to test various models, including ASP, MAC, BiLSTM-attention, and GSM.",
          "citing_paper_id": "259859069",
          "cited_paper_id": 8528277,
          "context_text": "Since we use ASP for logical reasoning, which easily works for any combination of logical rules, we focus on the robust reasoning task. et al., 2017), MAC (Hudson and Manning, 2018), BiLSTM-attention (Sinha et al., 2019), and GSM (Tian et al., 2021) on the original CLUTRR dataset, namely CLUTRR…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the CLUTRR dataset, which is used for evaluating robust reasoning tasks in neural networks. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2307.07696",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/007112213ece771be72cbecfd59f048209facabd",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "CLUTRR",
          "dataset_description": "Used to evaluate logical reasoning tasks in natural language understanding, focusing on robustness across clean, supporting, irrelevant, and mixed data instances. | Used to evaluate generative-symbolic models for logical reasoning in natural language understanding, focusing on the performance of different models on relational reasoning tasks. | Used to evaluate systematic generalization and robust reasoning in logical stories, focusing on unseen combinations of rules and noisy descriptions. | Used to evaluate robust reasoning in NLU tasks, focusing on four categories of data instances: clean, supporting, irrelevant, and disconnected. | Used to evaluate the performance of generative-symbolic models and BiLSTM encoders on logical reasoning tasks in natural language understanding.",
          "citing_paper_id": "259859069",
          "cited_paper_id": 235408314,
          "context_text": "CLUTRR consists of two subtasks, systematic generalization that evaluates stories containing un-seen combinations of logical rules (Minervini et al., 2020; Bergen et al., 2021) and robust reasoning that evaluates stories with noisy descriptions (Tian et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions CLUTRR, which is a dataset used for evaluating systematic generalization and robust reasoning in logical stories. The cited papers support the use of CLUTRR for these specific tasks.",
          "citing_paper_doi": "10.48550/arXiv.2307.07696",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ee4daad5dac290735e4912576e7d6b3739dc3010",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "259837542",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "iGibson"
      ],
      "dataset_details": [
        {
          "dataset_name": "iGibson",
          "dataset_description": "Used to provide 3D scenes for object placement, contributing to the creation of a dataset with rearrangement preferences for various objects in different rooms. | Used to curate object categories, contributing to the diversity of objects in the rearrangement preferences dataset.",
          "citing_paper_id": "248986485",
          "cited_paper_id": 9954873,
          "context_text": "This dataset contains rearrangement preferences for 1799 objects, in 585 placements, in 105 rooms from iGibson dataset scenes [25], constituting 1500+ hours of effort from 372 total annotators with 268 object categories curated from the Amazon-Berkeley [15], YCB objects [31], Google Scanned Objects [23], and iGibson [25] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions multiple datasets by name, all of which are used to curate a new dataset containing rearrangement preferences. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2205.10712",
          "cited_paper_doi": "10.1109/ICAR.2015.7251504",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7890ece03cfb88e0620f8e791105569bd7128c76",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ecb09b54ea93f0fb25ad5211bfbbf26a5483f59e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "iGibson",
          "dataset_description": "Used to curate object categories for a dataset containing rearrangement preferences, focusing on object placement in rooms with detailed annotations.",
          "citing_paper_id": "248986485",
          "cited_paper_id": null,
          "context_text": "This dataset contains rearrangement preferences for 1799 objects, in 585 placements, in 105 rooms, constituting 1500+ hours of effort from 372 total annotators with 268 object categories curated from the Amazon-Berkeley [30], YCB objects [83], Google Scanned Objects [54], and iGibson [62] datasets.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context describes a specific dataset with detailed characteristics and mentions other datasets used to curate it. The main dataset is unnamed but described in detail, while the others are clearly named and used as sources.",
          "citing_paper_doi": "10.48550/arXiv.2205.10712",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7890ece03cfb88e0620f8e791105569bd7128c76",
          "cited_paper_url": null,
          "citing_paper_year": 2022,
          "cited_paper_year": null
        },
        {
          "dataset_name": "iGibson",
          "dataset_description": "Used to evaluate LLM-based approaches in unseen apartments, with eight training scenes for module development and prompt engineering, and seven test scenes for evaluation.",
          "citing_paper_id": "268379149",
          "cited_paper_id": 259837542,
          "context_text": "In contrast to previous LLM-based works [5], [15], we evaluate all approaches in completely unseen apart-ments, following the data split of the iGibson challenge into eight training scenes for the development of all modules and prompt engineering and seven test scenes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the iGibson challenge, which is a specific dataset used for evaluating models in unseen environments. The dataset is used to train and test models in specific 3D scenes.",
          "citing_paper_doi": "10.1109/LRA.2024.3441495",
          "cited_paper_doi": "10.48550/arXiv.2307.06135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e0cbbf3460ca16d27269966fefcb9f4954cc293c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3843469976bd895851bfa08c8208350745bf649f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "iGibson",
          "dataset_description": "Used to evaluate LLM-based approaches in unseen apartments, with eight training scenes for module development and prompt engineering, and seven test scenes for evaluation.",
          "citing_paper_id": "268379149",
          "cited_paper_id": 261660608,
          "context_text": "In contrast to previous LLM-based works [5], [15], we evaluate all approaches in completely unseen apart-ments, following the data split of the iGibson challenge into eight training scenes for the development of all modules and prompt engineering and seven test scenes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the iGibson challenge, which is a specific dataset used for evaluating models in unseen environments. The dataset is used to train and test models in specific 3D scenes.",
          "citing_paper_doi": "10.1109/LRA.2024.3441495",
          "cited_paper_doi": "10.48550/arXiv.2309.04077",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e0cbbf3460ca16d27269966fefcb9f4954cc293c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5a9d4bcffa9989cac4139b2844358884ae023e8d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "235667974",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "R2R"
      ],
      "dataset_details": [
        {
          "dataset_name": "R2R",
          "dataset_description": "Used to evaluate vision-and-language navigation models, focusing on interpreting visually-grounded instructions in real environments.",
          "citing_paper_id": "258947250",
          "cited_paper_id": 4673790,
          "context_text": "5 on the R2R dataset (Anderson et al. 2018).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the R2R dataset, which is a specific dataset used for vision-and-language navigation tasks. The dataset is clearly identified and used for evaluating performance.",
          "citing_paper_doi": "10.48550/arXiv.2305.16986",
          "cited_paper_doi": "10.1109/CVPR.2018.00387",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8199c9d55dd998f69f703e0ad250ca0697e3ad27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c37c23b12e00168833eccff8025a830ce27c5abc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "R2R",
          "dataset_description": "Used to train models for identifying landmarks from observations in navigation tasks, focusing on cross-modality matching in Visual Language Navigation (VLN).",
          "citing_paper_id": "258947250",
          "cited_paper_id": 235667974,
          "context_text": "…R2R data (Hong et al. 2020b; He et al. 2021; Zhao et al. 2022), and plan to reach the destination by effectively identifying landmarks from observations, similar to works on utilizing objects information to perform cross-modality matching in VLN (Gao et al. 2021; Qi et al. 2020a, 2021).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'R2R data' which is a specific dataset used in navigation tasks involving visual and language instructions. The dataset is used to train models to identify landmarks from observations.",
          "citing_paper_doi": "10.48550/arXiv.2305.16986",
          "cited_paper_doi": "10.1109/CVPR46437.2021.00308",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8199c9d55dd998f69f703e0ad250ca0697e3ad27",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1c9cea1cfc9183f51e617b3cdfe6b1d045c23286",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "R2R",
          "dataset_description": "Used to train and evaluate models that generate action sequences for navigation tasks, focusing on end-to-end performance with Transformer models. | Used to evaluate models that generate action sequences for navigation tasks using Transformer models, focusing on performance in visually-grounded environments.",
          "citing_paper_id": "254408960",
          "cited_paper_id": 4673790,
          "context_text": "In navigation-only VLN datasets such as R2R [2], models that generate the action sequence end-to-end with a Transformer model can already achieve a good performance [25,35].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'navigation-only VLN datasets' and specifically cites 'R2R'. R2R is a known dataset in the field of vision-and-language navigation.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00280",
          "cited_paper_doi": "10.1109/CVPR.2018.00387",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c37c23b12e00168833eccff8025a830ce27c5abc",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "R2R",
          "dataset_description": "Used to train and evaluate models that generate action sequences for navigation tasks, focusing on end-to-end performance with Transformer models.",
          "citing_paper_id": "254408960",
          "cited_paper_id": 236975859,
          "context_text": "In navigation-only VLN datasets such as R2R [2], models that generate the action sequence end-to-end with a Transformer model can already achieve a good performance [37, 27].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'navigation-only VLN datasets such as R2R', which is a specific dataset used in vision-and-language navigation tasks. The dataset is used to train and evaluate models that generate action sequences for navigation.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00280",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0af51c6a6041a439796a334f65327a992c8a3e63",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "237142385",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "DS-1000"
      ],
      "dataset_details": [
        {
          "dataset_name": "DS-1000",
          "dataset_description": "Used for evaluation in data science scenarios, measuring the reliability and naturalness of code generation by LLMs. | Used to evaluate complex question answering, requiring multi-hop reasoning over multiple paragraphs. | Used to test theorem-driven question answering, focusing on logical reasoning and mathematical proofs. | Used to evaluate decision-making capabilities of large language models in a simulated environment, focusing on task completion and reasoning skills. | Used to assess multi-turn interaction and code generation capabilities of LLMs, specifically in the HumanEval subset for human-like code evaluation. | Used to evaluate decision-making and planning capabilities in interactive environments, focusing on text-based games. | Used to test LLMs in generating code that interacts with robot APIs, assessing the ability to call and control robotic functions through code. | Used to evaluate multi-turn interaction capabilities of LLMs in code generation tasks, focusing on the MBPP subset for benchmarking performance. | Used to evaluate reasoning capabilities in large language models, focusing on multi-choice questions across various subjects. | Used to evaluate LLMs in generating SQL and Bash scripts, focusing on the syntactic and semantic correctness of the generated code.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 235755472,
          "context_text": "• Code benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to test Python writing abilities, Spider (Yu et al., 2018) to assess database query capabilities, MultiPL-E (Cas-sano et al., 2023) to measure the multi-lingual coding capabilities, DS-1000 (Lai et al., 2023) for evaluation in data science scenarios 3.2 C ONNECTING LLM A GENTS TO E NVIRONMENT While the measurements in §3.1 provide valuable insights on each domain, they may not fully encapsulate the models’ capabilities in language agent scenarios due to their focus on single-turn interactions in fully observable settings.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks, but only 'Spider' and 'DS-1000' are specific datasets with clear identifiers and are used for evaluating specific capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DS-1000",
          "dataset_description": "Used for evaluation in data science scenarios, measuring the reliability and naturalness of code generation by LLMs. | Used to evaluate complex question answering, requiring multi-hop reasoning over multiple paragraphs. | Used to test theorem-driven question answering, focusing on logical reasoning and mathematical proofs. | Used to evaluate decision-making capabilities of large language models in a simulated environment, focusing on task completion and reasoning skills. | Used to assess multi-turn interaction and code generation capabilities of LLMs, specifically in the HumanEval subset for human-like code evaluation. | Used to evaluate decision-making and planning capabilities in interactive environments, focusing on text-based games. | Used to test LLMs in generating code that interacts with robot APIs, assessing the ability to call and control robotic functions through code. | Used to evaluate multi-turn interaction capabilities of LLMs in code generation tasks, focusing on the MBPP subset for benchmarking performance. | Used to evaluate reasoning capabilities in large language models, focusing on multi-choice questions across various subjects. | Used to evaluate LLMs in generating SQL and Bash scripts, focusing on the syntactic and semantic correctness of the generated code.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 237142385,
          "context_text": "• Code benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to test Python writing abilities, Spider (Yu et al., 2018) to assess database query capabilities, MultiPL-E (Cas-sano et al., 2023) to measure the multi-lingual coding capabilities, DS-1000 (Lai et al., 2023) for evaluation in data science scenarios 3.2 C ONNECTING LLM A GENTS TO E NVIRONMENT While the measurements in §3.1 provide valuable insights on each domain, they may not fully encapsulate the models’ capabilities in language agent scenarios due to their focus on single-turn interactions in fully observable settings.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks, but only 'Spider' and 'DS-1000' are specific datasets with clear identifiers and are used for evaluating specific capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "DS-1000",
          "dataset_description": "Used for evaluation in data science scenarios, measuring the reliability and naturalness of code generation by LLMs.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 52815560,
          "context_text": "• Code benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to test Python writing abilities, Spider (Yu et al., 2018) to assess database query capabilities, MultiPL-E (Cas-sano et al., 2023) to measure the multi-lingual coding capabilities, DS-1000 (Lai et al., 2023) for evaluation in data science scenarios 3.2 C ONNECTING LLM A GENTS TO E NVIRONMENT While the measurements in §3.1 provide valuable insights on each domain, they may not fully encapsulate the models’ capabilities in language agent scenarios due to their focus on single-turn interactions in fully observable settings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several benchmarks, but only 'Spider' and 'DS-1000' are specific datasets with clear identifiers and are used for evaluating specific capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.18653/v1/D18-1425",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e773b1840b894603c06b677a0f15ebcf0f26378",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "DS-1000",
          "dataset_description": "Used for evaluation in data science scenarios, measuring the reliability and naturalness of code generation by LLMs.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 253734939,
          "context_text": "• Code benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to test Python writing abilities, Spider (Yu et al., 2018) to assess database query capabilities, MultiPL-E (Cas-sano et al., 2023) to measure the multi-lingual coding capabilities, DS-1000 (Lai et al., 2023) for evaluation in data science scenarios 3.2 C ONNECTING LLM A GENTS TO E NVIRONMENT While the measurements in §3.1 provide valuable insights on each domain, they may not fully encapsulate the models’ capabilities in language agent scenarios due to their focus on single-turn interactions in fully observable settings.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several benchmarks, but only 'Spider' and 'DS-1000' are specific datasets with clear identifiers and are used for evaluating specific capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2211.11501",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8a4fc5f00cd4aca61e148e46a2125c3a406719f1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "237142385",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "ALF-World"
      ],
      "dataset_details": [
        {
          "dataset_name": "ALF-World",
          "dataset_description": "Used to evaluate interactive learning and decision-making in embodied environments, focusing on aligning text and physical actions. | Used to evaluate the model's ability to prove mathematical theorems, focusing on formal logic and proof construction. | Used to assess multi-hop question answering capabilities, focusing on diverse and explainable reasoning processes. | This dataset 'GSM8K' was mentioned in the citation context but no detailed description was generated.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 237142385,
          "context_text": "(Austin et al., 2021) 500 91 Decision Making ALFWorld (Shridhar et al., 2020) 134 134 Reasoning GSM8K (Cobbe et al., 2021) 1319 48 HotpotQA (Yang et al., 2018) 7,405 43 MATH (Hendrycks et al., 2021) 5,000 100 MMLU (Hendrycks et al., 2020) 13,985 76 TheoremQA (Chen et al., 2023a) 800 49 Total 29,307…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating decision-making and reasoning capabilities of LLMs. Each dataset is associated with a specific task or domain, such as multi-hop question answering, math problems, and theorem proving.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "ALF-World",
          "dataset_description": "Used to assess the performance of grounded language agents in scalable real-world web interaction tasks. | Used to evaluate language-based interactive decision-making systems, focusing on aligning text and embodied environments for interactive learning.",
          "citing_paper_id": "274060374",
          "cited_paper_id": 222208810,
          "context_text": "…experiments, we demonstrate that Prospec-tor outperforms recent advancements such as Re-Act (Yao et al., 2023) and Reflexion (Shinn et al., 2023) on the representative language-based interactive decision-making benchmarks including ALF-World (Shridhar et al., 2021) and WebShop (Yao et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ALF-World' and 'WebShop' as benchmarks for evaluating language-based interactive decision-making systems. These are specific datasets used for evaluation.",
          "citing_paper_doi": "10.18653/v1/2024.findings-emnlp.879",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/61349d9d243e344246502c461c21817ec94d4ba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ALF-World",
          "dataset_description": "Used to assess the performance of grounded language agents in scalable real-world web interaction tasks. | Used to evaluate language-based interactive decision-making systems, focusing on aligning text and embodied environments for interactive learning.",
          "citing_paper_id": "274060374",
          "cited_paper_id": 250264533,
          "context_text": "…experiments, we demonstrate that Prospec-tor outperforms recent advancements such as Re-Act (Yao et al., 2023) and Reflexion (Shinn et al., 2023) on the representative language-based interactive decision-making benchmarks including ALF-World (Shridhar et al., 2021) and WebShop (Yao et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ALF-World' and 'WebShop' as benchmarks for evaluating language-based interactive decision-making systems. These are specific datasets used for evaluation.",
          "citing_paper_doi": "10.18653/v1/2024.findings-emnlp.879",
          "cited_paper_doi": "10.48550/arXiv.2207.01206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/61349d9d243e344246502c461c21817ec94d4ba0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/23525374cfd3af714f3ffb7a203b1ef3253333fe",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ALF-World",
          "dataset_description": "Utilized to train agents in web browsing tasks, emphasizing the ability to follow complex instructions and navigate web interfaces effectively. | Used to train agents in household environments, focusing on navigation and manipulation tasks to enhance planning capabilities. | Applied to develop agents capable of performing real-world web interactions, specifically shopping tasks, to improve sequential decision-making.",
          "citing_paper_id": "260682249",
          "cited_paper_id": 250264533,
          "context_text": "The remaining 3 are recompiled from published datasets, which are house-holding (Alfworld [71]), web shopping (WebShop [96]), and web browsing (Mind2Web [19]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions three datasets by name, all of which are relevant to the topic of planning capabilities in language models. Each dataset is associated with a specific domain of interaction.",
          "citing_paper_doi": "10.48550/arXiv.2308.03688",
          "cited_paper_doi": "10.48550/arXiv.2207.01206",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dbf93a68b7fda600521f046dea35ea8ba9e884f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/23525374cfd3af714f3ffb7a203b1ef3253333fe",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "53734356",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "NL2Bash"
      ],
      "dataset_details": [
        {
          "dataset_name": "NL2Bash",
          "dataset_description": "Used for interactive coding, testing the model's ability to generate bash commands from natural language instructions. | Used for embodied task, evaluating the model's performance in interactive environments with text and visual inputs.",
          "citing_paper_id": "265128672",
          "cited_paper_id": 53734356,
          "context_text": "” Even if the agent calculated how many times James sprints (Xie et al., 2024) Travel Planning 225 SciQ (Welbl et al., 2017) Question Answering 11679 GrailQA (Gu et al., 2021) Knowledge Graph Reasoning 44337 NL2Bash (Lin et al., 2018) Interactive Coding 8090 AlfWorld (Shridhar et al., 2020) Embodied Task 3553 VCR (Zellers et al., 2019) Multimodal Reasoning 212923 LILA (Mishra et al., 2022) Math 93670 a week, which is 3 × 3 = 9 , the mere number 9 does not affect the next subgoal generation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets with specific names and their usage in various reasoning tasks, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.18653/v1/2024.acl-long.670",
          "cited_paper_doi": "10.1109/CVPR.2019.00688",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6dfc2ff03534a4325d06c6f88c3144831996629b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "NL2Bash",
          "dataset_description": "Used for interactive coding, testing the model's ability to generate bash commands from natural language instructions. | Applied to translate natural language commands into executable shell commands, emphasizing interactive coding and command synthesis. | Used for embodied task, evaluating the model's performance in interactive environments with text and visual inputs. | Used to highlight the need for new actions in embodied environments, such as open, take, and move, which are not present in existing annotations. | Utilized for embodied task learning in text-based environments, aligning textual instructions with actions in a simulated world. | Used for evaluating complex question answering over knowledge graphs, focusing on multi-hop reasoning and compositional generalization. | Applied to interactive coding, assessing the capability of models to translate natural language commands into executable shell scripts. | Used to demonstrate the requirement for travel-related actions like CitySearch and FlightSearch, which are also not covered in the existing training set. | Utilized for embodied task planning, testing the ability of models to navigate and interact with simulated environments using textual instructions.",
          "citing_paper_id": "265128672",
          "cited_paper_id": 222208810,
          "context_text": "” Even if the agent calculated how many times James sprints (Xie et al., 2024) Travel Planning 225 SciQ (Welbl et al., 2017) Question Answering 11679 GrailQA (Gu et al., 2021) Knowledge Graph Reasoning 44337 NL2Bash (Lin et al., 2018) Interactive Coding 8090 AlfWorld (Shridhar et al., 2020) Embodied Task 3553 VCR (Zellers et al., 2019) Multimodal Reasoning 212923 LILA (Mishra et al., 2022) Math 93670 a week, which is 3 × 3 = 9 , the mere number 9 does not affect the next subgoal generation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets with specific names and their usage in various reasoning tasks, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.18653/v1/2024.acl-long.670",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "NL2Bash",
          "dataset_description": "Used for interactive coding, testing the model's ability to generate bash commands from natural language instructions. | Used for embodied task, evaluating the model's performance in interactive environments with text and visual inputs.",
          "citing_paper_id": "265128672",
          "cited_paper_id": 253237047,
          "context_text": "” Even if the agent calculated how many times James sprints (Xie et al., 2024) Travel Planning 225 SciQ (Welbl et al., 2017) Question Answering 11679 GrailQA (Gu et al., 2021) Knowledge Graph Reasoning 44337 NL2Bash (Lin et al., 2018) Interactive Coding 8090 AlfWorld (Shridhar et al., 2020) Embodied Task 3553 VCR (Zellers et al., 2019) Multimodal Reasoning 212923 LILA (Mishra et al., 2022) Math 93670 a week, which is 3 × 3 = 9 , the mere number 9 does not affect the next subgoal generation.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets with specific names and their usage in various reasoning tasks, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.18653/v1/2024.acl-long.670",
          "cited_paper_doi": "10.48550/arXiv.2210.17517",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "NL2Bash",
          "dataset_description": "Used for interactive coding, specifically translating natural language commands into executable shell scripts. | Used for interactive environment navigation and task completion, focusing on embodied AI tasks in a simulated environment.",
          "citing_paper_id": "265128672",
          "cited_paper_id": 267406800,
          "context_text": "” Even if the agent calculated how many times James sprints (Xie et al., 2024) Travel Planning 225 SciQ (Welbl et al., 2017) Question Answering 11679 GrailQA (Gu et al., 2021) Knowledge Graph Reasoning 44337 NL2Bash (Lin et al., 2018) Interactive Coding 8090 AlfWorld (Shridhar et al., 2020)…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets with specific names and their usage in various tasks related to planning capabilities of language models.",
          "citing_paper_doi": "10.18653/v1/2024.acl-long.670",
          "cited_paper_doi": "10.48550/arXiv.2402.01622",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11155af5ccd1889277f4269f6bb349a7633554f4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "237142385",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "HumanEval"
      ],
      "dataset_details": [
        {
          "dataset_name": "HumanEval",
          "dataset_description": "Used to evaluate program synthesis techniques with 974 short Python functions, focusing on the effectiveness of large language models in generating correct code. | Used to evaluate the performance of baselines and LATS on programming tasks, focusing on the importance of external observations for complex reasoning. | Used to assess the effectiveness of baselines and LATS in solving programming problems, emphasizing the role of external observations in complex reasoning tasks.",
          "citing_paper_id": "263829963",
          "cited_paper_id": 237142385,
          "context_text": "To demonstrate the importance of external observations for complex reasoning tasks, we evaluate the baselines and LATS on programming with HumanEval (Chen et al., 2021) 1 and MBPP (Austin et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions HumanEval and MBPP as evaluation datasets for programming tasks, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.04406",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/700bd9681f1b9e9e2212e10415d27b11c7e6836b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "HumanEval",
          "dataset_description": "Used to evaluate the performance of large language models in generating correct Python programs, focusing on coding challenges and program synthesis. | Used to assess the ability of large language models to solve programming problems, specifically targeting the generation of correct Python code.",
          "citing_paper_id": "269921354",
          "cited_paper_id": 237142385,
          "context_text": "…3D [67] ✗ Planning Guo et al. [68] Multi-Agent Cooperation VirtualHome-Social ✗ Decision, Communication MetaGPT [69] Coding HumanEval [58], MBPP [70] In addition to LLM-based MARL, several works explored multi-agent interaction [72]–[74], e.g., multi-agent conversation and gaming.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions 'HumanEval' and 'MBPP', which are known datasets used for evaluating programming tasks. However, they are mentioned in the context of coding challenges, not planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2405.11106",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd38755291d108ab86c68d1aac7485921bb8e647",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "HumanEval",
          "dataset_description": "Used to assess GPT-4’s performance without self-reflection, highlighting a 3% drop in performance on code generation tasks. | Used to evaluate GPT-4’s performance with self-reflection, focusing on code generation tasks and assessing improvements in accuracy.",
          "citing_paper_id": "269921148",
          "cited_paper_id": null,
          "context_text": "For instance, while self-reflection boosts GPT-4’s performance on the HumanEval dataset, it drops by 3% on the MBPP dataset (Shinn et al., 2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two datasets, HumanEval and MBPP, which are used to evaluate the performance of GPT-4 with and without self-reflection.",
          "citing_paper_doi": "10.48550/arXiv.2405.11403",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e81c707040ce604c7102cfe14d78b72385c17b68",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "46761158",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "SCAN"
      ],
      "dataset_details": [
        {
          "dataset_name": "SCAN",
          "dataset_description": "Used to assess the ability of models to perform discrete reasoning over paragraphs, focusing on question answering tasks that require numerical reasoning and multi-step operations. | Used to evaluate compositional generalization in sequence-to-sequence models, focusing on navigation commands and their execution. The dataset tests the ability of models to generalize to unseen command-action pairs.",
          "citing_paper_id": "248986239",
          "cited_paper_id": 237385685,
          "context_text": "The two benchmarks considered in this paper, SCAN (Lake & Baroni, 2018) and DROP (Dua et al., 2019), have often been tackled by neural-symbolic methods (Andor et al., 2019; Chen et al., 2019, 2020; Nye et al., 2020; Shaw et al., 2021; Kim, 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions SCAN and DROP as benchmarks, which are typically used to evaluate the compositional generalization capabilities of models, including LLMs. These benchmarks are relevant to the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2205.10625",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SCAN",
          "dataset_description": "Used to evaluate systematic generalization in sequence-to-sequence models, focusing on compositional skills and rule-based generalization in neural networks. | Used to evaluate a wide range of linguistic generalizations by grounding the semantics of input instructions, focusing on compositional skills in sequence-to-sequence models.",
          "citing_paper_id": "212658007",
          "cited_paper_id": 46761158,
          "context_text": "Systematic, rule-based generalization is instead at the core of the recently introduced SCAN dataset (Lake & Baroni, 2018; see also Hupkes et al., 2019 for related ideas).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The SCAN dataset is explicitly mentioned and is a known benchmark for evaluating systematic generalization in neural networks, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4bc2bb6584774b0d8ad0b4f5215dc2075487c192",
          "cited_paper_url": "https://www.semanticscholar.org/paper/856fe866bcce5e7a540655bea6ecc7406bdcfcba",
          "citing_paper_year": 2020,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "SCAN",
          "dataset_description": "Used to study compositional generalization in language models, focusing on permutation equivariance to assess model capabilities in handling novel compositions.",
          "citing_paper_id": "212658007",
          "cited_paper_id": 213928056,
          "context_text": "Gordon et al. (2020) formalize the type of compositionality skills required by the SCAN dataset as equiv-ariance to a certain group of permutations.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the SCAN dataset, which is a specific, verifiable dataset used to study compositional generalization in language models.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4bc2bb6584774b0d8ad0b4f5215dc2075487c192",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3249dec80e963cbc86d941b819c549a325613f8c",
          "citing_paper_year": 2020,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "6401679",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "2Wiki"
      ],
      "dataset_details": [
        {
          "dataset_name": "2Wiki",
          "dataset_description": "Used to evaluate Auto-RAG's capability in multi-hop question answering, requiring reasoning over multiple documents. | Used to evaluate Auto-RAG's performance on multi-hop question answering using Wikipedia articles. | Used to assess multi-hop QA capabilities, emphasizing the need for reasoning across multiple documents to answer complex questions.",
          "citing_paper_id": "274422541",
          "cited_paper_id": 6401679,
          "context_text": "…effectiveness and robustness of Auto-RAG, we conducted assessments across six datasets: NQ, 2Wiki, TriviaQA (TQA) (Joshi et al., 2017), PopQA (PQA) (Mallen et al., 2023), Under review as a conference paper at ICLR 2025 HotpotQA (HQA) (Yang et al., 2018), and WebQuestions (WQ) (Berant et al., 2013).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions six datasets by name, all of which are specific and verifiable. These datasets are used to assess the effectiveness and robustness of Auto-RAG.",
          "citing_paper_doi": "10.48550/arXiv.2411.19443",
          "cited_paper_doi": "10.18653/v1/d13-1160",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d1beece295703c0cb3e545edaa12a4336b407bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b29447ba499507a259ae9d8f685d60cc1597d7d3",
          "citing_paper_year": 2024,
          "cited_paper_year": 2013
        },
        {
          "dataset_name": "2Wiki",
          "dataset_description": "Used to evaluate Auto-RAG's capability in multi-hop question answering, requiring reasoning over multiple documents. | Used to assess multi-hop QA capabilities, emphasizing the need for reasoning across multiple documents to answer complex questions. | Used to evaluate the effectiveness and robustness of Auto-RAG in answering complex questions from Wikipedia articles. | Used to evaluate Auto-RAG's capability in multi-hop question answering, requiring reasoning over multiple sentences. | Used to assess Auto-RAG's performance in multi-hop question answering using two Wikipedia articles. | Used to evaluate Auto-RAG's performance on multi-hop question answering using Wikipedia articles.",
          "citing_paper_id": "274422541",
          "cited_paper_id": 52822214,
          "context_text": "…effectiveness and robustness of Auto-RAG, we conducted assessments across six datasets: NQ, 2Wiki, TriviaQA (TQA) (Joshi et al., 2017), PopQA (PQA) (Mallen et al., 2023), Under review as a conference paper at ICLR 2025 HotpotQA (HQA) (Yang et al., 2018), and WebQuestions (WQ) (Berant et al., 2013).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions six datasets by name, all of which are specific and verifiable. These datasets are used to assess the effectiveness and robustness of Auto-RAG.",
          "citing_paper_doi": "10.48550/arXiv.2411.19443",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d1beece295703c0cb3e545edaa12a4336b407bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "2Wiki",
          "dataset_description": "Used to assess multi-hop QA capabilities, emphasizing the need for reasoning across multiple documents to answer complex questions. | Used to assess Auto-RAG's capability in answering complex questions requiring multi-hop reasoning and evidence combination. | Used to assess Auto-RAG's performance in generating answers from two-hop Wikipedia articles, focusing on multi-hop reasoning. | Used to evaluate the effectiveness and robustness of Auto-RAG in answering complex questions from Wikipedia articles. | Used to evaluate Auto-RAG's performance in answering questions using Freebase, focusing on the integration of structured and unstructured data. | Used to evaluate Auto-RAG's capability in multi-hop question answering, requiring reasoning over multiple sentences. | Used to assess Auto-RAG's performance in multi-hop question answering using two Wikipedia articles.",
          "citing_paper_id": "274422541",
          "cited_paper_id": 26501419,
          "context_text": "To evaluate the effectiveness and robustness of Auto-RAG, we conducted assessments across six datasets: NQ, 2Wiki, TriviaQA (TQA) (Joshi et al., 2017), PopQA (PQA) (Mallen et al., 2023), Under review as a conference paper at ICLR 2025 HotpotQA (HQA) (Yang et al., 2018), and WebQuestions (WQ) (Berant et al., 2013).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions six datasets by name, all of which are specific and verifiable. Each dataset is used to evaluate the effectiveness and robustness of Auto-RAG.",
          "citing_paper_doi": "10.48550/arXiv.2411.19443",
          "cited_paper_doi": "10.18653/v1/P17-1147",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d1beece295703c0cb3e545edaa12a4336b407bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f010affab57b5fcf1cd6be23df79d8ec98c7289c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "230799347",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "ASDIV"
      ],
      "dataset_details": [
        {
          "dataset_name": "ASDIV",
          "dataset_description": "Used to assess symbolic reasoning abilities, focusing on challenging tasks designed to test advanced cognitive skills. | Used to assess the performance of language models on arithmetic word problems, emphasizing the complexity and variability of problem-solving approaches. | Utilized to test the capability of solving arithmetic problems involving multiple operations, enhancing the evaluation of step-by-step reasoning. | Utilized to test solving of arithmetic story problems, specifically targeting diverse and challenging scenarios. | Employed to measure performance on a variety of arithmetic word problems, providing a benchmark for evaluating mathematical reasoning and problem-solving. | Employed to evaluate mathematical word problem-solving skills, covering a wide range of problem types and complexities. | Applied to assess performance on single-variable arithmetic problems, emphasizing accuracy and generalization.",
          "citing_paper_id": "253708270",
          "cited_paper_id": 220047831,
          "context_text": "…tasks: (1) mathematical problems (§4.1) from a wide range of datasets including GSM 8 K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), and MAWPS (Koncel-Kedziorski et al., 2016); (2) symbolic reasoning (§4.2) from BIG-Bench Hard (Suzgun et al., 2022) Q: On the…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluating mathematical problem-solving capabilities of models, which are directly relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2211.10435",
          "cited_paper_doi": "10.18653/v1/2020.acl-main.92",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f13e41d24e5d0a68ca662c1b49de398a6fb68251",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ASDIV",
          "dataset_description": "Used to assess symbolic reasoning abilities, focusing on challenging tasks designed to test advanced cognitive skills. | Utilized to test solving of arithmetic story problems, specifically targeting diverse and challenging scenarios. | Used to assess mathematical word problem-solving skills, emphasizing the ability to translate textual information into mathematical expressions. | Employed to evaluate mathematical word problem-solving skills, covering a wide range of problem types and complexities. | Used to test symbolic reasoning abilities, specifically evaluating the model's performance on complex and challenging tasks from the BIG-Bench Hard suite. | Applied to assess performance on single-variable arithmetic problems, emphasizing accuracy and generalization.",
          "citing_paper_id": "253708270",
          "cited_paper_id": 252917648,
          "context_text": "Data and in-context examples We experiment with three broad classes of reasoning tasks: (1) mathematical problems (§4.1) from a wide range of datasets including GSM 8 K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), and MAWPS (Koncel-Kedziorski et al., 2016); (2) symbolic reasoning (§4.2) from BIG-Bench Hard (Suzgun et al., 2022) Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating reasoning tasks, particularly in mathematical and symbolic reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2211.10435",
          "cited_paper_doi": "10.48550/arXiv.2210.09261",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ASDIV",
          "dataset_description": "Used to evaluate Flan-LongT5's ability to solve grade school math problems in few-shot/zero-shot settings, focusing on chain-of-thought reasoning. | Used to assess Flan-LongT5's capability to answer questions requiring strategic reasoning, emphasizing implicit reasoning strategies. | Used to assess Flan-LongT5's commonsense reasoning abilities, focusing on answering questions that require everyday knowledge. | Used to evaluate Flan-LongT5 in few-shot/zero-shot settings, focusing on complex arithmetic reasoning problems. | Used to test Flan-LongT5's performance on single-variable arithmetic word problems, emphasizing comprehension and calculation accuracy. | Used to evaluate Flan-LongT5's performance on a wide range of tasks across multiple domains, assessing its general knowledge and reasoning skills. | Used to challenge Flan-LongT5 with difficult tasks from the BIG-Bench suite, testing its general reasoning and problem-solving capabilities. | Used to challenge Flan-LongT5 with a variety of hard tasks, assessing its general reasoning and problem-solving abilities. | Used to measure Flan-LongT5's understanding of commonsense reasoning through multiple-choice questions. | Used to evaluate Flan-LongT5's ability to solve arithmetic word problems, particularly those involving diverse and complex scenarios. | Used to evaluate Flan-LongT5's capability in solving diverse arithmetic word problems, covering various difficulty levels and types. | Used to assess Flan-LongT5's ability to solve questions requiring strategic reasoning and multi-step thinking. | Used to test Flan-LongT5's performance on solving arithmetic word problems, focusing on multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 230799347,
          "context_text": "As a sanity check of instruction-tuning, we evaluate Flan-LongT5 with few-shot/zero-shot settings on CoT benchmark (GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), SVAMP (Patel et al., 2021), Asdiv (Miao et al., 2021), CommonsenseQA (Talmor et al., 2019)), BigBench-Hard (BBH) (Suzgun et al., 2022), and MMLU (Hendrycks et al., 2021b) as tested in Longpre et al. (2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and datasets used to evaluate Flan-LongT5 in few-shot/zero-shot settings. These are specific datasets with clear identifiers and are used for evaluating the model's performance.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "263605637",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "nuScenes"
      ],
      "dataset_details": [
        {
          "dataset_name": "nuScenes",
          "dataset_description": "Used to evaluate the effectiveness, generalization ability, and interpretability of GPT-Driver in large-scale and real-world autonomous driving scenarios.",
          "citing_paper_id": "263605637",
          "cited_paper_id": 246411621,
          "context_text": "In this section, we demonstrate the effectiveness, generalization ability, and interpretability of our GPT-Driver through extensive experiments on the large-scale and real-world nuScenes dataset Cae-sar et al. (2020).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'nuScenes dataset' which is a specific, verifiable dataset used for autonomous driving research.",
          "citing_paper_doi": "10.48550/arXiv.2310.01415",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/958ed4830ae80a189ecb9b93ab75a6ce2e3926fc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "nuScenes",
          "dataset_description": "Used to evaluate the GPT-Driver model, focusing on effectiveness, generalization, and interpretability in real-world autonomous driving scenarios.",
          "citing_paper_id": "263605637",
          "cited_paper_id": 85517967,
          "context_text": "In this section, we demonstrate the effectiveness, generalization ability, and interpretability of our GPT-Driver through extensive experiments on the large-scale and real-world nuScenes dataset Caesar et al. (2020). We first introduce the experimental settings and evaluation metrics, and then",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'nuScenes' dataset, which is a specific, verifiable resource used for autonomous driving research. The dataset is clearly identified and used in the context of evaluating the GPT-Driver model.",
          "citing_paper_doi": "10.48550/arXiv.2310.01415",
          "cited_paper_doi": "10.1109/cvpr42600.2020.01164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/958ed4830ae80a189ecb9b93ab75a6ce2e3926fc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9e475a514f54665478aac6038c262e5a6bac5e64",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "nuScenes",
          "dataset_description": "Used to fine-tune the LLM with human driving trajectories, focusing on motion planning capabilities through one epoch of training.",
          "citing_paper_id": "265294541",
          "cited_paper_id": 263605637,
          "context_text": "For motion planning, we follow [23] and fine-tune the LLM with human driving trajectories in the nuScenes training set for one epoch .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'nuScenes training set' which is a specific dataset used for training the LLM with human driving trajectories.",
          "citing_paper_doi": "10.48550/arXiv.2311.10813",
          "cited_paper_doi": "10.48550/arXiv.2310.01415",
          "citing_paper_url": "https://www.semanticscholar.org/paper/357e182a38219625dd37cba526befe5f8429aa4b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/958ed4830ae80a189ecb9b93ab75a6ce2e3926fc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "237504552",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "SmartPlay"
      ],
      "dataset_details": [
        {
          "dataset_name": "SmartPlay",
          "dataset_description": "Used to evaluate agent capabilities across various tasks, including Two-armed Bandits, Rock Paper Scissors, Messenger, Crafter, and Minecraft creative navigation tasks.",
          "citing_paper_id": "263608611",
          "cited_paper_id": 832073,
          "context_text": "Our initial release of SmartPlay consists of Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie et al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan et al., 2022) creative navigation tasks.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several environments/tasks but does not specify them as datasets. They are likely game environments or tasks used to evaluate agent capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2310.01557",
          "cited_paper_doi": "10.1007/s13218-010-0082-7",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0a2c3e734349232781eb95319dc41f8d8fb671a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/28c9c822fcaf69fb18a0fc4e385113f3f1fd2669",
          "citing_paper_year": 2023,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "SmartPlay",
          "dataset_description": "Used to evaluate agent capabilities across various tasks, including Two-armed Bandits, Rock Paper Scissors, Messenger, Crafter, and Minecraft creative navigation tasks.",
          "citing_paper_id": "263608611",
          "cited_paper_id": 237504552,
          "context_text": "Our initial release of SmartPlay consists of Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie et al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan et al., 2022) creative navigation tasks.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several environments/tasks but does not specify them as datasets. They are likely game environments or tasks used to evaluate agent capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2310.01557",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0a2c3e734349232781eb95319dc41f8d8fb671a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e128a1b2efb0ddf688902ade4405d22d5b61eec",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "SmartPlay",
          "dataset_description": "Used to evaluate agent capabilities across various tasks, including Two-armed Bandits, Rock Paper Scissors, Messenger, Crafter, and Minecraft creative navigation tasks. | Simplified for LLMs to control a hand-coded agent in finding specific biomes, focusing on creative tasks within the Minecraft environment.",
          "citing_paper_id": "263608611",
          "cited_paper_id": 249848263,
          "context_text": "Our initial release of SmartPlay consists of Two-armed Bandits, Rock Paper Scissors, Messenger (Hanjie et al., 2021), Crafter (Hafner, 2021), and Minecraft (Fan et al., 2022) creative navigation tasks.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several environments/tasks but does not specify them as datasets. They are likely game environments or tasks used to evaluate agent capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2310.01557",
          "cited_paper_doi": "10.48550/arXiv.2206.08853",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0a2c3e734349232781eb95319dc41f8d8fb671a0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "249889477",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "ACPBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "ACPBench",
          "dataset_description": "Standardizes evaluation tasks and metrics for assessing reasoning about actions, changes, and planning across 13 domains and 22 SOTA language models, including OpenAI’s o1 reasoning model.",
          "citing_paper_id": "277313841",
          "cited_paper_id": 269214439,
          "context_text": "ACPBench (Kokel et al. 2024) standardizes evaluation tasks and metrics for assessing reasoning about actions, changes (transitions), and planning—across 13 domains, on 22 SOTA language models including OpenAI’s o1 reasoning model.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "ACPBench is mentioned as a benchmark for evaluating reasoning about actions, changes, and planning across multiple domains and models. It is not a traditional dataset but a standardized evaluation framework.",
          "citing_paper_doi": "10.48550/arXiv.2503.18971",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5038e4830f01e44fe59bddc707128b8777d71d96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b0886f8c52f4fdae95ca8cb29a9200598713c966",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "ACPBench",
          "dataset_description": "Used to evaluate large language models on planning and reasoning tasks, focusing on improving performance over time and addressing poor performance on these tasks.",
          "citing_paper_id": "273228858",
          "cited_paper_id": 249889477,
          "context_text": "Improving perfor-Figure Performance over time One of our major motivations to generate and release the ACPBench collection is to encourage researchers to address the poor performance on these tasks and build models that are capable to perform reasoning required for better planning.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ACPBench collection, which is a specific benchmark collection. However, it is not clear if it is a dataset or a benchmark suite. Given the topic, it is likely a dataset used for evaluating planning capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2410.05669",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/7bee11da995d968394477a66e78aaab9721ecdf7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ACPBench",
          "dataset_description": "Used to generate an evaluation set of questions for assessing the planning capabilities of large language models, focusing on reliability in automated planning tasks. | Used to create a challenging evaluation set of questions to test the reliability of large language models in producing components for automated planners, emphasizing complex PDDL domains. | Used to generate an evaluation set of questions for assessing large language models' ability to produce reliable components for automated planners, focusing on planning domain definition language (PDDL) tasks. | Used to create a challenging subset of ACPBench for evaluating the ability of large language models to produce reliable components for automated planning.",
          "citing_paper_id": "277467626",
          "cited_paper_id": 2730999,
          "context_text": "We generate an evaluation set of questions based on a wide collection of 13 Planning Domain Definition Language (PDDL) (McDermott 2000) domains from ACPBench, which we call ACPBench Hard , in an attempt to evaluate the ability of large language models to produce reliable components for automated planners, as models that perform well on these tasks could in principle be integrated into a planner or be used directly as a policy.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ACPBench' and 'ACPBench Hard', which are specific collections of PDDL domains used for evaluating planning capabilities of large language models.",
          "citing_paper_doi": "10.48550/arXiv.2503.24378",
          "cited_paper_doi": "10.1609/aimag.v21i2.1506",
          "citing_paper_url": "https://www.semanticscholar.org/paper/151483ae8f33d60584f3c01ee017c521e2a28dfc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8995b12a886a177e98b83e6e1ae2f1eda3df20ef",
          "citing_paper_year": 2025,
          "cited_paper_year": 2000
        }
      ]
    },
    {
      "cited_paper_id": "262053695",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "MINT-MBPP"
      ],
      "dataset_details": [
        {
          "dataset_name": "MINT-MBPP",
          "dataset_description": "Used to assess multi-turn interaction and code generation capabilities of LLMs, specifically in the HumanEval subset for human-like code evaluation. | Used to test LLMs in generating code that interacts with robot APIs, assessing the ability to call and control robotic functions through code. | Used to evaluate multi-turn interaction capabilities of LLMs in code generation tasks, focusing on the MBPP subset for benchmarking performance. | Used to evaluate LLMs' planning capabilities in digital environments, focusing on complex interactive tasks and fast thinking processes. | Used to assess LLMs' performance in realistic web environments, emphasizing autonomous agent building and navigation tasks. | Used to test LLMs' planning abilities in physical environments, focusing on embodied agents and real-world interaction challenges. | Used to evaluate LLMs in generating SQL and Bash scripts, focusing on the syntactic and semantic correctness of the generated code.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 258960143,
          "context_text": "We use rich datasets from multiple benchmarks to evaluate this performance, including multi-turn MINT-MBPP and MINT-HumanEval in MINT (Wang et al., 2023b), SQL and Bash in InterCode (Yang et al., 2023), as well as RoboCodeGen that calls robot APIs through code (Liang et al., 2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating the performance of models in various programming and interaction tasks. These datasets are clearly named and used for benchmarking.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2305.17390",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d671d62a1eb4d57343e4a0928297266dffc0c118",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MINT-MBPP",
          "dataset_description": "Used to evaluate LLMs in multi-turn interactions involving theorem-driven question answering, requiring knowledge searches on Wikipedia and Python calculations. | Used to assess models' code generation and execution skills in multi-turn interactions, examining the effects of GPT-4 feedback on accuracy and efficiency. | Assesses the model’s capability to solve knowledge-based questions using Wikipedia searches, focusing on multi-hop reasoning and information retrieval. | Used to evaluate language agents' ability to solve complex multi-turn problems with tool-utilization, focusing on reasoning processes and interaction. | Used to test the model’s ability to solve mathematical problems using a Python interpreter, focusing on multi-step reasoning and problem-solving skills. | Used to evaluate LLM performance in multi-turn interactions with tools and language feedback, focusing on comprehensive scenario testing. | Used to assess multi-turn interaction and code generation capabilities of LLMs, specifically in the HumanEval subset for human-like code evaluation. | Assesses the model’s capability to solve knowledge-based questions using Wikipedia searches, focusing on multi-hop reasoning and multi-task learning. | Used to evaluate theorem-driven question answering capabilities in LLMs, focusing on multi-step reasoning and mathematical problem-solving. | Used to test LLMs in generating code that interacts with robot APIs, assessing the ability to call and control robotic functions through code. | Used to evaluate multi-turn interaction capabilities of LLMs in code generation tasks, focusing on the MBPP subset for benchmarking performance. | Used to evaluate LLMs in generating SQL and Bash scripts, focusing on the syntactic and semantic correctness of the generated code. | Used to evaluate models' reasoning capabilities in multi-turn interactions with language feedback, focusing on the impact of GPT-4 feedback on performance.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 262053695,
          "context_text": "We use rich datasets from multiple benchmarks to evaluate this performance, including multi-turn MINT-MBPP and MINT-HumanEval in MINT (Wang et al., 2023b), SQL and Bash in InterCode (Yang et al., 2023), as well as RoboCodeGen that calls robot APIs through code (Liang et al., 2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating the performance of models in various programming and interaction tasks. These datasets are clearly named and used for benchmarking.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2309.10691",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MINT-MBPP",
          "dataset_description": "Used to test LLMs in generating code that interacts with robot APIs, assessing the ability to call and control robotic functions through code. | Used to evaluate LLMs in generating SQL and Bash scripts, focusing on the syntactic and semantic correctness of the generated code. | Used to evaluate multi-turn interaction capabilities of LLMs in code generation tasks, focusing on the MBPP subset for benchmarking performance. | Used to assess multi-turn interaction and code generation capabilities of LLMs, specifically in the HumanEval subset for human-like code evaluation.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 252355542,
          "context_text": "We use rich datasets from multiple benchmarks to evaluate this performance, including multi-turn MINT-MBPP and MINT-HumanEval in MINT (Wang et al., 2023b), SQL and Bash in InterCode (Yang et al., 2023), as well as RoboCodeGen that calls robot APIs through code (Liang et al., 2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating the performance of models in various programming and interaction tasks. These datasets are clearly named and used for benchmarking.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.1109/ICRA48891.2023.10160591",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "3530344",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "MiniWoB++"
      ],
      "dataset_details": [
        {
          "dataset_name": "MiniWoB++",
          "dataset_description": "Used to evaluate large language models in executing computer tasks, comparing the number of demonstrations required against reinforcement learning methods.",
          "citing_paper_id": "270870063",
          "cited_paper_id": 268385171,
          "context_text": "Kim et al. (2024) showed that large language models can be prompted to execute computer tasks on Mini-WoB++ (Liu et al., 2018), requiring far fewer demonstrations than reinforcement learning methods.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'Mini-WoB++' as a dataset used to evaluate the performance of large language models in executing computer tasks. The dataset is clearly identified and used in the context of comparing different training methods.",
          "citing_paper_doi": "10.48550/arXiv.2407.01476",
          "cited_paper_doi": "10.48550/arXiv.2403.08978",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9345e55a21959948499cee997522aa5eac7ed588",
          "cited_paper_url": "https://www.semanticscholar.org/paper/157236d024e9043228570319e641b137b1ddcbd6",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "MiniWoB++",
          "dataset_description": "Used to evaluate models that generate grounded actions based on task and state information, demonstrating excellent performance in web-based tasks.",
          "citing_paper_id": "258179336",
          "cited_paper_id": 34953552,
          "context_text": "By considering the task information and state information, the models generate grounded actions and achieve excellent performance on MiniWoB++ (Shi et al., 2017); (6) notably, the recent birth of Auto-GPT6 further demonstrates the potential of foundation models in automating different tools and making long-term plans, pushing the boundaries of what is possible with tool learning.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions MiniWoB++, which is a known dataset for web-based agent tasks, but does not provide specific details on its usage in the current research context.",
          "citing_paper_doi": "10.1145/3704435",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/298a55ddc9777e39c5bad92a750827e1cae98ac1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "MiniWoB++",
          "dataset_description": "Used to assess the system's performance on web automation tasks, providing a standard benchmark for comparing different web interaction models. | Used to evaluate the performance of HTML-T5 on simulated web tasks, focusing on task completion and planning capabilities in web interfaces. | Used to assess the performance of HTML-T5 on a standard web automation benchmark, emphasizing the system's capabilities in web interaction tasks. | Used to evaluate the generalization of HTML-T5 to other websites, focusing on the system's ability to handle diverse web interfaces. | Used to evaluate the system's generalization to other websites, focusing on the performance and adaptability of HTML-T5 in diverse web environments. | Used to fine-tune HTML-T5 for web navigation tasks, specifically comparing performance across 56 MiniWoB++ tasks. | Used to fine-tune HTML-T5 for web automation tasks, specifically comparing performance across 56 MiniWoB++ tasks using reinforcement learning. | Used to assess the offline task planning abilities of HTML-T5, specifically evaluating its performance in complex web-based tasks.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 3530344,
          "context_text": "In addition, we quantify the performance of HTML-T5 itself on simulated web benchmark, MiniWoB++, and offline task planning benchmark, Mind2Web (Section 4.2).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two benchmarks, MiniWoB++ and Mind2Web, which are used to evaluate the performance of HTML-T5. These are specific, named benchmarks that are relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "202565722",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "SeqMultiWoz"
      ],
      "dataset_details": [
        {
          "dataset_name": "SeqMultiWoz",
          "dataset_description": "Used to build and evaluate multi-intent models, focusing on extending the capabilities of the original SNIPS dataset for more complex natural language processing tasks. | Used to assess model performance on schema-guided dialogue tasks, emphasizing scalability and multi-domain capabilities. | Curated from MixATIS to study spoken language understanding, focusing on task-oriented dialog systems and their planning capabilities. | Used to build and evaluate multi-intent models, focusing on extending the capabilities of the original ATIS dataset for more complex natural language processing tasks. | Used to convert mixed-intent utterances into a sequence of API calls, focusing on extracting parameters for each API call. | Curated from MixSNIPS to analyze spoken language understanding, particularly in the context of private-by-design voice interfaces and their planning capabilities. | Used to test in-distribution performance on multi-domain dialogue management, focusing on complex, multi-turn conversations. | Used to train and evaluate a spoken language understanding system, focusing on API sequence length and parameter count. The dataset supports research into the planning capabilities of language models. | Used to evaluate model performance on top-level dialogue tasks, likely involving complex conversational scenarios. | Used to evaluate in-distribution performance on topic-oriented dialogue systems, assessing the model's ability to handle specific topics.",
          "citing_paper_id": "267897727",
          "cited_paper_id": 44061213,
          "context_text": "Table 3 shows the evaluation results on five in-distribution datasets: SeqATIS, SeqSNIPS, Se-qSGD, SeqMultiWoz, and SeqTopV2.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used for evaluation, which are relevant to the topic of planning capabilities in language models.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/15c10b24ef645d83ff4059affd86945c33e00328",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "SeqMultiWoz",
          "dataset_description": "Used to train and evaluate multi-domain conversational agents, focusing on schema-guided dialogue management and natural language understanding. | Used to assess model performance on schema-guided dialogue tasks, emphasizing scalability and multi-domain capabilities. | Used to create SeqSGD, a tailored dataset for evaluating API sequences in multi-domain conversational agents, focusing on scalable dialogue management. | Used to test in-distribution performance on multi-domain dialogue management, focusing on complex, multi-turn conversations. | Used to evaluate model performance on top-level dialogue tasks, likely involving complex conversational scenarios. | Used to provide examples of schema-guided dialogue, focusing on understanding and generating structured responses in multi-domain conversational agents. | Used to evaluate in-distribution performance on topic-oriented dialogue systems, assessing the model's ability to handle specific topics.",
          "citing_paper_id": "267897727",
          "cited_paper_id": 202565722,
          "context_text": "Table 3 shows the evaluation results on five in-distribution datasets: SeqATIS, SeqSNIPS, Se-qSGD, SeqMultiWoz, and SeqTopV2.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used for evaluation, which are relevant to the topic of planning capabilities in language models.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": "10.1609/AAAI.V34I05.6394",
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/674b6321ae1d12c83f28ade1850a27256c20f0d4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "SeqMultiWoz",
          "dataset_description": "Used to support the hypothesis that the number of APIs in most datasets is not very large, specifically noting that Seq-MultiWoz has 12 APIs. | Used to test in-distribution performance on multi-domain dialogue management, focusing on complex, multi-turn conversations. | Used to evaluate in-distribution performance on topic-oriented dialogue systems, assessing the model's ability to handle specific topics.",
          "citing_paper_id": "267897727",
          "cited_paper_id": 233004444,
          "context_text": "Table 3 shows the evaluation results on five in-distribution datasets: SeqATIS, SeqSNIPS, Se-qSGD, SeqMultiWoz, and SeqTopV2.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions five specific datasets used for evaluation, which are relevant to the topic of planning capabilities in language models.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": "10.18653/v1/2022.sigdial-1.34",
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/322d5c9bbca1d5046416fb882523c87a48cbf86c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "235313679",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Ego4D"
      ],
      "dataset_details": [
        {
          "dataset_name": "Ego4D",
          "dataset_description": "Used to train a general-purpose visual encoder for robot perception, focusing on first-person video data to improve representation learning.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 235294299,
          "context_text": "1) Novel agent architecture: Decision Transformer [15, 50, 126] applies the powerful self-attention models to sequential decision making; 2) Pre-training for better representations: R3M [72] trains a general-purpose visual encoder for robot perception on Ego4D videos [36].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Ego4D videos' which is a specific dataset used for training a visual encoder. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Ego4D",
          "dataset_description": "Used to train a general-purpose visual encoder for robot perception, focusing on first-person video data to improve representation learning.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 235313679,
          "context_text": "1) Novel agent architecture: Decision Transformer [15, 50, 126] applies the powerful self-attention models to sequential decision making; 2) Pre-training for better representations: R3M [72] trains a general-purpose visual encoder for robot perception on Ego4D videos [36].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Ego4D videos' which is a specific dataset used for training a visual encoder. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f864d4d2267abba15eb43db54f58286aef78292b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Ego4D",
          "dataset_description": "Used to train a general-purpose visual encoder for robot perception, focusing on first-person video data to improve representation learning.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 246823673,
          "context_text": "1) Novel agent architecture: Decision Transformer [15, 50, 126] applies the powerful self-attention models to sequential decision making; 2) Pre-training for better representations: R3M [72] trains a general-purpose visual encoder for robot perception on Ego4D videos [36].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Ego4D videos' which is a specific dataset used for training a visual encoder. No other datasets are mentioned that meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eb92a453cf982126fa2125d4c8915352a52af54d",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "239998651",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "AddSub"
      ],
      "dataset_details": [
        {
          "dataset_name": "AddSub",
          "dataset_description": "Applied to assess addition and subtraction word problems, emphasizing step-by-step reasoning and numerical computation. | Utilized to test multi-step arithmetic problems, investigating complex problem-solving strategies and mathematical reasoning. | Applied to study simple and varied arithmetic problems, focusing on diverse problem types and solution methods. | Used to evaluate zero-shot and RCI methods in solving arithmetic word problems, focusing on the performance improvement over standard prompting.",
          "citing_paper_id": "257834038",
          "cited_paper_id": 428579,
          "context_text": "In the domain of arithmetic reasoning, we considered six datasets: SingleEq [34], AddSub [27], MultiArith [56], AQuA [35], GSM8K [9], and SVAMP [51].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly lists six datasets used for arithmetic reasoning, which are clearly named and specific.",
          "citing_paper_doi": "10.48550/arXiv.2303.17491",
          "cited_paper_doi": "10.3115/v1/D14-1058",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a7862e14b4c20cefd6dc4f611f8aa866fabf130b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        },
        {
          "dataset_name": "AddSub",
          "dataset_description": "Applied to assess addition and subtraction word problems, emphasizing step-by-step reasoning and numerical computation. | Utilized to test multi-step arithmetic problems, investigating complex problem-solving strategies and mathematical reasoning. | Applied to study simple and varied arithmetic problems, focusing on diverse problem types and solution methods. | Used to evaluate zero-shot and RCI methods in solving arithmetic word problems, focusing on the performance improvement over standard prompting.",
          "citing_paper_id": "257834038",
          "cited_paper_id": 4894130,
          "context_text": "In the domain of arithmetic reasoning, we considered six datasets: SingleEq [34], AddSub [27], MultiArith [56], AQuA [35], GSM8K [9], and SVAMP [51].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly lists six datasets used for arithmetic reasoning, which are clearly named and specific.",
          "citing_paper_doi": "10.48550/arXiv.2303.17491",
          "cited_paper_doi": "10.1162/tacl_a_00160",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/17230f5b3956188055a48c5f4f61d131cce0662f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "AddSub",
          "dataset_description": "Used to compare RCI prompting and baseline prompting methods, focusing on solving grade school math problems with language models. | Used to evaluate zero-shot and RCI methods in solving arithmetic word problems, focusing on the performance improvement over standard prompting. | Applied to assess addition and subtraction word problems, emphasizing step-by-step reasoning and numerical computation. | Utilized to test multi-step arithmetic problems, investigating complex problem-solving strategies and mathematical reasoning. | Applied to study simple and varied arithmetic problems, focusing on diverse problem types and solution methods.",
          "citing_paper_id": "257834038",
          "cited_paper_id": 239998651,
          "context_text": "In the domain of arithmetic reasoning, we considered six datasets: SingleEq [34], AddSub [27], MultiArith [56], AQuA [35], GSM8K [9], and SVAMP [51].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly lists six datasets used for arithmetic reasoning, which are clearly named and specific.",
          "citing_paper_doi": "10.48550/arXiv.2303.17491",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "271328989",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "DialogStudio"
      ],
      "dataset_details": [
        {
          "dataset_name": "DialogStudio",
          "dataset_description": "Used to enhance the instruction capability of xLAM by integrating diverse instruction-tuning datasets, focusing on conversational AI.",
          "citing_paper_id": "272424184",
          "cited_paper_id": 259982944,
          "context_text": "Specifically, to enhance the general instruction capability of xLAM, we integrate diverse instruction-tuning datasets from DialogStudio [38] and Data Provenance [39, 40].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'instruction-tuning datasets' from 'DialogStudio' and 'Data Provenance'. DialogStudio is a dataset collection, and Data Provenance is an initiative, but it may include datasets. The context suggests these are used to enhance the instruction capability of xLAM.",
          "citing_paper_doi": "10.48550/arXiv.2409.03215",
          "cited_paper_doi": "10.48550/arXiv.2307.10172",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a75da880b921d81426800a9893ce7c743339b278",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b34862afacf36e7011d40c67bb67c5ee9cf7da22",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "DialogStudio",
          "dataset_description": "Used to enhance the instruction capability of xLAM by integrating diverse instruction-tuning datasets, focusing on conversational AI.",
          "citing_paper_id": "272424184",
          "cited_paper_id": 264451840,
          "context_text": "Specifically, to enhance the general instruction capability of xLAM, we integrate diverse instruction-tuning datasets from DialogStudio [38] and Data Provenance [39, 40].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'instruction-tuning datasets' from 'DialogStudio' and 'Data Provenance'. DialogStudio is a dataset collection, and Data Provenance is an initiative, but it may include datasets. The context suggests these are used to enhance the instruction capability of xLAM.",
          "citing_paper_doi": "10.48550/arXiv.2409.03215",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a75da880b921d81426800a9893ce7c743339b278",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e817e7c898a6a52f0abd5acfb9de9e313b13ccf",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "DialogStudio",
          "dataset_description": "Used to enhance the instruction capability of xLAM by integrating diverse instruction-tuning datasets, focusing on conversational AI.",
          "citing_paper_id": "272424184",
          "cited_paper_id": 271328989,
          "context_text": "Specifically, to enhance the general instruction capability of xLAM, we integrate diverse instruction-tuning datasets from DialogStudio [38] and Data Provenance [39, 40].",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'instruction-tuning datasets' from 'DialogStudio' and 'Data Provenance'. DialogStudio is a dataset collection, and Data Provenance is an initiative, but it may include datasets. The context suggests these are used to enhance the instruction capability of xLAM.",
          "citing_paper_doi": "10.48550/arXiv.2409.03215",
          "cited_paper_doi": "10.48550/arXiv.2407.14933",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a75da880b921d81426800a9893ce7c743339b278",
          "cited_paper_url": "https://www.semanticscholar.org/paper/aa8ea913c82372466311f4aa8ebde62758791f90",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "216056509",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "E2ENLG"
      ],
      "dataset_details": [
        {
          "dataset_name": "E2ENLG",
          "dataset_description": "Used for few-shot learning experiments, specifically finetuning models with randomly sampled 0.1%, 0.5%, 1%, and 5% of training instances.",
          "citing_paper_id": "245218671",
          "cited_paper_id": 52908627,
          "context_text": "We follow the few-shot setup of Chen et al. (2020b) on the E2ENLG (Dušek et al., 2018) dataset, where we use randomly-sampled 0.1%, 0.5%, 1%, 5% of training instances for ﬁnetuning.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the E2ENLG dataset, which is a specific, verifiable resource used for few-shot learning experiments.",
          "citing_paper_doi": "10.18653/v1/2022.naacl-main.57",
          "cited_paper_doi": "10.18653/v1/W18-6539",
          "citing_paper_url": "https://www.semanticscholar.org/paper/304cf21da84961469ac9f43405df187441832b61",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92cfd6d2eb957805aaf4786dacb484081a469e80",
          "citing_paper_year": 2021,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "E2ENLG",
          "dataset_description": "Used for few-shot learning experiments, specifically finetuning models with randomly sampled 0.1%, 0.5%, 1%, and 5% of training instances.",
          "citing_paper_id": "245218671",
          "cited_paper_id": 216056509,
          "context_text": "We follow the few-shot setup of Chen et al. (2020b) on the E2ENLG (Dušek et al., 2018) dataset, where we use randomly-sampled 0.1%, 0.5%, 1%, 5% of training instances for ﬁnetuning.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the E2ENLG dataset, which is a specific, verifiable resource used for few-shot learning experiments.",
          "citing_paper_doi": "10.18653/v1/2022.naacl-main.57",
          "cited_paper_doi": "10.18653/v1/2020.acl-main.708",
          "citing_paper_url": "https://www.semanticscholar.org/paper/304cf21da84961469ac9f43405df187441832b61",
          "cited_paper_url": "https://www.semanticscholar.org/paper/342ec2f1c1b3d29d3269a2566c44f239f0141aeb",
          "citing_paper_year": 2021,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "E2ENLG",
          "dataset_description": "Used for few-shot learning experiments, specifically finetuning models with randomly sampled 0.1%, 0.5%, 1%, and 5% of training instances.",
          "citing_paper_id": "245218671",
          "cited_paper_id": null,
          "context_text": "We follow the few-shot setup of Chen et al. (2020b) on the E2ENLG (Dušek et al., 2018) dataset, where we use randomly-sampled 0.1%, 0.5%, 1%, 5% of training instances for ﬁnetuning.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the E2ENLG dataset, which is a specific, verifiable resource used for few-shot learning experiments.",
          "citing_paper_doi": "10.18653/v1/2022.naacl-main.57",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/304cf21da84961469ac9f43405df187441832b61",
          "cited_paper_url": null,
          "citing_paper_year": 2021,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "252693237",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PrOntoQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "PrOntoQA",
          "dataset_description": "Used to evaluate the reasoning capabilities of language models, focusing on formal logic and ontology-based questions. | Applied to assess the ability of language models to generate and verify logical proofs, emphasizing step-by-step reasoning. | Utilized to assess the ability of language models to handle first-order logic inference tasks. | Used to analyze the deductive reasoning capabilities of LLMs, specifically focusing on synthetic question answering tasks to evaluate model performance. | Used to evaluate reasoning capabilities in language models, focusing on formal logic and ontology-based questions.",
          "citing_paper_id": "258833332",
          "cited_paper_id": 252693237,
          "context_text": "PrOntoQA (Saparov and He, 2023) is a recent synthetic dataset created to analyze the capacity of LLMs for deductive reasoning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions PrOntoQA as a synthetic dataset used to analyze the deductive reasoning capabilities of LLMs, which aligns with the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2305.12295",
          "cited_paper_doi": "10.48550/arXiv.2210.01240",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9e9e4df2996bac794c4f04cb887df3e553bae4fd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "PrOntoQA",
          "dataset_description": "Used to evaluate logical reasoning capabilities of language models, focusing on systematic formal analysis of chain-of-thought reasoning.",
          "citing_paper_id": "264590280",
          "cited_paper_id": 252693237,
          "context_text": "(4) PrOntoQA (Saparov and He, 2022) is similar to ProofWriter for evaluating logical reasoning.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions PrOntoQA as a dataset for evaluating logical reasoning, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.18653/v1/2024.acl-long.531",
          "cited_paper_doi": "10.48550/arXiv.2210.01240",
          "citing_paper_url": "https://www.semanticscholar.org/paper/170c5b7e311a5004ed5db5d9eee1b736669273fb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "239998651",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MultiArith"
      ],
      "dataset_details": [
        {
          "dataset_name": "MultiArith",
          "dataset_description": "Used to evaluate commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Used to assess strategic thinking and reasoning, focusing on complex problem-solving and logical deduction. | Used to test multi-arithmetic problem-solving skills, focusing on complex arithmetic operations and reasoning. | Used to evaluate story-based math problems, focusing on contextual understanding and arithmetic reasoning. | Used to assess single equation problem-solving, focusing on simple algebraic reasoning and equation solving. | Used to assess the ability to solve complex math word problems, emphasizing step-by-step reasoning and verification. | Used to evaluate algebraic word problem solving, focusing on generating rationales and solutions using prompting techniques. | Used to assess commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Used to assess the ability to solve grade school math problems, emphasizing step-by-step reasoning and solution generation. | Used to evaluate the model's ability to solve story problems involving arithmetic operations, emphasizing comprehension and reasoning. | Used to test single-equation problem-solving, focusing on the ability to form and solve linear equations. | Used to test multi-step arithmetic problem-solving skills, focusing on the accuracy of intermediate calculations. | Used to evaluate strategic thinking and reasoning, focusing on complex questions that require multi-step logical deduction.",
          "citing_paper_id": "258558102",
          "cited_paper_id": 239998651,
          "context_text": "We experiment on six math reasoning datasets, including AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), MultiArith, AddSub, SingleEq, and SVAMP (Patel et al., 2021), two commonsense reasoning tasks (CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021)), and two symbolic…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for math and commonsense reasoning experiments. These datasets are clearly identified and used for evaluating model performance.",
          "citing_paper_doi": "10.48550/arXiv.2305.04091",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MultiArith",
          "dataset_description": "Used to assess verbal math problem solving, focusing on story problems and numerical reasoning. | Used to evaluate strategic question answering, focusing on multi-step reasoning and problem-solving techniques. | Used to assess complex arithmetic problem-solving skills, emphasizing step-by-step reasoning and solution accuracy. | Used to evaluate letter-based math problems, focusing on symbolic reasoning and problem-solving strategies. | Used to evaluate algebraic word problem solving, focusing on multi-step arithmetic reasoning and rationale generation.",
          "citing_paper_id": "266902900",
          "cited_paper_id": 12777818,
          "context_text": "We evaluate our proposal on eight dataset (MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), SingleEq (Koncel-Kedziorski et al., 2015), SAVMP (Patel et al., 2021), Letter (Wei et al., 2022), Coin (Wei et al., 2022), Strategyqa (Geva et al., 2021)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists multiple datasets used for evaluating the proposal, all of which are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2401.04925",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/40c0d1f38ab081e21cc3b1e2e5334a9b54b6ff08",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "259342833",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "TDW-MAP"
      ],
      "dataset_details": [
        {
          "dataset_name": "TDW-MAP",
          "dataset_description": "Used to evaluate multi-agent planning, focusing on decision-making and communication in complex environments. | Applied to assess multi-agent planning, emphasizing decision, communication, and memory in dynamic scenarios.",
          "citing_paper_id": "269921354",
          "cited_paper_id": 224803224,
          "context_text": "…Li et al. [61] Path Planning Close-source simulator ✗ Decision, Communication, Theory of Mind CoELA [62] Multi-Agent Planning TDW-MAT, C-WAH [63] ✓ Decision, Communication, Memory SMART-LLM [64] Multi-Agent Planning Proposed Benchmark Dataset ✗ Decision, Planning RoCo [65] Motion Planning…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'TDW-MAP' and 'C-WAH' as datasets used for multi-agent planning. However, 'Proposed Benchmark Dataset' is too generic and lacks a specific identifier.",
          "citing_paper_doi": "10.48550/arXiv.2405.11106",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd38755291d108ab86c68d1aac7485921bb8e647",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c562477737cc35e08d5a84aef01163ee4652d796",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "TDW-MAP",
          "dataset_description": "Utilized to assess multi-agent planning, emphasizing decision, communication, and memory in complex scenarios. | Used to evaluate multi-agent planning capabilities, focusing on decision-making and communication in a simulated environment.",
          "citing_paper_id": "269921354",
          "cited_paper_id": 259342833,
          "context_text": "…Consensus Seeking Generated Data ✗ Decision Li et al. [61] Path Planning Close-source simulator ✗ Decision, Communication, Theory of Mind CoELA [62] Multi-Agent Planning TDW-MAT, C-WAH [63] ✓ Decision, Communication, Memory SMART-LLM [64] Multi-Agent Planning Proposed Benchmark Dataset ✗…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'TDW-MAP' and 'C-WAH' as datasets used for multi-agent planning. However, 'Proposed Benchmark Dataset' is not a specific, verifiable dataset.",
          "citing_paper_doi": "10.48550/arXiv.2405.11106",
          "cited_paper_doi": "10.48550/arXiv.2307.02485",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd38755291d108ab86c68d1aac7485921bb8e647",
          "cited_paper_url": "https://www.semanticscholar.org/paper/587352c3b95c90de6d37f061c8e117f42be0b575",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "230799347",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "StrategyQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "StrategyQA",
          "dataset_description": "Used to evaluate implicit reasoning capabilities in question answering, focusing on multi-step reasoning processes required to answer yes/no questions.",
          "citing_paper_id": "263829338",
          "cited_paper_id": 230799347,
          "context_text": "• StrategyQA (Geva et al., 2021) is a yes/no QA dataset requiring implicit reasoning steps.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'StrategyQA' as a dataset used for question answering that requires implicit reasoning steps, which is directly relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.05915",
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/67daf8c4fe1958d20ebdf95c2a36dd490c73836f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "StrategyQA",
          "dataset_description": "Used to evaluate models on symbolic reasoning tasks involving memory and sequence reconstruction. | Used to evaluate models on symbolic reasoning tasks involving probability and logical operations. | Used to assess models' ability to answer strategic questions, emphasizing multi-step reasoning and commonsense knowledge. | Used to evaluate models on math word problems, focusing on diverse problem types and solution methods. | Used to assess models on symbolic reasoning tasks requiring string manipulation and pattern recognition.",
          "citing_paper_id": "269626390",
          "cited_paper_id": 220047831,
          "context_text": "…static test sets ranging from commonsense domains (Sports Understanding [41], StrategyQA [18], CommonSenseQA [44]), few-hop math word problems (AsDiv [31], GSM8k [10], MAWPS [27]), to a number of basic \"symbolic reasoning\" tasks (CoinFlip [26], LastLetterConcatenation [26], Shuffled Objects [41]).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating and developing models in various domains, including commonsense, math word problems, and symbolic reasoning tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/2020.acl-main.92",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f13e41d24e5d0a68ca662c1b49de398a6fb68251",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "259108190",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ToolAlpaca corpus"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolAlpaca corpus",
          "dataset_description": "Used to fine-tune Vicuna models, focusing on enhancing their planning capabilities through task-oriented dialogue and instruction-following tasks.",
          "citing_paper_id": "259108190",
          "cited_paper_id": null,
          "context_text": "Training We fine-tune Vicuna models (Vicuna-7B and Vicuna-13B) on ToolAlpaca corpus.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'ToolAlpaca corpus' as a specific dataset used for fine-tuning models. It is a multi-word proper noun and appears to be a verifiable resource.",
          "citing_paper_doi": "10.48550/arXiv.2306.05301",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/455866ca838f356b53a7e3e5b344834f9e93dbbc",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        },
        {
          "dataset_name": "ToolAlpaca corpus",
          "dataset_description": "Used to evaluate LLMs' tool usage and reasoning with 2,746 cases in English. | Used to assess LLMs' generalized tool learning capabilities through 3,938 simulated cases in English. | Used to evaluate LLMs' ability to interact with APIs, focusing on accuracy and tool usage in English. | Used to assess LLMs' tool usage and reasoning with 120 cases in English. | Used to train and evaluate language models on tool usage tasks, focusing on generalized tool learning with 3000 simulated cases. | Used to test LLMs' tool usage and reasoning skills with 12,657 cases in English. | Used to evaluate LLMs' tool usage and reasoning, though details are limited in the provided context. | Used to evaluate LLMs' performance on tool-related questions, emphasizing accuracy and tool usage in English.",
          "citing_paper_id": "271953942",
          "cited_paper_id": 259108190,
          "context_text": "…et al., 2023; Tang et al., 2023; Shen (Song et al., 2023) 2 94 157 English ✔ ✔ ✔ ✘ API-Bank (Li et al., 2023) 53 53 274 English ✔ ✘ ✔ ✔ ToolAlpaca (Tang et al., 2023) 400 400 3,938 English ✘ ✘ ✔ ✔ ToolQA (Zhuang et al., 2023) 13 13 1,600 English ✔ ✔ ✔ ✔ ToolBench1 (Qin et al., 2023a) 3,451 16,464…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several tools and datasets, but only 'ToolAlpaca' is a specific, verifiable dataset with a clear reference to a published paper.",
          "citing_paper_doi": "10.18653/v1/2024.findings-acl.928",
          "cited_paper_doi": "10.48550/arXiv.2306.05301",
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5676076ac122b8767126e76cb3a1867e1e50741",
          "cited_paper_url": "https://www.semanticscholar.org/paper/455866ca838f356b53a7e3e5b344834f9e93dbbc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "21435690",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Room-to-Room (R2R)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Room-to-Room (R2R)",
          "dataset_description": "Used to develop and evaluate vision-and-language navigation models, focusing on interpreting visually-grounded instructions in real indoor environments. | Used as a benchmark dataset for Vision-and-Language Navigation (VLN), focusing on interpreting visually-grounded navigation instructions in real indoor environments.",
          "citing_paper_id": "268512756",
          "cited_paper_id": 4673790,
          "context_text": "For instance, Room-to-Room (R2R) [1] is the first benchmark dataset for VLN, which is built on top of Matterport3D [4] scenes and it is equipped with discrete navigation graphs ( i.e., discrete environments) [1].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Room-to-Room (R2R)' and 'Matterport3D' as specific datasets. R2R is described as a benchmark dataset for Vision-and-Language Navigation (VLN) built on Matterport3D scenes.",
          "citing_paper_doi": "10.1109/IROS58592.2024.10801822",
          "cited_paper_doi": "10.1109/CVPR.2018.00387",
          "citing_paper_url": "https://www.semanticscholar.org/paper/32656886c8137f506f1bbc5e8eee63a38e18f191",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c37c23b12e00168833eccff8025a830ce27c5abc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "Room-to-Room (R2R)",
          "dataset_description": "Used to develop and evaluate vision-and-language navigation models, focusing on interpreting visually-grounded instructions in real indoor environments. | Used as a benchmark dataset for Vision-and-Language Navigation (VLN), focusing on interpreting visually-grounded navigation instructions in real indoor environments.",
          "citing_paper_id": "268512756",
          "cited_paper_id": 21435690,
          "context_text": "For instance, Room-to-Room (R2R) [1] is the first benchmark dataset for VLN, which is built on top of Matterport3D [4] scenes and it is equipped with discrete navigation graphs ( i.e., discrete environments) [1].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Room-to-Room (R2R)' and 'Matterport3D' as specific datasets. R2R is described as a benchmark dataset for Vision-and-Language Navigation (VLN) built on Matterport3D scenes.",
          "citing_paper_doi": "10.1109/IROS58592.2024.10801822",
          "cited_paper_doi": "10.1109/3DV.2017.00081",
          "citing_paper_url": "https://www.semanticscholar.org/paper/32656886c8137f506f1bbc5e8eee63a38e18f191",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8337441971f941716a9e525a67f37088eb01fd13",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "254221022",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Freebase"
      ],
      "dataset_details": [
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to construct a subgraph for multi-hop question answering, focusing on extracting triples within the maximum reasoning hops of question entities in WebQSP and CWQ.",
          "citing_paper_id": "263605944",
          "cited_paper_id": 231572861,
          "context_text": "To reduce the size of KGs, following previous works (He et al., 2021; Jiang et al., 2022), we construct a subgraph of Freebase by extracting all triples that contain within the max reasoning hops of question entities in WebQSP and CWQ. Similarly, we construct a subgraph of Wiki-Movies KGs for…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions constructing subgraphs from Freebase and Wiki-Movies KGs, which are specific knowledge bases used for multi-hop question answering.",
          "citing_paper_doi": "10.48550/arXiv.2310.01061",
          "cited_paper_doi": "10.1145/3437963.3441753",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/34ba905d57042bafc761e18284cfa29b6aaaa6e6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used to evaluate the performance of the method on multi-hop question answering tasks, specifically improving Hits@1 and F1 scores compared to the SOTA model UniKGQA. | Used to construct a subgraph for multi-hop question answering, focusing on extracting triples within the maximum reasoning hops of question entities in WebQSP and CWQ.",
          "citing_paper_id": "263605944",
          "cited_paper_id": 254221022,
          "context_text": "To reduce the size of KGs, following previous works (He et al., 2021; Jiang et al., 2022), we construct a subgraph of Freebase by extracting all triples that contain within the max reasoning hops of question entities in WebQSP and CWQ. Similarly, we construct a subgraph of Wiki-Movies KGs for…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions constructing subgraphs from Freebase and Wiki-Movies KGs, which are specific knowledge bases used for multi-hop question answering.",
          "citing_paper_doi": "10.48550/arXiv.2310.01061",
          "cited_paper_doi": "10.48550/arXiv.2212.00959",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "207167677",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CWQ"
      ],
      "dataset_details": [
        {
          "dataset_name": "CWQ",
          "dataset_description": "Used to evaluate complex question answering, focusing on reasoning over Freebase knowledge graphs.",
          "citing_paper_id": "263605944",
          "cited_paper_id": 207167677,
          "context_text": "Both WebQSP and CWQ can be reasoned based on Freebase KGs 7 (Bollacker et al., 2008).",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions WebQSP and CWQ, which are question answering datasets, but does not specify their usage. Freebase is mentioned as a knowledge graph, not a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2310.01061",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2023,
          "cited_paper_year": 2008
        },
        {
          "dataset_name": "CWQ",
          "dataset_description": "Used to fine-tune LLaMA2-Chat-7B, focusing on improving the model's ability to answer complex web questions through multi-hop reasoning. | Used to fine-tune LLaMA2-Chat-7B, enhancing the model's capability to handle complex and compositional questions from web sources.",
          "citing_paper_id": "263605944",
          "cited_paper_id": 259950998,
          "context_text": "For RoG , we use LLaMA2-Chat-7B (Touvron et al., 2023) as the LLM backbone, which is instruction finetuned on the training split of WebQSP and CWQ as well as Freebase for 3 epochs.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WebQSP and CWQ as datasets used for fine-tuning LLaMA2-Chat-7B. These are specific datasets with clear identifiers and are relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.01061",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/104b0bb1da562d53cbda87aec79ef6a2827d191a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "265466629",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "API-Bank"
      ],
      "dataset_details": [
        {
          "dataset_name": "API-Bank",
          "dataset_description": "Used to evaluate LLMs' tool usage and reasoning with 2,746 cases in English. | Used to assess LLMs' generalized tool learning capabilities through 3,938 simulated cases in English. | Used to evaluate LLMs' ability to interact with APIs, focusing on accuracy and tool usage in English. | Used to assess LLMs' tool usage and reasoning with 120 cases in English. | Used to test LLMs' tool usage and reasoning skills with 12,657 cases in English. | Used to evaluate LLMs' tool usage and reasoning, though details are limited in the provided context. | Used to evaluate LLMs' performance on tool-related questions, emphasizing accuracy and tool usage in English.",
          "citing_paper_id": "271953942",
          "cited_paper_id": null,
          "context_text": "In the second stage of evaluation, we start from the perspective of accuracy rather than having humans (Song et al., 2023; Tang et al., 2023; Shen (Song et al., 2023) 2 94 157 English ✔ ✔ ✔ ✘ API-Bank (Li et al., 2023) 53 53 274 English ✔ ✘ ✔ ✔ ToolAlpaca (Tang et al., 2023) 400 400 3,938 English ✘ ✘ ✔ ✔ ToolQA (Zhuang et al., 2023) 13 13 1,600 English ✔ ✔ ✔ ✔ ToolBench1 (Qin et al., 2023a) 3,451 16,464 12,657 English ✔ ✔ ✔ ✘ ToolBench2 (Xu et al., 2023) 8 232 2,746 English ✔ ✘ ✔ ✘ TPTU (Ruan et al., 2023) 12 12 120 English ✔ ✔ ✔ ✘ ToolEyes (Ye et al., With the proposed evaluation framework, we evaluate 11 LLMs with Chinese language capabilities.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating LLMs with Chinese language capabilities. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.18653/v1/2024.findings-acl.928",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d5676076ac122b8767126e76cb3a1867e1e50741",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "API-Bank",
          "dataset_description": "Used to evaluate the performance of FLAN-T5-XXL and MPT-30B, focusing on tool-related tasks and comparing their effectiveness. | Used to evaluate the performance of Falcon-40B and StarCoder-15B, focusing on API-based tasks and comparing against other models. | Used to assess the performance of FLAN-T5-XXL and MPT-30B, specifically for API-related tasks and benchmarking.",
          "citing_paper_id": "267897727",
          "cited_paper_id": 265466629,
          "context_text": "Falcon-40B and StarCoder-15B are showing better performance on our API-only test-set ToolLLM (we did not evaluate FLAN-T5-XXL on ToolLLM due to max sequence limit), whereas FLAN-T5-XXL and MPT-30B are doing well on API-Bank and ToolAlpaca.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ToolLLM', 'API-Bank', and 'ToolAlpaca' as specific datasets used for evaluating the performance of different language models. These names are specific and appear to be datasets used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": "10.48550/arXiv.2311.16867",
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c0312c604f9f7638bb4533b39e0ae81e7f6ab12",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "253098274",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Open Assistant crowdsourced annotated dialogue corpus"
      ],
      "dataset_details": [
        {
          "dataset_name": "Open Assistant crowdsourced annotated dialogue corpus",
          "dataset_description": "Included to train Lemur-Chat on complex coding tasks, using ChatGPT-generated problems and solutions to improve code generation and understanding. | Incorporated to improve chain of thought reasoning in Lemur-Chat, focusing on human-written tasks and logical problem-solving. | Utilized to incorporate real user interactions and ChatGPT history, enhancing the model's ability to handle diverse and realistic conversational scenarios. | Used to enhance dialogue capabilities in Lemur-Chat, providing annotated dialogues for training and evaluation.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 253098274,
          "context_text": "During the instruction fine-tuning phase, we include four data sources to construct Lemur-Chat, including the Open Assistant crowdsourced annotated dialogue corpus (K ¨ opf et al., 2023), Orca data with chain of thought reasoning for human-written tasks (Lian et al., 2023; Mukherjee et al., 2023), ShareGPT & Chatlogs containing real user and ChatGPT history records (ShareGPT data), as well as Evol-CodeAlpaca data (Luo et al., 2023) consisting of complex coding tasks generated by ChatGPT along with their solutions.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for constructing Lemur-Chat, which are relevant to the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.18653/v1/2022.emnlp-main.340",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Open Assistant crowdsourced annotated dialogue corpus",
          "dataset_description": "Included to train Lemur-Chat on complex coding tasks, using ChatGPT-generated problems and solutions to improve code generation and understanding. | Incorporated to improve chain of thought reasoning in Lemur-Chat, focusing on human-written tasks and logical problem-solving. | Utilized to incorporate real user interactions and ChatGPT history, enhancing the model's ability to handle diverse and realistic conversational scenarios. | Used to enhance dialogue capabilities in Lemur-Chat, providing annotated dialogues for training and evaluation.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 254877310,
          "context_text": "During the instruction fine-tuning phase, we include four data sources to construct Lemur-Chat, including the Open Assistant crowdsourced annotated dialogue corpus (K ¨ opf et al., 2023), Orca data with chain of thought reasoning for human-written tasks (Lian et al., 2023; Mukherjee et al., 2023), ShareGPT & Chatlogs containing real user and ChatGPT history records (ShareGPT data), as well as Evol-CodeAlpaca data (Luo et al., 2023) consisting of complex coding tasks generated by ChatGPT along with their solutions.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for constructing Lemur-Chat, which are relevant to the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2212.10560",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "218971783",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "VirtualHome dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "VirtualHome dataset",
          "dataset_description": "Used to simulate household activities via programs, showcasing tasks in eight different household scenes to study planning capabilities of LLMs.",
          "citing_paper_id": "272367167",
          "cited_paper_id": 49317780,
          "context_text": "VirtualHome (Puig et al., 2018a) VirtualHome is a collection of video simulations showcasing household tasks in eight different household scenes.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "VirtualHome is described as a collection of video simulations, which fits the criteria for a dataset. It is used to showcase household tasks in various scenes.",
          "citing_paper_doi": "10.48550/arXiv.2409.01806",
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8401ca54ccf4ee7237daadf0322ed186d587c47",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "VirtualHome dataset",
          "dataset_description": "Used to compare human-annotated action plans with those generated by GPT-3 models, focusing on task execution and planning capabilities.",
          "citing_paper_id": "246035276",
          "cited_paper_id": 218971783,
          "context_text": "Random Samples of Action Plans\nUncurated random sample action plans produced by human-annotators (from VirtualHome dataset), Vanilla GPT-3 175B, and Translated GPT-3 175B.\nHuman Vanilla GPT-3 175B Translated GPT-3 175B (Ours)\nTask: Go to sleep Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Find bed Step 4: Turn to bed Step 5: Look at bed Step 6: Sit on bed Step 7: Sleep\nTask: Go to sleep Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Find pajamas Step 4: Put on pajamas Step 5: Find slippers Step 6: Put on slippers Step 7: Get in bed Step 8: Sleep\nTask: Go to sleep Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Lie on bed Step 4: Sleep\nTask: Browse internet Step 1: Walk to home office Step 2: Walk to chair Step 3: Find chair Step 4: Sit on chair Step 5: Find computer Step 6: Switch on computer Step 7: Turn to computer Step 8: Look at computer Step 9: Find keyboard Step 10: Type on keyboard\nTask: Browse internet Step 1: Walk to home office Step 2: Walk to computer Step 3: Find computer Step 4: Grab computer Step 5: Find chair Step 6: Sit on chair Step 7: Browse internet\nTask: Browse internet Step 1: Walk to home office Step 2: Walk to desk Step 3: Find chair Step 4: Sit on chair Step 5: Find computer Step 6: Switch on computer Step 7: Find mouse Step 8: Grab mouse Step 9: Turn to computer Step 10: Point at computer\nTask: Wash teeth Step 1: Stand up Step 2: Walk to bathroom Step 3: Walk to sink Step 4: Find sink Step 5: Turn to sink Step 6: Look at sink Step 7: Find teeth Step 8: Wash teeth\nTask: Wash teeth Step 1: Walk to bathroom Step 2: Walk to sink Step 3: Turn on faucet Step 4: Grab toothbrush Step 5: Put toothpaste on toothbrush Step 6: Put toothbrush in mouth Step 7: Brush teeth Step 8: Rinse mouth out with water Step 9: Turn off faucet Step 10: Walk back to bedroom Task: Wash teeth Step 1: Walk to bathroom Step 2: Walk to sink Step 3: Find faucet Step 4: Switch on faucet Step 5: Walk to toothbrush Step 6: Grab toothbrush\nTask: Brush teeth Step 1: Walk to bathroom Step 2: Walk to tooth paste Step 3: Find tooth paste Step 4: Grab tooth paste Step 5: Find toothbrush Step 6: Grab toothbrush\nTask: Brush teeth Step 1: Walk to bathroom Step 2: Walk to sink Step 3: Turn on water Step 4: Grab toothbrush Step 5: Put toothpaste on toothbrush Step 6: Put toothbrush in mouth Step 7: Brush teeth Task: Brush teeth Step 1: Find toothbrush Step 2: Turn to toothbrush Step 3: Point at toothbrush Step 4: Wash toothbrush\nTask: Vacuum Step 1: Walk to home office Step 2: Walk to computer Step 3: Find vacuum cleaner Step 4: Switch on vacuum cleaner Step 5: Switch off vacuum cleaner\nTask: Vacuum Step 1: Walk to home office Step 2: Walk to vacuum Step 3: Find vacuum Step 4: Grab vacuum Step 5: Find chair Step 6: Sit on chair Step 7: Vacuum\nTask: Vacuum Step 1: Walk to home office Step 2: Walk to dresser Step 3: Open dresser Step 4: Find vacuum cleaner Step 5: Grab vacuum cleaner Step 6: Pull vacuum cleaner Step 7: Plug in vacuum cleaner Step 8: Switch on vacuum cleaner\nTask: Change sheets and pillow cases Step 1: Find pillow Step 2: Turn to pillow Step 3: Look at pillow Step 4: Grab pillow Task: Change sheets and pillow cases Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Change sheets and pillow cases Step 4: Walk to kitchen Step 5: Walk to sink Step 6: Wash sheets and pillow cases Step 7: Walk to bedroom Step 8: Put sheets and pillow cases on bed Step 9: Walk to living room Step 10: Sit on couch Task: Change sheets and pillow cases Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Find sheets Step 4: Grab sheets Step 5: Find pillow Step 6: Grab pillow\nTask: Wash dirty dishes Step 1: Walk to dining room Step 2: Walk to sink Step 3: Find sponge Step 4: Grab sponge Step 5: Find faucet Step 6: Switch on faucet Step 7: Put sponge on sink Step 8: Grab sponge Step 9: Find dish soap Step 10: Grab dish soap Step 11: Pour dish soap into sponge Step 12: Put back dish soap Step 13: Find plate Step 14: Grab plate Step 15: Scrub plate Step 16: Rinse plate Step 17: Find dishrack Step 18: Put plate on dishrack Step 19: Find plate Step 20: Grab plate Step 21: Scrub plate Step 22: Rinse plate Step 23: Put plate on dishrack Step 24: Rinse sponge Step 25: Put back sponge Step 26: Switch off faucet Task: Wash dirty dishes Step 1: Walk to kitchen Step 2: Walk to sink Step 3: Wash dishes Step 4: Dry dishes Step 5: Put dishes away\nTask: Wash dirty dishes Step 1: Walk to dining room Step 2: Walk to sink Step 3: Find faucet Step 4: Switch on faucet Step 5: Walk to plate Step 6: Grab plate\nTask: Feed me Step 1: Walk to dining room Step 2: Walk to plate Step 3: Find plate Step 4: Grab plate Step 5: Find food Step 6: Grab food Step 7: Put food on plate Step 8: Find food Step 9: Grab food Step 10: Put food on plate Step 11: Find fork Step 12: Grab fork Step 13: Eat plate Step 14: Eat plate\nTask: Feed me Step 1: Walk to kitchen Step 2: Walk to refrigerator Step 3: Open refrigerator Step 4: Grab food Step 5: Find plate Step 6: Put food on plate Step 7: Find fork Step 8: Put fork on plate Step 9: Find spoon Step 10: Put spoon on plate Step 11: Find cup Step 12: Pour water in cup Step 13: Walk to table Step 14: Sit at table Step 15: Eat food\nTask: Feed me Step 1: Walk to dining room Step 2: Walk to table Step 3: Find table Step 4: Turn to table Step 5: Find chair Step 6: Sit on chair Step 7: Find food Step 8: Grab food Step 9: Find plate Step 10: Put food on plate\nTask: Breakfast Step 1: Walk to dining room Step 2: Walk to food Step 3: Grab food Step 4: Walk to bedroom Step 5: Walk to television Step 6: Switch on television Step 7: Walk to bed Step 8: Lie on bed Step 9: Turn to television Step 10: Watch television Step 11: Eat food\nTask: Breakfast Step 1: Walk to kitchen Step 2: Find cereal Step 3: Pour cereal into bowl Step 4: Pour milk into bowl Step 5: Walk to living room Step 6: Sit on couch Step 7: Eat cereal\nTask: Breakfast Step 1: Walk to dining room Step 2: Walk to freezer Step 3: Find freezer Step 4: Open freezer Step 5: Find bread Step 6: Grab bread Step 7: Close freezer Step 8: Find toaster Step 9: Plug in toaster Step 10: Put bread on plate Step 11: Switch on toaster\nTask: Read Step 1: Find novel Step 2: Grab novel Step 3: Walk to bedroom Step 4: Walk to chair Step 5: Sit on chair Step 6: Read novel\nTask: Read Step 1: Walk to home office Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find chair Step 6: Sit on chair Step 7: Read novel\nTask: Read Step 1: Walk to home office Step 2: Walk to light Step 3: Find light Step 4: Switch on light Step 5: Find novel Step 6: Grab novel Step 7: Find chair Step 8: Sit on chair Step 9: Read novel\nTask: Style hair Step 1: Find towel Step 2: Turn to towel Step 3: Point at towel Step 4: Wash towel Step 5: Turn to towel Step 6: Look at towel Step 7: Squeeze towel Step 8: Walk to bathroom\nTask: Style hair Step 1: Walk to bathroom Step 2: Walk to mirror Step 3: Find hairbrush Step 4: Grab hairbrush Step 5: Find chair Step 6: Sit on chair Step 7: Style hair\nTask: Style hair Step 1: Walk to bathroom Step 2: Walk to mirror Step 3: Find hair Step 4: Grab hair Step 5: Find comb Step 6: Grab comb Step 7: Find brush Step 8: Grab brush\nTask: Pick up toys Step 1: Walk to bedroom Step 2: Walk to toy Step 3: Find toy Step 4: Grab toy Step 5: Walk to filing cabinet Step 6: Open filing cabinet Step 7: Put toy on filing cabinet Step 8: Find toy Step 9: Grab toy Step 10: Walk to filing cabinet Step 11: Put toy on filing cabinet Step 12: Find toy Step 13: Grab toy Step 14: Walk to filing cabinet Step 15: Put toy on filing cabinet Step 16: Close filing cabinet\nTask: Pick up toys Step 1: Walk to living room Step 2: Walk to toy box Step 3: Find toy box Step 4: Grab toy box Step 5: Find chair Step 6: Sit on chair Step 7: Pick up toys\nTask: Pick up toys Step 1: Find toy Step 2: Turn to toy Step 3: Point at toy Step 4: Wash toy\nTask: Decorate it Step 1: Walk to home office Step 2: Walk to centerpiece Step 3: Find centerpiece Step 4: Grab centerpiece Step 5: Walk to table Step 6: Put centerpiece on table Step 7: Find drawing Step 8: Grab drawing Step 9: Find wall Step 10: Turn to wall Step 11: Put drawing on wall Step 12: Find drawing Step 13: Grab drawing Step 14: Turn to wall Step 15: Put drawing on wall\nTask: Decorate it Step 1: Walk to home office Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find chair Step 6: Sit on chair Step 7: Read novel Step 8: Decorate it\nTask: Decorate it Step 1: Walk to table Step 2: Walk to centerpiece Step 3: Look at centerpiece Step 4: Grab centerpiece Step 5: Point at centerpiece\nTask: Put away groceries Step 1: Walk to dining room Step 2: Walk to freezer Step 3: Find freezer Step 4: Open freezer Step 5: Find food Step 6: Grab food Step 7: Put food in freezer Step 8: Close freezer\nTask: Put away groceries Step 1: Walk to kitchen Step 2: Walk to refrigerator Step 3: Open refrigerator Step 4: Grab milk and eggs Step 5: Close refrigerator Step 6: Walk to counter Step 7: Put milk and eggs on counter Step 8: Walk to cabinet Step 9: Open cabinet Step 10: Put away milk and eggs Step 11: Close cabinet Step 12: Walk to refrigerator Step 13: Grab orange juice and bread Step 14: Close refrigerator Task: Put away groceries Step 1: Walk to kitchen Step 2: Walk to fridge Step 3: Open fridge Step 4: Put food in fridge Step 5: Close fridge Step 6: Walk to dining room Step 7: Walk to livingroom Step 8: Walk to bedroom Step 9: Walk to bathroom\nTask: Draw picture Step 1: Walk to home office Step 2: Walk to chair Step 3: Find chair Step 4: Sit on chair Step 5: Find desk Step 6: Open desk Step 7: Find check Step 8: Grab check Step 9: Put check on desk Step 10: Find pencil Step 11: Grab pencil Step 12: Close desk\nTask: Draw picture Step 1: Walk to home office Step 2: Walk to pencils Step 3: Find pencils Step 4: Grab pencils Step 5: Find paper Step 6: Sit on chair Step 7: Draw picture\nTask: Draw picture Step 1: Walk to home office Step 2: Walk to wall Step 3: Walk to drawing Step 4: Find drawing Step 5: Turn to drawing Step 6: Look at drawing Step 7: Touch drawing Step 8: Pull drawing Step 9: Push drawing\nTask: Entertain Step 1: Walk to home office Step 2: Walk to cheese Step 3: Find cheese Step 4: Find food Step 5: Find couch Step 6: Sit on couch Step 7: Find wine glass Step 8: Grab wine glass Step 9: Drink wine glass\nTask: Entertain Step 1: Walk to living room Step 2: Walk to entertainment center Step 3: Find remote Step 4: Grab remote Step 5: Find chair Step 6: Sit on chair Step 7: Entertain self Task: Entertain Step 1: Walk to livingroom Step 2: Walk to couch Step 3: Find couch Step 4: Sit on couch\nTask: Type up document Step 1: Walk to home office Step 2: Walk to chair Step 3: Find chair Step 4: Sit on chair Step 5: Find computer Step 6: Switch on computer Step 7: Turn to computer Step 8: Look at computer Step 9: Find check Step 10: Grab check Step 11: Read check Step 12: Find coffee cup Step 13: Grab coffee cup Step 14: Drink coffee cup Step 15: Put back coffee cup Step 16: Find keyboard Step 17: Type on keyboard\nTask: Type up document Step 1: Walk to home office Step 2: Walk to keyboard Step 3: Find keyboard Step 4: Grab keyboard Step 5: Find chair Step 6: Sit on chair Step 7: Type document\nTask: Type up document Step 1: Walk to home office Step 2: Walk to computer Step 3: Find computer Step 4: Switch on computer Step 5: Find chair Step 6: Sit on chair Step 7: Turn to computer Step 8: Look at computer Step 9: Find keyboard Step 10: Grab keyboard Step 11: Type on keyboard Step 12: Stand up Step 13: Find printer Step 14: Switch on printer Step 15: Find printing paper\nTask: Put on your shoes Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes Step 4: Grab shoes Step 5: Find chair Step 6: Sit on chair Step 7: Find feet both Step 8: Put shoes on feet both\nTask: Put on your shoes Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes Step 4: Grab shoes Step 5: Find chair Step 6: Sit on chair Step 7: Put on shoes\nTask: Put on your shoes Step 1: Find shoes Step 2: Turn to shoes Step 3: Point at shoes Step 4: Wash shoes\nTask: Watch horror movie Step 1: Walk to home office Step 2: Walk to remote control Step 3: Find remote control Step 4: Grab remote control Step 5: Find couch Step 6: Sit on couch Step 7: Find television Step 8: Switch on television Step 9: Turn to television Step 10: Watch television\nTask: Watch horror movie Step 1: Walk to home office Step 2: Walk to horror movie Step 3: Find horror movie Step 4: Grab horror movie Step 5: Find chair Step 6: Sit on chair Step 7: Watch horror movie\nTask: Watch horror movie Step 1: Find television Step 2: Switch on television Step 3: Find cd player Step 4: Switch on cd player Step 5: Switch off cd player Step 6: Find remote control Step 7: Switch on remote control Step 8: Switch off remote control\nTask: Turn on radio Step 1: Walk to home office Step 2: Walk to stereo Step 3: Find stereo Step 4: Find electrical outlet Step 5: Turn to electrical outlet Step 6: Plug in stereo Step 7: Switch on electrical outlet Step 8: Switch on stereo\nTask: Turn on radio Step 1: Walk to home office Step 2: Walk to radio Step 3: Find radio Step 4: Grab radio Step 5: Find chair Step 6: Sit on chair Step 7: Turn on radio\nTask: Turn on radio Step 1: Walk to home office Step 2: Walk to chair Step 3: Find chair Step 4: Sit on chair Step 5: Find remote control Step 6: Grab remote control Step 7: Find radio Step 8: Point at radio Step 9: Switch on radio Step 10: Put back remote control\nTask: Look at mirror Step 1: Walk to bathroom Step 2: Walk to mirror Step 3: Find mirror Step 4: Turn to mirror Step 5: Look at mirror\nTask: Look at mirror Step 1: Walk to bathroom Step 2: Walk to mirror Step 3: Look in mirror\nTask: Look at mirror Step 1: Walk to bedroom Step 2: Walk to mirror Step 3: Find mirror Step 4: Turn to mirror Step 5: Look at mirror Step 6: Find face Step 7: Turn to face Step 8: Look at face Step 9: Find hair Step 10: Turn to hair Step 11: Look at hair Step 12: Find dress Step 13: Turn to dress Step 14: Look at dress\nTask: Leave home Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes Step 4: Grab shoes Step 5: Put on shoes\nTask: Leave home Step 1: Walk to front door Step 2: Open front door Step 3: Walk to car Step 4: Open car door Step 5: Get in car Step 6: Close car door Step 7: Drive to work\nTask: Leave home Step 1: Find shoes Step 2: Grab shoes Step 3: Put on shoes\nTask: Walk to room Step 1: Walk to home office Step 2: Walk to home office\nTask: Walk to room Step 1: Walk to home office Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find chair Step 6: Sit on chair Step 7: Read novel\nTask: Walk to room Step 1: Walk to dining room Step 2: Walk to livingroom Step 3: Walk to bedroom\nTask: Hang pictures Step 1: Walk to bedroom Step 2: Walk to drawing Step 3: Find drawing Step 4: Grab drawing Step 5: Find wall Step 6: Put drawing on wall Step 7: Turn to drawing Step 8: Look at drawing Step 9: Find drawing Step 10: Grab drawing Step 11: Put drawing on wall Step 12: Turn to drawing Step 13: Look at drawing\nTask: Hang pictures Step 1: Walk to home office Step 2: Walk to box of pictures Step 3: Find box of pictures Step 4: Grab box of pictures Step 5: Walk to wall Step 6: Hang pictures on wall\nTask: Hang pictures Step 1: Walk to home office Step 2: Walk to table Step 3: Find keys Step 4: Grab keys Step 5: Walk to wall Step 6: Find hanger Step 7: Put keys on hanger\nTask: Wipe down sink Step 1: Walk to dining room Step 2: Walk to rag Step 3: Find rag Step 4: Grab rag Step 5: Walk to sink Step 6: Wipe sink Step 7: Put back rag\nTask: Wipe down sink Step 1: Walk to kitchen Step 2: Walk to sink Step 3: Grab sponge Step 4: Wipe down sink Step 5: Rinse sponge Step 6: Put sponge away Step 7: Wash hands\nTask: Wipe down sink Step 1: Walk to bathroom Step 2: Walk to cleaning solution Step 3: Find cleaning solution Step 4: Grab cleaning solution Step 5: Find sink Step 6: Pour cleaning solution into sink Step 7: Find faucet Step 8: Switch on faucet Step 9: Scrub sink Step 10: Wash sink\nTask: Take shoes off Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes Step 4: Find shoes Step 5: Take off shoes Step 6: Find dresser Step 7: Open dresser Step 8: Grab shoes Step 9: Put shoes in dresser Step 10: Close dresser\nTask: Take shoes off Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes Step 4: Grab shoes Step 5: Find chair Step 6: Sit on chair Step 7: Take shoes off\nTask: Take shoes off Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes\nTask: Give milk to cat Step 1: Walk to home office Step 2: Walk to cat Step 3: Find cat Step 4: Turn to cat Step 5: Point at cat Step 6: Pull cat Step 7: Find milk Step 8: Grab milk Step 9: Find plate Step 10: Pour milk into plate Step 11: Touch cat Step 12: Scrub cat\nTask: Give milk to cat Step 1: Walk to kitchen Step 2: Walk to fridge Step 3: Open fridge Step 4: Grab milk Step 5: Walk to cat Step 6: Pour milk into bowl Step 7: Put bowl on floor Step 8: Walk away from cat\nTask: Give milk to cat Step 1: Find cat Step 2: Touch cat Step 3: Open fridge Step 4: Touch milk Step 5: Pour milk into bowl\nTask: Grab things Step 1: Walk to novel Step 2: Turn to novel Step 3: Grab novel\nTask: Grab things Step 1: Walk to home office Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find chair Step 6: Sit on chair Step 7: Read novel Step 8: Grab things\nTask: Grab things Step 1: Find toy Step 2: Turn to toy Step 3: Point at toy Step 4: Wash toy\nTask: Use laptop Step 1: Walk to home office Step 2: Walk to couch Step 3: Find couch Step 4: Sit on couch Step 5: Find laptop Step 6: Switch on laptop Step 7: Turn to laptop Step 8: Look at laptop Step 9: Find keyboard Step 10: Type on keyboard Step 11: Turn to keyboard Step 12: Look at keyboard Step 13: Type on keyboard Step 14: Turn to laptop Step 15: Look at laptop\nTask: Use laptop Step 1: Walk to home office Step 2: Walk to laptop Step 3: Find laptop Step 4: Grab laptop Step 5: Find chair Step 6: Sit on chair Step 7: Use laptop\nTask: Use laptop Step 1: Walk to home office Step 2: Walk to chair Step 3: Find chair Step 4: Sit on chair Step 5: Find laptop Step 6: Switch on laptop Step 7: Find mouse Step 8: Grab mouse Step 9: Find mousepad Step 10: Put mouse on mousepad Step 11: Turn to laptop Step 12: Point at laptop\nTask: Organize pantry Step 1: Walk to dining room Step 2: Walk to pantry Step 3: Find food Step 4: Grab food Step 5: Turn to food Step 6: Look at food Step 7: Find garbage can Step 8: Put food on garbage can Step 9: Find vegetable Step 10: Grab vegetable Step 11: Turn to vegetable Step 12: Look at vegetable Step 13: Put vegetable on garbage can Step 14: Find dry pasta Step 15: Grab dry pasta Step 16: Turn to dry pasta Step 17: Look at dry pasta Step 18: Put back dry pasta Step 19: Find food Step 20: Turn to food Step 21: Look at food Step 22: Push food Step 23: Find noodles Step 24: Grab noodles Step 25: Turn to noodles Step 26: Look at noodles Step 27: Put back noodles Task: Organize pantry Step 1: Walk to kitchen Step 2: Walk to pantry Step 3: Find pantry Step 4: Grab pantry Step 5: Find chair Step 6: Sit on chair Step 7: Organize pantry\nTask: Organize pantry Step 1: Walk to pantry Step 2: Find pantry\nTask: Change clothes Step 1: Walk to bedroom Step 2: Walk to dresser Step 3: Open dresser Step 4: Find dress Step 5: Grab dress Step 6: Put on dress Step 7: Close dresser\nTask: Change clothes Step 1: Walk to bedroom Step 2: Walk to closet Step 3: Find clothes Step 4: Grab clothes Step 5: Find chair Step 6: Sit on chair Step 7: Change clothes\nTask: Change clothes Step 1: Walk to bedroom Step 2: Walk to dresser Step 3: Find dresser Step 4: Open dresser Step 5: Find pants Step 6: Grab pants Step 7: Put on pants Step 8: Find shirt Step 9: Grab shirt Step 10: Put on shirt Step 11: Close dresser\nTask: Put up decoration Step 1: Walk to home office Step 2: Walk to drawing Step 3: Find drawing Step 4: Find centerpiece Step 5: Grab centerpiece Step 6: Walk to wall Step 7: Grab drawing Step 8: Put drawing on wall Step 9: Find centerpiece Step 10: Grab centerpiece Step 11: Walk to table Step 12: Put centerpiece on table\nTask: Put up decoration Step 1: Walk to home office Step 2: Walk to decoration Step 3: Find decoration Step 4: Grab decoration Step 5: Find chair Step 6: Sit on chair Step 7: Put up decoration\nTask: Put up decoration Step 1: Walk to livingroom Step 2: Walk to table\nTask: Make bed Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Find sheets Step 4: Grab sheets Step 5: Pull sheets\nTask: Make bed Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Find sheets Step 4: Find blanket Step 5: Find pillow Step 6: Put sheets on bed Step 7: Put blanket on bed Step 8: Put pillow on bed\nTask: Make bed Step 1: Stand up Step 2: Find sheets Step 3: Find blanket Step 4: Find pillow Step 5: Find comforter Step 6: Find bed\nTask: Scrubbing living room tile floor is once week activity for me Step 1: Find mop Step 2: Turn to mop Step 3: Point at mop Step 4: Wash mop Step 5: Turn to mop Step 6: Look at mop Task: Scrubbing living room tile floor is once week activity for me Step 1: Walk to living room Step 2: Walk to tile floor Step 3: Find scrub brush Step 4: Grab scrub brush Step 5: Scrub tile floor Step 6: Put down scrub brush Step 7: Walk to kitchen Task: Scrubbing living room tile floor is once week activity for me Step 1: Walk to livingroom Step 2: Walk to mop Step 3: Find mop Step 4: Grab mop Step 5: Walk to bathroom Step 6: Find bucket Step 7: Switch on faucet\nTask: Read yourself to sleep Step 1: Walk to bedroom Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find bed Step 6: Sit on bed Step 7: Find bookmark Step 8: Grab bookmark Step 9: Read novel Step 10: Put bookmark on novel Step 11: Sleep\nTask: Read yourself to sleep Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Find book Step 4: Grab book Step 5: Find chair Step 6: Sit on chair Step 7: Read book Step 8: Sleep\nTask: Read yourself to sleep Step 1: Walk to bedroom Step 2: Walk to bed Step 3: Lie on bed Step 4: Sleep Step 5: Read book\nTask: Hand washing Step 1: Walk to bathroom Step 2: Walk to sink Step 3: Find faucet Step 4: Switch on faucet Step 5: Find soap Step 6: Grab soap Step 7: Find hands both Step 8: Scrub hands both Step 9: Put back soap Step 10: Wash hands both Step 11: Switch off faucet Step 12: Find towel Step 13: Grab towel Step 14: Wipe hands both\nTask: Hand washing Step 1: Walk to bathroom Step 2: Walk to sink Step 3: Turn on faucet Step 4: Grab soap Step 5: Wash hands Step 6: Rinse hands Step 7: Dry hands\nTask: Hand washing Step 1: Walk to bathroom Step 2: Walk to sink Step 3: Find faucet Step 4: Turn to faucet Step 5: Find hands both Step 6: Wash hands both\nTask: Throw away paper Step 1: Walk to home office Step 2: Walk to desk Step 3: Find desk Step 4: Turn to desk Step 5: Find chair Step 6: Sit on chair Step 7: Find check Step 8: Grab check Step 9: Squeeze check Step 10: Stand up Step 11: Walk to trashcan Step 12: Put check on trashcan\nTask: Throw away paper Step 1: Walk to home office Step 2: Walk to wastebasket Step 3: Find wastebasket Step 4: Grab wastebasket Step 5: Walk to desk Step 6: Drop paper in wastebasket\nTask: Throw away paper Step 1: Walk to home office Step 2: Walk to table Step 3: Find table Step 4: Turn to table Step 5: Find paper Step 6: Grab paper Step 7: Walk to trashcan Step 8: Open trashcan Step 9: Put paper on trashcan Step 10: Close trashcan\nTask: Tale off shoes Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes Step 4: Find shoes Step 5: Take off shoes Step 6: Grab shoes Step 7: Find mat Step 8: Put shoes on mat\nTask: Tale off shoes Step 1: Walk to home office Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find chair Step 6: Sit on chair Step 7: Read novel\nTask: Tale off shoes Step 1: Walk to home office Step 2: Walk to shoes Step 3: Find shoes\nTask: Hang keys Step 1: Walk to bedroom Step 2: Walk to dresser Step 3: Find purse Step 4: Grab purse Step 5: Open purse Step 6: Find keys Step 7: Grab keys Step 8: Find bowl Step 9: Put keys on bowl Step 10: Close purse Step 11: Put back purse\nTask: Hang keys Step 1: Walk to home office Step 2: Walk to key hook Step 3: Find key hook Step 4: Grab key hook Step 5: Find keys Step 6: Hang keys on key hook Step 7: Walk to front door Step 8: Leave house\nTask: Hang keys Step 1: Walk to home office Step 2: Walk to table Step 3: Find keys Step 4: Grab keys Step 5: Walk to wall Step 6: Find hanger Step 7: Put keys on hanger\nTask: Make popcorn Step 1: Walk to dining room Step 2: Walk to food Step 3: Find food Step 4: Grab food Step 5: Walk to microwave Step 6: Turn to microwave Step 7: Open microwave Step 8: Put food in microwave Step 9: Close microwave Step 10: Switch on microwave\nTask: Make popcorn Step 1: Walk to kitchen Step 2: Walk to microwave Step 3: Open microwave Step 4: Put bag of popcorn in microwave Step 5: Close microwave Step 6: Push buttons on microwave Step 7: Wait for popcorn to pop Step 8: Get bowl out of cupboard Step 9: Get butter out of fridge Step 10: Get salt out of cupboard Step 11: Pour popcorn into bowl Step 12: Add butter and salt to bowl Step 13: Task: Make popcorn Step 1: Walk to kitchen Step 2: Walk to microwave Step 3: Find microwave Step 4: Open microwave\nTask: Receive credit card Step 1: Walk to home office Step 2: Walk to envelope Step 3: Find envelope Step 4: Grab envelope Step 5: Walk to home office Step 6: Walk to scissors Step 7: Find scissors Step 8: Open envelope Step 9: Find creditcard Step 10: Grab creditcard\nTask: Receive credit card Step 1: Walk to mailbox Step 2: Open mailbox Step 3: Grab credit card Step 4: Walk to home office Step 5: Walk to desk Step 6: Put credit card on desk Step 7: Sit at desk Step 8: Look at credit card\nTask: Receive credit card Step 1: Walk to home office Step 2: Walk to cabinet Step 3: Find cabinet Step 4: Open cabinet\nTask: Read to child Step 1: Walk to bedroom Step 2: Walk to child Step 3: Find child Step 4: Greet child Step 5: Walk to filing cabinet Step 6: Open filing cabinet Step 7: Find novel Step 8: Grab novel Step 9: Walk to chair Step 10: Pull chair Step 11: Sit on chair Step 12: Turn to novel Step 13: Look at novel Step 14: Turn to novel Step 15: Point at novel Step 16: Turn to novel Step 17: Point at novel Step 18: Read novel Step 19: Read novel Step 20: Read novel\nTask: Read to child Step 1: Walk to home office Step 2: Walk to novel Step 3: Find novel Step 4: Grab novel Step 5: Find chair Step 6: Sit on chair Step 7: Read novel\nTask: Read to child Step 1: Walk to child Step 2: Read book\nTask: Eat cheese Step 1: Walk to dining room Step 2: Walk to freezer Step 3: Open freezer Step 4: Find cheese Step 5: Grab cheese Step 6: Eat cheese\nTask: Eat cheese Step 1: Walk to kitchen Step 2: Walk to refrigerator Step 3: Open refrigerator Step 4: Grab cheese Step 5: Eat cheese\nTask: Eat cheese Step 1: Walk to dining room Step 2: Walk to table Step 3: Find table Step 4: Turn to table Step 5: Find chair Step 6: Sit on chair Step 7: Find food Step 8: Grab food Step 9: Find plate Step 10: Put food on plate\nTask: Unload various items from pockets and place them in bowl on table Step 1: Walk to home office Step 2: Walk to table Step 3: Find bowl Step 4: Grab bowl Step 5: Put bowl on table Step 6: Find pants Step 7: Find bills Step 8: Grab bills Step 9: Find phone Step 10: Grab phone Task: Unload various items from pockets and place them in bowl on table Step 1: Walk to home office Step 2: Walk to bowl on table Step 3: Find bowl on table Step 4: Grab bowl on table Step 5: Unload various items from pockets and place them in bowl on table Task: Unload various items from pockets and place them in bowl on table Step 1: Find bowl Step 2: Turn to bowl Step 3: Point at bowl Step 4: Wash bowl\nTask: Switch on lamp Step 1: Walk to bedroom Step 2: Walk to floor lamp Step 3: Find floor lamp Step 4: Switch on floor lamp\nTask: Switch on lamp Step 1: Walk to lamp Step 2: Switch on lamp Step 3: Walk to chair Step 4: Sit on chair Step 5: Read book\nTask: Switch on lamp Step 1: Walk to bedroom Step 2: Walk to floor lamp Step 3: Find floor lamp Step 4: Turn to floor lamp Step 5: Switch on floor lamp Step 6: Find bed Step 7: Lie on bed\nTask: Take jacket off Step 1: Walk to bedroom Step 2: Walk to jacket Step 3: Find jacket Step 4: Find jacket Step 5: Take off jacket Step 6: Find dresser Step 7: Open dresser Step 8: Close dresser\nTask: Take jacket off Step 1: Walk to home office Step 2: Walk to jacket Step 3: Find jacket Step 4: Grab jacket Step 5: Find chair Step 6: Sit on chair Step 7: Take jacket off\nTask: Take jacket off Step 1: Walk to home office Step 2: Walk to jacket Step 3: Find jacket",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'VirtualHome dataset' as a source of human-annotated action plans, which is a specific, verifiable dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "258564677",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CoScript"
      ],
      "dataset_details": [
        {
          "dataset_name": "CoScript",
          "dataset_description": "Used to train and evaluate models for constrained language planning, focusing on goal-oriented scripts. The dataset comprises 55,000 scripts, enabling the study of script knowledge in large language models.",
          "citing_paper_id": "272367167",
          "cited_paper_id": 258564677,
          "context_text": "Yuan et al. (2023b) introduce the task of constrained language planning and present CoScript , a dataset comprising 55,000 goal-oriented scripts.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, CoScript, which is introduced for the task of constrained language planning. The dataset is clearly named and described.",
          "citing_paper_doi": "10.48550/arXiv.2409.01806",
          "cited_paper_doi": "10.48550/arXiv.2305.05252",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8401ca54ccf4ee7237daadf0322ed186d587c47",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f06c38a0fd49dd1468e72696913169c0d5588fc3",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "CoScript",
          "dataset_description": "Used to train and evaluate smaller models like T5, demonstrating their ability to achieve good performance, even surpassing larger language models.",
          "citing_paper_id": "258564677",
          "cited_paper_id": 204838007,
          "context_text": "Experiments show that, when trained on CoScript, smaller models such as T5 (Raffel et al., 2020) can achieve good performance, even surpassing that of LLMs.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'CoScript' as a dataset used for training and evaluating models. It is specific and appears to be a verifiable resource.",
          "citing_paper_doi": "10.48550/arXiv.2305.05252",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f06c38a0fd49dd1468e72696913169c0d5588fc3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "1460418",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "bAbI datasets"
      ],
      "dataset_details": [
        {
          "dataset_name": "bAbI datasets",
          "dataset_description": "Used to evaluate models on multiple types of knowledge and abilities, including embodied knowledge, logic reasoning, and linguistic knowledge, to assess planning capabilities.",
          "citing_paper_id": "258762577",
          "cited_paper_id": 3178759,
          "context_text": "Additionally, we evaluate our models on bAbI [51], a dataset for testing multiple types of knowledge and abilities including embodied knowledge, logic reasoning, linguistic knowledge, etc.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "bAbI is a specific dataset used for evaluating models on various types of knowledge and abilities, which aligns with the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2305.10626",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f821d75968bc8de070af3ce5aa7f57bc031fafb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "bAbI datasets",
          "dataset_description": "Used to compare the performance of the proposed method with GPT-3 baselines and state-of-the-art models, focusing on question answering and reasoning tasks.",
          "citing_paper_id": "259859069",
          "cited_paper_id": 1460418,
          "context_text": "Table 1 compares our method with the two GPT-3 baselines, as well as two state-of-the-art methods on bAbI datasets, STM and QRN. Interestingly, the new GPT-3, text-davinci-003",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'bAbI datasets' which is a known dataset used for evaluating question answering and reasoning capabilities of models. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2307.07696",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4bf7edee5a4c4cfdbdd43a607c402420129fa277",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "229371222",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PROOF WRITER"
      ],
      "dataset_details": [
        {
          "dataset_name": "PROOF WRITER",
          "dataset_description": "Inspired the creation of P R O NTO QA, providing a basis for generating natural language implications and proofs. | Used to test reasoning abilities in QA systems, focusing on natural language reasoning with first-order logic. | Used to generate examples from an ontology with unique proofs, focusing on the generation of natural language implications and abductive statements. | Used to test reasoning abilities in QA systems, focusing on generating implications, proofs, and abductive statements over natural language.",
          "citing_paper_id": "252693237",
          "cited_paper_id": 229371222,
          "context_text": "Our proposed dataset is most closely related to P ROOF W RITER (Tafjord et al., 2021) and FOLIO (Han et al., 2022) which are QA datasets designed to test reasoning ability.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, PROOF WRITER and FOLIO, which are used to test reasoning abilities in QA systems.",
          "citing_paper_doi": "10.48550/arXiv.2210.01240",
          "cited_paper_doi": "10.18653/v1/2021.findings-acl.317",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "PROOF WRITER",
          "dataset_description": "Used to test reasoning abilities in QA systems, focusing on natural language reasoning with first-order logic. | Used to test reasoning ability in QA tasks, focusing on first-order logic to evaluate the planning capabilities of LLMs. | Used to test reasoning abilities in QA systems, focusing on generating implications, proofs, and abductive statements over natural language.",
          "citing_paper_id": "252693237",
          "cited_paper_id": 252070866,
          "context_text": "Our proposed dataset is most closely related to P ROOF W RITER (Tafjord et al., 2021) and FOLIO (Han et al., 2022) which are QA datasets designed to test reasoning ability.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, PROOF WRITER and FOLIO, which are used to test reasoning abilities in QA systems.",
          "citing_paper_doi": "10.48550/arXiv.2210.01240",
          "cited_paper_doi": "10.48550/arXiv.2209.00840",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "49317780",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ActivityPrograms dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "ActivityPrograms dataset",
          "dataset_description": "Intended for future evaluation of the method, focusing on simulating household activities via programs to assess planning capabilities.",
          "citing_paper_id": "258187415",
          "cited_paper_id": 49317780,
          "context_text": "For future work, we would like to evaluate our method using more tasks, potentially those from existing benchmarks such as ActivityPrograms [35].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'ActivityPrograms' as a potential benchmark for future evaluation, but does not specify it as a dataset currently used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2304.08587",
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ff31e27301de5fd6c813a82e026bb56d6b7a875",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "ActivityPrograms dataset",
          "dataset_description": "Used to simulate 12 everyday dining tasks, focusing on household activity programs to enhance the realism of virtual environments. | Used to evaluate 12 tasks extracted for simulating household activities, focusing on the planning capabilities of models in virtual environments. | Used to conduct systematic evaluations of dining-focused tasks, providing a structured set of activities for simulation and assessment.",
          "citing_paper_id": "258960174",
          "cited_paper_id": 49317780,
          "context_text": "To conduct systematic evaluations, we selected 12 dining-focused tasks from the ActivityPrograms dataset [23].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'ActivityPrograms dataset' which is a specific, verifiable dataset used for evaluating dining-focused tasks.",
          "citing_paper_doi": "10.1007/s10514-023-10133-5",
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a79c7062809548fdfb593d68c106034012a401d9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "221738970",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ProScript"
      ],
      "dataset_details": [
        {
          "dataset_name": "ProScript",
          "dataset_description": "Used to collect human-annotated partial-order plans, enhancing the dataset for training and evaluating LLMs on complex planning tasks. | Used to supplement WikiHow data, providing additional context for deriving planning tasks in the research on planning capabilities of LLMs. | Serves as the base dataset to derive planning tasks, focusing on goal-oriented steps and temporal ordering in the context of LLM planning capabilities. | Used to collect planning tasks, providing a rich source of step-by-step instructions and goal-oriented content for training and evaluation.",
          "citing_paper_id": "267411736",
          "cited_paper_id": 221738970,
          "context_text": "In addition to existing data in ProScript(Sakaguchi et al., 2021), we use WikiHow (Koupaee & Wang, 2018; Zhang et al., 2020) as a base dataset to derive the planning tasks we need.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ProScript' and 'WikiHow' as datasets used to derive planning tasks. Both are multi-word proper nouns and are used in the context of planning capabilities research.",
          "citing_paper_doi": "10.48550/arXiv.2402.02805",
          "cited_paper_doi": "10.18653/V1/2020.EMNLP-MAIN.374",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8679575a82d9a2ba5970a96fcc5ac461aecbc90e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/498c525afce5be80951babc666d865b86a38bdcc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ProScript",
          "dataset_description": "Serves as the base dataset to derive planning tasks, focusing on goal-oriented steps and temporal ordering in the context of LLM planning capabilities. | Used to study the planning capabilities of LLMs, specifically analyzing the structure and order of plans collected via crowdsourcing for ProScript. | Used to collect human-annotated partial-order plans, enhancing the dataset for training and evaluating LLMs on complex planning tasks. | Used to provide asynchronous instances with complete time annotations for meaningful steps, combined with WikiHow instances to create a collection of 1.6K instances for further analysis. | Used alongside LLM-annotated WikiHow data to create natural language prompts, emphasizing the role of human annotations in improving planning accuracy. | Used to supplement WikiHow data, providing additional context for deriving planning tasks in the research on planning capabilities of LLMs. | Used to collect planning tasks, providing a rich source of step-by-step instructions and goal-oriented content for training and evaluation. | Used to assess step dependency annotation through randomized experiments, sampling 100 instances from the dev and test sets to evaluate annotation quality. | Used to combine with human-annotated ProScript data to generate natural language prompts, focusing on enhancing planning capabilities through LLM annotations. | Used to generate asynchronous instances, which were then combined with ProScript instances to form a collection of 1.6K instances for further analysis.",
          "citing_paper_id": "267411736",
          "cited_paper_id": null,
          "context_text": "In addition to existing data in ProScript(Sakaguchi et al., 2021), we use WikiHow (Koupaee & Wang, 2018; Zhang et al., 2020) as a base dataset to derive the planning tasks we need.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ProScript' and 'WikiHow' as datasets used to derive planning tasks. Both are multi-word proper nouns and are used in the context of planning capabilities research.",
          "citing_paper_doi": "10.48550/arXiv.2402.02805",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8679575a82d9a2ba5970a96fcc5ac461aecbc90e",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "53296520",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CommonsenseQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CommonsenseQA",
          "dataset_description": "Used to evaluate commonsense reasoning capabilities, focusing on questions that require understanding of everyday situations and knowledge. | Used to evaluate LLMs' ability to solve math word problems, focusing on scaling instances to larger problems and assessing procedural extension capabilities. | Used to assess basic symbolic reasoning, focusing on simple probability and logical deduction tasks. | Utilized to test few-hop math word problems, specifically designed to challenge models with complex arithmetic and logical reasoning. | Employed to evaluate few-hop math word problems, targeting basic arithmetic and algebraic reasoning tasks. | Used to assess LLMs' commonsense reasoning, specifically evaluating their ability to handle questions requiring everyday knowledge and understanding. | Applied to assess performance on few-hop math word problems, emphasizing multi-step reasoning and problem-solving skills.",
          "citing_paper_id": "269626390",
          "cited_paper_id": 53296520,
          "context_text": "…mainly constrained its evaluations to static test sets ranging from commonsense domains (Sports Understanding [41], StrategyQA [18], CommonSenseQA [44]), few-hop math word problems (AsDiv [31], GSM8k [10], MAWPS [27]), to a number of basic \"symbolic reasoning\" tasks (CoinFlip [26],…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for evaluating models, which are relevant to the topic of planning capabilities of LLMs. These datasets are used to assess various aspects of reasoning and understanding.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/N19-1421",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "CommonsenseQA",
          "dataset_description": "Utilized to assess the ability to solve strategic questions, emphasizing multi-step reasoning and planning. | Used to evaluate commonsense reasoning capabilities, focusing on questions that require understanding of everyday situations and knowledge.",
          "citing_paper_id": "257834038",
          "cited_paper_id": 53296520,
          "context_text": "For commonsense reasoning, we utilized the CommonsenseQA dataset [67] and the StrategyQA dataset [21].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context explicitly mentions the use of two datasets, CommonsenseQA and StrategyQA, for commonsense reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2303.17491",
          "cited_paper_doi": "10.18653/v1/N19-1421",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "239009562",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "P3"
      ],
      "dataset_details": [
        {
          "dataset_name": "P3",
          "dataset_description": "Used as an NLP task instruction dataset, reformatted for a wide range of downstream tasks using diverse human-written templates. | Used for general-purpose instructions, assessing the model's capability to handle a wide range of instructional tasks. | Used for NLP downstream tasks, specifically to evaluate the model's performance on a variety of linguistic tasks.",
          "citing_paper_id": "266359670",
          "cited_paper_id": 239009562,
          "context_text": "We first select representative instruction datasets: P3 (Sanh et al., 2022) for NLP down-stream tasks, CodeAlpaca (Chaudhary, 2023) for code generation, and Alpaca (Taori et al., 2023) for general-purpose instructions.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for different purposes in the research. Each dataset is clearly named and referenced for a specific task.",
          "citing_paper_doi": "10.18653/v1/2024.acl-srw.15",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ec9414654469692d8f1de8e2401a3dcbc58ee11a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "P3",
          "dataset_description": "Used for general-purpose instructions, assessing the model's capability to handle a wide range of instructional tasks. | Used for NLP downstream tasks, specifically to evaluate the model's performance on a variety of linguistic tasks.",
          "citing_paper_id": "266359670",
          "cited_paper_id": null,
          "context_text": "We first select representative instruction datasets: P3 (Sanh et al., 2022) for NLP down-stream tasks, CodeAlpaca (Chaudhary, 2023) for code generation, and Alpaca (Taori et al., 2023) for general-purpose instructions.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for different purposes in the research. Each dataset is clearly named and referenced for a specific task.",
          "citing_paper_doi": "10.18653/v1/2024.acl-srw.15",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ec9414654469692d8f1de8e2401a3dcbc58ee11a",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "252846090",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Language-Table"
      ],
      "dataset_details": [
        {
          "dataset_name": "Language-Table",
          "dataset_description": "Used to study multi-object tabletop pushing environments, focusing on complex dynamics and large language cardinality in robotic interactions.",
          "citing_paper_id": "257364842",
          "cited_paper_id": 252846090,
          "context_text": "The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset (Lynch et al., 2022) and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, 'Language-Table', which is used for a multi-object tabletop pushing environment. The dataset is described as challenging due to its complexity and language cardinality.",
          "citing_paper_doi": "10.48550/arXiv.2303.03378",
          "cited_paper_doi": "10.48550/arXiv.2210.06407",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e34addae55a571d7efd3a5e2543e86dd7d41a83",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Language-Table",
          "dataset_description": "Utilized to simulate and evaluate robotic manipulation tasks, focusing on the integration of language and physical action. | Used as a textual corpus to train language models, enhancing their ability to generate coherent and contextually relevant text. | Used in a mixture for training or evaluation, contributing 3.1% of the data, likely for robotic manipulation tasks.",
          "citing_paper_id": "257364842",
          "cited_paper_id": 2210455,
          "context_text": "…10 5.2 CC3M (Sharma et al., 2018) 25 13.1 Object Aware (Piergiovanni et al., 2022) 10 5.2 OK-VQA (Marino et al., 2019) 1 0.5 VQAv2 (Goyal et al., 2017) 1 0.5 COCO (Chen et al., 2015) 1 0.5 Wikipedia text 1 0.5 (robot) Mobile Manipulator, real 6 3.1 (robot) Language Table (Lynch et al., 2022 In Tab.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and a corpus, all of which are specific and verifiable. The citation intent is to reference reusable resources, and the resource type is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2303.03378",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "234741852",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "AQuA"
      ],
      "dataset_details": [
        {
          "dataset_name": "AQuA",
          "dataset_description": "Used as a testbed to evaluate the performance of PoT on algebraic word problems, focusing on the types of questions PoT can solve better. | Used as a testbed for solving and explaining algebraic word problems, focusing on program induction by rationale generation. | Used to assess model capabilities in table-based question answering, emphasizing the difficulty of handling tabular data and complex queries. | Used to evaluate few-shot learning in hybrid tabular and textual content in finance, addressing complex questions with 8 shots. | Used to evaluate model performance on algebraic word problems, focusing on the challenge of solving and explaining complex questions. | Used to evaluate the model's performance on solving and explaining algebraic word problems, emphasizing the complexity and variety of questions. | Used to evaluate few-shot learning in algebraic word problems, covering more diverse and challenging problems with 8 shots.",
          "citing_paper_id": "253801709",
          "cited_paper_id": 12777818,
          "context_text": "For simple datasets like FinQA (Chen et al., 2021b), we tend to use fewer shots, while for more challenging datasets like AQuA (Ling et al., 2017) and TATQA (Zhu et al., 2021), we use 8 shots to cover more diverse problems.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: FinQA, AQuA, and TATQA. These are specific datasets used for evaluating few-shot learning in different domains.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "AQuA",
          "dataset_description": "Used to assess the performance of PoT prompting on multi-step arithmetic problems, focusing on complex reasoning and calculation abilities. | Used to evaluate PoT prompting on hybrid tabular and textual financial content, assessing the model's ability to integrate and reason over diverse data types. | Used to evaluate multi-step reasoning over tabular and textual content, focusing on mathematical word problems in financial contexts. | Used to evaluate PoT prompting on mathematical word problems, focusing on the accuracy and reasoning capabilities of the model. | Used to evaluate few-shot learning in hybrid tabular and textual content in finance, addressing complex questions with 8 shots. | Used to evaluate LLMs on math-driven financial questions, focusing on the ability to process hybrid tabular and textual content in finance. | Used to evaluate PoT prompting on financial questions, testing the model's ability to handle financial data and perform accurate calculations. | Used to evaluate PoT prompting on tabular mathematical word problems, assessing the model's ability to integrate tabular data with textual information. | Used to assess the performance of PoT prompting on algebraic word problems, emphasizing logical reasoning and problem-solving skills. | Used to test PoT prompting on single-variable arithmetic problems, evaluating the model's ability to handle basic arithmetic operations. | Used to evaluate few-shot learning in algebraic word problems, covering more diverse and challenging problems with 8 shots.",
          "citing_paper_id": "253801709",
          "cited_paper_id": 234741852,
          "context_text": "For simple datasets like FinQA (Chen et al., 2021b), we tend to use fewer shots, while for more challenging datasets like AQuA (Ling et al., 2017) and TATQA (Zhu et al., 2021), we use 8 shots to cover more diverse problems.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three datasets: FinQA, AQuA, and TATQA. These are specific datasets used for evaluating few-shot learning in different domains.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/2021.acl-long.254",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b3213c84a6ff7a2f11099de783c93166e4fc02a4",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "237274213",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "NetHack"
      ],
      "dataset_details": [
        {
          "dataset_name": "NetHack",
          "dataset_description": "Used to evaluate the success rate of BLINDER and other baselines in generalizing to unseen tasks within the NetHack environment, focusing on reinforcement learning capabilities.",
          "citing_paper_id": "260125969",
          "cited_paper_id": 220042384,
          "context_text": "Figure 2 shows success rate on test tasks in the NetHack domain, comparing BLINDER to other baselines that can generalize to unseen tasks.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'NetHack domain' which is likely a specific environment or dataset used for reinforcement learning tasks. However, it does not explicitly mention a dataset name.",
          "citing_paper_doi": "10.48550/arXiv.2307.11922",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5da8d559955f0501ad6b2a97fe8a106f067cb5dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/822bdb6e8c39e272ebfee127666e032bd3aa0107",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "NetHack",
          "dataset_description": "Used to evaluate the success rate of BLINDER and other baselines in generalizing to unseen tasks within the NetHack environment, focusing on reinforcement learning capabilities.",
          "citing_paper_id": "260125969",
          "cited_paper_id": 237274213,
          "context_text": "Figure 2 shows success rate on test tasks in the NetHack domain, comparing BLINDER to other baselines that can generalize to unseen tasks.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'NetHack domain' which is likely a specific environment or dataset used for reinforcement learning tasks. However, it does not explicitly mention a dataset name.",
          "citing_paper_doi": "10.48550/arXiv.2307.11922",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5da8d559955f0501ad6b2a97fe8a106f067cb5dd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/43ea4f5d999d35d4fc6c544eedbc100d8c3a5e00",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "253098632",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PlanBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "PlanBench",
          "dataset_description": "Used to evaluate large language models on planning and reasoning about change, focusing on the brittleness and unreliability of LLMs in these tasks.",
          "citing_paper_id": "258968043",
          "cited_paper_id": 253098632,
          "context_text": "Despite these successes, LLMs can however be brittle and unreliable in their reasoning, especially when reasoning about agents, social contexts [28] and planning [37].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'planning' and cites a paper titled 'PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change', which suggests the use of a specific dataset for evaluating LLMs in planning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2305.19165",
          "cited_paper_doi": "10.48550/arXiv.2210.13312",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f1a3cd5cc340f47e3e966709f7dfddef23460aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/311fd5f6f114ae51f8cbd95a0da69d7b556d25f1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "PlanBench",
          "dataset_description": "Used to evaluate large language models on planning and reasoning about change, focusing on the brittleness and unreliability of LLMs in these tasks.",
          "citing_paper_id": "258968043",
          "cited_paper_id": 249889477,
          "context_text": "Despite these successes, LLMs can however be brittle and unreliable in their reasoning, especially when reasoning about agents, social contexts [28] and planning [37].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'planning' and cites a paper titled 'PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change', which suggests the use of a specific dataset for evaluating LLMs in planning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2305.19165",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f1a3cd5cc340f47e3e966709f7dfddef23460aa2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "348944",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "GrailQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "GrailQA",
          "dataset_description": "Used to compile a dataset for evaluating LLMs' long-term planning capabilities, focusing on complex question answering from Freebase.",
          "citing_paper_id": "260682249",
          "cited_paper_id": 348944,
          "context_text": "In an effort to gauge the decision-making abilities of LLMs, specifically their proficiency in long-term planning, we have meticulously compiled a dataset sourced from preexisting knowledge base question answering (KBQA) datasets on FREEBASE, including GrailQA [28], ComplexWebQuestions [77], and GraphQuestions [76].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific KBQA datasets used to compile a new dataset for evaluating LLMs' long-term planning capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2308.03688",
          "cited_paper_doi": "10.18653/v1/D16-1054",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dbf93a68b7fda600521f046dea35ea8ba9e884f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ce23476759263bd3f5e95fc758385eb62b3ab59a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "GrailQA",
          "dataset_description": "Used to compile a dataset for evaluating LLMs' long-term planning capabilities, focusing on complex question answering from Freebase.",
          "citing_paper_id": "260682249",
          "cited_paper_id": 3986974,
          "context_text": "In an effort to gauge the decision-making abilities of LLMs, specifically their proficiency in long-term planning, we have meticulously compiled a dataset sourced from preexisting knowledge base question answering (KBQA) datasets on FREEBASE, including GrailQA [28], ComplexWebQuestions [77], and GraphQuestions [76].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific KBQA datasets used to compile a new dataset for evaluating LLMs' long-term planning capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2308.03688",
          "cited_paper_doi": "10.18653/v1/N18-1059",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dbf93a68b7fda600521f046dea35ea8ba9e884f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8725f13be7434b69738491c66b45c9225258253",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "215785913",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "WikiSQL"
      ],
      "dataset_details": [
        {
          "dataset_name": "WikiSQL",
          "dataset_description": "Used to acquire source queries and databases, contributing to the diversity of instructions and data for training models.",
          "citing_paper_id": "260682249",
          "cited_paper_id": 2623009,
          "context_text": "We acquire the source queries and databases via reusing and amalgamating several established datasets: WikiSQL [103], WikiTableQuestions [60], SQA [37], HybridaQA [13], and FeTaQA [53], ensuring the diversity of instructions and data.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to acquire source queries and databases for ensuring the diversity of instructions and data.",
          "citing_paper_doi": "10.48550/arXiv.2308.03688",
          "cited_paper_doi": "10.18653/v1/P17-1167",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dbf93a68b7fda600521f046dea35ea8ba9e884f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8ff54aa8045b1e30c348cf2ca42259c946cd7a9e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "WikiSQL",
          "dataset_description": "Used to acquire source queries and databases, contributing to the diversity of instructions and data for training models.",
          "citing_paper_id": "260682249",
          "cited_paper_id": 215785913,
          "context_text": "We acquire the source queries and databases via reusing and amalgamating several established datasets: WikiSQL [103], WikiTableQuestions [60], SQA [37], HybridaQA [13], and FeTaQA [53], ensuring the diversity of instructions and data.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to acquire source queries and databases for ensuring the diversity of instructions and data.",
          "citing_paper_doi": "10.48550/arXiv.2308.03688",
          "cited_paper_doi": "10.18653/v1/2020.findings-emnlp.91",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5dbf93a68b7fda600521f046dea35ea8ba9e884f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/27db72a2f643f9dfebc0cc2e8b98a9db307f0f07",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "239998651",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TheoremQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "TheoremQA",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs in solving complex mathematical problems, focusing on theorem proving and logical reasoning. | Used to evaluate math reasoning skills, focusing on solving grade school math problems with step-by-step solutions. | Used for interactive learning in embodied environments, focusing on aligning text and actions in a virtual world. | Used to assess multi-hop question answering capabilities, focusing on diverse and explainable reasoning processes. | Used for multi-hop question answering, emphasizing diverse and explainable reasoning processes. | This dataset 'GSM8K' was mentioned in the citation context but no detailed description was generated. | Used to test knowledge problem-solving, evaluating the model's ability to answer questions across various subjects and domains. | Used to evaluate the model's ability to prove mathematical theorems, focusing on formal logic and proof construction. | Used to set the solution range variable according to instance requirements (float, integer, list of integers, option) for evaluating planning capabilities. | Used to test theorem proving and logical reasoning, focusing on advanced mathematical and logical challenges. | Used to evaluate interactive learning and decision-making in embodied environments, focusing on aligning text and physical actions. | Used to assess multi-hop question answering, requiring the model to integrate information from multiple documents to answer questions. | Used to measure mathematical problem-solving abilities, covering a wide range of math topics and difficulty levels.",
          "citing_paper_id": "262053695",
          "cited_paper_id": 239998651,
          "context_text": "(Austin et al., 2021) 500 91 Decision Making ALFWorld (Shridhar et al., 2020) 134 134 Reasoning GSM8K (Cobbe et al., 2021) 1319 48 HotpotQA (Yang et al., 2018) 7,405 43 MATH (Hendrycks et al., 2021) 5,000 100 MMLU (Hendrycks et al., 2020) 13,985 76 TheoremQA (Chen et al., 2023a) 800 49 Total 29,307 586 across different interaction limits k .",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating different aspects of decision making and reasoning in AI models. Each dataset is associated with a specific task or domain.",
          "citing_paper_doi": "10.48550/arXiv.2309.10691",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "247939706",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SayCan dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "SayCan dataset",
          "dataset_description": "Used to ground language in robotic affordances, specifically investigating how robots interpret and execute commands in a physical environment.",
          "citing_paper_id": "256416127",
          "cited_paper_id": 247939706,
          "context_text": "We use the SayCan dataset (Ahn et al., 2022), which assumes a scenario of a robot operat-7",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'SayCan dataset' which is a specific, verifiable dataset used in the research. The dataset is relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2301.13379",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "272368347",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ToolACE"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolACE",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs in complex agent environments, focusing on instruction-following and task completion. | Used to enhance function-calling capabilities of LLM agents, focusing on evaluating planning capabilities of LLMs, employing single-tool conversations to improve task execution. | Used to enhance the function calling abilities of LLMs, specifically in the context of instruction fine-tuning for complex agent environments. | Used to improve the model's instruction-following capabilities in agent environments, focusing on complex and diverse instruction-completion tasks. | This dataset 'ShareGPT' was mentioned in the citation context but no detailed description was generated. | Used to improve the instruction-following capabilities of LLMs in agent-based scenarios, emphasizing high-quality instruction-completion tasks. | Used to enhance the model's function calling capabilities, specifically to win points in LLM function calling tasks.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 272368347,
          "context_text": "• ToolACE (Liu et al., 2024c) is a single-tool conversation dataset designed to enhance the function-calling capabilities of LLM agents.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ToolACE is mentioned as a dataset designed for enhancing function-calling capabilities of LLM agents, which is directly relevant to the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": "10.48550/arXiv.2409.00920",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0350636522997217df53553ddf3e472338bca97b",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "14113767",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LAION-400M"
      ],
      "dataset_details": [
        {
          "dataset_name": "LAION-400M",
          "dataset_description": "Used for pre-training with 491 thousand filtered image-text pairs obtained by re-captioning using BLIP-2, improving the model's performance in generating accurate captions. | Used to provide image caption tasks, focusing on generating accurate captions for diverse images, enhancing the evaluation of planning capabilities in language models.",
          "citing_paper_id": "258865718",
          "cited_paper_id": 14113767,
          "context_text": "In the first stage, we focus on image-text conversation alignment pre-training, which involves using three datasets: COCO Caption [44], 595 thousand finely filtered image-text pairs from CC3M [45], and 491 thousand filtered image-text pairs obtained by re-captioning LAION-400M using BLIP-2 [17].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for pre-training in image-text conversation alignment. Each dataset is clearly identified and has a specific role in the research.",
          "citing_paper_doi": "10.48550/arXiv.2305.15021",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "204955905",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Meta-World"
      ],
      "dataset_details": [
        {
          "dataset_name": "Meta-World",
          "dataset_description": "Used to evaluate embodied control tasks, focusing on the success rate of long-horizon tasks using imitation and reinforcement learning.",
          "citing_paper_id": "258865718",
          "cited_paper_id": 204955905,
          "context_text": "…task-specific features, leading to a significant improvement in the success rate of embodied control tasks, outperforming both R3M [12] (a video-language contrastive learned model) and BLIP-2 [13] (a multi-modal foundation model) on Franka Kitchen [14] and Meta-World [15] environments. data.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Franka Kitchen' and 'Meta-World' as environments where the models were evaluated. These are specific, identifiable environments used for evaluating embodied control tasks.",
          "citing_paper_doi": "10.48550/arXiv.2305.15021",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8c54e8575e7c17a4097838305915e6e7b00fd4af",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "257219404",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GPT-4 generated 52K English instruction-following dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "GPT-4 generated 52K English instruction-following dataset",
          "dataset_description": "Used to fine-tune a pre-trained LLaMA-7B model, focusing on enhancing its instruction-following capabilities through additional training data. | Used to fine-tune a pre-trained LLaMA-7B model, specifically to improve its performance on English instruction-following tasks with a large, synthetically generated dataset.",
          "citing_paper_id": "258865718",
          "cited_paper_id": 257219404,
          "context_text": "For the frozen language model, we adopt a pre-trained LLaMA-7B [43] model and fine-tune it using the ShareGPT dataset and a GPT-4 generated 52K English instruction-following dataset [52].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets used for fine-tuning a pre-trained LLaMA-7B model. 'ShareGPT' and 'GPT-4 generated 52K English instruction-following dataset' are both specific and identifiable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2305.15021",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "257364842",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "EgoCOT"
      ],
      "dataset_details": [
        {
          "dataset_name": "EgoCOT",
          "dataset_description": "Used to build a scalable dataset for embodied multimodal language models, focusing on part-level interactions and sub-goal sequences in robotic tasks.",
          "citing_paper_id": "258865718",
          "cited_paper_id": 257364842,
          "context_text": "…part level, such as the gripper of a robotic arm or the handle of a door, manifested in sub-goal sequences. ii) the proposed EgoCOT dataset is built based on an open-source large-scale dataset, which offers greater scalability compared to the PaLM-E [2] model trained on proprietary robot data.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'EgoCOT' as a dataset and refers to an open-source large-scale dataset, but does not specify its name. 'PaLM-E' is a model, not a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2305.15021",
          "cited_paper_doi": "10.48550/arXiv.2303.03378",
          "citing_paper_url": "https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "221139844",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DROP"
      ],
      "dataset_details": [
        {
          "dataset_name": "DROP",
          "dataset_description": "Used to assess the ability of models to perform discrete reasoning over paragraphs, focusing on question answering tasks that require numerical reasoning and multi-step operations. | Used to evaluate compositional generalization in sequence-to-sequence models, focusing on navigation commands and their execution. The dataset tests the ability of models to generalize to unseen command-action pairs.",
          "citing_paper_id": "248986239",
          "cited_paper_id": 221139844,
          "context_text": "The two benchmarks considered in this paper, SCAN (Lake & Baroni, 2018) and DROP (Dua et al., 2019), have often been tackled by neural-symbolic methods (Andor et al., 2019; Chen et al., 2019, 2020; Nye et al., 2020; Shaw et al., 2021; Kim, 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions SCAN and DROP as benchmarks, which are typically used to evaluate the compositional generalization capabilities of models, including LLMs. These benchmarks are relevant to the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2205.10625",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b35b0a19425129432eefc21c3a9a1825f328c4b1",
          "citing_paper_year": 2022,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "3922816",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AI2 Reasoning Challenge"
      ],
      "dataset_details": [
        {
          "dataset_name": "AI2 Reasoning Challenge",
          "dataset_description": "Used to evaluate the performance of Auto-RAG on general reasoning tasks, focusing on complex question answering and problem-solving capabilities.",
          "citing_paper_id": "274422541",
          "cited_paper_id": 3922816,
          "context_text": "To evaluate the performance of Auto-RAG on general tasks, we conducted experiments on several general task evaluation benchmarks, including the AI2 Reasoning Challenge (ARC, Clark et al., 2018",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the AI2 Reasoning Challenge (ARC) as a benchmark for evaluating the performance of Auto-RAG on general tasks. The context indicates that ARC is used for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2411.19443",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d1beece295703c0cb3e545edaa12a4336b407bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88bb0a28bb58d847183ec505dda89b63771bb495",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "86611921",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Natural Questions"
      ],
      "dataset_details": [
        {
          "dataset_name": "Natural Questions",
          "dataset_description": "Used to synthesize reasoning-based instructions for training Auto-RAG, focusing on multi-hop question answering tasks. | Used to assess multi-hop QA capabilities, emphasizing the need for reasoning across multiple documents to answer complex questions. | Used to synthesize reasoning-based instructions for training Auto-RAG, focusing on complex question answering tasks.",
          "citing_paper_id": "274422541",
          "cited_paper_id": 86611921,
          "context_text": "To train Auto-RAG, we synthesized 10,000 reasoning-based instructions derived from two representative datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and 2WikiMultihopQA (2Wiki) (Ho et al., 2020).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used to synthesize reasoning-based instructions for training Auto-RAG. Both datasets are well-known and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2411.19443",
          "cited_paper_doi": "10.1162/tacl_a_00276",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1d1beece295703c0cb3e545edaa12a4336b407bc",
          "cited_paper_url": "https://www.semanticscholar.org/paper/17dbd7b72029181327732e4d11b52a08ed4630d0",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "49317780",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RobotHow"
      ],
      "dataset_details": [
        {
          "dataset_name": "RobotHow",
          "dataset_description": "Used to conduct experiments on completing daily household goals, focusing on human-generated instructions and planning capabilities. | Used as a knowledge base for common household tasks, specifically to simulate activities in the VirtualHome environment, enhancing the planning capabilities of AI models. | Used for zero-shot experiments on procedural information, focusing on natural language instructions and steps without prior training. | Used for zero-shot experiments on procedural information, focusing on the ability of models to understand and generate instructions without prior training. | Used for zero-shot experiments on procedural information, focusing on robot instructions and actions without prior training. | Used to conduct experiments on completing daily household goals, focusing on robotic task execution and planning capabilities.",
          "citing_paper_id": "252681067",
          "cited_paper_id": 49317780,
          "context_text": "We conduct experiments on two datasets that involve completing daily household goals, RobotHow (Puig et al., 2018) and WikiHow (Koupaee & Wang, 2018).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, 'RobotHow' and 'WikiHow', which are used for experiments involving completing daily household goals.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/418085c9726669bf53f3d66e0018f2b08ffc4ce6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "15206880",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ConceptNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "ConceptNet",
          "dataset_description": "Used to represent a graph of general knowledge with concept nodes and commonsense relations, supporting the development of planning capabilities in language models. | Used as an external knowledge base to sample local subgraphs for entities, focusing on concept nodes and commonsense relations to enhance the model's understanding.",
          "citing_paper_id": "252681067",
          "cited_paper_id": 15206880,
          "context_text": ", ConceptNet (Speer et al., 2017)), where N andR represent the number of concept nodes and commonsense relations respectively.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ConceptNet is a knowledge base, not a traditional dataset, but it is used in the context of representing concepts and relations which could be relevant for planning capabilities in LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1609/aaai.v31i1.11164",
          "citing_paper_url": "https://www.semanticscholar.org/paper/418085c9726669bf53f3d66e0018f2b08ffc4ce6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "citing_paper_year": 2022,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "259991740",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "L-Eval"
      ],
      "dataset_details": [
        {
          "dataset_name": "L-Eval",
          "dataset_description": "Re-annotated data and instructions from similar public datasets to ensure quality, used for standardized evaluation of long-context language models.",
          "citing_paper_id": "265067352",
          "cited_paper_id": 259991740,
          "context_text": "L-Eval L-Eval (An et al., 2023) differs in re-annotating the data and instructions from similar public datasets with smaller sizes to ensure the quality.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'L-Eval' as a dataset that has been re-annotated for quality assurance, making it a specific, verifiable resource.",
          "citing_paper_doi": "10.48550/arXiv.2311.04939",
          "cited_paper_doi": "10.48550/arXiv.2307.11088",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0484f05b6b3c11a9344cb623649ae867f172046f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b0db25e317cf856f1ec1ca3df0e573d850ed4696",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "76665670",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CHALET"
      ],
      "dataset_details": [
        {
          "dataset_name": "CHALET",
          "dataset_description": "Used to train and evaluate navigation and interaction tasks in indoor environments, providing a platform for benchmarking AI agents. | Used as a virtual house environment for training agents in household tasks, emphasizing the development of planning and navigation skills. | Used to train and evaluate task-oriented learning in a 3D virtual kitchen environment, focusing on interactive and realistic scenarios. | Used to train and evaluate household agents in a simulated home environment, focusing on navigation and interaction tasks. | Used as an interactive 3D virtual environment for task-oriented learning, focusing on agent training and planning capabilities in kitchen settings.",
          "citing_paper_id": "258762577",
          "cited_paper_id": 76665670,
          "context_text": "Other indoor household World Models include VRKitchen [13], CHALET [57], MINOS [40], House3D [54], etc.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'VRKitchen' and 'CHALET' as indoor household World Models. These are environments used for task-oriented learning and agent training, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2305.10626",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f821d75968bc8de070af3ce5aa7f57bc031fafb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b5a30dc9bb96f686da830b93033569475688424d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "20681260",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VRKitchen"
      ],
      "dataset_details": [
        {
          "dataset_name": "VRKitchen",
          "dataset_description": "Used to train and evaluate navigation and interaction tasks in indoor environments, providing a platform for benchmarking AI agents. | Used as a virtual house environment for training agents in household tasks, emphasizing the development of planning and navigation skills. | Used as an interactive 3D virtual environment for task-oriented learning, focusing on agent training and planning capabilities in kitchen settings. | Used to train and evaluate task-oriented learning in a 3D virtual kitchen environment, focusing on interactive and realistic scenarios. | Used to train and evaluate household agents in a simulated home environment, focusing on navigation and interaction tasks. | Used to train and evaluate agents in a large-scale 3D house environment, focusing on complex navigation and interaction tasks.",
          "citing_paper_id": "258762577",
          "cited_paper_id": 20681260,
          "context_text": "Other indoor household World Models include VRKitchen [13], CHALET [56], MINOS [41], House3D [53], etc.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'World Models' which are environments used for training and evaluating agents. These are not traditional datasets but are used similarly in the context of reinforcement learning and AI planning.",
          "citing_paper_doi": "10.48550/arXiv.2305.10626",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f821d75968bc8de070af3ce5aa7f57bc031fafb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b58e80ad8c6e6844c41535080ccbdef06bce3b6e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "230435736",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Pile"
      ],
      "dataset_details": [
        {
          "dataset_name": "Pile",
          "dataset_description": "Used to evaluate perplexity on a subset of the test set, focusing on the performance of GPT-Neo and GPT-J models in language modeling. | Used to evaluate perplexity on a subset of the Pile test set, focusing on the performance of GPT-Neo and GPT-J models in language modeling. | Used to evaluate language model performance, specifically measuring perplexity on a diverse text dataset designed for language modeling. | Used to evaluate model performance, focusing on diverse text for language modeling. Sampled 5000 examples for assessment. | Used to verify the preservation of generality and language modeling abilities of LMs, ensuring improved performance on downstream tasks through evaluation on a diverse text corpus.",
          "citing_paper_id": "258762577",
          "cited_paper_id": 230435736,
          "context_text": "Therefore, following previous work [42], we evaluate perplexity on a subset of Pile [12] test set, which is the pretraining dataset for both GPT-Neo and GPT-J.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions using a subset of the Pile test set for evaluating perplexity, which is a specific, verifiable dataset.",
          "citing_paper_doi": "10.48550/arXiv.2305.10626",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f821d75968bc8de070af3ce5aa7f57bc031fafb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "40952164",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "P bw 2"
      ],
      "dataset_details": [
        {
          "dataset_name": "P bw 2",
          "dataset_description": "Used to evaluate the STRIPS-HGN network on test problems for the Blocksworld domain, focusing on planning capabilities and performance in solving specific planning tasks.",
          "citing_paper_id": "208512881",
          "cited_paper_id": 40952164,
          "context_text": "To determine whether this is the case, we train each STRIPS-HGN on the training problems for Zenotravel and Gripper, while we evaluate the network on the test problems P bw 2 for Blocksworld.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Blocksworld' and 'Zenotravel' as problem domains but does not specify them as datasets. 'P bw 2' is mentioned as a test problem for Blocksworld, which seems to be a specific instance or subset of problems.",
          "citing_paper_doi": "10.1609/icaps.v30i1.6754",
          "cited_paper_doi": "10.1016/S0004-3702(00)00079-5",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e26f5517a6ff40529b297af26b771f46a3a1dee6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a3ee0f8c5efa5605182d0fdc93cdb8df3c5f4b37",
          "citing_paper_year": 2019,
          "cited_paper_year": 2001
        }
      ]
    },
    {
      "cited_paper_id": "249848263",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MineDojo"
      ],
      "dataset_details": [
        {
          "dataset_name": "MineDojo",
          "dataset_description": "Used to decompose and extract structured actions, focusing on building open-ended embodied agents with internet-scale knowledge. | Used to build open-ended embodied agents with internet-scale knowledge, specifically focusing on crafting and smelting recipes to enhance agent capabilities.",
          "citing_paper_id": "258959262",
          "cited_paper_id": 249848263,
          "context_text": "The related knowledge is from: 1) Crafting/smelting recipes in MineDojo [5], written in the form “Crafting {quantity} {object} requires {material} as the material and {tool} as the tool”; 2) Wiki on the Internet 5.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Crafting/smelting recipes in MineDojo', which refers to a specific dataset of crafting and smelting recipes used in the MineDojo project. The context indicates that these recipes are used to build open-ended embodied agents.",
          "citing_paper_doi": "10.48550/arXiv.2305.17144",
          "cited_paper_doi": "10.48550/arXiv.2206.08853",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c695c4e68561347564ea0daa50dc339dff73d8c5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "260887370",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PEB"
      ],
      "dataset_details": [
        {
          "dataset_name": "PEB",
          "dataset_description": "Used to assess LLM-based agents in penetration testing scenarios, focusing on diverse targets from leading platforms to evaluate planning and execution capabilities.",
          "citing_paper_id": "261064713",
          "cited_paper_id": 260887370,
          "context_text": "PEB [29] is a benchmark tailored for assessing LLM-based agents in penetration testing scenarios, comprising 13 diverse targets from leading platforms.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions PEB as a benchmark for assessing LLM-based agents in penetration testing, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.1007/s11704-024-40231-1",
          "cited_paper_doi": "10.48550/arXiv.2308.06782",
          "citing_paper_url": "https://www.semanticscholar.org/paper/28c6ac721f54544162865f41c5692e70d61bccab",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b85f3a66245d483f3eb3447eaf9950bd55f2b21e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "215768690",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "RoboTHOR"
      ],
      "dataset_details": [
        {
          "dataset_name": "RoboTHOR",
          "dataset_description": "Used as a simulation environment for experiments, focusing on the validation set to test planning capabilities of LLMs in embodied AI tasks.",
          "citing_paper_id": "257378363",
          "cited_paper_id": 215768690,
          "context_text": "Simulation Setup: We use the RoboTHOR [16] validation set as a simulation environment for our experiments.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "RoboTHOR is mentioned as a simulation environment used for experiments, which fits the criteria for a reusable resource and a dataset.",
          "citing_paper_doi": "10.1109/LRA.2023.3346800",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00323",
          "citing_paper_url": "https://www.semanticscholar.org/paper/32524aa3ae8522542753ed7e6f4cca3970e4acab",
          "cited_paper_url": "https://www.semanticscholar.org/paper/98789b46f7be983300a0e93e1c53bab56b36efd1",
          "citing_paper_year": 2023,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "4894130",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SingleEq"
      ],
      "dataset_details": [
        {
          "dataset_name": "SingleEq",
          "dataset_description": "Used to assess models on single-equation grade-school algebra word problems involving multiple math operations over nonnegative rational numbers. | Used to test symbolic reasoning, focusing on tasks that involve manipulating sequences of characters. | Used for few-shot learning experiments, containing multiple-choice questions to assess the model's reasoning and comprehension. | Used to evaluate arithmetic problem-solving, emphasizing multi-step reasoning and real-world scenarios. | Used to evaluate commonsense reasoning, focusing on questions that require everyday knowledge and logical inference. | Used to assess complex arithmetic problem-solving skills, emphasizing multi-step reasoning and accurate computation. | Used to evaluate multi-step reasoning with questions that require strategic thinking, but the reasoning steps are not provided. | Used to test symbolic reasoning, focusing on pattern recognition and sequence completion. | Used for few-shot learning experiments, providing simple and varied arithmetic problems to evaluate the model's generalization. | Used for few-shot learning experiments, focusing on strategic questions to evaluate the model's higher-order reasoning and planning. | Used to assess single-equation problems, focusing on straightforward algebraic reasoning. | Used to test math word problems requiring multiple reasoning steps and operations, focusing on complex problem-solving skills. | Used to test basic arithmetic reasoning, focusing on multi-operation problems and numerical understanding. | Used to assess strategic thinking and reasoning, focusing on questions that require planning and multi-step problem-solving. | Used to evaluate models on algebraic word problems with natural language rationales, focusing on the ability to generate correct equations and solutions. | Used for few-shot learning experiments, providing algebraic word problems to test the model's ability to parse and solve equations. | Used for few-shot learning experiments, focusing on letter-based puzzles to test the model's pattern recognition and logical reasoning. | Used to assess complex arithmetic problem-solving skills, emphasizing multi-step reasoning and numerical accuracy. | Used to evaluate arithmetic reasoning with high-quality, linguistically diverse grade school math word problems created by human problem writers. | Used to evaluate strategic thinking and commonsense reasoning, focusing on complex, multi-step problems. | Used for few-shot learning experiments, targeting commonsense knowledge and reasoning through multiple-choice questions. | Used for few-shot learning experiments, featuring single-equation problems to test the model's equation-solving abilities. | Used to assess one-unknown arithmetic word problems for up-to-4 grade level students, derived from another dataset with simple modifications. | Used to evaluate arithmetic word problems, emphasizing real-world scenarios and numerical reasoning. | Used to evaluate models on solving single-equation grade-school algebra word problems, involving multiple math operations over nonnegative rational numbers and one variable. | Used to assess commonsense reasoning, focusing on everyday situations and logical inference. | Used to train and evaluate models on parsing algebraic word problems, focusing on generating natural language rationales for solutions.",
          "citing_paper_id": "258558102",
          "cited_paper_id": 4894130,
          "context_text": "…word problems, (5) the AQUA (Ling et al., 2017) dataset of algebraic word problems with natural language rationales, and (6) the SingleEq (Koncel-Kedziorski et al., 2015) dataset of single-equation grade-school algebra word problems with multiple math operations over nonnegative rational…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, AQUA and SingleEq, which are used for evaluating models on algebraic word problems. Both datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2305.04091",
          "cited_paper_doi": "10.1162/tacl_a_00160",
          "citing_paper_url": "https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/17230f5b3956188055a48c5f4f61d131cce0662f",
          "citing_paper_year": 2023,
          "cited_paper_year": 2015
        }
      ]
    },
    {
      "cited_paper_id": "230799347",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MAWPS"
      ],
      "dataset_details": [
        {
          "dataset_name": "MAWPS",
          "dataset_description": "Used to assess CoT reasoning in strategic problem-solving, particularly in questions requiring multi-step logical reasoning. | Used to evaluate CoT reasoning in commonsense reasoning tasks, focusing on questions that require everyday knowledge. | Used to evaluate CoT reasoning in few-hop math word problems, focusing on arithmetic and algebraic problem-solving. | Used to assess CoT reasoning in complex math word problems, focusing on multi-step calculations and logical reasoning. | Used to evaluate CoT reasoning in basic math word problems, focusing on single-step and multi-step arithmetic problems.",
          "citing_paper_id": "269626390",
          "cited_paper_id": 230799347,
          "context_text": "Previous work on CoT mainly constrained its evaluations to static test sets ranging from commonsense domains (Sports Understanding [41], StrategyQA [18], CommonSenseQA [44]), few-hop math word problems (AsDiv [31], GSM8k [10], MAWPS [27]), to a number of basic \"symbolic reasoning\" tasks (CoinFlip…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for evaluating Chain-of-Thought (CoT) reasoning, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "259501567",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Habitat-Matterport 3D"
      ],
      "dataset_details": [
        {
          "dataset_name": "Habitat-Matterport 3D",
          "dataset_description": "Used to evaluate dialectic multi-robot collaboration with large language models, focusing on motion planning and decision-making tasks. | Utilized for semantic navigation tasks, providing realistic 3D environments to test planning capabilities of LLMs.",
          "citing_paper_id": "269921354",
          "cited_paper_id": 259501567,
          "context_text": "…TDW-MAT, C-WAH [63] ✓ Decision, Communication, Memory SMART-LLM [64] Multi-Agent Planning Proposed Benchmark Dataset ✗ Decision, Planning RoCo [65] Motion Planning RoCoBench ✗ Decision, Planning Co-NavGPT [66] Semantic Navigation Habitat-Matterport 3D [67] ✗ Planning Guo et al. [68]…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'RoCo' and 'Habitat-Matterport 3D', which are specific resources. However, 'SMART-LLM' and 'Co-NavGPT' are excluded as they are likely models or methods.",
          "citing_paper_doi": "10.48550/arXiv.2405.11106",
          "cited_paper_doi": "10.1109/ICRA57147.2024.10610855",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd38755291d108ab86c68d1aac7485921bb8e647",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c5d18dbb92d0cd5393baa1e69de33d6922ac3e57",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "264172518",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TDW-MAT"
      ],
      "dataset_details": [
        {
          "dataset_name": "TDW-MAT",
          "dataset_description": "Used to evaluate multi-agent planning capabilities, focusing on decision-making and theory of mind in complex environments. | Used to assess multi-agent planning, emphasizing decision-making and communication in collaborative tasks.",
          "citing_paper_id": "269921354",
          "cited_paper_id": 264172518,
          "context_text": "…Game, Driving BabyAI-Text, Traffic Junction [39] ✓ Decision, Communication Chen et al. [60] Consensus Seeking Generated Data ✗ Decision Li et al. [61] Path Planning Close-source simulator ✗ Decision, Communication, Theory of Mind CoELA [62] Multi-Agent Planning TDW-MAT, C-WAH [63] ✓ Decision,…",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Generated Data' and 'TDW-MAT, C-WAH', but 'Generated Data' is too generic. 'TDW-MAT' and 'C-WAH' are specific datasets used in multi-agent planning research.",
          "citing_paper_doi": "10.48550/arXiv.2405.11106",
          "cited_paper_doi": "10.18653/v1/2023.emnlp-main.13",
          "citing_paper_url": "https://www.semanticscholar.org/paper/dd38755291d108ab86c68d1aac7485921bb8e647",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e17c58d7a48b6b811df023484161a3b9c03e0d6b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "199000710",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MineRL"
      ],
      "dataset_details": [
        {
          "dataset_name": "MineRL",
          "dataset_description": "Used as a reference for setting up the Minecraft environment, providing a large-scale dataset of Minecraft demonstrations for training and evaluation.",
          "citing_paper_id": "265129059",
          "cited_paper_id": 199000710,
          "context_text": "Our Minecraft environment is a hybrid between MineRL [19] and the MCP-Reborn (github.com/Hexeption/MCP-Reborn) Minecraft modding package.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions MineRL, which is a dataset of Minecraft demonstrations, but does not indicate that it is used in the current research. The mention is more about the environment setup.",
          "citing_paper_doi": "10.1109/TPAMI.2024.3511593",
          "cited_paper_doi": "10.24963/ijcai.2019/339",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fde266447d68b2728752a7835e8c97e1af316ac4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2c87b6b48dba6315a6581098861f8bf2994799f6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "251979509",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "G-PlanET"
      ],
      "dataset_details": [
        {
          "dataset_name": "G-PlanET",
          "dataset_description": "Used to represent structured inputs for LLMs in task planning, focusing on grounding language models in 3D environments to enhance planning capabilities. | Used for comparison with the proposed method, focusing on grounded planning for embodied tasks using language models. Contains simulation environments in a tabular form.",
          "citing_paper_id": "271854813",
          "cited_paper_id": 251979509,
          "context_text": "For comparison, we use G-PlanET [15], which contains all the simulation environments of ALFRED but represented in a tabular form.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'G-PlanET' as a resource containing simulation environments, which appears to be a specific dataset used for comparison in the research.",
          "citing_paper_doi": "10.3233/FAIA240916",
          "cited_paper_doi": "10.1609/aaai.v37i11.26549",
          "citing_paper_url": "https://www.semanticscholar.org/paper/63f05ba4d7ec1af82c756348e7fb6c90c8ab4c4e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/59b71e2a248d67a2692bc7e35faa504ee2dbc98d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "259837542",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "3D Scene Graphs"
      ],
      "dataset_details": [
        {
          "dataset_name": "3D Scene Graphs",
          "dataset_description": "Used to represent structured inputs for LLMs in task planning, focusing on grounding language models in 3D environments to enhance planning capabilities.",
          "citing_paper_id": "271854813",
          "cited_paper_id": 259837542,
          "context_text": "LLMs’ capabilities change when the query in input is not completely textual, but can assume a more structured form, e.g., a tabular structure [15], a graph-like structure [23] (such as 3D Scene Graphs [4]), or even LTL formulas [8].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions '3D Scene Graphs' which is a specific type of structured data used in the cited papers. However, it does not explicitly state that it is a dataset. The context focuses on the use of structured inputs for LLMs.",
          "citing_paper_doi": "10.3233/FAIA240916",
          "cited_paper_doi": "10.48550/arXiv.2307.06135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/63f05ba4d7ec1af82c756348e7fb6c90c8ab4c4e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3843469976bd895851bfa08c8208350745bf649f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "258108259",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Vicuna"
      ],
      "dataset_details": [
        {
          "dataset_name": "Vicuna",
          "dataset_description": "Used to assess Orca's performance on a variety of tasks, emphasizing natural language understanding and generation. | Used to evaluate Orca's response quality and coherence across a wide range of prompts and scenarios. | Used to evaluate generative, reasoning, and comprehension abilities of Orca, focusing on diverse prompts and responses. | Used to test Orca's reasoning and problem-solving skills on challenging academic tasks and puzzles.",
          "citing_paper_id": "259075316",
          "cited_paper_id": 258108259,
          "context_text": "Evaluation: We assess the generative, reasoning, and comprehension abilities of Orca, under a range of settings: (i) AutoEvaluation with GPT-4 on existing evaluation sets from Vicuna, WizardLM and the awesome prompts collection8; (ii) Academic benchmarks like Big-Bench Hard [4] and TruthfulQA [20]; (iii) Professional and Academic exams like SAT, LSAT, GRE, GMAT from AGIEval [1]; (iv) Safety evaluation with ToxiGen [21] to test toxic language generation and hate speech detection across different minority groups.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several evaluation sets and benchmarks used to assess the capabilities of Orca. These are specific datasets or collections used for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2306.02707",
          "cited_paper_doi": "10.48550/arXiv.2304.06364",
          "citing_paper_url": "https://www.semanticscholar.org/paper/0244aeb7c6927e2fb0c2e668687e160a00737dbe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c834c19cd126bbd6d25a3572d7205cfed76271",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "263625818",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WizardLM"
      ],
      "dataset_details": [
        {
          "dataset_name": "WizardLM",
          "dataset_description": "Used to test Orca's reasoning and problem-solving skills on challenging academic tasks and puzzles. | Used to evaluate Orca's response quality and coherence across a wide range of prompts and scenarios. | Used to assess Orca's performance on a variety of tasks, emphasizing natural language understanding and generation. | Used to evaluate generative, reasoning, and comprehension abilities of Orca, focusing on diverse prompts and responses.",
          "citing_paper_id": "259075316",
          "cited_paper_id": 263625818,
          "context_text": "Evaluation: We assess the generative, reasoning, and comprehension abilities of Orca, under a range of settings: (i) AutoEvaluation with GPT-4 on existing evaluation sets from Vicuna, WizardLM and the awesome prompts collection8; (ii) Academic benchmarks like Big-Bench Hard [4] and TruthfulQA [20]; (iii) Professional and Academic exams like SAT, LSAT, GRE, GMAT from AGIEval [1]; (iv) Safety evaluation with ToxiGen [21] to test toxic language generation and hate speech detection across different minority groups.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several evaluation sets and benchmarks used to assess the capabilities of Orca. These are specific datasets or collections used for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2306.02707",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/0244aeb7c6927e2fb0c2e668687e160a00737dbe",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "4710439",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "EPIC-KITCHENS"
      ],
      "dataset_details": [
        {
          "dataset_name": "EPIC-KITCHENS",
          "dataset_description": "Used to collect video-action pairs, specifically focusing on egocentric kitchen activities to enhance the understanding of complex, sequential actions.",
          "citing_paper_id": "270521777",
          "cited_paper_id": 4710439,
          "context_text": "Besides Panda-70M, we also collect video-action pairs from existing action-annotated datasets, including Something-Something V2 [26], BridgeData V2 [73], and EPIC-KITCHENS [20].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions specific datasets used for collecting video-action pairs, which are relevant to the research on planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2406.09455",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/da223f21296066984bf8d698010b35dd2e8b2394",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fc50c9392fd23b6c88915177c6ae904a498aacea",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "268041615",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LoCoMo"
      ],
      "dataset_details": [
        {
          "dataset_name": "LoCoMo",
          "dataset_description": "Used to evaluate long-term conversational memory in LLM agents, focusing on various reasoning types including single-hop, multi-hop, temporal, commonsense, world knowledge, and adversarial reasoning.",
          "citing_paper_id": "273345961",
          "cited_paper_id": 268041615,
          "context_text": "LoCoMo (Maharana et al., 2024) includes 50 long-term chat histories and questions testing single-hop, multi-hop, temporal, commonsense, world knowledge, and adversarial reasoning.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "LoCoMo is a specific dataset with a clear name and purpose, used for evaluating long-term conversational memory in LLM agents.",
          "citing_paper_doi": "10.48550/arXiv.2410.10813",
          "cited_paper_doi": "10.48550/arXiv.2402.17753",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fd48dc6b433cfcd220b6e769c6179d8ef6fcf862",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "258762525",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "4nums.com"
      ],
      "dataset_details": [
        {
          "dataset_name": "4nums.com",
          "dataset_description": "Used to source games ranked by relative difficulty for evaluating problem-solving capabilities of large language models, focusing on the subset of games indexed 901-1000. | Used to select a subset of 100 games (indices 901-1000) from a larger set of 1362 games ranked by relative difficulty for evaluating problem-solving capabilities of LLMs.",
          "citing_paper_id": "261049794",
          "cited_paper_id": 258762525,
          "context_text": "Adhering to the setup detailed in (Yao et al. 2023), we use games from indices 901-1000, sourced from the 1362 games ranked by relative difficulty at 4nums.com .",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific subset of games from a larger set ranked by difficulty, which appears to be a dataset. However, the name '4nums.com' is not a traditional dataset identifier.",
          "citing_paper_doi": "10.48550/arXiv.2308.10379",
          "cited_paper_doi": "10.48550/arXiv.2305.10601",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "257532815",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TravelPlanner"
      ],
      "dataset_details": [
        {
          "dataset_name": "TravelPlanner",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs in U.S. domestic travel scenarios, demonstrating that even GPT-4 has a low success rate without pre-collected information.",
          "citing_paper_id": "279621061",
          "cited_paper_id": 257532815,
          "context_text": "To investigate this problem, Xie et al. (2024) proposed a U.S. domestic travel planning benchmark, TravelPlanner, and showed that LLMs are not capable of handling this task and even strongest LLM GPT-4 can only achieve a success rate of 0.6% by itself without access to pre-collected information.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'TravelPlanner', which is described as a benchmark for U.S. domestic travel planning. It is used to evaluate the planning capabilities of LLMs, specifically showing the limitations of GPT-4.",
          "citing_paper_doi": "10.48550/arXiv.2404.11891",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fbf7e7f2f156443ee2e31297e8eb4a2819a18845",
          "cited_paper_url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "8953884",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MetaQA-3hop"
      ],
      "dataset_details": [
        {
          "dataset_name": "MetaQA-3hop",
          "dataset_description": "Used to evaluate multi-hop reasoning capabilities in question answering systems, specifically focusing on complex queries over the Wiki-Movies knowledge graph.",
          "citing_paper_id": "263605944",
          "cited_paper_id": 8953884,
          "context_text": "We further select the MetaQA-3hop dataset (Zhang et al., 2018) which is based on Wiki-Movies KGs 6 .",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MetaQA-3hop dataset, which is a specific dataset used for question answering with knowledge graphs. The dataset is clearly identified and used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2310.01061",
          "cited_paper_doi": "10.1609/aaai.v32i1.12057",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/95280565aa3d120c6d7e8d87ea3423f16977f19a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "268379413",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Live-CodeBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "Live-CodeBench",
          "dataset_description": "Used to evaluate large language models in coding tasks, focusing on contamination-free and holistic assessment of model performance.",
          "citing_paper_id": "277999502",
          "cited_paper_id": 268379413,
          "context_text": "• Multi-step MARFT with reflective agents on coding tasks : Introduce multi-step MARFT incorporating a \"reflector\" agent to handle coding tasks and benchmarks such as Live-CodeBench (Jain et al., 2024) and CodeForces 5 .",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Live-CodeBench' and 'CodeForces' as benchmarks for evaluating coding tasks. However, 'CodeForces' is a platform, not a dataset. 'Live-CodeBench' is a specific dataset used for evaluating large language models in coding tasks.",
          "citing_paper_doi": "10.48550/arXiv.2504.16129",
          "cited_paper_doi": "10.48550/arXiv.2403.07974",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a9a6f3178221ee4bacbdddd7b8fc0d6861e39b25",
          "cited_paper_url": "https://www.semanticscholar.org/paper/afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "428579",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CSQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CSQA",
          "dataset_description": "Applied to assess problem-solving skills in algebraic word problems, emphasizing logical and mathematical reasoning. | Applied to evaluate the robustness of models on a diverse set of arithmetic word problems, using shared exemplars with GSM8K. | Utilized to evaluate the ability of models to solve complex arithmetic and algebraic word problems, emphasizing the role of contextual information. | Used to solve arithmetic word problems, focusing on the effectiveness of exemplars in improving model performance. | Used to test the capability of models to solve single-equation word problems, leveraging shared exemplars with GSM8K. | Used to assess performance on arithmetic word problems, particularly those involving simple and complex operations. | Utilized to assess the generalization of models across different types of arithmetic word problems, using shared exemplars with GSM8K. | Employed to assess the performance of models on addition and subtraction word problems, using shared exemplars with GSM8K. | Used to assess arithmetic reasoning in story problems, emphasizing the ability to handle various numerical and textual complexities.",
          "citing_paper_id": "258297976",
          "cited_paper_id": 428579,
          "context_text": "In Table 1, for GSM8K, CSQA, Letter Concate-nation and other datasets sharing the same exem-plars with GSM8K (AddSub, SingleEq, SVAMP, and ASDiv), we utilize the best exemplars in this section.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, which are used to solve arithmetic and algebraic word problems. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2304.11657",
          "cited_paper_doi": "10.3115/v1/D14-1058",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a7862e14b4c20cefd6dc4f611f8aa866fabf130b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "245329531",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ELI5"
      ],
      "dataset_details": [
        {
          "dataset_name": "ELI5",
          "dataset_description": "Used to train models to answer complex questions in simple terms, focusing on the ability to infer answers from web pages using natural language processing techniques.",
          "citing_paper_id": "252762395",
          "cited_paper_id": 245329531,
          "context_text": "WebGPT (Nakano et al., 2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 (Fan et al., 2019).",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ELI5' which is a dataset used for training models to answer complex questions in simple terms. However, it is not explicitly stated that it is used for planning capabilities of LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22",
          "citing_paper_year": 2022,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "222208810",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Household"
      ],
      "dataset_details": [
        {
          "dataset_name": "Household",
          "dataset_description": "Used to simulate and evaluate planning capabilities in a typical household environment, focusing on the production of textual observations using PDDL semantics.",
          "citing_paper_id": "277313841",
          "cited_paper_id": 222208810,
          "context_text": "ALFWorld and Household (Shridhar et al. 2021; Guan et al. 2023) tackles the complexities of real-world typical household environment that uses PDDL semantics to produce textual observations.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Household' as a dataset used in the ALFWorld framework, which is relevant to the planning capabilities of LLMs in a household environment.",
          "citing_paper_doi": "10.48550/arXiv.2503.18971",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5038e4830f01e44fe59bddc707128b8777d71d96",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "211096967",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SciBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "SciBench",
          "dataset_description": "Evaluates college-level scientific problem-solving abilities in physics and chemistry, assessing the capabilities of large language models in these domains. | Used to measure mathematical problem-solving abilities, focusing on a variety of math problems to evaluate the performance of large language models.",
          "citing_paper_id": "270123654",
          "cited_paper_id": 211096967,
          "context_text": "…approaches for complex task solving and our adaptive build approach with Captain Agent on six real-world scenarios, including many mathematics problem-solving [45], data analysis [46], programming [47], scientific problem-solving [48] (Physics and Chemistry), and world-information retrieval [49].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several domains where the adaptive build approach with Captain Agent is applied, including mathematics problem-solving, data analysis, programming, scientific problem-solving, and world-information retrieval. However, only 'MATH' and 'SciBench' are specific datasets mentioned in the cited papers.",
          "citing_paper_doi": "10.48550/arXiv.2405.19425",
          "cited_paper_doi": "10.1145/3383458",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/010c9f63c51ccc504de36d2d1693e0ea9d525da1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "257834038",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MiniWobChat"
      ],
      "dataset_details": [
        {
          "dataset_name": "MiniWobChat",
          "dataset_description": "Used to evaluate the planning capabilities of language models across various tasks, providing a comprehensive analysis of performance and difficulty levels.",
          "citing_paper_id": "263611068",
          "cited_paper_id": 257834038,
          "context_text": "In our evaluation, we use all available tasks in the official RCI code, with varying degrees of difficulty, to conduct a comprehensive analysis against MiniWobChat.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MiniWobChat' as a resource used for evaluation, but it does not specify if it is a dataset, method, or tool. Given the topic of planning capabilities of LLMs, it is likely a dataset or benchmark, but the context is not explicit enough to confirm.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2303.17491",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9ea0757c750ab1222a7442d3485a74d1c526b04c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "252595921",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ScienceQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "ScienceQA",
          "dataset_description": "Used to evaluate GPT-4's performance on mathematical reasoning tasks with tabular contexts, focusing on the model's ability to understand and solve problems presented in tables. | Used to evaluate Chameleon's performance on science-related questions, focusing on reasoning capabilities and problem-solving skills. | Used to evaluate QA accuracy on semi-structured mathematical reasoning problems, focusing on the test set to measure model performance. | Used to evaluate the effectiveness and adaptability of the Chameleon model in handling complex reasoning tasks, focusing on scientific questions and problem-solving. | Applied to assess Chameleon's ability to solve mathematical word problems, emphasizing logical reasoning and numerical computation. | Used to assess the Chameleon model's performance in semi-structured mathematical reasoning tasks, emphasizing adaptability and problem-solving skills.",
          "citing_paper_id": "258212542",
          "cited_paper_id": 252595921,
          "context_text": "We conduct experiments on two complex reasoning tasks, ScienceQA [27] and TabMWP [28], to evaluate our Chameleon in terms of effectiveness and adaptability.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, ScienceQA and TabMWP, which are used to evaluate the effectiveness and adaptability of the Chameleon model.",
          "citing_paper_doi": "10.48550/arXiv.2304.09842",
          "cited_paper_doi": "10.48550/arXiv.2209.14610",
          "citing_paper_url": "https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "3432876",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MNLI"
      ],
      "dataset_details": [
        {
          "dataset_name": "MNLI",
          "dataset_description": "Used to fine-tune a DeBERTa-large model for determining semantic entailment between generated actions, focusing on natural language inference.",
          "citing_paper_id": "264405734",
          "cited_paper_id": 3432876,
          "context_text": "Inspired by Kuhn et al. (2022), we apply a DeBERTa-large model (He et al., 2020) fine-tuned on natural language inference (NLI) dataset MNLI (Williams et al., 2018) to determine whether the two generated actions entail each other semantically.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MNLI dataset, which is a specific, verifiable resource used for fine-tuning a model to perform semantic entailment.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": "10.18653/v1/N18-1101",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "258833200",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MINT-TheoremQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MINT-TheoremQA",
          "dataset_description": "Used to evaluate LLMs in multi-turn interactions involving theorem-driven question answering, requiring knowledge searches on Wikipedia and Python calculations. | Used to evaluate complex question answering, requiring multi-hop reasoning over multiple paragraphs. | Used to evaluate reasoning capabilities in theorem-driven question answering, focusing on the ability to solve mathematical problems and understand logical proofs. | Used to test theorem-driven question answering, focusing on logical reasoning and mathematical proofs. | Used to evaluate decision-making and planning capabilities in interactive environments, focusing on text-based games. | Used to evaluate theorem-driven question answering capabilities in LLMs, focusing on multi-step reasoning and mathematical problem-solving. | Used to evaluate reasoning capabilities in large language models, focusing on multi-choice questions across various subjects.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 258833200,
          "context_text": "At the same time, for the MINT-TheoremQA (Chen et al., 2023a), the model needs to perform knowledge searches on Wikipedia and use a Python interpreter to calculate and draw conclusions.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MINT-TheoremQA' which is a specific dataset used for evaluating LLMs in multi-turn interactions involving theorem-driven question answering.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2305.12524",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a52dd1e900200e0733eea927edc7d6c27aeba187",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "257833781",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ToolBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolBench",
          "dataset_description": "Used to evaluate tool-use environments, focusing on the model's ability to interact with tools in simulated settings. | Used to assess reasoning capabilities, specifically evaluating the model's performance on complex arithmetic problems.",
          "citing_paper_id": "264405734",
          "cited_paper_id": 257833781,
          "context_text": "We evaluate ToolChain ∗ on four tool-use environments in ToolBench (Xu et al., 2023) and one reasoning task in GSM8K (Cobbe et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, ToolBench and GSM8K, which are used for evaluating a model. ToolBench is used for tool-use environments, and GSM8K is used for a reasoning task.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": "10.48550/arXiv.2303.17580",
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ToolQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolQA",
          "dataset_description": "Used to conduct experiments on problem-solving capabilities, focusing on the entire set of questions to evaluate the model's performance. | Used to conduct experiments on a subset of hard questions, specifically to test the model's ability to handle challenging problems. | Used to evaluate model performance on a subset of hard questions, focusing on the planning capabilities of LLMs in solving complex problems.",
          "citing_paper_id": "264405734",
          "cited_paper_id": null,
          "context_text": "We conduct experiments on the entire set of GSM8K and also a subset of hard questions from GSM8K collected in ToolQA (Zhuang et al., 2023).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets: GSM8K and a subset from ToolQA. Both are used for conducting experiments, particularly focusing on hard questions.",
          "citing_paper_doi": "10.48550/arXiv.2310.13227",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "263625818",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Movie Recommendation"
      ],
      "dataset_details": [
        {
          "dataset_name": "Movie Recommendation",
          "dataset_description": "Used to evaluate LLMCompiler on embarrassingly parallel patterns, focusing on speedup and cost reduction compared to ReAct.",
          "citing_paper_id": "266044180",
          "cited_paper_id": 263625818,
          "context_text": "• We evaluate LLMCompiler on embarrassingly parallel patterns using HotpotQA [54] and Movie Recommendation [4], where we observe 1.80 × / 3.74 × speedup and 3.37 × / 6.73 × cost reduction compared to ReAct (Sec.",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'HotpotQA' and 'Movie Recommendation' as datasets used to evaluate LLMCompiler. HotpotQA is a well-known dataset, while Movie Recommendation is less specific but appears to be a dataset used in the evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2312.04511",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/36f71673d9337b432babc51da77ef38b2070b5ed",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "259983067",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AITW"
      ],
      "dataset_details": [
        {
          "dataset_name": "AITW",
          "dataset_description": "Used to provide human demonstrations of device interactions through static images, supporting research into the planning capabilities of LLMs.",
          "citing_paper_id": "267617061",
          "cited_paper_id": 259983067,
          "context_text": "AITW [20] stands out as a static image dataset that offers human demonstrations of device interactions.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions AITW as a static image dataset offering human demonstrations of device interactions, which aligns with the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.1145/3637528.3671650",
          "cited_paper_doi": "10.48550/arXiv.2307.10088",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7147a7ccbac8add23010c5fcc25137e754afb3ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/060e0df71620f1844839f6a993dfa5fb8e4c3bf6",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "254044610",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "The Stack"
      ],
      "dataset_details": [
        {
          "dataset_name": "The Stack",
          "dataset_description": "Used to build a code-centric corpus with 90B tokens and a 10:1 code-to-text ratio, enhancing coding ability while preserving natural language performance. | Used as the basis for the code part of the research, providing a collection of permissively licensed source codes from GitHub to support the development and evaluation of the models.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 254044610,
          "context_text": "Specifically, we built a code-centric corpus based on The Stack (Kocetkov et al., 2022), comprising 90B tokens with a 10:1 code-to-text ratio, ensuring improved capabilities in coding ability while maintaining performance in natural language ability.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'The Stack' as a corpus used to build a code-centric dataset. It is a specific, named resource with clear provenance.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2211.15533",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f3a6115e5fb2237df938976e005468f0b18da797",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "260164780",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "InterCode-CTF"
      ],
      "dataset_details": [
        {
          "dataset_name": "InterCode-CTF",
          "dataset_description": "Used to assess LLMs' performance in realistic web environments, emphasizing autonomous agent building and navigation tasks. | Used to test LLMs' planning abilities in physical environments, focusing on embodied agents and real-world interaction challenges. | Used to evaluate LLMs' planning capabilities in digital environments, focusing on complex interactive tasks and fast thinking processes.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 260164780,
          "context_text": "To measure this ability, we use three datasets: InterCode-CTF (Yang et al., 2023) and WebArena (Zhou et al., 2023) in digital environments, as well as ALFWorld (Shridhar et al., 2020b) in physical environments.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used to measure the planning capabilities of LLMs in different environments. These datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2307.13854",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e41482f4ee984f17382f6cdd900df094d928be06",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "259075316",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Orca data"
      ],
      "dataset_details": [
        {
          "dataset_name": "Orca data",
          "dataset_description": "Used to provide real user interactions and ChatGPT history records, enhancing the model's understanding of real-world dialogue patterns. | Used to improve code generation capabilities through progressive learning, focusing on evolving code samples and explanations. | Used to train models with chain of thought reasoning for human-written tasks, focusing on complex explanation traces generated by GPT-4.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 259075316,
          "context_text": "…crowdsourced annotated dialogue corpus (K ¨ opf et al., 2023), Orca data with chain of thought reasoning for human-written tasks (Lian et al., 2023; Mukherjee et al., 2023), ShareGPT & Chatlogs containing real user and ChatGPT history records (ShareGPT data), as well as Evol-CodeAlpaca data (Luo…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for training and evaluating models with planning capabilities, including crowdsourced dialogues, Orca data, ShareGPT & Chatlogs, and Evol-CodeAlpaca data.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": "10.48550/arXiv.2306.02707",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0244aeb7c6927e2fb0c2e668687e160a00737dbe",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "259164815",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Evol-Instruct-Code-80k-v1"
      ],
      "dataset_details": [
        {
          "dataset_name": "Evol-Instruct-Code-80k-v1",
          "dataset_description": "Used to train models on complex coding tasks generated by ChatGPT, evaluating the model's ability to generate correct and efficient code solutions. | Used to train and evaluate models on real user interactions with ChatGPT, focusing on the quality and relevance of responses in a conversational setting. | Used to train and evaluate models on complex coding tasks generated by ChatGPT, including task solutions and code generation. | Used to train and evaluate models on real user interactions with ChatGPT, focusing on human reasoning tasks and user history records. | Used to enhance code generation and instruction-following abilities, specifically for Python programming tasks, through evolutionary instruction tuning. | Used to train and evaluate code generation capabilities of large language models, focusing on evolutionary instruction tuning and code synthesis.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 259164815,
          "context_text": "We use two open-sourced Evolution-Instruct (Luo et al., 2023) datasets, i.e., Evol-Instruct-Code-80k-v1 and evol-codealpaca-v1, and an execution-verified Python dataset constructed by us.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, 'Evol-Instruct-Code-80k-v1' and 'evol-codealpaca-v1', which are used for training and evaluation. The third dataset is a custom Python dataset, which is excluded as it lacks a specific, verifiable name.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/454c8fef2957aa2fb13eb2c7a454393a2ee83805",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MINT-Reasoning"
      ],
      "dataset_details": [
        {
          "dataset_name": "MINT-Reasoning",
          "dataset_description": "Used to assess models' code generation and execution skills in multi-turn interactions, examining the effects of GPT-4 feedback on accuracy and efficiency. | Used to evaluate models' reasoning capabilities in multi-turn interactions with language feedback, focusing on the impact of GPT-4 feedback on performance.",
          "citing_paper_id": "263831032",
          "cited_paper_id": null,
          "context_text": "We evaluate models on MINT-Reasoning and MINT-Code with and without GPT-4 feedback and the experimental results are in Table 6.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MINT-Reasoning' and 'MINT-Code', which are specific datasets used for evaluating models. These names are consistent with the cited paper's title, indicating they are datasets designed for evaluating LLMs in multi-turn interactions.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "235097370",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TaskLAMA"
      ],
      "dataset_details": [
        {
          "dataset_name": "TaskLAMA",
          "dataset_description": "Used as a subset within TaskLAMA, providing 711 annotated complex tasks for model training and evaluation. | Used to develop and evaluate models on complex task decomposition and organization, comprising 1,612 annotated tasks.",
          "citing_paper_id": "272367167",
          "cited_paper_id": 235097370,
          "context_text": "TaskLAMA (Yuan et al., 2023a) has developed a dataset comprising 1,612 annotated complex tasks, which includes 711 tasks from the MSComplex-Tasks dataset (Zhang et al., 2021) and 901 tasks derived from how-to search queries.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets: TaskLAMA and MSComplex-Tasks. Both are used for developing and evaluating models on complex task decomposition and organization.",
          "citing_paper_doi": "10.48550/arXiv.2409.01806",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.217",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8401ca54ccf4ee7237daadf0322ed186d587c47",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c58269be7bb0b7c896e5d74bfdcfb023c6787cf6",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "252693237",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TO QA"
      ],
      "dataset_details": [
        {
          "dataset_name": "TO QA",
          "dataset_description": "Used to assess LLMs' reasoning capabilities through synthetic questions, focusing on the ability to perform chain-of-thought reasoning.",
          "citing_paper_id": "272367167",
          "cited_paper_id": 252693237,
          "context_text": "TO QA (Saparov and He, 2023) is a synthetic QA dataset designed to assess LLMs’ reasoning capability.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset 'TO QA' which is designed to assess LLMs' reasoning capabilities. The dataset is clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2409.01806",
          "cited_paper_doi": "10.48550/arXiv.2210.01240",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8401ca54ccf4ee7237daadf0322ed186d587c47",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "262053695",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MINT"
      ],
      "dataset_details": [
        {
          "dataset_name": "MINT",
          "dataset_description": "Used to evaluate LLMs in multi-turn interactions with tools and language feedback, focusing on planning capabilities and performance in complex scenarios.",
          "citing_paper_id": "273228858",
          "cited_paper_id": 262053695,
          "context_text": "This PDDL domain is publicly available 1 and PDDL problem files are obtained from the MINT benchmark (Wang et al. 2024b).",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MINT benchmark', which is a specific, identifiable dataset used in the evaluation of LLMs in multi-turn interactions.",
          "citing_paper_doi": "10.48550/arXiv.2410.05669",
          "cited_paper_doi": "10.48550/arXiv.2309.10691",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7bee11da995d968394477a66e78aaab9721ecdf7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "258865971",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ByteSized32"
      ],
      "dataset_details": [
        {
          "dataset_name": "ByteSized32",
          "dataset_description": "Used to generate task-specific world models expressed as text games, focusing on reasoning capabilities in a controlled environment with 20k lines of Python code.",
          "citing_paper_id": "272367167",
          "cited_paper_id": 258865971,
          "context_text": "ByteSized32-SP (Wang et al., 2023a) ByteSized32 is a corpus of 32 reasoning-focused text games totaling 20k lines of Python code.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ByteSized32' as a corpus of text games, which is a specific, verifiable dataset. It is used for generating task-specific world models expressed as text games.",
          "citing_paper_doi": "10.48550/arXiv.2409.01806",
          "cited_paper_doi": "10.48550/arXiv.2305.14879",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c8401ca54ccf4ee7237daadf0322ed186d587c47",
          "cited_paper_url": "https://www.semanticscholar.org/paper/070b91f80ac118b910c1d2ab5be9f65f685979fe",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "259370794",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "natural language based question answering style dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "natural language based question answering style dataset",
          "dataset_description": "Used to evaluate LLMs on reasoning tasks such as projection, execution, planning, and goal recognition, focusing on the planning capabilities of LLMs.",
          "citing_paper_id": "273228858",
          "cited_paper_id": 259370794,
          "context_text": "He et al. (2023) proposed a natural language based question answering style dataset to evaluate LLMs on 4 tasks of projection, execution, planning, and goal recognition.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a dataset created by He et al. (2023) to evaluate LLMs on specific reasoning tasks, including planning.",
          "citing_paper_doi": "10.48550/arXiv.2410.05669",
          "cited_paper_doi": "10.18653/v1/2023.acl-long.255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7bee11da995d968394477a66e78aaab9721ecdf7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2a8b771c1645f3a7b7f6aaf9241e3f88d11b14c1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "54447604",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PartNet"
      ],
      "dataset_details": [
        {
          "dataset_name": "PartNet",
          "dataset_description": "Used to generate data for microwave assets, focusing on fine-grained and hierarchical part-level 3D object understanding in the context of planning capabilities of LLMs.",
          "citing_paper_id": "259243610",
          "cited_paper_id": 54447604,
          "context_text": "We use 10 microwave assets from PartNet [30] for data generation.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "PartNet is a specific, verifiable dataset used for generating data in the context of microwave assets. The citation indicates the dataset is used for data generation, which is relevant to the research.",
          "citing_paper_doi": "10.1109/IROS58592.2024.10802746",
          "cited_paper_doi": "10.1109/CVPR.2019.00100",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f1ba488b092620a655ae48dfcf8fbd8d7e9afbe6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3ecbd9b6153e9ff2f490950a87853e68c808db4d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "8953884",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MQA-2H"
      ],
      "dataset_details": [
        {
          "dataset_name": "MQA-2H",
          "dataset_description": "Used to evaluate two-hop questions in the MetaQA dataset, focusing on more complex relationships and reasoning within the knowledge graph. | Used to test three-hop questions in the MetaQA dataset, focusing on advanced multi-relational reasoning and complex query structures.",
          "citing_paper_id": "268379197",
          "cited_paper_id": 8953884,
          "context_text": "MetaQA (Zhang et al., 2018) is a KGQA dataset from movie domain, with 3 levels of difficulty, denoted as MQA-1H, MQA-2H and MQA-3H.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions MetaQA as a KGQA dataset from the movie domain, which is a specific, verifiable dataset with clear identifiers and usage details.",
          "citing_paper_doi": "10.48550/arXiv.2403.08593",
          "cited_paper_doi": "10.1609/aaai.v32i1.12057",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef1d74ddfc09a367050bc54b7be846769061d95e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/95280565aa3d120c6d7e8d87ea3423f16977f19a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "207167677",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WebQSP"
      ],
      "dataset_details": [
        {
          "dataset_name": "WebQSP",
          "dataset_description": "Used to retrieve top 5 similar relations for those generated by LLMs, enhancing the evaluation of LLM-generated content by comparing against a structured knowledge base. | Used to evaluate complex knowledge graph question answering, focusing on multi-hop reasoning and multi-constrained questions on Freebase. | Used to evaluate single-constrained reasoning paths in knowledge graph questions, focusing on 2-hop reasoning on Freebase.",
          "citing_paper_id": "268379197",
          "cited_paper_id": 207167677,
          "context_text": "WebQuestionsSP (WebQSP) (Yih et al., 2016) contains KGQA questions from Google query logs with up to 2-hop reasoning on Freebase, mostly requiring a single-constrained reasoning path.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions WebQuestionsSP (WebQSP) as a dataset containing KGQA questions from Google query logs, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2403.08593",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ef1d74ddfc09a367050bc54b7be846769061d95e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2024,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "258987659",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "competition-level mathematics problem set"
      ],
      "dataset_details": [
        {
          "dataset_name": "competition-level mathematics problem set",
          "dataset_description": "Used to evaluate verifiers trained on annotated intermediate solutions, focusing on performance improvements over verifiers trained on final solutions in a challenging mathematical context.",
          "citing_paper_id": "267412590",
          "cited_paper_id": 258987659,
          "context_text": "Importantly, Lightman et al. [2023] showed that on a challenging competition-level mathematics problem set Hendrycks et al. [2021], verifiers trained on annotated intermediate solutions (PSV) surpasses verifiers trained on final solutions by a large margin, and both substantially better than…",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'competition-level mathematics problem set' from Hendrycks et al. [2021], which is likely a specific dataset used for evaluating verifiers trained on annotated intermediate solutions.",
          "citing_paper_doi": "10.48550/arXiv.2402.02658",
          "cited_paper_doi": "10.48550/arXiv.2305.20050",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ba1ce9f45b5a84d8c8609c1db23aebc887c0ae4d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "3219429",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Geolife"
      ],
      "dataset_details": [
        {
          "dataset_name": "Geolife",
          "dataset_description": "Used to analyze LLM-Mob’s reasoning ability and interpretability, focusing on user, location, and trajectory data to understand the model's performance and decision-making processes.",
          "citing_paper_id": "261276812",
          "cited_paper_id": 3219429,
          "context_text": "We select and analyse some example results from Geolife (as shown in Table 4) to gain a deeper understanding of LLM-Mob ’s reasoning ability and interpretability.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "Geolife is mentioned as a specific dataset used to analyze LLM-Mob’s reasoning ability and interpretability. The context indicates that it is a collaborative social networking service involving user, location, and trajectory data.",
          "citing_paper_doi": "10.48550/arXiv.2308.15197",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f4eea3bda63af47bd6ddc0b5a34ab04c558f0da2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/27faf0300710a345ff878e3a09f3803e36a0a064",
          "citing_paper_year": 2023,
          "cited_paper_year": 2010
        }
      ]
    },
    {
      "cited_paper_id": "15458100",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CLEVR"
      ],
      "dataset_details": [
        {
          "dataset_name": "CLEVR",
          "dataset_description": "Used to generate language statements for compositional language and elementary visual reasoning, modified for the current research on planning capabilities in LLMs. | Used to inspire a benchmark environment for integrating language understanding and control, focusing on compositional language and elementary visual reasoning.",
          "citing_paper_id": "189998275",
          "cited_paper_id": 15458100,
          "context_text": "This scheme for generating language statements is reminiscent of the CLEVR dataset [25] whose code we drew on and modiﬁed for our use case.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the CLEVR dataset, which is a specific, verifiable dataset used for generating language statements. The dataset is relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2017.215",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c2c8482c713b94073f3d59895b373db4398ddfbb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/03eb382e04cca8cca743f7799070869954f1402a",
          "citing_paper_year": 2019,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "249889477",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "static dataset of 600 three to five block problems"
      ],
      "dataset_details": [
        {
          "dataset_name": "static dataset of 600 three to five block problems",
          "dataset_description": "Used to evaluate large language models on planning and reasoning about change, focusing on a specific subset of block manipulation problems.",
          "citing_paper_id": "272770270",
          "cited_paper_id": 249889477,
          "context_text": "3 Since PlanBench [1] first debuted on arXiv in 2022, we have continually retested new models on one particular subset: a static dataset of 600 three to five block problems.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a 'static dataset' but does not provide a specific name. The dataset is described as containing 600 three to five block problems, which is domain-qualified but lacks a specific identifier.",
          "citing_paper_doi": "10.48550/arXiv.2409.13373",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5329cea2b868ce408163420e6af7e9bd00a1940c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "1026139",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Pororo-SV"
      ],
      "dataset_details": [
        {
          "dataset_name": "Pororo-SV",
          "dataset_description": "Used to generate multi-scene videos from coreference-based scene prompts, enhancing narrative coherence in video generation. | Used to generate multi-scene videos from lists of sentences describing events, focusing on event captioning and scene generation. | Used to generate multi-scene videos from coreference-based scene video prompts, emphasizing coherent scene transitions and narrative structure. | Used to generate multi-scene videos from lists of sentences describing events, focusing on coreference resolution and scene generation.",
          "citing_paper_id": "262825203",
          "cited_paper_id": 1026139,
          "context_text": "For multi-scene video generation, we experiment with two types of input prompts: (1) a list of sentences describing events – ActivityNet Captions (Krishna et al., 2017) and Coref-SV prompts based on Pororo-SV (Li et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ActivityNet Captions' and 'Coref-SV prompts based on Pororo-SV'. Both are specific datasets used for generating multi-scene video content.",
          "citing_paper_doi": "10.48550/arXiv.2309.15091",
          "cited_paper_doi": "10.1109/ICCV.2017.83",
          "citing_paper_url": "https://www.semanticscholar.org/paper/16753e0317730e8c1b297338300a8c6163dd06f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "198489709",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MSR-VTT"
      ],
      "dataset_details": [
        {
          "dataset_name": "MSR-VTT",
          "dataset_description": "Used to evaluate video generation quality and video-text alignment, focusing on the visual quality and alignment scores in the context of planning capabilities of LLMs. | Used to evaluate the performance of GPT models on video captioning tasks, specifically measuring FID, FVD, and CLIPSIM metrics to assess in-context learning skills and layout generation quality.",
          "citing_paper_id": "262825203",
          "cited_paper_id": 198489709,
          "context_text": "In addition to the visual quality (FID/FVD) and video-text alignment (CLIPSIM) scores on MSR-VTT and movement direction accuracy score on ActionBench-Directions, we also report the number of samples whose output layouts can be successfully parsed ( e.g ., #Samples).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions MSR-VTT and ActionBench-Directions, which are specific datasets used for evaluating video generation and action recognition tasks. However, ActionBench-Directions is a benchmark rather than a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2309.15091",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/16753e0317730e8c1b297338300a8c6163dd06f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d4dbdeb772105a7ee780543483c6142743b20298",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "206594535",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ActionBench-Direction"
      ],
      "dataset_details": [
        {
          "dataset_name": "ActionBench-Direction",
          "dataset_description": "Utilized to evaluate open-domain video generation, providing a wide range of action categories for comprehensive assessment. | Used to assess object dynamics through skill-based prompts, focusing on directional actions in videos. | Adapted to create ActionBench-Direction prompts, enhancing the evaluation of object dynamics in video sequences. | Employed to examine open-domain video generation, bridging video and language by generating captions for diverse video content.",
          "citing_paper_id": "262825203",
          "cited_paper_id": 206594535,
          "context_text": "…layout control via VPEval Skill-based prompts (Cho et al., 2023b), (2) assess object dynamics through ActionBench-Direction prompts adapted from ActionBench-SSV2 (Wang et al., 2023c), and (3) examine open-domain video generation using MSR-VTT and UCF-101 (Xu et al., 2016; Soomro et al., 2012).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating different aspects of LLM capabilities, including layout control, object dynamics, and open-domain video generation.",
          "citing_paper_doi": "10.48550/arXiv.2309.15091",
          "cited_paper_doi": "10.1109/CVPR.2016.571",
          "citing_paper_url": "https://www.semanticscholar.org/paper/16753e0317730e8c1b297338300a8c6163dd06f2",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "28328610",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AI2-THOR"
      ],
      "dataset_details": [
        {
          "dataset_name": "AI2-THOR",
          "dataset_description": "Used to evaluate multi-agent task planning systems, covering a range of tasks from simple to complex in a 3D simulation environment, focusing on the planning capabilities of LLMs.",
          "citing_paper_id": "262055166",
          "cited_paper_id": 28328610,
          "context_text": "…decomposition, coalition formation, and skill-based task assignment, by leveraging LLMs. • Benchmark Dataset: A benchmark dataset designed for evaluating multi-agent task planning systems, covering a spectrum of tasks, ranging from elemental to complex ones in the AI2-THOR [8] simulation platform.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a 'benchmark dataset' designed for evaluating multi-agent task planning systems, which is relevant to the topic of planning capabilities of LLMs. AI2-THOR is referenced as the simulation platform used.",
          "citing_paper_doi": "10.1109/IROS58592.2024.10802322",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/755853c6b30f5a186131e23a63c68a3f2737068e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/89c8aad71433f7638d2e2c009e1ea20e039f832d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "265050668",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AVALONBENCH"
      ],
      "dataset_details": [
        {
          "dataset_name": "AVALONBENCH",
          "dataset_description": "Used to develop and evaluate advanced LLMs and multi-agent frameworks in the context of playing Resistance Avalon, focusing on strategic decision-making and interaction.",
          "citing_paper_id": "267412980",
          "cited_paper_id": 265050668,
          "context_text": "[ Light et al. , 2023b ] explores the potential of LLM agents in playing Resistance Avalon, introducing AVALONBENCH, a comprehensive game environment and benchmark for further developing advanced LLMs and multi-agent frameworks.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "AVALONBENCH is introduced as a game environment and benchmark, which fits the criteria for a dataset or reusable resource. It is specific and relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2402.01680",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8f070e301979732e0dd73f6aa6170309cf73aa7d",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "247519241",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FanLang-9"
      ],
      "dataset_details": [
        {
          "dataset_name": "FanLang-9",
          "dataset_description": "Used to fine-tune the ChatGLM-6B model, focusing on improving action prediction accuracy through a set of 100 test games.",
          "citing_paper_id": "267413027",
          "cited_paper_id": 247519241,
          "context_text": "We fine-tune the ChatGLM-6B (Du et al., 2021) model using data from the FanLang-9 dataset and then tested the action prediction accuracy on a set of 100 test games.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'FanLang-9' as a specific dataset used for fine-tuning the ChatGLM-6B model. The dataset is clearly identified and used for a specific purpose in the research.",
          "citing_paper_doi": "10.48550/arXiv.2402.02330",
          "cited_paper_doi": "10.18653/v1/2022.acl-long.26",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/50796b0f3edf9cb5ff1e447c298b33755378aa4f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "257050264",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "48 game logs"
      ],
      "dataset_details": [
        {
          "dataset_name": "48 game logs",
          "dataset_description": "Used to fine-tune a RoBERTa-like pretrained model for constructing a value network in the Werewolf game, focusing on language understanding and action prediction.",
          "citing_paper_id": "267413027",
          "cited_paper_id": 257050264,
          "context_text": "Deep Wolf (Shibata et al., 2023) fine-tunes a RoBERTa-like pretrained model with 48 game logs to con-1 struct a value network given the current game state, human speeches, and candidate actions.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions '48 game logs' which is a specific, domain-qualified dataset used for training a value network. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2402.02330",
          "cited_paper_doi": "10.48550/arXiv.2302.10646",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/04a343ffba0120ac02e3138ffd6d7b85f753846b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "258959321",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "contrastive trajectory dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "contrastive trajectory dataset",
          "dataset_description": "Used to compute the outcome-DPO loss and impart process-level supervision in training agents to learn from incorrect trajectories.",
          "citing_paper_id": "270558898",
          "cited_paper_id": 258959321,
          "context_text": "Initially, to facilitate agent’s learning from incorrect trajectories, we compute the outcome-DPO loss using the contrastive trajectory dataset: ) , (7) Next, the step-DPO loss imparts process-level supervision.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'contrastive trajectory dataset' which seems specific but lacks a clear identifier or name. It is used for computing the outcome-DPO loss and imparting process-level supervision.",
          "citing_paper_doi": "10.48550/arXiv.2406.11176",
          "cited_paper_doi": "10.48550/arXiv.2305.18290",
          "citing_paper_url": "https://www.semanticscholar.org/paper/394e14ae60bae4c41162056717d9e30a8168abaa",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "270823634",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "agent trajectory data"
      ],
      "dataset_details": [
        {
          "dataset_name": "agent trajectory data",
          "dataset_description": "Used to train and fine-tune open-source LLMs for specific agent abilities, such as reasoning, by constructing trajectories from teacher agents like GPT-4.",
          "citing_paper_id": "270558898",
          "cited_paper_id": 270823634,
          "context_text": "Chen et al. (2023) and Yin et al. (2023) construct agent trajectory data from teacher agents (e.g., GPT-4) and fine-tune open-source LLMs for specific agent abilities, such as reasoning.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'agent trajectory data' which is constructed from teacher agents like GPT-4. This appears to be a specific dataset used for training and fine-tuning LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2406.11176",
          "cited_paper_doi": "10.48550/arXiv.2311.05657",
          "citing_paper_url": "https://www.semanticscholar.org/paper/394e14ae60bae4c41162056717d9e30a8168abaa",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "102352148",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CommonGen"
      ],
      "dataset_details": [
        {
          "dataset_name": "CommonGen",
          "dataset_description": "Used to evaluate CIDEr and SPICE metrics in generating natural scenarios, focusing on common sense reasoning and contextualized outputs.",
          "citing_paper_id": "251979509",
          "cited_paper_id": 102352148,
          "context_text": "The ﬁrst two metrics are CIDEr (Vedantam, Zitnick, and Parikh 2015) and SPICE (Anderson et al. 2016), which are both widely used for tasks where the outputs are highly contextualized and describe natural scenarios in everyday life, e.g., VaTex (Wang et al. 2019) and CommonGen (Lin et al. 2020).",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions VaTex and CommonGen as datasets used for evaluating metrics in tasks involving contextualized outputs and natural scenarios. VaTex is confirmed as a dataset through the cited paper title.",
          "citing_paper_doi": "10.1609/aaai.v37i11.26549",
          "cited_paper_doi": "10.1109/ICCV.2019.00468",
          "citing_paper_url": "https://www.semanticscholar.org/paper/59b71e2a248d67a2692bc7e35faa504ee2dbc98d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "233219849",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Mind2Web"
      ],
      "dataset_details": [
        {
          "dataset_name": "Mind2Web",
          "dataset_description": "Used to build the MT-Mind2Web dataset, contributing to the development of conversational agents for task-oriented dialogues. | Used to build the MT-Mind2Web dataset, providing a foundation for web interaction and navigation tasks.",
          "citing_paper_id": "267897510",
          "cited_paper_id": 233219849,
          "context_text": "…existing conversation datasets, such as Hy-briDialogue (Nakamura et al., 2022) from OTT-QA (Chen et al., 2021), MMCoQA (Li et al., 2022) from MMQA (Talmor et al., 2021), and PACIFIC (Deng et al., 2022) from TAT-QA (Zhu et al., 2021), we build the MT-Mind2Web dataset from the Mind2Web dataset…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets by name, all of which are specific and verifiable. These datasets are used to build a new dataset, MT-Mind2Web, for the research on planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2402.15057",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e6100eb9dc6667b24c26fcea683c139451e89652",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d365978adf0a5c9c6028820857e015617856256b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "235593036",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WOMD"
      ],
      "dataset_details": [
        {
          "dataset_name": "WOMD",
          "dataset_description": "Used for training and testing models in planning tasks, providing a comprehensive evaluation benchmark for autonomous vehicle planning capabilities. | Used for training and testing models in planning tasks, focusing on selected interactive scenarios to evaluate model performance.",
          "citing_paper_id": "257482793",
          "cited_paper_id": 235593036,
          "context_text": "For the planning tasks, we train and test the models on both WOMD with selected interactive scenarios and the nuPlan dataset [2] with a comprehensive evaluation benchmark.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets: WOMD and nuPlan. Both are used for training and testing models in planning tasks, with nuPlan providing a comprehensive evaluation benchmark.",
          "citing_paper_doi": "10.1109/ICCV51070.2023.00361",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4c667a69a3d788e4ddbaf900dd36b78d845fd287",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b88b38ec61a4881173ab94647d1e97500f4af15b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "255825985",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Wikitext"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikitext",
          "dataset_description": "Used to collect covariance statistics for optimizing factual associations in GPT, focusing on the impact of sample size and precision on model performance.",
          "citing_paper_id": "252873467",
          "cited_paper_id": 255825985,
          "context_text": "Similar to ROME, covariance statistics are collected using 100,000 samples of Wikitext in fp32 . δ i optimization proceeds for 25 steps with a learning rate of 5 × 10 − 1 .",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Wikitext' as a source of samples, which is a plausible dataset name. However, it does not specify a particular version or release, making it less specific than preferred.",
          "citing_paper_doi": "10.48550/arXiv.2210.07229",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413",
          "cited_paper_url": "https://www.semanticscholar.org/paper/996445d847f06e99b0bd259345408a0cf1bce87e",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "173188048",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MathQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MathQA",
          "dataset_description": "Used to evaluate the accuracy of LLMs in solving math word problems, focusing on operation-based formalisms to enhance interpretability.",
          "citing_paper_id": "258179336",
          "cited_paper_id": 173188048,
          "context_text": ", 2020) and MathQA (Amini et al., 2019) and choose accuracy as the metric.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MathQA' which is a specific dataset used for evaluating math word problem solving. The dataset is used to measure accuracy, indicating its relevance to the research on planning capabilities of LLMs.",
          "citing_paper_doi": "10.1145/3704435",
          "cited_paper_doi": "10.18653/v1/N19-1245",
          "citing_paper_url": "https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eef7cfe8267954adbb4675576072a1d80ca7a3a8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MixATIS"
      ],
      "dataset_details": [
        {
          "dataset_name": "MixATIS",
          "dataset_description": "Used to measure the generalizability of API models, focusing on out-of-distribution performance in a tool-related context. | Used to evaluate the out-of-distribution performance of API models, specifically in a sequential tool-related question-answering context. | Used to build and evaluate multi-intent models, focusing on extending the capabilities of the original SNIPS dataset for more complex natural language processing tasks. | Used to assess the generalizability of API models, focusing on out-of-distribution performance in a tool-related context. | Used to build and evaluate multi-intent models, focusing on extending the capabilities of the original ATIS dataset for more complex natural language processing tasks. | Used to evaluate the generalizability of API models, focusing on out-of-distribution performance in a tool-related context.",
          "citing_paper_id": "267897727",
          "cited_paper_id": null,
          "context_text": "MixATIS and MixSNIPS are multi-intent datasets (Qin et al., 2020) built based on ATIS and SNIPS, respectively.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MixATIS' and 'MixSNIPS' as multi-intent datasets, which are specific and plausible dataset names. These datasets are derived from ATIS and SNIPS, which are well-known datasets in the field of spoken language understanding.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "258588247",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ToolLLM"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolLLM",
          "dataset_description": "Used to evaluate the performance of FLAN-T5-XXL and MPT-30B, focusing on tool-related tasks and comparing their effectiveness. | Used to evaluate the performance of Falcon-40B and StarCoder-15B, focusing on API-based tasks and comparing against other models. | Used to assess the performance of FLAN-T5-XXL and MPT-30B, specifically for API-related tasks and benchmarking.",
          "citing_paper_id": "267897727",
          "cited_paper_id": 258588247,
          "context_text": "Falcon-40B and StarCoder-15B are showing better performance on our API-only test-set ToolLLM (we did not evaluate FLAN-T5-XXL on ToolLLM due to max sequence limit), whereas FLAN-T5-XXL and MPT-30B are doing well on API-Bank and ToolAlpaca.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'ToolLLM', 'API-Bank', and 'ToolAlpaca' as specific datasets used for evaluating the performance of different language models. These names are specific and appear to be datasets used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "27300853",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Kinetics Human Action Video Dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Kinetics Human Action Video Dataset",
          "dataset_description": "Used to train and evaluate models on recognizing human actions in videos, focusing on the planning capabilities of large language models in understanding and predicting complex human behaviors.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 27300853,
          "context_text": "[55] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a video dataset focused on human actions, which is relevant for training and evaluating models on action recognition tasks, including those involving planning capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "28695052",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "M INE CLIP"
      ],
      "dataset_details": [
        {
          "dataset_name": "M INE CLIP",
          "dataset_description": "Used to train the policy with Proximal Policy Optimization (PPO) algorithms, focusing on reward shaping for planning capabilities in large language models.",
          "citing_paper_id": "249848263",
          "cited_paper_id": 28695052,
          "context_text": "The policy is trained with PPO [102] on the M INE CLIP rewards.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'M INE CLIP rewards' which appears to be a specific dataset or reward system used for training the policy. However, without additional context, it is unclear if this is a verifiable dataset or a method. The name does not follow typical dataset naming conventions.",
          "citing_paper_doi": "10.48550/arXiv.2206.08853",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b",
          "citing_paper_year": 2022,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "260334759",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Go-rilla"
      ],
      "dataset_details": [
        {
          "dataset_name": "Go-rilla",
          "dataset_description": "Utilizes synthetic, single-sequence API data, specifically from Deep Learning libraries' APIs, generated from language models. Used to compare with multi-sequence REST-API data from GPT-4.",
          "citing_paper_id": "267897727",
          "cited_paper_id": 260334759,
          "context_text": "For instance, ToolLLM (Qin et al., 2023) produces multi-sequence REST-API data sourced from GPT-4 (Achiam et al., 2023), while datasets like Go-rilla (Patil et al., 2023) utilize synthetic, single-sequence API data, specifically Deep Learning li-braries’ APIs, generated from language models.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Go-rilla' as a dataset utilizing synthetic, single-sequence API data. However, it does not provide a clear, specific usage description or research context for this dataset.",
          "citing_paper_doi": "10.48550/arXiv.2402.15491",
          "cited_paper_doi": "10.48550/arXiv.2307.16789",
          "citing_paper_url": "https://www.semanticscholar.org/paper/45ea1b47195d3d381e4b08f8cc0be3568c780ea9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "254853659",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Unnatural Instructions"
      ],
      "dataset_details": [
        {
          "dataset_name": "Unnatural Instructions",
          "dataset_description": "Used to decode GPT-4 answers, focusing on unnatural instructions to evaluate model performance and behavior under non-standard conditions. | Used to evaluate the effectiveness of synthetic instructions generated by text-davinci-002, focusing on the quality and diversity of 68,478 synthesized samples using 3-shot in-context-learning.",
          "citing_paper_id": "257985497",
          "cited_paper_id": 254853659,
          "context_text": "• Unnatural Instructions4 (Honovich et al., 2022) is a dataset of 68,478 samples synthesized by text-davinci-002 using 3-shot in-context-learning from 15 manually-constructed examples.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation clearly identifies 'Unnatural Instructions' as a dataset with specific characteristics and usage.",
          "citing_paper_doi": "10.48550/arXiv.2304.03277",
          "cited_paper_doi": "10.48550/arXiv.2212.09689",
          "citing_paper_url": "https://www.semanticscholar.org/paper/9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6f4cc536f9ed83d0dbf7e919dc609be12aa0848a",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "53046555",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "wikiHow"
      ],
      "dataset_details": [
        {
          "dataset_name": "wikiHow",
          "dataset_description": "Used as the initial dataset for providing examples of instructional articles on various topics, focusing on the content and structure of how-to guides.",
          "citing_paper_id": "258564677",
          "cited_paper_id": 53046555,
          "context_text": "Data Source for Examples We adopt wikiHow (Koupaee and Wang, 2018), a data source of instructional articles on various topics, as the initial dataset for providing examples.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'wikiHow' as a data source of instructional articles, which is a specific, named dataset used for providing examples.",
          "citing_paper_doi": "10.48550/arXiv.2305.05252",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f06c38a0fd49dd1468e72696913169c0d5588fc3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c0199a7a37c22797c899571e51dba9690e606fa2",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "91184540",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Matterport3D"
      ],
      "dataset_details": [
        {
          "dataset_name": "Matterport3D",
          "dataset_description": "Used to evaluate multi-object and spatial goal navigation tasks in the Habitat simulator, focusing on embodied AI research methodologies.",
          "citing_paper_id": "252846548",
          "cited_paper_id": 91184540,
          "context_text": "We use the Habitat simulator [45] with the Matterport3D dataset [46] for the evaluation of multi-object and spatial goal navigation tasks.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of the Matterport3D dataset for evaluating navigation tasks in the Habitat simulator. The dataset is clearly identified and used for a specific research purpose.",
          "citing_paper_doi": "10.1109/ICRA48891.2023.10160969",
          "cited_paper_doi": "10.1109/ICCV.2019.00943",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c305ab1bdba79442bec72ec7f5c5ee7c49c2a566",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4a35e548de27b6924e5f2ee41d37238a5c4a1d5",
          "citing_paper_year": 2022,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "237940946",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "gSCAN test dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "gSCAN test dataset",
          "dataset_description": "Used to evaluate systematic generalization capabilities of models across eight different splits, focusing on language and vision tasks.",
          "citing_paper_id": "259859069",
          "cited_paper_id": 237940946,
          "context_text": "(Qiu et al., 2021), on the gSCAN test dataset in eight splits.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'gSCAN test dataset' which is a specific dataset used for evaluating systematic generalization in language and vision tasks.",
          "citing_paper_doi": "10.48550/arXiv.2307.07696",
          "cited_paper_doi": "10.18653/v1/2021.emnlp-main.166",
          "citing_paper_url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d41417f22f898125c4d34f672938ebb2a3764961",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "248228026",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "StepGame"
      ],
      "dataset_details": [
        {
          "dataset_name": "StepGame",
          "dataset_description": "Used to evaluate robust multi-hop spatial reasoning in texts, focusing on the planning capabilities of language models through a series of complex reasoning tasks. | Used for pretraining SynSup, focusing on robust multi-hop spatial reasoning in texts, enhancing the model's ability to understand complex spatial relationships.",
          "citing_paper_id": "259859069",
          "cited_paper_id": 248228026,
          "context_text": "To address the issues, Shi et al. (2022) propose the StepGame dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the StepGame dataset, which is proposed in the cited paper. The dataset is clearly identified and relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2307.07696",
          "cited_paper_doi": "10.48550/arXiv.2204.08292",
          "citing_paper_url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2a1acf6dce142d9a87d353c7edbda0127dbe69d7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "5647908",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OlympiadBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "OlympiadBench",
          "dataset_description": "Used to evaluate model performance on complex problem-solving tasks, focusing on logical reasoning and mathematical skills. | Used to assess the reasoning capabilities of LLMs, specifically examining the consistency of correct answers across training checkpoints. | Used to evaluate the performance of LLMs on mathematical problems, focusing on the accuracy of solutions at different checkpoints. | Used to test general problem-solving abilities, including logical reasoning and quantitative analysis, across various domains.",
          "citing_paper_id": "278911988",
          "cited_paper_id": 5647908,
          "context_text": "These evaluations were conducted on the OlympiadBench [10], MATH-500 [12], and GPQA [33] benchmarks.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific benchmarks that are used for evaluation, which are likely to be datasets or collections of problems used to assess the performance of models.",
          "citing_paper_doi": "10.48550/arXiv.2505.20196",
          "cited_paper_doi": "10.1007/978-0-387-39940-9_2108",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5bd465c1d9f74d4769d5a1887a13f84b8c7d1a73",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d97951108dc85389aedda2395467f218f624f70c",
          "citing_paper_year": 2025,
          "cited_paper_year": 1988
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DeepscaleR-40k"
      ],
      "dataset_details": [
        {
          "dataset_name": "DeepscaleR-40k",
          "dataset_description": "Used to train a model with 4k randomly selected samples, focusing on the planning capabilities of LLMs. | Used to fine-tune the Qwen-7B-Base model, focusing on improving planning capabilities through specific training settings.",
          "citing_paper_id": "278911988",
          "cited_paper_id": null,
          "context_text": "The training data consisted of 4k samples randomly selected from the DeepscaleR-40k dataset [24].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset, 'DeepscaleR-40k', which is used for training a model with 4k samples.",
          "citing_paper_doi": "10.48550/arXiv.2505.20196",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5bd465c1d9f74d4769d5a1887a13f84b8c7d1a73",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "51876975",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Wikipedia text"
      ],
      "dataset_details": [
        {
          "dataset_name": "Wikipedia text",
          "dataset_description": "Used in a mixture for training or evaluation, contributing 0.5% of the data, likely for language modeling and text generation tasks. | Used in a mixture for training or evaluation, contributing 3.1% of the data, likely for robotic manipulation tasks.",
          "citing_paper_id": "257364842",
          "cited_paper_id": 51876975,
          "context_text": "Dataset in full mixture Sampling frequency % Webli (Chen et al., 2022) 100 52.4 VQ 2 A (Changpinyo et al., 2022) 25 13.1 VQG (Changpinyo et al., 2022) 10 5.2 CC3M (Sharma et al., 2018) 25 13.1 Object Aware (Piergiovanni et al., 2022) 10 5.2 OK-VQA (Marino et al., 2019) 1 0.5 VQAv2 (Goyal et al., 2017) 1 0.5 COCO (Chen et al., 2015) 1 0.5 Wikipedia text 1 0.5 (robot) Mobile Manipulator, real 6 3.1 (robot) Language Table (Lynch et al., 2022 In Tab.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context lists several datasets with their sampling frequencies. These datasets are used in a mixture for training or evaluation, likely in a multimodal setting involving vision and language tasks.",
          "citing_paper_doi": "10.48550/arXiv.2303.03378",
          "cited_paper_doi": "10.18653/v1/P18-1238",
          "citing_paper_url": "https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "14113767",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "GQA",
          "dataset_description": "Used to train DriveLM-Agent, providing questions and answers about images to improve the model's ability to reason about visual content.",
          "citing_paper_id": "266435584",
          "cited_paper_id": 14113767,
          "context_text": "Taking advantage of the more general architecture of a VLM, we include samples from COCO [44] and GQA [28] along with DriveLM-CARLA during training for DriveLM-Agent.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the use of COCO and GQA datasets alongside DriveLM-CARLA for training DriveLM-Agent. COCO is a well-known dataset, while GQA is also a specific dataset.",
          "citing_paper_doi": "10.48550/arXiv.2312.14150",
          "cited_paper_doi": "10.1007/978-3-319-10602-1_48",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
          "citing_paper_year": 2023,
          "cited_paper_year": 2014
        }
      ]
    },
    {
      "cited_paper_id": "256390509",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DriveLM-Data"
      ],
      "dataset_details": [
        {
          "dataset_name": "DriveLM-Data",
          "dataset_description": "Used to fine-tune BLIP-2 for the motion task, specifically applying the trajectory tokenization scheme described in the paper.",
          "citing_paper_id": "266435584",
          "cited_paper_id": 256390509,
          "context_text": "…check-point, which requires video inputs, we train a single-frame version (‘ UniAD-Single ’) for a fair comparison to our single-frame VLMs. Finally, BLIP-RT-2 denotes BLIP-2 [40] fine-tuned on DriveLM-Data with the trajectory tok-enization scheme described in Section 3.3 for only the motion task.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DriveLM-Data' as a dataset used for fine-tuning BLIP-2. No other datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2312.14150",
          "cited_paper_doi": "10.48550/arXiv.2301.12597",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "264600413",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OpenLane-V2"
      ],
      "dataset_details": [
        {
          "dataset_name": "OpenLane-V2",
          "dataset_description": "Used to generate a portion of the Perception QAs, providing ground truth data for topology reasoning in unified 3D HD mapping. | Used to generate questions about observational facets of objects within a scene, leveraging ground truth annotations for planning capabilities of LLMs. | Used to generate questions about observational facets of objects within a scene, focusing on topology reasoning and unified 3D HD mapping for LLM planning.",
          "citing_paper_id": "266435584",
          "cited_paper_id": 264600413,
          "context_text": "A portion of the Perception QAs are generated from the nuScenes [5] and OpenLane-V2 [70] ground truth, while the remaining QAs are manually annotated.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, nuScenes and OpenLane-V2, which are used to generate Perception QAs. The datasets are clearly identified and their usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2312.14150",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bc8bbd71148f7f170bbe52e955f4babc9324e61b",
          "citing_paper_year": 2023,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "2454882",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DealOrNotDeal"
      ],
      "dataset_details": [
        {
          "dataset_name": "DealOrNotDeal",
          "dataset_description": "Used to implement bargaining over multiple issues, focusing on end-to-end learning of negotiation dialogues.",
          "citing_paper_id": "270357425",
          "cited_paper_id": 2454882,
          "context_text": "Bargaining: DealOrNotDeal We use DealOrNotDeal [39] to implement the bargaining over multiple issues.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DealOrNotDeal' as a resource used for implementing bargaining over multiple issues. The cited paper title confirms it is a dataset used for negotiation dialogues.",
          "citing_paper_doi": "10.48550/arXiv.2406.04784",
          "cited_paper_doi": "10.18653/v1/D17-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/09a6e95e87e569dca187f86ce3b155c8be145a9e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6a8dbea5e40831bd6e987c03b76487f45ac49599",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "268532288",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GAMA-Bench"
      ],
      "dataset_details": [
        {
          "dataset_name": "GAMA-Bench",
          "dataset_description": "Used as the implemented environment for the Public Goods Game to evaluate LLMs' decision-making and gaming abilities in multi-agent settings.",
          "citing_paper_id": "270357425",
          "cited_paper_id": 268532288,
          "context_text": "Public Goods Game: GAMA-Bench We use GAMA-Bench [37] as the implemented environment for this game.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "GAMA-Bench is mentioned as the implemented environment for the Public Goods Game, which is relevant to evaluating LLMs' gaming ability in multi-agent environments.",
          "citing_paper_doi": "10.48550/arXiv.2406.04784",
          "cited_paper_doi": "10.48550/arXiv.2403.11807",
          "citing_paper_url": "https://www.semanticscholar.org/paper/09a6e95e87e569dca187f86ce3b155c8be145a9e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f8359e7d74b2be343379472be3d2b452fcfa4801",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "FOLIO"
      ],
      "dataset_details": [
        {
          "dataset_name": "FOLIO",
          "dataset_description": "Utilizes first-order logic inference problems to test LLMs' capacity for complex logical reasoning, including quantifiers and logical connectives. | Applies a set of progressively challenging questions to measure LLMs' reasoning skills, particularly in handling multi-step logical deductions. | Tests LLMs' ability to perform logical deduction through a series of problems designed to challenge their reasoning mechanisms. | Used to assess LLMs' logical reasoning capabilities through multiple-choice questions, focusing on deductive, inductive, and abductive reasoning. | Employs a dataset of formal proofs to evaluate LLMs' ability to generate and understand logical arguments, emphasizing structured reasoning. | Used to train and evaluate natural language reasoning models, focusing on logical complexity and first-order logic annotations to enhance planning capabilities.",
          "citing_paper_id": "264590280",
          "cited_paper_id": null,
          "context_text": "FOLIO (Han et al., 2022) is a human-annotated and logically complex datasets for natu-ral language reasoning, equipped with first-order logic (FOL) annotations.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "FOLIO is identified as a specific, human-annotated dataset with logical complexity and FOL annotations, which is relevant to the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.18653/v1/2024.acl-long.531",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/170c5b7e311a5004ed5db5d9eee1b736669273fb",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "TP-RAG"
      ],
      "dataset_details": [
        {
          "dataset_name": "TP-RAG",
          "dataset_description": "Used to create a travel planning benchmark for retrieval-augmented and spatiotemporal-aware travel planning, constructed using around 1 billion GPT-4 tokens.",
          "citing_paper_id": "277740771",
          "cited_paper_id": null,
          "context_text": "Our main contributions are three-fold: (1) TP-RAG , the first travel planning benchmark for retrieval-augmented and spatiotemporal-aware travel planning, using around 1 billion GPT-4o to-kens for dataset construction.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a new benchmark called TP-RAG, which is a travel planning benchmark. However, it does not mention any existing datasets, only the creation of a new one.",
          "citing_paper_doi": "10.48550/arXiv.2504.08694",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/2a919d50d2f757269615646ae40d4f2d33a273e8",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "49317780",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ActivityPrograms knowledge base"
      ],
      "dataset_details": [
        {
          "dataset_name": "ActivityPrograms knowledge base",
          "dataset_description": "Used to provide executable plans for household tasks in the VirtualHome simulation platform, focusing on the planning capabilities of the system.",
          "citing_paper_id": "258823133",
          "cited_paper_id": 49317780,
          "context_text": "VirtualHome [49] is a simulation platform for typical household activities, and ActivityPrograms knowledge base [49] consists of many tasks with plans executable in VirtualHome.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ActivityPrograms knowledge base' which is a specific, named resource used in conjunction with the VirtualHome simulation platform. It is used to provide executable plans for tasks.",
          "citing_paper_doi": "10.48550/arXiv.2305.11554",
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "253571680",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "KAMEL"
      ],
      "dataset_details": [
        {
          "dataset_name": "KAMEL",
          "dataset_description": "Used to analyze the planning capabilities of language models by leveraging structured knowledge from Wikidata, focusing on relation-based question templates.",
          "citing_paper_id": "258823133",
          "cited_paper_id": 253571680,
          "context_text": "KAMEL contains knowledge about 243 relations from Wikidata, each of which is associated with a question template (e.g. winner_of : \" Who is the winner of [S]?",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "KAMEL is a knowledge base, not a traditional dataset, but it is used in the context of language models and contains structured information that could be relevant to planning capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2305.11554",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ceab663aada792b409891c96847ee6e8e18998ed",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "54137939",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Abstraction and Reasoning Corpus (ARC)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Abstraction and Reasoning Corpus (ARC)",
          "dataset_description": "Used to evaluate methods in few-shot visual reasoning, focusing on the ability to solve complex, abstract problems with minimal examples.",
          "citing_paper_id": "273970146",
          "cited_paper_id": 54137939,
          "context_text": "We evaluate these methods in the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), a collection of extremely challenging few-shot visual reasoning problems.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the Abstraction and Reasoning Corpus (ARC) as a specific dataset used for evaluating methods in few-shot visual reasoning problems.",
          "citing_paper_doi": "10.48550/arXiv.2411.07279",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d3a2f01e393cef87a5a9ca7cb3d48ea80a53360b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f48eed915cbb9c6592cdb9df80c1edaeb46959af",
          "citing_paper_year": 2024,
          "cited_paper_year": 2012
        }
      ]
    },
    {
      "cited_paper_id": "235755472",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NaturalCodeBench (NCB)"
      ],
      "dataset_details": [
        {
          "dataset_name": "NaturalCodeBench (NCB)",
          "dataset_description": "Used to measure models' capacities to solve practical programming tasks, focusing on real-world coding challenges. | Used to evaluate programming problems in languages beyond Python, assessing the model's ability to generate correct code solutions.",
          "citing_paper_id": "270562306",
          "cited_paper_id": 235755472,
          "context_text": "…of LLMs, AlignBench [1] to measure the alignment quality of ChatGLM with Chinese language content, HumanEval-X [58] to evaluate HumanEval [4] problems in programming languages beyond Python, as well as NaturalCodeBench (NCB) to measure models’ capacities to solve practical programming…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several benchmarks and challenges, but only HumanEval-X and NaturalCodeBench (NCB) are specific datasets used for evaluating programming capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2406.12793",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c7f9706898bdfa3241601e075b1305649b174ff1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "265157752",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "IFEval"
      ],
      "dataset_details": [
        {
          "dataset_name": "IFEval",
          "dataset_description": "Used to assess the instruction-following capabilities of GLM-4, focusing on the model's proficiency in executing complex tasks and understanding diverse instructions.",
          "citing_paper_id": "270562306",
          "cited_paper_id": 265157752,
          "context_text": "We assess the proficiency of GLM-4 in following instructions with the recently-introduced IFEval dataset [62].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "IFEval is mentioned as a dataset used to assess the proficiency of GLM-4 in following instructions. The context clearly indicates its use as a reusable resource.",
          "citing_paper_doi": "10.48550/arXiv.2406.12793",
          "cited_paper_doi": "10.48550/arXiv.2311.07911",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c7f9706898bdfa3241601e075b1305649b174ff1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a9b8c545ba9a6779f202e04639c2d67e6d34f63",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "267335080",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "HumanEval-X"
      ],
      "dataset_details": [
        {
          "dataset_name": "HumanEval-X",
          "dataset_description": "Used to evaluate programming problems across multiple languages, focusing on the performance of LLMs in solving coding challenges.",
          "citing_paper_id": "270562306",
          "cited_paper_id": 267335080,
          "context_text": "…including AgentBench [25] for evaluating LLMs as agents, LongBench [2] for evaluating the long context handling performance of LLMs, AlignBench [1] to measure the alignment quality of ChatGLM with Chinese language content, HumanEval-X [58] to evaluate HumanEval [4] problems in programming…",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions several benchmarks, but only HumanEval-X is a specific, downloadable dataset used for evaluating programming problems. Others are excluded as they are primarily benchmark suites or methods.",
          "citing_paper_doi": "10.48550/arXiv.2406.12793",
          "cited_paper_doi": "10.48550/arXiv.2401.18058",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c7f9706898bdfa3241601e075b1305649b174ff1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec9203f6c25a353325dd23ed38e5036b79d9e79b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "195507797",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VirtualHome-Env"
      ],
      "dataset_details": [
        {
          "dataset_name": "VirtualHome-Env",
          "dataset_description": "Used to synthesize environment-aware activities in household settings, focusing on daily activities across 7 scenes gathered through crowdsourcing.",
          "citing_paper_id": "261100610",
          "cited_paper_id": 195507797,
          "context_text": "We utilize the VirtualHome-Env dataset (Liao et al. 2019), comprising daily household activities from 7 scenes gathered via crowdsourcing.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset, 'VirtualHome-Env', which is used for synthesizing environment-aware activities in household settings.",
          "citing_paper_doi": "10.48550/arXiv.2308.12682",
          "cited_paper_doi": "10.1109/CVPR.2019.00645",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7569d62c7651625456a2cbd4922d9d65351593ac",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f68e35cdd571c3cf66f071384725135191ebac4d",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "258987659",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PRM12K"
      ],
      "dataset_details": [
        {
          "dataset_name": "PRM12K",
          "dataset_description": "Used as the training dataset to generate self-training data, focusing on improving the planning capabilities of LLMs through consistent step-by-step verification.",
          "citing_paper_id": "275133600",
          "cited_paper_id": 258987659,
          "context_text": "Consistent with previous studies, we employ the PRM12K dataset (Lightman et al., 2024) as our training dataset to generate self-training data.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the PRM12K dataset, which is a specific, verifiable dataset used for training. The dataset is clearly identified and used in the context of generating self-training data.",
          "citing_paper_doi": "10.48550/arXiv.2412.21187",
          "cited_paper_doi": "10.48550/arXiv.2305.20050",
          "citing_paper_url": "https://www.semanticscholar.org/paper/84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "253237047",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LiLA"
      ],
      "dataset_details": [
        {
          "dataset_name": "LiLA",
          "dataset_description": "Used to unify various mathematical datasets for evaluating and enhancing the planning capabilities of LLMs in mathematical reasoning tasks.",
          "citing_paper_id": "253801709",
          "cited_paper_id": 253237047,
          "context_text": "LiLA (Mishra et al., 2022) proposes to assemble a large set of mathematical datasets into a unified dataset.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions LiLA as a unified dataset, which is relevant to the topic of planning capabilities in LLMs, especially in the domain of mathematical reasoning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2210.17517",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
          "cited_paper_url": "https://www.semanticscholar.org/paper/52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7",
          "citing_paper_year": 2022,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "257532815",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MWP datasets"
      ],
      "dataset_details": [
        {
          "dataset_name": "MWP datasets",
          "dataset_description": "Used to evaluate the performance of PoT+SC on math word problems, focusing on accuracy and effectiveness in solving complex problems.",
          "citing_paper_id": "253801709",
          "cited_paper_id": 257532815,
          "context_text": "Our PoT+SC achieves the best-known results on all the evaluated MWP datasets and near best-known results on the financial datasets (excluding GPT-4 (OpenAI, 2023)).",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MWP datasets' and 'financial datasets', which are domain-qualified data phrases. However, no specific dataset names are provided.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
          "cited_paper_url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
          "citing_paper_year": 2022,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "BIRD"
      ],
      "dataset_details": [
        {
          "dataset_name": "BIRD",
          "dataset_description": "Used to construct instructions for the Database task, focusing on SELECT-only queries to evaluate the planning capabilities of LLMs. | Used to collect interaction trajectories of GPT-4 with the database, focusing on the model's ability to interact with structured data. | Used to generate correct trajectories, focusing on the planning capabilities of LLMs by providing trajectory data for model training and evaluation.",
          "citing_paper_id": "264306101",
          "cited_paper_id": null,
          "context_text": "Thus to construct instructions on the Database (DB) task, we derive instructions from BIRD (Li et al., 2023), a SELECT-only database benchmark.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'BIRD' as a SELECT-only database benchmark, which is a specific dataset used for constructing instructions on the Database task.",
          "citing_paper_doi": "10.48550/arXiv.2310.12823",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/46fe9ce789408b8a50fb4259e6bf0cc5855f4ed5",
          "cited_paper_url": null,
          "citing_paper_year": 2023,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "252780086",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "12K-episode dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "12K-episode dataset",
          "dataset_description": "Used to fine-tune HTML-T5 for web navigation tasks, specifically comparing performance across 56 MiniWoB++ tasks.",
          "citing_paper_id": "260126067",
          "cited_paper_id": 252780086,
          "context_text": "We compare the web navigation performance among 56 MiniWoB++ tasks [24], by finetuning HTML-T5 with public 12K-episode dataset [42].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MiniWoB++ tasks' and a 'public 12K-episode dataset'. MiniWoB++ is a benchmark, but the 12K-episode dataset is a specific dataset used for fine-tuning.",
          "citing_paper_doi": "10.48550/arXiv.2307.12856",
          "cited_paper_doi": "10.48550/arXiv.2210.03945",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a53c8ba374d430d6c3786d13c04edb200d547750",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3ee3f425482cf86989d809155cc8cf2bf8d8113e",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "232134851",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MINT-HotpotQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MINT-HotpotQA",
          "dataset_description": "Used to test the model’s ability to answer complex questions requiring multi-hop reasoning, focusing on natural language understanding.",
          "citing_paper_id": "263831032",
          "cited_paper_id": 232134851,
          "context_text": "This part includes several adapted datasets for testing: MINT-{ GSM8K, MATH } (Cobbe et al., 2021; Hendrycks et al., 2021b) is used to test the model’s ability to solve mathematical problems using a Python interpreter, and MINT-{ HotpotQA, MMLU } (Yang et al., 2018; Hendrycks et al., 2021a)…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions specific datasets used for testing the model's problem-solving capabilities, which are directly relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2310.06830",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8147cec9245d34d13732a08e915c920a1a499bb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "225040341",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ObstructedSuite"
      ],
      "dataset_details": [
        {
          "dataset_name": "ObstructedSuite",
          "dataset_description": "Used to evaluate the agent's planning, movement, and interaction capabilities in environments with obstacles, focusing on reinforcement learning and motion planning.",
          "citing_paper_id": "269502645",
          "cited_paper_id": 225040341,
          "context_text": "ObstructedSuite: Yamada et al. [41] contains tasks that evaluate our agent’s ability to plan, move and interact with the environment in the presence of obstacles.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'tasks' from ObstructedSuite, which appears to be a collection of challenges rather than a traditional dataset. However, it is used to evaluate planning capabilities, making it relevant to the topic.",
          "citing_paper_doi": "10.48550/arXiv.2405.01534",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e3116a5d5784392d9190ca35997f65a252291032",
          "cited_paper_url": "https://www.semanticscholar.org/paper/64d16d0bef64c9a36ef91877c3687e260430534e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "218971783",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GSM240K"
      ],
      "dataset_details": [
        {
          "dataset_name": "GSM240K",
          "dataset_description": "Used to train and improve the performance of the SFT baseline model, focusing on the impact of additional training data on model performance.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 218971783,
          "context_text": "As shown in Table 3, while training with GSM240K improved the performance of the SFT baseline, ReVISE still exhibited better performance.",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'GSM240K' which appears to be a dataset used for training. However, there is no explicit description of how the dataset is used or its specific characteristics.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "237142385",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MBPP"
      ],
      "dataset_details": [
        {
          "dataset_name": "MBPP",
          "dataset_description": "Used to evaluate the effectiveness of ReVISE in enhancing reasoning capabilities in the coding domain, focusing on program synthesis tasks.",
          "citing_paper_id": "276482377",
          "cited_paper_id": 237142385,
          "context_text": "Also, we verify that ReVISE effectively enhances reasoning in the coding domain (i.e., MBPP (Austin et al., 2021)).",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions MBPP, which is a known dataset for program synthesis tasks, but does not provide additional details about its usage.",
          "citing_paper_doi": "10.48550/arXiv.2502.14565",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a0ba3a62fc6761b1c0528b554d9ff335d26893e8",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "204915921",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Blocksworld"
      ],
      "dataset_details": [
        {
          "dataset_name": "Blocksworld",
          "dataset_description": "Used to test question answering through sentence composition, focusing on logical reasoning and comprehension in mathematical and logical problems. | Used to assess planning and problem-solving skills in the Tower of Hanoi puzzle, focusing on recursive algorithms and strategic thinking. | Used to assess the ability to solve complex arithmetic problems, emphasizing step-by-step reasoning and numerical computation. | Used to evaluate planning capabilities in a block manipulation environment, focusing on sequential decision-making and problem-solving strategies. | Used to evaluate question answering via sentence composition, focusing on integrating information from multiple sentences to answer complex questions.",
          "citing_paper_id": "273025556",
          "cited_paper_id": 204915921,
          "context_text": "Dataset: The datasets we used are the open source datasets Blocksworld(Valmeekam et al., 2024), GSM8K(Cobbe et al., 2021a), AQUA(Ling et al., 2017), QASC(Khot et al., 2020), and our own production of Hanoi Tower. where the experiments for AQUA and QASC are in the Table 4.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for experiments, including Blocksworld, GSM8K, AQUA, QASC, and Hanoi Tower. These datasets are clearly identified and used for evaluating planning capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2410.01696",
          "cited_paper_doi": "10.1609/AAAI.V34I05.6319",
          "citing_paper_url": "https://www.semanticscholar.org/paper/39bef6bc1f298eb0a9a46f7a9ce108eda27170e6",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c9b051b29feda7b62a4b683b1dfc37408724d8f5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "246411621",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Letter Concatenation"
      ],
      "dataset_details": [
        {
          "dataset_name": "Letter Concatenation",
          "dataset_description": "Used to evaluate symbolic reasoning capabilities in large language models, focusing on the ability to concatenate letters in a logical sequence. | Used to evaluate the effectiveness of revised examples in improving date understanding tasks, focusing on the reasoning capabilities of large language models.",
          "citing_paper_id": "258297976",
          "cited_paper_id": 246411621,
          "context_text": ", 2022); (3) one symbolic reasoning datasets: Letter Concatenation (Wei et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Letter Concatenation' as a symbolic reasoning dataset, which is specific and relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2304.11657",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "230799347",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Musique"
      ],
      "dataset_details": [
        {
          "dataset_name": "Musique",
          "dataset_description": "Used to assess models' ability to answer strategy-based questions with supporting facts and Wikipedia paragraph indices, focusing on logical reasoning. | Used to evaluate models on questions requiring implicit reasoning strategies, focusing on the ability to infer and apply logical steps not explicitly stated in the data. | Used to demonstrate substantial improvement in performance, specifically addressing implicit reasoning strategies in question answering. | Used to evaluate LUMOS agents on a previously unseen dataset, focusing on multi-hop reasoning and complex question answering. | Used to assess the model's ability to answer complex questions on 1000 randomly selected examples, emphasizing multi-hop reasoning. | Used to train models on solving math problems with natural language solution steps and formulas, focusing on step-by-step reasoning processes. | Used to evaluate LUMOS's performance in complex question answering tasks requiring implicit reasoning strategies. | Used to evaluate complex question answering with decomposed questions and supporting facts, focusing on multi-step reasoning and evidence integration. | Used to evaluate the model's reasoning strategies on 300 randomly selected examples, focusing on implicit reasoning capabilities. | Used to evaluate LUMOS's performance in solving grade school math word problems, emphasizing step-by-step reasoning. | Applied to assess the ability to answer strategic questions using decomposed queries and relevant Wikipedia paragraphs, emphasizing implicit reasoning strategies. | Used to test models on web interaction tasks with ground-truth action sequences, focusing on the execution of complex web-based actions. | Used to assess LUMOS's ability to handle multi-hop reasoning in complex question answering scenarios. | Used to test LUMOS's capabilities in web navigation and interaction tasks, focusing on complex user interactions. | Used to evaluate models on complex question answering tasks with decomposed questions and supporting facts, emphasizing multi-hop reasoning. | Used to assess LUMOS's ability to solve simple math word problems, focusing on arithmetic and algebraic reasoning.",
          "citing_paper_id": "270823634",
          "cited_paper_id": 230799347,
          "context_text": "…maths dataset containing the solution steps written in natural language, inter-leaved with formulas; Musique (Trivedi et al., 2022) and StrategyQA (Geva et al., 2021) are complex QA datasets annotated with decomposed questions, supporting facts, and relevant Wikipedia para-graph indices; Mind2Web…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for complex question answering and reasoning tasks, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2311.05657",
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "236771976",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PRM800K"
      ],
      "dataset_details": [
        {
          "dataset_name": "PRM800K",
          "dataset_description": "Used to assess models' ability to answer strategy-based questions with supporting facts and Wikipedia paragraph indices, focusing on logical reasoning. | Used to train models on solving math problems with natural language solution steps and formulas, focusing on step-by-step reasoning processes. | Used to train and evaluate LLMs on solving math problems, focusing on natural language solution steps and formulas. | Applied to assess LLMs' ability to answer complex questions through decomposed sub-questions and supporting facts. | Used to test models on web interaction tasks with ground-truth action sequences, focusing on the execution of complex web-based actions. | Used to evaluate models on complex question answering tasks with decomposed questions and supporting facts, emphasizing multi-hop reasoning. | Utilized to test LLMs' reasoning skills in answering strategy-based questions with supporting evidence.",
          "citing_paper_id": "270823634",
          "cited_paper_id": 236771976,
          "context_text": "For example, PRM800K (Light-man et al., 2023) is a maths dataset containing the solution steps written in natural language, inter-leaved with formulas; Musique (Trivedi et al., 2022) and StrategyQA (Geva et al., 2021) are complex QA datasets annotated with decomposed questions, supporting facts, and relevant Wikipedia para-graph indices; Mind2Web includes ground-truth action sequences such as “ [combobox] Reservation Type → SELECT: Pickup ”.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training and evaluating models on complex reasoning tasks, which aligns with the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2311.05657",
          "cited_paper_doi": "10.1162/tacl_a_00475",
          "citing_paper_url": "https://www.semanticscholar.org/paper/7dd3b54233a71c532a15adc6faa7284af7c02f15",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ec307b17f193b14292206b65a1bcc95bfd8f02ed",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "249889477",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Mystery Blocksworld"
      ],
      "dataset_details": [
        {
          "dataset_name": "Mystery Blocksworld",
          "dataset_description": "Used to draw instances for evaluating large language models on planning tasks, focusing on reasoning about change in a domain similar to Blocksworld. | Used to evaluate large language models on planning and reasoning tasks, specifically focusing on the ability to reason about change using a set of 100 instances.",
          "citing_paper_id": "267637077",
          "cited_paper_id": 249889477,
          "context_text": "For Mystery Blocksworld, we use a set of 100 instances from [31] for our evaluations.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Mystery Blocksworld' and refers to a set of 100 instances used for evaluation. The cited paper title suggests this is part of a benchmark for evaluating LLMs on planning and reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2402.08115",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/112e0260c960c02a808cbf191420b13ef824da1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "258762525",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "data scraped from 4nums.com"
      ],
      "dataset_details": [
        {
          "dataset_name": "data scraped from 4nums.com",
          "dataset_description": "Used to evaluate problem-solving capabilities of large language models, focusing on numerical reasoning tasks and algorithmic challenges.",
          "citing_paper_id": "267637077",
          "cited_paper_id": 258762525,
          "context_text": "Following [37], we use data scraped from 4nums.com .",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'data scraped from 4nums.com', which appears to be a specific, named resource. However, it lacks clear provenance and public accessibility details.",
          "citing_paper_doi": "10.48550/arXiv.2402.08115",
          "cited_paper_doi": "10.48550/arXiv.2305.10601",
          "citing_paper_url": "https://www.semanticscholar.org/paper/112e0260c960c02a808cbf191420b13ef824da1c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "259129428",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Tell Me More"
      ],
      "dataset_details": [
        {
          "dataset_name": "Tell Me More",
          "dataset_description": "Used to evaluate language agents' capability of asking clarification questions, focusing on improving user interaction through better understanding of ambiguous instructions. | Assumes clear and explicit user instructions, used to test language agents' planning capabilities in travel-related scenarios without requiring clarification. | Used to assess language agents' ability to ask follow-up questions, enhancing the clarity and effectiveness of user interactions. | Assumes clear and explicit user instructions, used to evaluate language agents' ability to perform web-based tasks without needing additional clarifications.",
          "citing_paper_id": "270561990",
          "cited_paper_id": 259129428,
          "context_text": "3 Ask-before-Plan Dataset Existing benchmarks on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets/benchmarks but does not specify their usage in the current research. It only describes their focus areas.",
          "citing_paper_doi": "10.48550/arXiv.2406.12639",
          "cited_paper_doi": "10.48550/arXiv.2306.06070",
          "citing_paper_url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/58f8925a8b87054ad0635a6398a7fe24935b1604",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "270123654",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Clamber"
      ],
      "dataset_details": [
        {
          "dataset_name": "Clamber",
          "dataset_description": "Assumes clear and explicit user instructions, used to evaluate language agents' ability to perform web-based tasks without needing additional clarifications. | Assumes clear and explicit user instructions, used to test language agents' planning capabilities in travel-related scenarios without requiring clarification. | Used to assess language agents' ability to ask follow-up questions, enhancing the clarity and effectiveness of user interactions. | Used to evaluate language agents' capability of asking clarification questions, focusing on improving user interaction through better understanding of ambiguous instructions.",
          "citing_paper_id": "270561990",
          "cited_paper_id": 270123654,
          "context_text": "3 Ask-before-Plan Dataset Existing benchmarks on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets/benchmarks but does not specify their usage in the current research. It only describes their focus areas.",
          "citing_paper_doi": "10.48550/arXiv.2406.12639",
          "cited_paper_doi": "10.48550/arXiv.2405.19425",
          "citing_paper_url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Ask-before-Plan"
      ],
      "dataset_details": [
        {
          "dataset_name": "Ask-before-Plan",
          "dataset_description": "Used to build a dataset for evaluating planning capabilities in language models, focusing on interactive travel planning scenarios. | Served as the foundation for the Ask-before-Plan dataset, providing travel planning data for model training and evaluation.",
          "citing_paper_id": "270561990",
          "cited_paper_id": null,
          "context_text": "The Ask-before-Plan dataset was built from the TravelPlanner dataset (Xie et al., 2024), which is publicly available.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets: 'Ask-before-Plan' and 'TravelPlanner'. Both are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2406.12639",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    }
  ],
  "citation_count_distribution": {
    "2320080": 1,
    "18976919": 2,
    "24331484": 1,
    "37925315": 2,
    "160025533": 7,
    "204838007": 6,
    "218971783": 25,
    "220618602": 1,
    "222066988": 2,
    "239998651": 7,
    "246035276": 17,
    "246485514": 3,
    "248366629": 2,
    "250451569": 8,
    "251979509": 1,
    "257532815": 15,
    "258298051": 10,
    "258564677": 1,
    "258762760": 4,
    "258865812": 8,
    "258865907": 3,
    "258947337": 4,
    "258959321": 3,
    "259138811": 3,
    "259950998": 5,
    "261100610": 1,
    "261245497": 2,
    "262825203": 2,
    "263671594": 2,
    "263829338": 2,
    "263829697": 3,
    "264172681": 1,
    "264405734": 2,
    "265128575": 3,
    "267782588": 2,
    "267897975": 2,
    "269214439": 2,
    "269982497": 1,
    "270357425": 2,
    "270371268": 1,
    "270562133": 1,
    "271745636": 1,
    "272367167": 1,
    "272770270": 1,
    "273374872": 2,
    "273502635": 1,
    "273507560": 1,
    "274234014": 1,
    "274763098": 1,
    "277313841": 1,
    "279621061": 2,
    "16946362": 1,
    "59656859": 9,
    "209485573": 1,
    "218470560": 1,
    "257496672": 2,
    "269626390": 2,
    "11080756": 1,
    "201646309": 4,
    "236258746": 1,
    "247155069": 1,
    "258218323": 1,
    "258535542": 1,
    "3752019": 1,
    "235294299": 2,
    "254854675": 2,
    "51876975": 1,
    "204852201": 1,
    "218869575": 4,
    "222208810": 12,
    "231591445": 9,
    "246411621": 14,
    "252762395": 22,
    "258179774": 1,
    "258833055": 13,
    "261064713": 9,
    "211204736": 1,
    "221191193": 1,
    "254854219": 1,
    "254877499": 1,
    "258832372": 2,
    "259129428": 4,
    "260164780": 3,
    "260351380": 1,
    "261048935": 1,
    "265295561": 1,
    "267406772": 1,
    "7164502": 2,
    "14337532": 1,
    "204241631": 1,
    "231879586": 1,
    "237194835": 1,
    "248476411": 2,
    "252439240": 1,
    "253735327": 1,
    "257364842": 3,
    "257631498": 1,
    "257687420": 2,
    "258587978": 1,
    "261706176": 1,
    "264490392": 1,
    "266725320": 1,
    "267750682": 2,
    "229853": 1,
    "5340880": 1,
    "7828197": 1,
    "10956233": 1,
    "13195796": 1,
    "20817484": 1,
    "38193578": 2,
    "212658007": 1,
    "251647156": 1,
    "257687798": 1,
    "259341893": 1,
    "263830368": 1,
    "264305599": 1,
    "268531200": 1,
    "58377815": 1,
    "208617407": 6,
    "231839855": 1,
    "246275593": 1,
    "252968000": 1,
    "254408960": 7,
    "255198985": 2,
    "259342833": 2,
    "270123427": 1,
    "271212307": 1,
    "271571434": 6,
    "271769045": 1,
    "271854813": 1,
    "15912959": 1,
    "132910614": 1,
    "256697342": 11,
    "2329216": 1,
    "123106881": 2,
    "229363334": 1,
    "247939706": 14,
    "252355542": 10,
    "253801709": 5,
    "257663729": 5,
    "258833332": 1,
    "267406155": 2,
    "536614": 1,
    "2307186": 1,
    "2366653": 1,
    "2738870": 1,
    "4706694": 1,
    "9225837": 1,
    "12316599": 1,
    "13771609": 1,
    "14217178": 1,
    "16238232": 1,
    "16261041": 1,
    "16716972": 1,
    "33551774": 1,
    "49575415": 1,
    "52012313": 1,
    "61817892": 1,
    "160018920": 1,
    "189048448": 1,
    "199405608": 1,
    "201093978": 1,
    "204960852": 1,
    "206805969": 1,
    "210926937": 1,
    "218981884": 1,
    "221308729": 1,
    "235359276": 1,
    "235651885": 1,
    "236034184": 1,
    "236090307": 1,
    "236447420": 1,
    "236945351": 1,
    "236985291": 1,
    "238353829": 1,
    "240302757": 1,
    "245144651": 1,
    "245986665": 1,
    "246240065": 1,
    "247022700": 1,
    "247897432": 1,
    "251253219": 1,
    "251371611": 1,
    "252089119": 1,
    "257365015": 1,
    "257912912": 1,
    "258685266": 1,
    "258762525": 8,
    "258841776": 1,
    "259137864": 1,
    "259241297": 1,
    "259262186": 2,
    "260334759": 14,
    "261030303": 3,
    "261100760": 1,
    "262055731": 1,
    "263671690": 1,
    "263834585": 1,
    "263909014": 2,
    "264930510": 1,
    "265019126": 1,
    "265607979": 2,
    "265659006": 1,
    "266176951": 1,
    "266435934": 1,
    "266693892": 1,
    "267060885": 1,
    "267069400": 1,
    "268239825": 1,
    "271533450": 1,
    "271544382": 1,
    "142610973": 1,
    "256598146": 7,
    "258865184": 7,
    "258967184": 6,
    "259262376": 1,
    "276344352": 2,
    "1397894": 3,
    "40099475": 1,
    "59524001": 1,
    "67788344": 1,
    "84186410": 1,
    "184469323": 1,
    "210971709": 1,
    "220828823": 1,
    "246681316": 1,
    "249461514": 1,
    "252693035": 1,
    "259991167": 1,
    "266051233": 1,
    "266162896": 1,
    "267616761": 1,
    "277626231": 1,
    "1100293": 1,
    "1906145": 1,
    "12198630": 1,
    "15693605": 1,
    "233324485": 2,
    "246426909": 8,
    "257219404": 8,
    "258841029": 1,
    "258887849": 5,
    "259095869": 1,
    "260293142": 2,
    "1553193": 2,
    "52055325": 1,
    "202541184": 1,
    "202660943": 1,
    "221516475": 5,
    "232134851": 5,
    "247011290": 1,
    "247595263": 13,
    "253237047": 1,
    "256415991": 1,
    "258832847": 1,
    "258833200": 1,
    "258967391": 1,
    "261030818": 1,
    "261696697": 1,
    "262084051": 1,
    "264172893": 1,
    "265157752": 1,
    "265295009": 3,
    "265608902": 1,
    "266209760": 1,
    "266359670": 1,
    "267412590": 1,
    "267412607": 2,
    "267770504": 1,
    "269457450": 1,
    "270380229": 1,
    "270560678": 1,
    "273228858": 2,
    "274234789": 1,
    "275789950": 4,
    "275932560": 1,
    "276079693": 1,
    "276116814": 1,
    "276422318": 1,
    "277451641": 1,
    "278715317": 1,
    "278768680": 1,
    "278788524": 1,
    "278911988": 1,
    "279070806": 1,
    "279154475": 1,
    "279447674": 1,
    "280000246": 1,
    "199541993": 1,
    "210836616": 1,
    "218595872": 1,
    "233322292": 1,
    "245023520": 1,
    "252284068": 1,
    "257512189": 1,
    "258558102": 4,
    "269005338": 1,
    "9944290": 1,
    "15120295": 1,
    "250390581": 1,
    "257445349": 1,
    "264491049": 1,
    "267069469": 1,
    "269292966": 1,
    "6138957": 1,
    "28328610": 3,
    "201782024": 1,
    "219964473": 1,
    "254591260": 2,
    "257205781": 2,
    "257757298": 1,
    "258841057": 1,
    "263134620": 1,
    "267068760": 1,
    "9953039": 1,
    "235458009": 4,
    "245329531": 4,
    "249017698": 2,
    "259370794": 1,
    "260682249": 1,
    "265129059": 4,
    "265351664": 2,
    "267406800": 5,
    "277467626": 1,
    "10097708": 1,
    "26553672": 1,
    "60447873": 1,
    "80628296": 1,
    "140952910": 1,
    "189656355": 1,
    "199437458": 1,
    "221668003": 1,
    "253410677": 1,
    "255826858": 1,
    "259737661": 1,
    "259837542": 4,
    "265295011": 1,
    "268777626": 1,
    "1033682": 1,
    "2682274": 1,
    "7338543": 1,
    "9026666": 1,
    "127986044": 1,
    "233296711": 1,
    "248097655": 1,
    "248986576": 1,
    "249926846": 1,
    "253510037": 1,
    "255372955": 1,
    "258059885": 1,
    "258865950": 1,
    "263151865": 1,
    "15238391": 1,
    "222066674": 1,
    "256827430": 1,
    "258179056": 4,
    "263625818": 1,
    "265019021": 1,
    "103456": 2,
    "17746168": 1,
    "42886148": 1,
    "49317780": 5,
    "252519594": 5,
    "258564887": 2,
    "259089333": 1,
    "259141622": 5,
    "259274760": 3,
    "260125969": 1,
    "260438420": 1,
    "8347993": 1,
    "233305616": 1,
    "248986485": 1,
    "249017743": 4,
    "250311260": 1,
    "258960174": 1,
    "261100919": 3,
    "264935717": 2,
    "265185500": 1,
    "266906759": 2,
    "268042527": 3,
    "268531996": 1,
    "246634264": 1,
    "259360395": 1,
    "259936967": 1,
    "261817592": 7,
    "265067168": 1,
    "267412980": 4,
    "269921354": 2,
    "276489554": 1,
    "13756489": 8,
    "230799347": 3,
    "264935408": 1,
    "267094920": 1,
    "267759583": 1,
    "270703266": 1,
    "274788614": 1,
    "4673790": 1,
    "91184540": 2,
    "121987403": 1,
    "254877310": 1,
    "259342058": 3,
    "269293529": 1,
    "270258182": 1,
    "17305": 2,
    "1478442": 1,
    "2126705": 1,
    "2432261": 1,
    "2730999": 2,
    "5642010": 1,
    "5753706": 1,
    "6384075": 1,
    "6552475": 1,
    "7138783": 1,
    "8065594": 1,
    "8623866": 1,
    "10448069": 1,
    "10684261": 1,
    "10791590": 1,
    "11337237": 2,
    "11481542": 1,
    "16652519": 1,
    "17099383": 1,
    "19104534": 1,
    "19182808": 1,
    "21698093": 1,
    "28467205": 1,
    "32556638": 1,
    "126643659": 1,
    "206799161": 3,
    "208512881": 1,
    "210928826": 1,
    "219188633": 1,
    "235349223": 1,
    "235428843": 1,
    "235586367": 1,
    "237581157": 1,
    "245827334": 1,
    "246527904": 1,
    "247778891": 1,
    "248721898": 1,
    "250334236": 1,
    "250406114": 1,
    "254017827": 1,
    "256274690": 1,
    "256846992": 2,
    "259320426": 1,
    "259854663": 1,
    "260440590": 4,
    "262269888": 2,
    "265130666": 1,
    "265957829": 1,
    "266223700": 1,
    "266359688": 1,
    "268297180": 2,
    "268680720": 1,
    "268961846": 1,
    "269614003": 1,
    "270158113": 1,
    "270170904": 1,
    "270620065": 1,
    "271504408": 1,
    "271571035": 3,
    "271719990": 4,
    "273098440": 1,
    "273589540": 1,
    "274776284": 1,
    "276079564": 1,
    "276482470": 1,
    "276618008": 1,
    "6326401": 1,
    "19100351": 1,
    "47012216": 1,
    "53513571": 2,
    "209366827": 1,
    "209515572": 1,
    "214774912": 1,
    "219124336": 1,
    "220280457": 1,
    "220404390": 1,
    "220551746": 1,
    "231698518": 1,
    "231730714": 1,
    "237581476": 1,
    "246575985": 1,
    "258987581": 1,
    "260865987": 1,
    "261395685": 1,
    "261823722": 1,
    "263832826": 1,
    "264128019": 1,
    "264146567": 1,
    "267406588": 1,
    "267411892": 3,
    "267413218": 1,
    "267897865": 1,
    "269539563": 1,
    "270285926": 1,
    "274437509": 1,
    "276450249": 1,
    "276742020": 1,
    "278904757": 1,
    "3628605": 1,
    "10878315": 1,
    "12515065": 1,
    "13253834": 1,
    "17341026": 1,
    "19182772": 1,
    "38899272": 1,
    "207424229": 1,
    "211132951": 1,
    "212703531": 1,
    "237581307": 1,
    "255825556": 1,
    "268856519": 1,
    "13405741": 1,
    "250114239": 1,
    "255124952": 1,
    "267642722": 1,
    "271162268": 1,
    "272559061": 1,
    "272827463": 1,
    "273482255": 1,
    "274823073": 1,
    "276913297": 1,
    "53208380": 1,
    "67000854": 1,
    "237142385": 1,
    "254096365": 1,
    "256105296": 1,
    "264172455": 1,
    "276482377": 1,
    "279586232": 1,
    "256868474": 1,
    "266359151": 3,
    "266844877": 1,
    "266902900": 1,
    "273042295": 1,
    "214754592": 1,
    "227347434": 1,
    "232404173": 1,
    "233324338": 1,
    "236957210": 1,
    "248721683": 1,
    "248913107": 2,
    "256390607": 1,
    "258947447": 1,
    "268379149": 1,
    "268417079": 1,
    "268889483": 1,
    "272367253": 1,
    "273098318": 2,
    "274131897": 1,
    "276580691": 1,
    "277150647": 1,
    "258685337": 2,
    "258987659": 2,
    "260887105": 1,
    "263609132": 3,
    "264590387": 1,
    "265609521": 1,
    "267212063": 1,
    "267320303": 1,
    "12777818": 4,
    "34953552": 1,
    "52897360": 1,
    "233231380": 1,
    "234093776": 1,
    "236034497": 1,
    "252917648": 3,
    "258108259": 1,
    "259360665": 1,
    "262053695": 1,
    "264439655": 1,
    "267199749": 1,
    "267897727": 1,
    "268031860": 1,
    "268363855": 2,
    "269148675": 1,
    "270620912": 1,
    "270738094": 1,
    "271270048": 1,
    "272593197": 1,
    "273507547": 1,
    "275606655": 1,
    "275757481": 1,
    "276421817": 1,
    "226965153": 1,
    "249097975": 3,
    "257900871": 4,
    "258714753": 1,
    "259203671": 1,
    "263605944": 2,
    "268379197": 1,
    "274514455": 1,
    "233289729": 1,
    "257637012": 1,
    "258179336": 1,
    "258959262": 2,
    "259108190": 2,
    "261241602": 1,
    "263310365": 1,
    "264306101": 3,
    "277276996": 1,
    "235755472": 4,
    "258040990": 6,
    "269921148": 2,
    "270371956": 1,
    "265506220": 1,
    "270562306": 1,
    "271212406": 1,
    "271953942": 1,
    "272368347": 2,
    "272424184": 1,
    "5082661": 1,
    "28695052": 1,
    "204972004": 1,
    "257663442": 4,
    "258179085": 1,
    "259144814": 1,
    "259145016": 1,
    "259947046": 1,
    "262053612": 2,
    "263310497": 1,
    "263708879": 1,
    "264146113": 1,
    "267897401": 1,
    "268032502": 1,
    "269502645": 1,
    "269536190": 1,
    "272330427": 2,
    "1767517": 1,
    "265067391": 1,
    "265294665": 1,
    "267211622": 1,
    "267898017": 1,
    "268379413": 1,
    "269283058": 1,
    "270062853": 1,
    "271515809": 1,
    "273502415": 1,
    "276107438": 1,
    "277780948": 1,
    "278237252": 1,
    "202770731": 1,
    "208158225": 2,
    "237504552": 3,
    "248722148": 3,
    "249674500": 1,
    "249848263": 2,
    "252735112": 1,
    "253107613": 1,
    "257532801": 1,
    "259262142": 1,
    "263334319": 1,
    "263909278": 1,
    "266174368": 1,
    "267413027": 1,
    "267617061": 2,
    "267627652": 1,
    "268230464": 2,
    "271064506": 2,
    "274437383": 1,
    "34198369": 1,
    "49470584": 1,
    "225062560": 1,
    "225076003": 1,
    "235606348": 1,
    "237277983": 1,
    "246634577": 1,
    "247618840": 2,
    "252200013": 1,
    "252718704": 1,
    "257079001": 2,
    "257952310": 1,
    "259187664": 1,
    "259308821": 1,
    "259837330": 2,
    "261681849": 1,
    "263218031": 1,
    "263626099": 1,
    "268856732": 1,
    "270440391": 1,
    "3517962": 1,
    "59788741": 1,
    "198953378": 2,
    "231718747": 1,
    "235212182": 1,
    "248496292": 3,
    "254823489": 2,
    "257687695": 1,
    "258170084": 1,
    "264590280": 1,
    "265019477": 1,
    "268248810": 1,
    "268297061": 1,
    "14588414": 1,
    "25648541": 1,
    "52127932": 1,
    "67769538": 1,
    "214802269": 1,
    "218908860": 1,
    "247749019": 1,
    "251719353": 2,
    "252407646": 1,
    "256827239": 1,
    "259129651": 2,
    "259342486": 1,
    "259342528": 1,
    "260202961": 1,
    "261823330": 1,
    "263787277": 1,
    "264406064": 1,
    "267412137": 1,
    "270441137": 1,
    "276248853": 1,
    "259262077": 2,
    "265050948": 1,
    "266044180": 1,
    "267740648": 1,
    "268249090": 1,
    "6628106": 2,
    "33285731": 1,
    "215737187": 2,
    "233427561": 1,
    "235415270": 1,
    "236477750": 1,
    "259187717": 1,
    "259833781": 1,
    "263610099": 1,
    "266521240": 1,
    "266551228": 1,
    "267068458": 1,
    "267211869": 1,
    "267411962": 1,
    "268553748": 1,
    "270870063": 2,
    "1373518": 1,
    "52822214": 4,
    "258060250": 1,
    "258822910": 2,
    "258823133": 2,
    "258865718": 1,
    "259243960": 2,
    "260887189": 1,
    "267548105": 1,
    "268232499": 2,
    "270521777": 1,
    "271532761": 1,
    "273654757": 1,
    "964287": 1,
    "990084": 1,
    "13866508": 1,
    "17756042": 1,
    "18506861": 1,
    "49313245": 3,
    "52183757": 1,
    "62039799": 1,
    "204854503": 1,
    "208290939": 2,
    "221555387": 1,
    "225066679": 1,
    "226278099": 1,
    "237205744": 1,
    "237532606": 1,
    "238531697": 1,
    "239010011": 1,
    "252780839": 1,
    "253080612": 1,
    "256661805": 1,
    "257572753": 1,
    "257952500": 1,
    "258041354": 1,
    "258171641": 1,
    "258556812": 1,
    "258947222": 4,
    "259129531": 1,
    "259129602": 1,
    "259983087": 1,
    "260164518": 1,
    "264107446": 1,
    "264825354": 1,
    "265038146": 1,
    "265104049": 1,
    "266362535": 1,
    "266999586": 1,
    "267406814": 1,
    "267412025": 1,
    "267523467": 1,
    "267750939": 1,
    "204578308": 1,
    "222140924": 1,
    "247451124": 2,
    "258291566": 1,
    "259950380": 1,
    "260063238": 1,
    "265019383": 1,
    "267770308": 1,
    "268417347": 1,
    "269302548": 1,
    "270371213": 1,
    "270620354": 1,
    "46997949": 1,
    "53533716": 1,
    "153257084": 1,
    "202538486": 1,
    "225066700": 1,
    "238408283": 1,
    "253759631": 2,
    "254277011": 1,
    "257833781": 2,
    "258041253": 1,
    "258509586": 1,
    "259088724": 1,
    "262044464": 1,
    "265213232": 1,
    "266690872": 1,
    "267061073": 1,
    "267081115": 1,
    "267938212": 1,
    "268042457": 1,
    "268681670": 1,
    "268819803": 1,
    "268876709": 1,
    "269745421": 1,
    "269757480": 1,
    "269757934": 1,
    "270062501": 1,
    "252596089": 1,
    "258212642": 1,
    "983645": 1,
    "144110805": 1,
    "218665356": 1,
    "235266260": 1,
    "238259902": 2,
    "256808659": 2,
    "257805102": 1,
    "259224572": 1,
    "261049794": 1,
    "263310943": 2,
    "264306288": 1,
    "265610018": 1,
    "547566": 2,
    "2548749": 1,
    "7559418": 1,
    "14827857": 1,
    "17055038": 1,
    "257255456": 2,
    "258968043": 2,
    "259714625": 1,
    "260881158": 1,
    "263829963": 2,
    "266163085": 1,
    "267548095": 1,
    "254246305": 1,
    "258212542": 1,
    "258740978": 1,
    "258947657": 2,
    "258999153": 1,
    "264555202": 1,
    "1916803": 1,
    "15080556": 1,
    "15117338": 1,
    "15184765": 1,
    "118922379": 1,
    "222133157": 2,
    "234680400": 1,
    "247951931": 5,
    "251881108": 2,
    "254877723": 1,
    "254877753": 4,
    "256416127": 1,
    "258740735": 2,
    "258762577": 2,
    "263098481": 1,
    "263868305": 1,
    "208910339": 1,
    "254823156": 1,
    "260887370": 1,
    "261076387": 1,
    "263671701": 2,
    "264490502": 1,
    "268042522": 1,
    "268307249": 1,
    "1543962": 1,
    "5182891": 1,
    "7038773": 1,
    "21740766": 1,
    "31204656": 1,
    "52198622": 1,
    "53715584": 1,
    "64316736": 1,
    "205223625": 1,
    "215768690": 1,
    "220714040": 1,
    "257378363": 1,
    "258079021": 2,
    "258967487": 1,
    "264146047": 1,
    "265294541": 2,
    "279052856": 1,
    "26419660": 1,
    "232092445": 2,
    "235417602": 1,
    "256615643": 2,
    "262055166": 1,
    "267411736": 1,
    "3883991": 1,
    "29167732": 1,
    "49670925": 1,
    "60035920": 1,
    "116908168": 1,
    "208527654": 1,
    "233289456": 1,
    "250287185": 1,
    "256389594": 1,
    "258297976": 1,
    "246652372": 2,
    "253264914": 1,
    "258841328": 1,
    "262824801": 1,
    "266191787": 1,
    "268532485": 1,
    "271497434": 1,
    "11309330": 1,
    "259982665": 1,
    "262828493": 1,
    "263620434": 1,
    "264564300": 1,
    "265149884": 1,
    "273811174": 1,
    "5276660": 1,
    "19135734": 1,
    "53115163": 1,
    "216559511": 1,
    "255569874": 1,
    "256846700": 1,
    "263608611": 2,
    "263830494": 4,
    "18709606": 1,
    "60440549": 1,
    "213176860": 1,
    "254685791": 1,
    "256783608": 1,
    "257219432": 2,
    "260682436": 1,
    "270440269": 1,
    "270576866": 1,
    "273025556": 1,
    "273695554": 1,
    "276482111": 1,
    "276766708": 1,
    "6936600": 1,
    "9885884": 1,
    "266198147": 1,
    "266435868": 1,
    "267311877": 1,
    "270561990": 1,
    "271270974": 1,
    "258947250": 1,
    "267413178": 2,
    "267637077": 1,
    "67872126": 1,
    "76665670": 1,
    "259243610": 1,
    "259501163": 1,
    "259689601": 1,
    "260378734": 1,
    "261898118": 1,
    "267212047": 1,
    "268531565": 1,
    "270621087": 1,
    "32893583": 1,
    "57366648": 1,
    "119181611": 1,
    "214770841": 1,
    "238634419": 1,
    "246863587": 1,
    "249304070": 1,
    "252904698": 1,
    "253098091": 1,
    "253954372": 1,
    "258841284": 1,
    "5550767": 1,
    "7646250": 1,
    "32182109": 1,
    "52967399": 3,
    "174803437": 1,
    "195984066": 1,
    "213662188": 1,
    "215786368": 1,
    "218889832": 1,
    "232045968": 1,
    "235795331": 1,
    "237420687": 1,
    "237589920": 1,
    "253734315": 1,
    "256826987": 1,
    "258461606": 1,
    "258823112": 1,
    "259203115": 1,
    "264935466": 1,
    "266550843": 1,
    "253708270": 3,
    "254853816": 1,
    "263831032": 1,
    "266051661": 1,
    "1762453": 1,
    "15252590": 1,
    "15597128": 1,
    "21174343": 1,
    "36102848": 2,
    "199370376": 1,
    "238857096": 2,
    "264305982": 1,
    "3986974": 1,
    "7278297": 1,
    "13905064": 1,
    "158046772": 1,
    "202539519": 1,
    "207167677": 1,
    "208006241": 1,
    "237562927": 1,
    "239011786": 1,
    "247082469": 1,
    "251518434": 1,
    "259095910": 1,
    "259138909": 1,
    "259924559": 1,
    "259936842": 1,
    "260315904": 1,
    "260350986": 1,
    "261076103": 1,
    "261214582": 1,
    "264995471": 1,
    "266176297": 1,
    "267411800": 1,
    "268296710": 1,
    "268417087": 1,
    "268691587": 1,
    "268697551": 1,
    "269687228": 1,
    "269704509": 1,
    "274060374": 1,
    "3694591": 1,
    "6627476": 1,
    "70350059": 1,
    "173991054": 1,
    "189998275": 2,
    "207870268": 1,
    "220042384": 1,
    "232170216": 1,
    "244921108": 1,
    "207716": 1,
    "9926549": 1,
    "10248021": 1,
    "232335456": 1,
    "237364324": 1,
    "251979702": 1,
    "258947227": 1,
    "259765919": 1,
    "260925901": 1,
    "261034676": 1,
    "261075905": 1,
    "1739732": 1,
    "59536625": 1,
    "249889477": 2,
    "253734939": 1,
    "258741194": 1,
    "259164815": 1,
    "262217135": 1,
    "268249221": 1,
    "270357954": 1,
    "270823634": 1,
    "3752864": 1,
    "235574660": 1,
    "252814432": 1,
    "254070076": 1,
    "240088413": 1,
    "246867455": 1,
    "257636839": 1,
    "257834038": 2,
    "216356": 1,
    "3674213": 1,
    "4662765": 1,
    "5210390": 2,
    "7228830": 1,
    "15654531": 1,
    "18394514": 1,
    "18433083": 1,
    "44451104": 1,
    "144296456": 1,
    "210023849": 1,
    "222379602": 1,
    "225588887": 1,
    "227305659": 1,
    "233224125": 1,
    "233547699": 1,
    "233987077": 1,
    "235599489": 1,
    "237416499": 1,
    "239020696": 1,
    "245062830": 1,
    "245212848": 1,
    "248177934": 1,
    "248377511": 1,
    "255941863": 1,
    "259139714": 1,
    "262041623": 1,
    "266999465": 1,
    "269329951": 1,
    "270638927": 1,
    "272201868": 1,
    "274131547": 1,
    "275954224": 1,
    "202773799": 1,
    "214148289": 1,
    "237492197": 1,
    "250264533": 2,
    "252918278": 1,
    "257038525": 1,
    "257920051": 1,
    "258479667": 1,
    "259949693": 1,
    "267365239": 1,
    "267897510": 1,
    "268732915": 1,
    "269757778": 1,
    "270123654": 1,
    "248118878": 1,
    "261697361": 1,
    "268536974": 1,
    "270558898": 1,
    "248986239": 2,
    "252715485": 1,
    "252762102": 2,
    "258180548": 1,
    "3297437": 1,
    "17362777": 1,
    "30196678": 1,
    "127986954": 1,
    "232223322": 3,
    "256416408": 1,
    "257427208": 1,
    "260866107": 1,
    "265128672": 1,
    "270620509": 1,
    "271923851": 1,
    "2188447": 1,
    "13905150": 1,
    "49902052": 1,
    "52035171": 1,
    "235593036": 1,
    "237263814": 1,
    "248426863": 1,
    "249209900": 1,
    "256868369": 1,
    "257482793": 1,
    "263830880": 1,
    "266435584": 2,
    "266335848": 2,
    "270379625": 1,
    "270688227": 1,
    "273234147": 1,
    "274024100": 1,
    "278532827": 1,
    "5034059": 1,
    "269294048": 1,
    "270440318": 1,
    "272986898": 1,
    "218551201": 1,
    "246634179": 1,
    "265220706": 1,
    "271039484": 1,
    "3098522": 3,
    "7646504": 1,
    "14105338": 1,
    "20483647": 1,
    "34600346": 1,
    "37954891": 1,
    "96452977": 1,
    "205763978": 1,
    "219179089": 1,
    "238583814": 1,
    "238634478": 1,
    "247957917": 1,
    "248085271": 1,
    "258187415": 1,
    "258991231": 1,
    "265157583": 1,
    "266899650": 1,
    "268512756": 1,
    "273229503": 1,
    "273374421": 1,
    "260126067": 2,
    "262825568": 1,
    "268248325": 1,
    "268296913": 1,
    "268296997": 1,
    "273821674": 1,
    "274859421": 2,
    "277824060": 1,
    "12508462": 1,
    "259202547": 1,
    "261065228": 1,
    "263611068": 1,
    "266844635": 1,
    "270257715": 1,
    "271092420": 1,
    "273228480": 1,
    "273821797": 1,
    "273970146": 1,
    "274823033": 1,
    "275515996": 1,
    "276482713": 1,
    "276742133": 1,
    "276884818": 1,
    "276937204": 1,
    "277113131": 1,
    "277634095": 1,
    "277993666": 1,
    "277999502": 1,
    "278602855": 1,
    "32417": 1,
    "27250165": 1,
    "85499781": 1,
    "140254691": 1,
    "144787755": 1,
    "196127175": 1,
    "211258652": 1,
    "260775783": 1,
    "264555641": 1,
    "270226047": 1,
    "270923717": 2,
    "7575616": 1,
    "16355120": 1,
    "18688952": 1,
    "54767605": 1,
    "63614656": 1,
    "142669959": 1,
    "143781031": 1,
    "146578268": 1,
    "153933608": 1,
    "221904092": 1,
    "231693275": 1,
    "239009871": 1,
    "239459629": 1,
    "247085270": 1,
    "248780453": 1,
    "250311134": 1,
    "253420678": 1,
    "258179241": 1,
    "258217017": 1,
    "258217984": 1,
    "258833277": 1,
    "260697887": 1,
    "262826094": 1,
    "268533038": 1,
    "9426935": 1,
    "57825693": 1,
    "236586513": 1,
    "252409485": 1,
    "253499063": 1,
    "260939243": 1,
    "2218552": 1,
    "11039301": 1,
    "76660838": 1,
    "202540203": 1,
    "212645349": 1,
    "246411155": 1,
    "249642147": 1,
    "252873467": 1,
    "258865984": 1,
    "261049680": 1,
    "261245264": 1,
    "264288947": 2,
    "265067352": 1,
    "267334785": 1,
    "267523037": 1,
    "267770255": 1,
    "268041615": 1,
    "269032933": 1,
    "269363075": 2,
    "269484643": 1,
    "270062331": 1,
    "270064259": 1,
    "270703648": 1,
    "270710703": 1,
    "271404773": 1,
    "271860017": 1,
    "272770285": 1,
    "273023170": 1,
    "273098148": 1,
    "273098808": 1,
    "273345961": 1,
    "273901309": 1,
    "274422541": 1,
    "274859535": 1,
    "275907122": 1,
    "276094378": 1,
    "276235546": 1,
    "276482449": 1,
    "277857467": 1,
    "278165315": 1,
    "279071173": 1,
    "3110807": 1,
    "7435203": 1,
    "11552928": 1,
    "14526353": 1,
    "14758992": 1,
    "15202972": 1,
    "16224144": 1,
    "36031679": 1,
    "43543364": 1,
    "61059855": 1,
    "61567783": 1,
    "206875703": 1,
    "256358611": 1,
    "268033675": 1,
    "269005522": 1,
    "271488373": 1,
    "235899403": 1,
    "251135295": 1,
    "255595818": 1,
    "255912658": 1,
    "263136146": 1,
    "263605637": 1,
    "266521410": 1,
    "267028230": 1,
    "269009845": 1,
    "327319": 1,
    "3922816": 1,
    "4897444": 1,
    "5445756": 1,
    "6675645": 1,
    "6866988": 1,
    "12358271": 1,
    "13838309": 1,
    "15367821": 1,
    "16299141": 1,
    "21722946": 1,
    "24604537": 1,
    "29799633": 1,
    "37065565": 1,
    "40344783": 1,
    "51903808": 1,
    "53232749": 1,
    "53787096": 1,
    "54434517": 1,
    "94929253": 1,
    "108374190": 1,
    "119413748": 1,
    "125046626": 1,
    "135497159": 1,
    "173188048": 1,
    "174797974": 1,
    "174802445": 1,
    "195069360": 1,
    "195791303": 1,
    "198999551": 1,
    "201754999": 1,
    "204105343": 1,
    "204960716": 1,
    "206756462": 1,
    "207220720": 1,
    "211146177": 1,
    "212415210": 1,
    "215768677": 1,
    "217680306": 1,
    "218487092": 1,
    "219955663": 1,
    "220347237": 1,
    "224803102": 1,
    "224803470": 1,
    "226059428": 1,
    "230528669": 1,
    "232104726": 1,
    "233444273": 1,
    "234207025": 1,
    "235220930": 1,
    "237476761": 1,
    "237578994": 1,
    "237747003": 1,
    "238226735": 1,
    "238634584": 1,
    "243865204": 1,
    "245960612": 1,
    "248065796": 1,
    "249674746": 1,
    "252030240": 1,
    "252383606": 1,
    "252544920": 1,
    "253237094": 1,
    "254877346": 1,
    "256389950": 1,
    "256416564": 1,
    "256827363": 1,
    "257637220": 1,
    "258021826": 1,
    "258059792": 1,
    "258213830": 1,
    "258381985": 1,
    "258587994": 1,
    "258685532": 1,
    "259075316": 1,
    "259316643": 1,
    "259501816": 1,
    "259766332": 1,
    "261681740": 1,
    "261925357": 1,
    "263134037": 1,
    "263835191": 1,
    "263890629": 1,
    "263908945": 1,
    "263909166": 1,
    "264591559": 1,
    "264973960": 1,
    "265041119": 1,
    "265104899": 1,
    "265308533": 1,
    "265308911": 1,
    "265505046": 1,
    "265505419": 1,
    "265544461": 1,
    "265551641": 1,
    "265601068": 1,
    "265698016": 1,
    "266448819": 1,
    "266693920": 1,
    "266862195": 1,
    "267500390": 1,
    "267522772": 1,
    "267547479": 1,
    "268297055": 1,
    "268509923": 1,
    "268531713": 1,
    "268680784": 1,
    "269605576": 1,
    "269605607": 1,
    "269626268": 1,
    "269626676": 1,
    "270223881": 1,
    "270358007": 1,
    "270620677": 1,
    "270878784": 1,
    "271064894": 1,
    "271544431": 1,
    "271865537": 1,
    "271874369": 1,
    "272367007": 1,
    "272886429": 1,
    "273375070": 1,
    "273798212": 1,
    "273993329": 1,
    "273993701": 1,
    "274023350": 1,
    "274140377": 1,
    "274140947": 1,
    "274220905": 1,
    "274656164": 1,
    "274966586": 1,
    "276422100": 1,
    "276741674": 1,
    "276855039": 1,
    "277356221": 1,
    "278220803": 1,
    "245353475": 2,
    "215827489": 1,
    "258418299": 1,
    "261696947": 1,
    "268247491": 1,
    "272911169": 1,
    "33081038": 1,
    "198967841": 1,
    "220769401": 1,
    "229298019": 1,
    "246210055": 1,
    "252693237": 1,
    "254017497": 2,
    "258426922": 2,
    "258509700": 1,
    "258865395": 1,
    "268230308": 1,
    "270620269": 2,
    "271051190": 1,
    "275133600": 1,
    "1680775": 1,
    "2711679": 1,
    "6257157": 1,
    "7498624": 1,
    "12246493": 1,
    "12490249": 1,
    "15299054": 1,
    "15975768": 1,
    "31002043": 1,
    "34088202": 1,
    "54480269": 1,
    "64008149": 1,
    "142242370": 1,
    "145184426": 1,
    "213174884": 1,
    "256459681": 1,
    "258686311": 1,
    "2316322": 1,
    "6107832": 1,
    "195218865": 1,
    "209324317": 1,
    "220919878": 1,
    "232379138": 1,
    "269527804": 1,
    "267782782": 1,
    "273638611": 1,
    "276250494": 1,
    "143424870": 1,
    "210861095": 2,
    "211204954": 1,
    "225067460": 1,
    "233444226": 1,
    "261064970": 1,
    "261242328": 1,
    "265050721": 1,
    "265498394": 1,
    "266436034": 1,
    "271038835": 1,
    "280272508": 1,
    "2069491": 1,
    "220047831": 1,
    "220483148": 1,
    "226096901": 1,
    "275820389": 1,
    "25784016": 1,
    "120478295": 1,
    "121328056": 1,
    "198229805": 1,
    "210931542": 1,
    "221806544": 1,
    "224911624": 1,
    "226227046": 1,
    "235435927": 1,
    "249335344": 1,
    "250471540": 1,
    "252125685": 1,
    "256274581": 1,
    "258192578": 1,
    "259881565": 1,
    "265132727": 1,
    "271963189": 1,
    "272344390": 1,
    "273662196": 1,
    "274192378": 1,
    "276635791": 1,
    "4076251": 1,
    "203837042": 3,
    "231632715": 1,
    "257985497": 2,
    "258060965": 1,
    "259262396": 1,
    "4807711": 1,
    "13928442": 1,
    "16623047": 1,
    "19449905": 1,
    "40992894": 1,
    "205261034": 1,
    "206853161": 1,
    "212945787": 1,
    "245218671": 1,
    "247058662": 1,
    "256900800": 1,
    "263605885": 1,
    "267211867": 1,
    "268364153": 1,
    "268819892": 1,
    "259601920": 1,
    "9968823": 1,
    "23892230": 1,
    "210861196": 2,
    "634360": 1,
    "246411325": 1,
    "247778764": 1,
    "5160783": 1,
    "9377590": 1,
    "271693407": 1,
    "57825680": 1,
    "236493269": 1,
    "253734854": 1,
    "258960371": 1,
    "259064000": 1,
    "263672149": 1,
    "268264163": 1,
    "19706": 1,
    "232269696": 1,
    "237091588": 1,
    "257900969": 1,
    "258212828": 1,
    "258291425": 1,
    "258832478": 1,
    "258841249": 1,
    "258947425": 1,
    "264146554": 1,
    "264591439": 1,
    "247500268": 1,
    "249097545": 1,
    "252681067": 1,
    "255341638": 1,
    "256358492": 1,
    "265715696": 1,
    "266176989": 1,
    "268520018": 1,
    "13753671": 1,
    "52099638": 1,
    "220845633": 1,
    "229156165": 1,
    "250426345": 1,
    "252846548": 1,
    "1563120": 1,
    "3334421": 1,
    "4635503": 1,
    "9082946": 1,
    "10242377": 1,
    "13029170": 1,
    "13241655": 1,
    "14709421": 1,
    "15551372": 1,
    "18595016": 1,
    "53026976": 1,
    "57189241": 1,
    "60121883": 1,
    "109965942": 1,
    "114735598": 1,
    "130096094": 1,
    "143983583": 1,
    "153312312": 1,
    "155295771": 1,
    "155298218": 1,
    "189762063": 1,
    "195847962": 1,
    "204142413": 1,
    "206456257": 1,
    "207215142": 1,
    "207231872": 1,
    "212705090": 1,
    "213389645": 1,
    "215791332": 1,
    "218907828": 1,
    "221342993": 1,
    "229156802": 1,
    "229923103": 1,
    "231877730": 1,
    "232269781": 1,
    "233592247": 1,
    "237416585": 1,
    "239768231": 1,
    "243476338": 1,
    "246634238": 1,
    "246863881": 1,
    "250596535": 1,
    "251518264": 1,
    "251518408": 1,
    "251719382": 1,
    "252683292": 1,
    "257632892": 1,
    "257767087": 1,
    "259129807": 1,
    "259251823": 1,
    "260499637": 1,
    "260499706": 1,
    "260548557": 1,
    "260775522": 1,
    "261276812": 1,
    "261705716": 1,
    "263322443": 1,
    "263830637": 1,
    "263908782": 1,
    "264146527": 1,
    "264590333": 1,
    "264615596": 1,
    "265609836": 1,
    "266362729": 1,
    "266375217": 1,
    "266693228": 1,
    "266844108": 1,
    "266933252": 1,
    "267034871": 1,
    "267069344": 1,
    "267617080": 1,
    "267627170": 1,
    "267682348": 1,
    "267750165": 1,
    "267782436": 1,
    "268032947": 1,
    "268063792": 1,
    "268091158": 1,
    "268091311": 1,
    "268379562": 1,
    "268560147": 1,
    "268680708": 1,
    "268722652": 1,
    "268724471": 1,
    "269449477": 1,
    "269605040": 1,
    "270045529": 1,
    "270081134": 1,
    "270199466": 1,
    "270379543": 1,
    "270440657": 1,
    "270440703": 1,
    "270619481": 1,
    "270619971": 1,
    "271051324": 1,
    "271064200": 1,
    "271329051": 1,
    "271533766": 1,
    "271571077": 1,
    "271854887": 1,
    "271954939": 1,
    "271955275": 1,
    "271961973": 1,
    "273229366": 1,
    "273404602": 1,
    "273506998": 1,
    "273654966": 1,
    "273662115": 1,
    "273680557": 1,
    "273811763": 1,
    "273822178": 1,
    "274117080": 1,
    "274198497": 1,
    "274464979": 1,
    "274656234": 1,
    "274776648": 1,
    "274822996": 1,
    "274964904": 1,
    "275133196": 1,
    "275936018": 1,
    "276250403": 1,
    "276317785": 1,
    "276333529": 1,
    "276569762": 1,
    "276617519": 1,
    "276708938": 1,
    "276742424": 1,
    "276885142": 1,
    "276885258": 1,
    "276993547": 1,
    "277042081": 1,
    "277104104": 1,
    "277740771": 1,
    "279074991": 1,
    "270391544": 1,
    "272910918": 1,
    "276161350": 1,
    "276625983": 1,
    "56552072": 1,
    "257404891": 1,
    "258823123": 1,
    "259859069": 1,
    "269790943": 1,
    "2014543": 1,
    "2449317": 1,
    "11518222": 1,
    "49552345": 1,
    "216553223": 1,
    "265221159": 1,
    "259129398": 1,
    "7866026": 1,
    "11243437": 1,
    "18116614": 1,
    "49865049": 1,
    "70219313": 1,
    "215998752": 1,
    "244352923": 1,
    "267740683": 1,
    "268554279": 1,
    "268723699": 1,
    "791679": 1,
    "1934251": 1,
    "18941952": 1,
    "207609497": 1,
    "700325": 1,
    "1842081": 1,
    "4300572": 1,
    "7188883": 1,
    "7846914": 1,
    "13202755": 1,
    "16900887": 1,
    "27294004": 1,
    "61897015": 1,
    "206852649": 1,
    "214429751": 1,
    "252212141": 1,
    "265609805": 1,
    "4959317": 1,
    "5803269": 1,
    "53691145": 1,
    "202558505": 1,
    "222341867": 1,
    "252542956": 1,
    "256504063": 1,
    "258331653": 1,
    "258960339": 2,
    "259089245": 1,
    "211171605": 1,
    "216284142": 1,
    "245218925": 1,
    "258841374": 1,
    "260775975": 1,
    "263829588": 1,
    "263830033": 1
  },
  "merged_dataset_groups": [
    {
      "display_name": "GSM8K",
      "normalized_name": "gsm8k",
      "name_variants": [
        "GSM8K",
        "GSM8k"
      ],
      "mention_count": 29,
      "cited_papers_count": 19,
      "topic_summary": "The GSM8K dataset is primarily used to train and evaluate large language models (LLMs) on solving and explaining algebraic word problems, emphasizing multi-step reasoning, logical deduction, and the generation of natural language rationales. It is employed to assess the models' accuracy, robustness, and generalization across diverse arithmetic problems, including fractions, decimals, and complex operations. The dataset also supports few-shot learning experiments, evaluates commonsense reasoning, and tests strategic thinking and symbolic reasoning tasks. Additionally, it is used to train process reward models that provide intermediate signals for reasoning steps, enhancing the models' planning capabilities."
    },
    {
      "display_name": "HotpotQA",
      "normalized_name": "hotpotqa",
      "name_variants": [
        "Hot-potQA",
        "HotpotQA"
      ],
      "mention_count": 23,
      "cited_papers_count": 17,
      "topic_summary": "The HotpotQA dataset is primarily used to evaluate multi-hop question answering capabilities, focusing on diverse and explainable reasoning processes. It is employed to assess models' ability to integrate information from multiple documents, solve complex mathematical problems, and perform logical reasoning tasks. The dataset supports research on planning capabilities of LLMs, particularly in complex agent environments and instruction-following tasks. It emphasizes fair comparisons and the challenges of random test example selection."
    },
    {
      "display_name": "MATH",
      "normalized_name": "math",
      "name_variants": [
        "MATH"
      ],
      "mention_count": 12,
      "cited_papers_count": 8,
      "topic_summary": "The MATH dataset is extensively used to evaluate and enhance the mathematical reasoning and problem-solving capabilities of large language models (LLMs). It focuses on multi-step reasoning, theorem proving, and logical challenges, covering a wide range of math topics from grade school to college level. The dataset is utilized to benchmark performance, assess accuracy, and measure improvements in solving complex mathematical problems, including multi-hop question answering and evidence combination. It also tests the robustness and scalability of models, highlighting both strengths and limitations in handling diverse and challenging tasks."
    },
    {
      "display_name": "Common Crawl",
      "normalized_name": "commoncrawl",
      "name_variants": [
        "Common Crawl",
        "CommonCrawl"
      ],
      "mention_count": 8,
      "cited_papers_count": 8,
      "topic_summary": "The Common Crawl dataset is primarily used for pre-training large language models, providing a vast and diverse collection of web-scraped text data. It is utilized to improve long-span denoising objectives, enhancing the models' efficiency in handling long sequences. Additionally, it is applied for training multimodal models, focusing on the understanding of instructional videos and their corresponding transcripts. This dataset enables researchers to evaluate and enhance the task planning capabilities of models like HTML-T5, comparing their performance against other models on offline task planning tasks."
    },
    {
      "display_name": "ALFRED",
      "normalized_name": "alfred",
      "name_variants": [
        "ALFRED",
        "Al-fred dataset"
      ],
      "mention_count": 7,
      "cited_papers_count": 7,
      "topic_summary": "The ALFRED dataset is primarily used to develop and evaluate AI agents capable of understanding and executing grounded instructions in embodied environments, focusing on vision-language tasks. It supports the creation of agents that can interact in complex scenarios using both text and visuals. The dataset is also used to evaluate hierarchical planning models, generate questions based on PDDL tasks, and study planning capabilities in multi-turn interactions and simulated 3D environments. It provides goals and plans for the Alfworld and AI2THOR environments, enabling researchers to assess the planning and performance of LLMs and other AI models in various tasks."
    },
    {
      "display_name": "MMLU",
      "normalized_name": "mmlu",
      "name_variants": [
        "MMLU"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The MMLU dataset is primarily used to evaluate the performance of large language models (LLMs) across a wide range of tasks, including advanced language understanding, multitask language understanding, interactive and feedback-based tasks, frame-based reasoning, software engineering benchmarks, and live coding. It is also used to assess planning capabilities, particularly in task-oriented dialogues and complex environments. The dataset supports scaling law experiments by providing a benchmark for evaluating model performance and generalization. Pre-training on MMLU enhances core agentic capabilities and improves model performance on diverse and complex tasks."
    },
    {
      "display_name": "WebShop",
      "normalized_name": "webshop",
      "name_variants": [
        "WebShop",
        "Webshop",
        "Webshop benchmark"
      ],
      "mention_count": 5,
      "cited_papers_count": 3,
      "topic_summary": "The WebShop dataset is primarily used to evaluate and train models in various interactive and embodied environments, focusing on tasks such as web navigation, question answering, and embodied household activities. It assesses models' planning capabilities, logical reasoning, and ability to follow instructions in both web and virtual home settings. The dataset supports out-of-distribution testing, multi-hop reasoning, and fact verification, often comparing different models and strategies like ReAct and Act. It is also used to train agents for tasks requiring structured knowledge and to align text with actions in virtual worlds."
    },
    {
      "display_name": "CLUTRR",
      "normalized_name": "clutrr",
      "name_variants": [
        "CLUTRR"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The CLUTRR dataset is used to evaluate the inductive and systematic reasoning capabilities of models in natural language understanding, focusing on robustness and generalization. It assesses models' ability to handle logical rules, unseen combinations, and noisy descriptions, particularly in relational reasoning tasks. The dataset supports research on various models, including ASP, MAC, BiLSTM-attention, and GSM, across clean, supporting, irrelevant, and disconnected data instances."
    },
    {
      "display_name": "iGibson",
      "normalized_name": "igibson",
      "name_variants": [
        "iGibson"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The iGibson dataset is used to create and evaluate datasets for object rearrangement preferences in 3D scenes, focusing on diverse object categories and detailed room annotations. It provides 3D environments for object placement and curation, enabling the development and testing of LLM-based approaches in both training and unseen apartment scenes."
    },
    {
      "display_name": "R2R",
      "normalized_name": "r2r",
      "name_variants": [
        "R2R",
        "R2R data"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The R2R dataset is used to train and evaluate vision-and-language navigation models, focusing on generating action sequences for navigation tasks in visually-grounded environments. It supports cross-modality matching, such as identifying landmarks from observations, and is particularly effective for end-to-end performance evaluation with Transformer models. This dataset enables researchers to assess model capabilities in interpreting and executing visually-grounded instructions in real-world settings."
    },
    {
      "display_name": "DS-1000",
      "normalized_name": "ds1000",
      "name_variants": [
        "DS-1000"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "Dataset DS-1000 is used to evaluate various capabilities of large language models (LLMs), including code generation, complex question answering, theorem-driven reasoning, and decision-making in simulated environments. It measures the reliability, naturalness, and correctness of LLM outputs, particularly in multi-turn interactions, multi-hop reasoning, and task completion. The dataset includes subsets like HumanEval and MBPP, which focus on human-like code evaluation and benchmarking performance, respectively. It also assesses LLMs in generating SQL and Bash scripts, and interacting with robot APIs."
    },
    {
      "display_name": "ALF-World",
      "normalized_name": "alfworld",
      "name_variants": [
        "ALF-World",
        "ALFWorld",
        "Alfworld"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The ALF-World dataset is primarily used to evaluate and train agents in interactive learning and decision-making tasks within embodied environments. It focuses on aligning text and physical actions, enhancing multi-hop question answering, and improving formal logic and proof construction. The dataset is also utilized to assess the performance of grounded language agents in web interaction tasks, including complex instruction following and navigation, as well as household navigation and manipulation tasks to enhance planning capabilities."
    },
    {
      "display_name": "NL2Bash",
      "normalized_name": "nl2bash",
      "name_variants": [
        "NL2Bash"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The NL2Bash dataset is primarily used for interactive coding, where it tests and evaluates models' abilities to translate natural language instructions into executable bash commands. It is also applied in embodied AI tasks, assessing models' performance in navigating and interacting with simulated environments using textual and visual inputs. The dataset highlights the need for new actions in embodied environments, such as opening, taking, moving, and travel-related actions, which are often missing from existing annotations. This enables research on interactive coding, command synthesis, and embodied task planning, focusing on the alignment of textual instructions with actions in both coding and simulated environments."
    },
    {
      "display_name": "HumanEval",
      "normalized_name": "humaneval",
      "name_variants": [
        "HumanEval"
      ],
      "mention_count": 3,
      "cited_papers_count": 2,
      "topic_summary": "The HumanEval dataset, comprising 974 short Python functions, is primarily used to evaluate the performance of large language models in generating correct Python code. It focuses on program synthesis and coding challenges, assessing the effectiveness of models like GPT-4 with and without self-reflection. The dataset also highlights the importance of external observations for complex reasoning tasks, enabling researchers to compare different approaches and baselines in solving programming problems."
    },
    {
      "display_name": "SCAN",
      "normalized_name": "scan",
      "name_variants": [
        "SCAN"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The SCAN dataset is primarily used to evaluate compositional generalization in sequence-to-sequence models, focusing on tasks such as navigation command execution, numerical reasoning, and multi-step operations. It assesses models' abilities to generalize to unseen command-action pairs and handle novel compositions, emphasizing systematic and rule-based generalization. This dataset enables researchers to test and improve models' capabilities in understanding and executing complex instructions, particularly in linguistic and navigational contexts."
    },
    {
      "display_name": "2Wiki",
      "normalized_name": "2wiki",
      "name_variants": [
        "2Wiki"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The 2Wiki dataset is primarily used to evaluate and assess the multi-hop question answering (QA) capabilities of Auto-RAG, focusing on the system's ability to reason across multiple documents or sentences. It emphasizes complex question answering tasks that require integrating information from multiple Wikipedia articles. The dataset helps researchers test the effectiveness and robustness of Auto-RAG in generating accurate answers, particularly in scenarios where multi-hop reasoning and evidence combination are necessary."
    },
    {
      "display_name": "ASDIV",
      "normalized_name": "asdiv",
      "name_variants": [
        "ASDIV",
        "Asdiv"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The ASDIV dataset is primarily used to assess the symbolic reasoning and problem-solving abilities of language models, particularly in solving arithmetic word problems. It evaluates models on a wide range of tasks, including single-variable arithmetic, multi-step reasoning, and complex problem-solving scenarios. The dataset emphasizes the complexity and variability of problems, serving as a benchmark for advanced cognitive and mathematical reasoning skills. It is also used to test models like Flan-LongT5 in few-shot and zero-shot settings, focusing on chain-of-thought reasoning and generalization."
    },
    {
      "display_name": "nuScenes",
      "normalized_name": "nuscenes",
      "name_variants": [
        "nuScenes"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The nuScenes dataset is used to evaluate and fine-tune large language models (LLMs) like GPT-Driver in the context of autonomous driving. It focuses on assessing the model's effectiveness, generalization ability, and interpretability in real-world scenarios. The dataset is also used to enhance motion planning capabilities by fine-tuning the LLM with human driving trajectories."
    },
    {
      "display_name": "SmartPlay",
      "normalized_name": "smartplay",
      "name_variants": [
        "SmartPlay"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The SmartPlay dataset is used to evaluate agent capabilities across various tasks, including Two-armed Bandits, Rock Paper Scissors, Messenger, Crafter, and Minecraft creative navigation tasks. It simplifies LLMs to control a hand-coded agent, focusing on creative tasks such as finding specific biomes in Minecraft. This dataset enables researchers to assess and enhance the performance of agents in diverse and complex environments."
    },
    {
      "display_name": "ACPBench",
      "normalized_name": "acpbench",
      "name_variants": [
        "ACPBench"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "ACPBench is used to standardize evaluation tasks and metrics for assessing the reasoning and planning capabilities of large language models across 13 domains and 22 state-of-the-art models. It generates a challenging set of questions to evaluate the reliability of these models in producing components for automated planners, particularly in complex PDDL domains. The dataset focuses on improving model performance over time and addressing poor performance in planning and reasoning tasks."
    },
    {
      "display_name": "MINT-MBPP",
      "normalized_name": "mintmbpp",
      "name_variants": [
        "MINT-MBPP"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The MINT-MBPP dataset is used to evaluate the multi-turn interaction and code generation capabilities of large language models (LLMs). It focuses on various subsets, including HumanEval and MBPP, to assess human-like code evaluation, interaction with robot APIs, and the generation of SQL and Bash scripts. The dataset tests LLMs' ability to perform complex tasks such as theorem-driven question answering, multi-hop reasoning, and solving mathematical problems using Python. It also evaluates the impact of feedback, particularly from GPT-4, on accuracy and efficiency in multi-turn interactions and tool utilization."
    },
    {
      "display_name": "MiniWoB++",
      "normalized_name": "miniwob",
      "name_variants": [
        "Mini-WoB++",
        "MiniWoB++"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "MiniWoB++ is used to evaluate and compare the performance of large language models and other systems in executing web-based tasks. It serves as a benchmark for assessing task completion, planning capabilities, and generalization to diverse web interfaces. The dataset is employed to fine-tune models like HTML-T5, compare them against reinforcement learning methods, and evaluate their adaptability across 56 web automation tasks."
    },
    {
      "display_name": "SeqMultiWoz",
      "normalized_name": "seqmultiwoz",
      "name_variants": [
        "SeqMultiWoz"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The SeqMultiWoz dataset is primarily used to build, train, and evaluate multi-domain conversational agents and models, focusing on schema-guided dialogue management and natural language understanding. It is employed to assess model performance on complex, multi-turn conversations and top-level dialogue tasks, emphasizing scalability and multi-domain capabilities. The dataset supports research into spoken language understanding, particularly in converting mixed-intent utterances into API sequences and handling specific topics. It contains 12 APIs, which aids in studying the impact of API count on model performance."
    },
    {
      "display_name": "Ego4D",
      "normalized_name": "ego4d",
      "name_variants": [
        "Ego4D"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Ego4D dataset is used to train a general-purpose visual encoder for robot perception, leveraging first-person video data to enhance representation learning. This dataset enables researchers to focus on improving the visual understanding capabilities of robots by providing extensive, real-world first-person perspective videos, which are crucial for developing robust and context-aware robotic systems."
    },
    {
      "display_name": "AddSub",
      "normalized_name": "addsub",
      "name_variants": [
        "AddSub"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The AddSub dataset is used to assess and evaluate the performance of language models in solving arithmetic word problems, particularly focusing on addition and subtraction. It emphasizes step-by-step reasoning, numerical computation, and complex problem-solving strategies. The dataset is utilized to compare zero-shot and RCI (Retrieval-Augmented Chain-of-Thought) prompting methods, examining performance improvements over standard prompting techniques. It includes a variety of problem types, from simple to multi-step, to study diverse solution methods and mathematical reasoning."
    },
    {
      "display_name": "DialogStudio",
      "normalized_name": "dialogstudio",
      "name_variants": [
        "DialogStudio"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The DialogStudio dataset is used to enhance the instruction capability of xLAM by integrating diverse instruction-tuning datasets, specifically focusing on conversational AI. This integration improves the model's ability to handle and respond to complex instructions in a conversational context, thereby advancing the development of more sophisticated conversational agents."
    },
    {
      "display_name": "E2ENLG",
      "normalized_name": "e2enlg",
      "name_variants": [
        "E2ENLG"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The E2ENLG dataset is used for few-shot learning experiments, where models are fine-tuned with varying percentages (0.1%, 0.5%, 1%, and 5%) of training instances. This approach helps researchers evaluate the effectiveness of small amounts of data in improving model performance, focusing on the impact of limited training samples on model accuracy and generalization."
    },
    {
      "display_name": "PrOntoQA",
      "normalized_name": "prontoqa",
      "name_variants": [
        "PrOntoQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "PrOntoQA is used to evaluate the reasoning capabilities of language models, particularly in formal logic and ontology-based questions. It assesses the ability of models to generate and verify logical proofs, handle first-order logic inference tasks, and perform synthetic question answering. The dataset emphasizes step-by-step and chain-of-thought reasoning, enabling researchers to systematically analyze deductive reasoning and logical inference in language models."
    },
    {
      "display_name": "MultiArith",
      "normalized_name": "multiarith",
      "name_variants": [
        "MultiArith"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MultiArith dataset is primarily used to evaluate and assess various aspects of mathematical and logical reasoning, including commonsense reasoning, strategic thinking, and multi-step problem-solving. It focuses on solving complex arithmetic and algebraic word problems, emphasizing step-by-step reasoning, solution accuracy, and the ability to generate rationales. The dataset is employed to test both single-equation and multi-equation problem-solving skills, as well as the comprehension and logical deduction required for story-based math problems. This enables researchers to assess models' capabilities in handling real-world, context-rich mathematical challenges."
    },
    {
      "display_name": "TDW-MAP",
      "normalized_name": "tdwmap",
      "name_variants": [
        "TDW-MAP"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The TDW-MAP dataset is used to evaluate multi-agent planning capabilities, focusing on decision-making, communication, and memory in complex and dynamic environments. It is applied in simulated settings to assess how agents make decisions, communicate, and retain information, enabling researchers to analyze and improve multi-agent systems in intricate scenarios."
    },
    {
      "display_name": "StrategyQA",
      "normalized_name": "strategyqa",
      "name_variants": [
        "StrategyQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The StrategyQA dataset is used to evaluate models' implicit reasoning capabilities, particularly in multi-step reasoning processes. It assesses models on various symbolic reasoning tasks, including memory, sequence reconstruction, probability, logical operations, and string manipulation. The dataset also tests models on math word problems and strategic questions, emphasizing commonsense knowledge and diverse solution methods. This enables researchers to comprehensively analyze the reasoning and problem-solving abilities of language models."
    },
    {
      "display_name": "ToolAlpaca corpus",
      "normalized_name": "toolalpaca",
      "name_variants": [
        "ToolAlpaca",
        "ToolAlpaca corpus"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The ToolAlpaca corpus is primarily used to evaluate and enhance the tool usage and reasoning capabilities of large language models (LLMs). It contains a diverse set of cases in English, ranging from 120 to 12,657, which are used to assess LLMs' performance in interacting with APIs, following instructions, and executing task-oriented dialogues. The dataset supports both training and evaluation, focusing on accuracy, generalized tool learning, and planning capabilities."
    },
    {
      "display_name": "Room-to-Room (R2R)",
      "normalized_name": "roomtoroomr2r",
      "name_variants": [
        "Room-to-Room (R2R)"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Room-to-Room (R2R) dataset is used to develop and evaluate vision-and-language navigation models, specifically focusing on interpreting visually-grounded instructions in real indoor environments. It serves as a benchmark for Vision-and-Language Navigation (VLN), enabling researchers to test and improve models' ability to follow navigation instructions in complex, real-world settings."
    },
    {
      "display_name": "Freebase",
      "normalized_name": "freebase",
      "name_variants": [
        "Freebase"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Freebase dataset is used to construct subgraphs for multi-hop question answering, focusing on extracting relevant triples within the maximum reasoning hops of question entities in WebQSP and CWQ. It is employed to improve the performance of models on multi-hop question answering tasks, specifically enhancing Hits@1 and F1 scores compared to state-of-the-art models like UniKGQA."
    },
    {
      "display_name": "CWQ",
      "normalized_name": "cwq",
      "name_variants": [
        "CWQ"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CWQ dataset is used to evaluate and enhance complex question answering capabilities, particularly focusing on reasoning over Freebase knowledge graphs and handling complex, compositional web questions. It is employed to fine-tune models like LLaMA2-Chat-7B, improving their ability to perform multi-hop reasoning and answer intricate queries from web sources. This dataset enables researchers to assess and refine the reasoning and compositional skills of language models in real-world contexts."
    },
    {
      "display_name": "API-Bank",
      "normalized_name": "apibank",
      "name_variants": [
        "API-Bank"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The API-Bank dataset is used to evaluate large language models (LLMs) on their tool usage and reasoning capabilities, particularly in interacting with APIs. It contains a large number of cases (ranging from 120 to 12,657) in English, which are used to assess generalized tool learning, accuracy, and performance on API-related tasks. The dataset is employed to compare different LLMs, such as FLAN-T5-XXL, MPT-30B, Falcon-40B, and StarCoder-15B, focusing on their effectiveness in handling tool-related questions and API interactions."
    },
    {
      "display_name": "Open Assistant crowdsourced annotated dialogue corpus",
      "normalized_name": "openassistantcrowdsourcedannotateddialogue",
      "name_variants": [
        "Open Assistant crowdsourced annotated dialogue corpus"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Open Assistant crowdsourced annotated dialogue corpus is used to enhance various capabilities in the Lemur-Chat model, including complex coding tasks, chain of thought reasoning, and handling diverse conversational scenarios. It incorporates human-written tasks, logical problem-solving, and real user interactions, improving code generation, dialogue management, and overall conversational proficiency. The dataset's annotated dialogues are crucial for both training and evaluation, enabling the model to better understand and respond to realistic user inputs."
    },
    {
      "display_name": "VirtualHome dataset",
      "normalized_name": "virtualhome",
      "name_variants": [
        "VirtualHome",
        "VirtualHome dataset"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The VirtualHome dataset is used to simulate household activities through programs in eight different household scenes, primarily to study the planning capabilities of large language models (LLMs). It compares human-annotated action plans with those generated by models like GPT-3, focusing on task execution and planning accuracy. This dataset enables researchers to evaluate and enhance the planning and execution skills of LLMs in realistic domestic settings."
    },
    {
      "display_name": "CoScript",
      "normalized_name": "coscript",
      "name_variants": [
        "CoScript"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CoScript dataset, comprising 55,000 scripts, is used to train and evaluate models for constrained language planning, particularly focusing on goal-oriented scripts. It enables researchers to study script knowledge in large language models and demonstrates the effectiveness of smaller models like T5, which can outperform larger models in specific tasks. This dataset facilitates the assessment of model performance in generating coherent and contextually appropriate scripts."
    },
    {
      "display_name": "bAbI datasets",
      "normalized_name": "babi",
      "name_variants": [
        "bAbI",
        "bAbI datasets"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The bAbI datasets are used to evaluate and compare models on various cognitive tasks, including embodied knowledge, logic reasoning, and linguistic knowledge, with a focus on assessing planning capabilities. These datasets enable researchers to test and benchmark model performance, particularly in question answering and reasoning tasks, against baselines like GPT-3 and other state-of-the-art models."
    },
    {
      "display_name": "PROOF WRITER",
      "normalized_name": "proofwriter",
      "name_variants": [
        "PROOF WRITER"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The PROOF WRITER dataset is used to test reasoning abilities in QA systems, focusing on natural language reasoning with first-order logic. It generates examples from an ontology with unique proofs, including natural language implications and abductive statements. This dataset enables researchers to evaluate the reasoning and planning capabilities of LLMs by generating and testing complex logical statements and implications."
    },
    {
      "display_name": "ActivityPrograms dataset",
      "normalized_name": "activityprograms",
      "name_variants": [
        "ActivityPrograms",
        "ActivityPrograms dataset"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The ActivityPrograms dataset is used to simulate and evaluate household activities, particularly dining tasks, in virtual environments. It provides a structured set of 12 everyday activities to assess the planning capabilities of models. The dataset enhances the realism of simulations, enabling systematic evaluations of model performance in executing complex, real-world tasks."
    },
    {
      "display_name": "ProScript",
      "normalized_name": "proscript",
      "name_variants": [
        "ProScript"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The ProScript dataset is used to collect and enhance human-annotated partial-order plans, serving as a base for deriving planning tasks focused on goal-oriented steps and temporal ordering. It supplements WikiHow data, providing rich, step-by-step instructions for training and evaluating LLMs on complex planning tasks. The dataset supports research on LLM planning capabilities by combining human and LLM annotations, assessing step dependencies, and generating natural language prompts. It includes asynchronous instances with complete time annotations, contributing to a collection of 1.6K instances for analysis."
    },
    {
      "display_name": "CommonsenseQA",
      "normalized_name": "commonsenseqa",
      "name_variants": [
        "CommonsenseQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "The CommonsenseQA dataset is primarily used to evaluate the commonsense reasoning capabilities of language models, focusing on questions that require understanding of everyday situations and knowledge. It is also utilized to assess models' abilities in solving few-hop math word problems, emphasizing multi-step reasoning, basic arithmetic, algebraic reasoning, and procedural extension capabilities. The dataset enables researchers to test both symbolic reasoning and strategic question-solving, highlighting the models' proficiency in handling complex, real-world scenarios."
    },
    {
      "display_name": "P3",
      "normalized_name": "p3",
      "name_variants": [
        "P3"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "Dataset P3 is used as an NLP task instruction dataset, reformatted for a wide range of downstream tasks using diverse human-written templates. It is employed to assess the model's capability to handle general-purpose instructions and evaluate its performance on various linguistic tasks. This dataset enables researchers to test and improve the model's versatility in handling different instructional and linguistic challenges."
    },
    {
      "display_name": "Language-Table",
      "normalized_name": "languagetable",
      "name_variants": [
        "Language Table",
        "Language-Table"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Language-Table dataset is primarily used to study multi-object tabletop pushing environments and robotic manipulation tasks, integrating language and physical actions. It is employed to simulate and evaluate complex dynamics and large language cardinality in robotic interactions. Additionally, it serves as a textual corpus to train language models, enhancing their coherence and contextual relevance. In some studies, it contributes 3.1% of the data for training or evaluation, particularly for robotic manipulation tasks."
    },
    {
      "display_name": "AQuA",
      "normalized_name": "aqua",
      "name_variants": [
        "AQuA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The AQuA dataset is primarily used to evaluate models on algebraic word problems, focusing on complex reasoning, problem-solving, and explanation capabilities. It serves as a testbed for few-shot learning, particularly with 8 shots, and is also used to assess performance in hybrid tabular and textual content, especially in financial contexts. The dataset emphasizes the integration of diverse data types and the handling of complex queries, enabling researchers to test and improve models' reasoning and calculation abilities."
    },
    {
      "display_name": "NetHack",
      "normalized_name": "nethack",
      "name_variants": [
        "NetHack"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The NetHack dataset is used to evaluate the success rate of reinforcement learning algorithms, such as BLINDER and other baselines, in generalizing to unseen tasks within the NetHack environment. This evaluation focuses on the algorithms' ability to adapt and perform in complex, dynamic scenarios, providing insights into their generalization capabilities and effectiveness in reinforcement learning settings."
    },
    {
      "display_name": "PlanBench",
      "normalized_name": "planbench",
      "name_variants": [
        "PlanBench"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The PlanBench dataset is used to evaluate large language models (LLMs) on their planning and reasoning capabilities, particularly focusing on the brittleness and unreliability of these models in handling changes. This dataset enables researchers to assess how well LLMs can reason about dynamic scenarios and plan effectively, providing insights into the limitations and robustness of current LLMs in these tasks."
    },
    {
      "display_name": "GrailQA",
      "normalized_name": "grailqa",
      "name_variants": [
        "GrailQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The GrailQA dataset is used to evaluate the long-term planning capabilities of large language models (LLMs), specifically focusing on complex question answering tasks sourced from Freebase. Researchers compile this dataset to assess how effectively LLMs can handle intricate queries, emphasizing the models' ability to plan and execute multi-step reasoning processes. This dataset enables the examination of LLMs' performance in retrieving and integrating information from structured knowledge bases."
    },
    {
      "display_name": "WikiSQL",
      "normalized_name": "wikisql",
      "name_variants": [
        "WikiSQL"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The WikiSQL dataset is used to acquire source queries and databases, enhancing the diversity of instructions and data for training models. It contributes to the development of natural language processing models by providing a rich set of SQL queries and corresponding database entries, which helps in improving model performance on query generation and understanding tasks."
    },
    {
      "display_name": "TheoremQA",
      "normalized_name": "theoremqa",
      "name_variants": [
        "TheoremQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "TheoremQA is used to evaluate the planning capabilities and reasoning skills of language models, particularly in solving complex mathematical problems and theorem proving. It focuses on logical reasoning, step-by-step solutions, and formal logic. The dataset is also employed for multi-hop question answering, interactive learning in embodied environments, and assessing knowledge problem-solving across various domains. It emphasizes diverse and explainable reasoning processes, aligning text with actions, and integrating information from multiple sources."
    },
    {
      "display_name": "SayCan dataset",
      "normalized_name": "saycan",
      "name_variants": [
        "SayCan dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SayCan dataset is used to ground language in robotic affordances, focusing on how robots interpret and execute commands in physical environments. It enables researchers to investigate the interaction between natural language processing and robotic action, specifically addressing the challenges in command interpretation and execution. This dataset facilitates the development and evaluation of models that enhance the ability of robots to understand and perform tasks based on human instructions."
    },
    {
      "display_name": "ToolACE",
      "normalized_name": "toolace",
      "name_variants": [
        "ToolACE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ToolACE dataset is used to evaluate and enhance the planning capabilities of LLMs in complex agent environments, focusing on instruction-following and task completion. It employs single-tool conversations and function-calling tasks to improve the model's ability to execute instructions effectively, particularly in diverse and complex scenarios. This dataset supports research aimed at refining LLMs' function-calling and instruction-completion abilities, enabling more efficient and accurate task execution in agent-based settings."
    },
    {
      "display_name": "LAION-400M",
      "normalized_name": "laion400m",
      "name_variants": [
        "LAION-400M"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LAION-400M dataset is used for pre-training and evaluating models in generating accurate image captions. It involves filtering and re-captioning 491 thousand image-text pairs using BLIP-2, which enhances the model's performance in caption generation. This dataset supports research focused on improving and evaluating the planning capabilities of language models through image caption tasks, ensuring diverse and accurate captions."
    },
    {
      "display_name": "Meta-World",
      "normalized_name": "metaworld",
      "name_variants": [
        "Meta-World"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Meta-World dataset is used to evaluate embodied control tasks, particularly focusing on the success rate of long-horizon tasks. Researchers employ imitation and reinforcement learning methodologies to assess performance. This dataset enables the study of complex, sequential decision-making processes in robotic control, providing a benchmark for comparing different algorithms and approaches."
    },
    {
      "display_name": "GPT-4 generated 52K English instruction-following dataset",
      "normalized_name": "gpt4generated52kenglishinstructionfollowing",
      "name_variants": [
        "GPT-4 generated 52K English instruction-following dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GPT-4 generated 52K English instruction-following dataset is used to fine-tune pre-trained LLaMA-7B models, specifically to enhance their instruction-following capabilities. This large, synthetically generated dataset improves the model's performance on English instruction-following tasks by providing additional training data. The dataset's extensive size and synthetic nature enable researchers to effectively augment the model's ability to follow complex instructions."
    },
    {
      "display_name": "EgoCOT",
      "normalized_name": "egocot",
      "name_variants": [
        "EgoCOT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The EgoCOT dataset is used to build a scalable dataset for embodied multimodal language models, specifically focusing on part-level interactions and sub-goal sequences in robotic tasks. This dataset enables researchers to develop and evaluate models that can understand and execute complex, multi-step actions in physical environments, enhancing the capabilities of robotic systems through detailed interaction data."
    },
    {
      "display_name": "DROP",
      "normalized_name": "drop",
      "name_variants": [
        "DROP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DROP dataset is used to assess models' discrete reasoning capabilities over paragraphs, particularly in question answering tasks that involve numerical reasoning and multi-step operations. It also evaluates compositional generalization in sequence-to-sequence models, testing their ability to handle unseen command-action pairs in navigation tasks. This dataset enables researchers to measure and improve model performance in complex reasoning and generalization scenarios."
    },
    {
      "display_name": "AI2 Reasoning Challenge",
      "normalized_name": "ai2reasoningchallenge",
      "name_variants": [
        "AI2 Reasoning Challenge"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AI2 Reasoning Challenge dataset is used to evaluate the performance of Auto-RAG on general reasoning tasks, particularly focusing on complex question answering and problem-solving capabilities. This dataset enables researchers to assess how well models can handle intricate reasoning problems, providing insights into their effectiveness in real-world applications."
    },
    {
      "display_name": "Natural Questions",
      "normalized_name": "naturalquestions",
      "name_variants": [
        "Natural Questions"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Natural Questions dataset is used to synthesize reasoning-based instructions for training models like Auto-RAG, specifically focusing on multi-hop question answering tasks. It assesses the model's ability to reason across multiple documents to answer complex questions, emphasizing the need for advanced reasoning capabilities in natural language processing."
    },
    {
      "display_name": "RobotHow",
      "normalized_name": "robothow",
      "name_variants": [
        "RobotHow"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RobotHow dataset is used to conduct experiments on completing daily household goals, focusing on human-generated instructions and planning capabilities. It serves as a knowledge base for common household tasks, enabling zero-shot experiments on procedural information. The dataset enhances the planning capabilities of AI models by simulating activities in the VirtualHome environment and testing the ability to understand and generate instructions without prior training."
    },
    {
      "display_name": "ConceptNet",
      "normalized_name": "conceptnet",
      "name_variants": [
        "ConceptNet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "ConceptNet is used as a graph-based knowledge base, representing concepts and commonsense relations. It supports the development of planning capabilities in language models by providing structured external knowledge. Researchers sample local subgraphs for specific entities, enhancing the model's understanding through concept nodes and their relationships. This dataset enables the integration of commonsense reasoning into AI systems, improving their ability to plan and reason."
    },
    {
      "display_name": "L-Eval",
      "normalized_name": "leval",
      "name_variants": [
        "L-Eval"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The L-Eval dataset is used for the standardized evaluation of long-context language models. It involves re-annotated data and instructions from similar public datasets to ensure high-quality evaluations. This dataset enables researchers to assess the performance and capabilities of language models in handling extended contexts, providing a benchmark for comparing different models."
    },
    {
      "display_name": "CHALET",
      "normalized_name": "chalet",
      "name_variants": [
        "CHALET"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CHALET dataset is used to train and evaluate AI agents in navigation and interaction tasks within 3D virtual indoor environments, such as houses and kitchens. It provides a platform for benchmarking task-oriented learning, emphasizing the development of planning and navigation skills in realistic, interactive scenarios. This dataset enables researchers to focus on the training and evaluation of household agents, enhancing their ability to perform complex tasks in simulated home settings."
    },
    {
      "display_name": "VRKitchen",
      "normalized_name": "vrkitchen",
      "name_variants": [
        "VRKitchen"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VRKitchen dataset is used to train and evaluate AI agents in navigation and interaction tasks within 3D virtual kitchen and household environments. It provides a platform for benchmarking and developing planning and navigation skills, focusing on task-oriented learning and realistic scenarios. This dataset enables researchers to assess agent performance in complex, interactive settings, emphasizing the development of robust planning capabilities in domestic contexts."
    },
    {
      "display_name": "Pile",
      "normalized_name": "pile",
      "name_variants": [
        "Pile"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Pile dataset is used to evaluate the performance of language models, particularly GPT-Neo and GPT-J, by measuring perplexity on a diverse text corpus. Researchers sample subsets of the test set, typically 5000 examples, to assess language modeling capabilities and ensure the preservation of generality and improved performance on downstream tasks. The dataset's diversity supports comprehensive evaluation of model performance."
    },
    {
      "display_name": "P bw 2",
      "normalized_name": "pbw2",
      "name_variants": [
        "P bw 2"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'P bw 2' dataset is used to evaluate the STRIPS-HGN network on test problems within the Blocksworld domain. It focuses on assessing planning capabilities and performance in solving specific planning tasks. This dataset enables researchers to measure the effectiveness of the STRIPS-HGN network in handling complex planning scenarios, providing insights into its strengths and limitations."
    },
    {
      "display_name": "MineDojo",
      "normalized_name": "minedojo",
      "name_variants": [
        "MineDojo"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MineDojo dataset is used to build open-ended embodied agents with internet-scale knowledge, focusing on decomposing and extracting structured actions. It specifically enhances agent capabilities by teaching them crafting and smelting recipes. This dataset enables researchers to develop agents that can perform complex tasks in dynamic environments, leveraging large-scale data to improve their decision-making and action execution."
    },
    {
      "display_name": "PEB",
      "normalized_name": "peb",
      "name_variants": [
        "PEB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PEB dataset is used to assess the planning and execution capabilities of LLM-based agents in penetration testing scenarios. It evaluates these agents across diverse targets from leading platforms, focusing on their ability to plan and execute complex tasks. This dataset enables researchers to systematically analyze and compare the performance of LLMs in cybersecurity contexts, providing insights into their effectiveness and limitations."
    },
    {
      "display_name": "RoboTHOR",
      "normalized_name": "robothor",
      "name_variants": [
        "RoboTHOR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The RoboTHOR dataset is used as a simulation environment to test and validate the planning capabilities of large language models (LLMs) in embodied AI tasks. Researchers employ the validation set to conduct experiments, assessing how effectively LLMs can plan and execute actions in simulated environments. This dataset enables the evaluation of LLMs' performance in complex, interactive settings, providing insights into their ability to handle real-world tasks."
    },
    {
      "display_name": "SingleEq",
      "normalized_name": "singleeq",
      "name_variants": [
        "SingleEq"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SingleEq dataset is used to assess models on single-equation grade-school algebra word problems, emphasizing multi-step reasoning, symbolic manipulation, and numerical accuracy. It is employed in few-shot learning experiments, testing the model's ability to generalize from limited examples. The dataset evaluates various reasoning skills, including strategic thinking, pattern recognition, and commonsense inference, using a mix of arithmetic problems, algebraic word problems, and letter-based puzzles. It focuses on tasks that require parsing, solving equations, and generating natural language rationales, often involving real-world scenarios and multiple math operations over nonnegative rational numbers."
    },
    {
      "display_name": "MAWPS",
      "normalized_name": "mawps",
      "name_variants": [
        "MAWPS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MAWPS dataset is used to evaluate Chain-of-Thought (CoT) reasoning in various types of math word problems, ranging from basic arithmetic to complex multi-step calculations. It assesses the ability to solve problems requiring logical reasoning, multi-step calculations, and everyday knowledge. The dataset supports research in strategic problem-solving and commonsense reasoning, focusing on the accuracy and coherence of reasoning processes in both simple and complex scenarios."
    },
    {
      "display_name": "Habitat-Matterport 3D",
      "normalized_name": "habitatmatterport3d",
      "name_variants": [
        "Habitat-Matterport 3D"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Habitat-Matterport 3D dataset is used to evaluate dialectic multi-robot collaboration and semantic navigation tasks with large language models. It provides realistic 3D environments to test motion planning and decision-making capabilities, enabling researchers to assess how LLMs can effectively navigate and collaborate in complex, real-world settings."
    },
    {
      "display_name": "TDW-MAT",
      "normalized_name": "tdwmat",
      "name_variants": [
        "TDW-MAT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TDW-MAT dataset is used to evaluate multi-agent planning capabilities, particularly focusing on decision-making and theory of mind in complex environments. It assesses how agents make decisions and communicate in collaborative tasks, enabling researchers to analyze and improve multi-agent systems' performance in intricate scenarios."
    },
    {
      "display_name": "MineRL",
      "normalized_name": "minerl",
      "name_variants": [
        "MineRL"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MineRL dataset is used to set up the Minecraft environment, providing a large-scale dataset of Minecraft demonstrations for training and evaluation. It serves as a benchmark for reinforcement learning algorithms, enabling researchers to evaluate agent performance in complex, interactive tasks within the Minecraft environment."
    },
    {
      "display_name": "G-PlanET",
      "normalized_name": "gplanet",
      "name_variants": [
        "G-PlanET"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The G-PlanET dataset is used to represent structured inputs for language models (LLMs) in task planning, particularly for grounding these models in 3D environments. It is utilized to enhance and evaluate the planning capabilities of LLMs in embodied tasks. The dataset includes simulation environments in a tabular format, enabling researchers to compare different methods and assess their performance in grounded planning scenarios."
    },
    {
      "display_name": "3D Scene Graphs",
      "normalized_name": "3dscenegraphs",
      "name_variants": [
        "3D Scene Graphs"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 3D Scene Graphs dataset is used to represent structured inputs for language models (LLMs) in task planning, specifically to ground these models in 3D environments. This enhances the planning capabilities of LLMs by providing them with context-rich, structured data that reflects real-world scenarios. The dataset enables researchers to explore how LLMs can better understand and interact with complex 3D scenes, improving their ability to plan and execute tasks in these environments."
    },
    {
      "display_name": "Vicuna",
      "normalized_name": "vicuna",
      "name_variants": [
        "Vicuna"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Vicuna dataset is used to evaluate Orca's performance in natural language understanding, generation, and reasoning. It assesses response quality, coherence, and problem-solving skills across diverse prompts, scenarios, and challenging academic tasks. This dataset enables researchers to comprehensively test and analyze Orca's capabilities in handling complex linguistic and cognitive challenges."
    },
    {
      "display_name": "WizardLM",
      "normalized_name": "wizardlm",
      "name_variants": [
        "WizardLM"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WizardLM dataset is used to evaluate Orca's performance across various tasks, focusing on reasoning, problem-solving, response quality, and coherence. It assesses Orca's natural language understanding and generation through diverse prompts and scenarios, emphasizing academic tasks, puzzles, and generative abilities. This dataset enables researchers to comprehensively test and analyze the model's capabilities in complex linguistic and cognitive tasks."
    },
    {
      "display_name": "EPIC-KITCHENS",
      "normalized_name": "epickitchens",
      "name_variants": [
        "EPIC-KITCHENS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The EPIC-KITCHENS dataset is used to collect video-action pairs focusing on egocentric kitchen activities. It enhances the understanding of complex, sequential actions through detailed visual data. Researchers employ this dataset to analyze and model human behavior in kitchen environments, leveraging its rich, first-person perspective videos to improve action recognition and understanding in real-world settings."
    },
    {
      "display_name": "LoCoMo",
      "normalized_name": "locomo",
      "name_variants": [
        "LoCoMo"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LoCoMo dataset is used to evaluate long-term conversational memory in LLM agents, specifically focusing on reasoning types such as single-hop, multi-hop, temporal, commonsense, world knowledge, and adversarial reasoning. This dataset enables researchers to assess the agents' ability to maintain and utilize context over extended conversations, providing insights into their reasoning capabilities and memory retention."
    },
    {
      "display_name": "4nums.com",
      "normalized_name": "4numscom",
      "name_variants": [
        "4nums.com"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 4nums.com dataset is used to evaluate the problem-solving capabilities of large language models by sourcing games ranked by relative difficulty. Specifically, it focuses on a subset of 100 games (indices 901-1000) from a larger set of 1362 games. This subset is selected to assess the models' performance on more challenging problems, enabling researchers to analyze their planning and reasoning abilities."
    },
    {
      "display_name": "TravelPlanner",
      "normalized_name": "travelplanner",
      "name_variants": [
        "TravelPlanner"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TravelPlanner dataset is used to evaluate the planning capabilities of large language models (LLMs) in U.S. domestic travel scenarios. It demonstrates the effectiveness and limitations of LLMs like GPT-4 in generating travel plans, highlighting their low success rates without pre-collected information. This dataset enables researchers to assess and improve the practical planning abilities of LLMs in real-world contexts."
    },
    {
      "display_name": "MetaQA-3hop",
      "normalized_name": "metaqa3hop",
      "name_variants": [
        "MetaQA-3hop"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MetaQA-3hop dataset is used to evaluate multi-hop reasoning capabilities in question answering systems, particularly focusing on complex queries over the Wiki-Movies knowledge graph. It enables researchers to assess how effectively these systems can integrate information from multiple sources to answer intricate questions, thereby advancing the field of natural language processing and knowledge retrieval."
    },
    {
      "display_name": "Live-CodeBench",
      "normalized_name": "livecodebench",
      "name_variants": [
        "Live-CodeBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "Live-CodeBench is used to evaluate large language models in coding tasks, ensuring a contamination-free and holistic assessment of model performance. The dataset supports research by providing a robust framework to measure the effectiveness of these models in coding scenarios, focusing on their ability to generate accurate and functional code without prior exposure to similar problems."
    },
    {
      "display_name": "CSQA",
      "normalized_name": "csqa",
      "name_variants": [
        "CSQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CSQA dataset is primarily used to assess the problem-solving and arithmetic reasoning capabilities of models, particularly in solving algebraic and arithmetic word problems. It emphasizes logical and mathematical reasoning, the role of contextual information, and the effectiveness of exemplars in improving model performance. The dataset is often used alongside GSM8K to evaluate model robustness and generalization across diverse and complex arithmetic tasks, including single-equation, addition, subtraction, and story problems."
    },
    {
      "display_name": "ELI5",
      "normalized_name": "eli5",
      "name_variants": [
        "ELI5"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ELI5 dataset is used to train models to answer complex questions in simple terms, leveraging natural language processing techniques to infer answers from web pages. This focuses on enhancing the model's ability to understand and simplify complex information, making it accessible to a broader audience. The dataset's emphasis on simplification and inference supports research in improving the clarity and comprehensibility of AI-generated responses."
    },
    {
      "display_name": "Household",
      "normalized_name": "household",
      "name_variants": [
        "Household"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Household' dataset is used to simulate and evaluate planning capabilities in a typical household environment. It focuses on generating textual observations using PDDL semantics, enabling researchers to assess the effectiveness of planning algorithms in realistic domestic settings. This dataset supports the development and testing of automated planning systems by providing a structured representation of household tasks and environments."
    },
    {
      "display_name": "SciBench",
      "normalized_name": "scibench",
      "name_variants": [
        "SciBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SciBench dataset is used to evaluate the scientific and mathematical problem-solving abilities of large language models, particularly in college-level physics and chemistry. It measures performance across a variety of math problems, assessing the models' capabilities in these domains. This dataset enables researchers to systematically test and compare the problem-solving skills of different models, providing insights into their strengths and limitations in scientific contexts."
    },
    {
      "display_name": "MiniWobChat",
      "normalized_name": "miniwobchat",
      "name_variants": [
        "MiniWobChat"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MiniWobChat dataset is used to evaluate the planning capabilities of language models across various tasks. It provides a comprehensive analysis of model performance and task difficulty, enabling researchers to assess how well language models can plan and execute complex sequences of actions. This dataset supports the examination of specific planning challenges and helps identify areas where models excel or struggle."
    },
    {
      "display_name": "ScienceQA",
      "normalized_name": "scienceqa",
      "name_variants": [
        "ScienceQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ScienceQA dataset is used to evaluate the performance of language models, particularly GPT-4 and Chameleon, on mathematical reasoning and science-related questions. It focuses on the models' abilities to understand, reason, and solve problems presented in various formats, including tabular contexts and semi-structured tasks. The dataset emphasizes logical reasoning, numerical computation, and adaptability in handling complex scientific and mathematical problems, enabling researchers to measure and compare model performance on these specific tasks."
    },
    {
      "display_name": "MNLI",
      "normalized_name": "mnli",
      "name_variants": [
        "MNLI"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MNLI dataset is used to fine-tune DeBERTa-large models for natural language inference tasks, specifically to determine semantic entailment between generated actions. This involves training models to understand and infer relationships between textual statements, enhancing their ability to process and analyze complex linguistic data."
    },
    {
      "display_name": "MINT-TheoremQA",
      "normalized_name": "minttheoremqa",
      "name_variants": [
        "MINT-TheoremQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MINT-TheoremQA dataset is used to evaluate large language models (LLMs) in theorem-driven question answering, focusing on multi-step reasoning, logical proofs, and mathematical problem-solving. It assesses LLMs' abilities in multi-turn interactions, including knowledge searches and Python calculations, and tests their reasoning capabilities across various subjects, including multi-hop reasoning over multiple paragraphs. The dataset also evaluates decision-making and planning in text-based games."
    },
    {
      "display_name": "ToolBench",
      "normalized_name": "toolbench",
      "name_variants": [
        "ToolBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ToolBench dataset is used to evaluate models' tool-use capabilities in simulated environments and to assess their reasoning abilities, particularly in solving complex arithmetic problems. It focuses on the model's interaction with tools and its performance on specific tasks, providing insights into the effectiveness of these interactions and reasoning processes."
    },
    {
      "display_name": "ToolQA",
      "normalized_name": "toolqa",
      "name_variants": [
        "ToolQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ToolQA dataset is used to evaluate the problem-solving capabilities of language models, particularly focusing on their performance on challenging questions. Researchers employ the dataset to conduct experiments on both the full set of questions and subsets of hard questions, specifically to assess the model's planning capabilities in tackling complex problems. This dataset enables detailed analysis of model performance in handling intricate tasks, providing insights into the effectiveness of language models in problem-solving scenarios."
    },
    {
      "display_name": "Movie Recommendation",
      "normalized_name": "movierecommendation",
      "name_variants": [
        "Movie Recommendation"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Movie Recommendation dataset is used to evaluate LLMCompiler's performance on embarrassingly parallel patterns, specifically focusing on achieving speedup and reducing computational costs compared to the ReAct method. This dataset enables researchers to benchmark and optimize large language model compilers for efficiency in parallel processing tasks."
    },
    {
      "display_name": "AITW",
      "normalized_name": "aitw",
      "name_variants": [
        "AITW"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AITW dataset is used to provide human demonstrations of device interactions through static images, supporting research into the planning capabilities of LLMs. This dataset enables researchers to evaluate how LLMs can understand and predict human interaction sequences with devices, enhancing their ability to generate coherent and contextually appropriate plans."
    },
    {
      "display_name": "The Stack",
      "normalized_name": "thestack",
      "name_variants": [
        "The Stack"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Stack is used to construct a code-centric corpus with 90B tokens and a 10:1 code-to-text ratio, which enhances coding ability while maintaining natural language performance. It serves as the foundation for the code component in research, providing permissively licensed source codes from GitHub to develop and evaluate models. This dataset supports the creation of large-scale, high-quality training data for improving model performance in both coding and natural language tasks."
    },
    {
      "display_name": "InterCode-CTF",
      "normalized_name": "intercodectf",
      "name_variants": [
        "InterCode-CTF"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The InterCode-CTF dataset is used to evaluate the planning capabilities of large language models (LLMs) in various environments. It assesses LLM performance in realistic web environments, physical environments, and digital environments, focusing on tasks such as autonomous agent building, navigation, embodied agent interactions, and complex interactive tasks. The dataset emphasizes fast thinking processes and real-world interaction challenges, enabling researchers to test and improve LLMs' planning and execution skills in diverse scenarios."
    },
    {
      "display_name": "Orca data",
      "normalized_name": "orca",
      "name_variants": [
        "Orca data"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Orca data dataset is used to enhance the training of language models, particularly in understanding real-world dialogue patterns through real user interactions and ChatGPT history records. It also improves code generation capabilities via progressive learning with evolving code samples and explanations. Additionally, the dataset trains models in chain-of-thought reasoning for complex human-written tasks, leveraging detailed explanation traces generated by GPT-4. These methodologies focus on refining the model's ability to handle nuanced and evolving tasks, thereby improving its performance in dialogue, code generation, and complex reasoning tasks."
    },
    {
      "display_name": "Evol-Instruct-Code-80k-v1",
      "normalized_name": "evolinstructcode80kv1",
      "name_variants": [
        "Evol-Instruct-Code-80k-v1"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Evol-Instruct-Code-80k-v1 dataset is used to train and evaluate large language models on complex coding tasks and real user interactions with ChatGPT. It focuses on enhancing code generation, instruction-following, and conversational response quality, particularly for Python programming tasks. The dataset employs evolutionary instruction tuning to improve model performance in generating correct, efficient, and contextually relevant code solutions and responses."
    },
    {
      "display_name": "MINT-Reasoning",
      "normalized_name": "mintreasoning",
      "name_variants": [
        "MINT-Reasoning"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MINT-Reasoning dataset is used to evaluate models' reasoning and code generation skills in multi-turn interactions, particularly focusing on the impact of GPT-4 feedback on accuracy, efficiency, and overall performance. This dataset enables researchers to assess how language feedback influences model behavior, providing insights into the effectiveness of interactive learning and feedback mechanisms."
    },
    {
      "display_name": "TaskLAMA",
      "normalized_name": "tasklama",
      "name_variants": [
        "TaskLAMA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TaskLAMA dataset is used for developing and evaluating models on complex task decomposition and organization. It comprises 1,612 annotated tasks, with a subset of 711 complex tasks specifically designed for model training and evaluation. This dataset enables researchers to assess how models handle intricate tasks, focusing on their ability to break down and organize steps effectively."
    },
    {
      "display_name": "TO QA",
      "normalized_name": "toqa",
      "name_variants": [
        "TO QA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TO QA dataset is used to assess the reasoning capabilities of large language models (LLMs) by generating synthetic questions. It focuses on evaluating the LLMs' ability to perform chain-of-thought reasoning, providing a structured approach to test and analyze their cognitive processes. This dataset enables researchers to systematically evaluate and compare the reasoning skills of different LLMs."
    },
    {
      "display_name": "MINT",
      "normalized_name": "mint",
      "name_variants": [
        "MINT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MINT dataset is used to evaluate the planning capabilities of large language models (LLMs) in multi-turn interactions involving tools and language feedback. It focuses on assessing LLM performance in complex scenarios, providing a structured environment to analyze how these models plan and execute tasks over multiple steps. This dataset enables researchers to test and improve the sequential reasoning and adaptive behavior of LLMs in interactive settings."
    },
    {
      "display_name": "ByteSized32",
      "normalized_name": "bytesized32",
      "name_variants": [
        "ByteSized32"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ByteSized32 dataset is used to generate task-specific world models expressed as text games, focusing on reasoning capabilities within a controlled environment. It involves 20k lines of Python code, enabling researchers to evaluate and enhance the reasoning abilities of language models in complex, text-based scenarios."
    },
    {
      "display_name": "natural language based question answering style dataset",
      "normalized_name": "naturallanguagebasedquestionansweringstyle",
      "name_variants": [
        "natural language based question answering style dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The natural language based question answering style dataset is used to evaluate large language models (LLMs) on reasoning tasks, specifically focusing on their planning capabilities. It assesses LLMs in areas such as projection, execution, planning, and goal recognition. This dataset enables researchers to systematically test and analyze the reasoning and planning abilities of LLMs through structured question-answer interactions."
    },
    {
      "display_name": "PartNet",
      "normalized_name": "partnet",
      "name_variants": [
        "PartNet"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PartNet dataset is used to generate data for microwave assets, enabling fine-grained and hierarchical part-level 3D object understanding. This supports research in the planning capabilities of large language models (LLMs), specifically by providing detailed 3D object data necessary for enhancing LLMs' ability to reason about and plan interactions with complex objects."
    },
    {
      "display_name": "MQA-2H",
      "normalized_name": "mqa2h",
      "name_variants": [
        "MQA-2H"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MQA-2H dataset is used to evaluate and test complex multi-hop questions in the MetaQA dataset, focusing on advanced reasoning and query structures within knowledge graphs. It specifically addresses two-hop and three-hop questions, enabling researchers to assess the ability of models to handle more intricate relationships and multi-relational reasoning tasks."
    },
    {
      "display_name": "WebQSP",
      "normalized_name": "webqsp",
      "name_variants": [
        "WebQSP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WebQSP dataset is used to evaluate and enhance the performance of language models and knowledge graph question answering systems. It supports multi-hop reasoning and multi-constrained questions on Freebase, as well as single-constrained 2-hop reasoning paths. Researchers use it to retrieve similar relations for LLM-generated content, compare against structured knowledge bases, and assess complex reasoning capabilities."
    },
    {
      "display_name": "competition-level mathematics problem set",
      "normalized_name": "competitionlevelmathematicsproblemset",
      "name_variants": [
        "competition-level mathematics problem set"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The competition-level mathematics problem set is used to evaluate verifiers trained on annotated intermediate solutions, specifically assessing performance improvements over verifiers trained on final solutions. This dataset enables researchers to test and enhance the accuracy and reliability of verifiers in complex mathematical contexts, focusing on the effectiveness of intermediate solution annotations."
    },
    {
      "display_name": "Geolife",
      "normalized_name": "geolife",
      "name_variants": [
        "Geolife"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Geolife dataset is used to analyze the reasoning ability and interpretability of LLM-Mob, focusing on user, location, and trajectory data. Researchers employ this dataset to evaluate the model's performance and decision-making processes, leveraging its rich spatiotemporal information to understand how the model processes and reasons about mobility patterns."
    },
    {
      "display_name": "CLEVR",
      "normalized_name": "clevr",
      "name_variants": [
        "CLEVR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CLEVR dataset is used to generate language statements for compositional language and elementary visual reasoning, and to inspire benchmark environments that integrate language understanding and control. It supports research on planning capabilities in LLMs by providing a foundation for tasks requiring both linguistic and visual reasoning."
    },
    {
      "display_name": "static dataset of 600 three to five block problems",
      "normalized_name": "static",
      "name_variants": [
        "static dataset of 600 three to five block problems"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The static dataset of 600 three to five block problems is used to evaluate large language models on planning and reasoning tasks, specifically focusing on block manipulation. It assesses the models' ability to reason about changes in block configurations, providing a benchmark for their planning capabilities. The dataset's structured nature allows researchers to systematically test and compare model performance on these specific reasoning challenges."
    },
    {
      "display_name": "Pororo-SV",
      "normalized_name": "pororosv",
      "name_variants": [
        "Pororo-SV"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Pororo-SV dataset is used to generate multi-scene videos from various inputs, including coreference-based scene prompts and lists of sentences describing events. It focuses on enhancing narrative coherence, coreference resolution, and coherent scene transitions. The dataset supports research in video generation, particularly in event captioning and scene generation, by providing structured data that facilitates the creation of narratively consistent videos."
    },
    {
      "display_name": "MSR-VTT",
      "normalized_name": "msrvtt",
      "name_variants": [
        "MSR-VTT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MSR-VTT dataset is used to evaluate video generation quality and video-text alignment, focusing on visual quality and alignment scores. It is also employed to assess the performance of GPT models on video captioning tasks, specifically measuring FID, FVD, and CLIPSIM metrics to evaluate in-context learning skills and layout generation quality."
    },
    {
      "display_name": "ActionBench-Direction",
      "normalized_name": "actionbenchdirection",
      "name_variants": [
        "ActionBench-Direction"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ActionBench-Direction dataset is utilized to evaluate open-domain video generation, focusing on a wide range of action categories and object dynamics through skill-based prompts. It enhances the assessment of directional actions in videos and bridges video and language by generating captions for diverse video content. This dataset enables comprehensive evaluation and improvement of video generation models."
    },
    {
      "display_name": "AI2-THOR",
      "normalized_name": "ai2thor",
      "name_variants": [
        "AI2-THOR"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AI2-THOR dataset is used to evaluate multi-agent task planning systems in a 3D simulation environment. It focuses on assessing the planning capabilities of large language models (LLMs) across a spectrum of tasks, from simple to complex. This dataset enables researchers to test and refine algorithms that enhance the coordination and decision-making processes of multiple agents in dynamic environments."
    },
    {
      "display_name": "AVALONBENCH",
      "normalized_name": "avalonbench",
      "name_variants": [
        "AVALONBENCH"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AVALONBENCH dataset is used to develop and evaluate advanced LLMs and multi-agent frameworks in the context of playing Resistance Avalon. It focuses on strategic decision-making and interaction, enabling researchers to assess the planning capabilities and collaborative performance of AI agents in complex, social-deduction games."
    },
    {
      "display_name": "FanLang-9",
      "normalized_name": "fanlang9",
      "name_variants": [
        "FanLang-9"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FanLang-9 dataset is used to fine-tune the ChatGLM-6B model, specifically to enhance its action prediction accuracy. This is achieved by training the model on a set of 100 test games, which helps in evaluating and improving the model's performance in predicting actions within game environments. The dataset's focus on game scenarios makes it particularly useful for refining the model's decision-making capabilities in interactive contexts."
    },
    {
      "display_name": "48 game logs",
      "normalized_name": "48gamelogs",
      "name_variants": [
        "48 game logs"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The '48 game logs' dataset is used to fine-tune a RoBERTa-like pretrained model for constructing a value network in the Werewolf game. This involves enhancing the model's language understanding and action prediction capabilities. The dataset enables researchers to improve the model's ability to predict optimal actions based on game logs, focusing on the integration of linguistic and strategic elements."
    },
    {
      "display_name": "contrastive trajectory dataset",
      "normalized_name": "contrastivetrajectory",
      "name_variants": [
        "contrastive trajectory dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The contrastive trajectory dataset is used to compute the outcome-DPO loss, providing process-level supervision in training agents. This enables agents to learn effectively from incorrect trajectories, enhancing their ability to navigate and make decisions in complex environments. The dataset's focus on incorrect trajectories is crucial for improving the robustness and adaptability of the trained agents."
    },
    {
      "display_name": "agent trajectory data",
      "normalized_name": "agenttrajectory",
      "name_variants": [
        "agent trajectory data"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'agent trajectory data' dataset is used to train and fine-tune open-source language models (LLMs) for specific agent abilities, such as reasoning. This is achieved by constructing trajectories from more advanced teacher agents like GPT-4. The dataset enables researchers to enhance the reasoning capabilities of LLMs by leveraging expert-generated sequences of actions and decisions."
    },
    {
      "display_name": "CommonGen",
      "normalized_name": "commongen",
      "name_variants": [
        "CommonGen"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CommonGen dataset is used to evaluate CIDEr and SPICE metrics in generating natural scenarios, focusing on common sense reasoning and contextualized outputs. It enables researchers to assess the performance of these metrics in creating coherent and contextually appropriate scenarios, emphasizing the importance of common sense reasoning in natural language generation tasks."
    },
    {
      "display_name": "Mind2Web",
      "normalized_name": "mind2web",
      "name_variants": [
        "Mind2Web"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Mind2Web dataset is used to build the MT-Mind2Web dataset, which contributes to the development of conversational agents for task-oriented dialogues and provides a foundation for web interaction and navigation tasks. This dataset enables researchers to enhance the capabilities of conversational agents in performing specific web-related tasks through improved dialogue management and interaction."
    },
    {
      "display_name": "WOMD",
      "normalized_name": "womd",
      "name_variants": [
        "WOMD"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WOMD dataset is used for training and testing models in planning tasks, particularly for evaluating autonomous vehicle planning capabilities. It provides a comprehensive benchmark and focuses on selected interactive scenarios to assess model performance. This dataset enables researchers to systematically evaluate and compare different planning algorithms in complex, real-world driving situations."
    },
    {
      "display_name": "Wikitext",
      "normalized_name": "wikitext",
      "name_variants": [
        "Wikitext"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Wikitext dataset is used to collect covariance statistics for optimizing factual associations in GPT models. Research focuses on the impact of sample size and precision on model performance, employing statistical methods to enhance the accuracy and reliability of language models. This dataset enables researchers to analyze and improve the factual consistency of large language models."
    },
    {
      "display_name": "MathQA",
      "normalized_name": "mathqa",
      "name_variants": [
        "MathQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MathQA dataset is used to evaluate the accuracy of large language models (LLMs) in solving math word problems. Researchers focus on operation-based formalisms to enhance the interpretability of the models' solutions. This dataset enables the assessment of LLMs' ability to understand and solve complex mathematical problems, providing insights into their reasoning capabilities."
    },
    {
      "display_name": "MixATIS",
      "normalized_name": "mixatis",
      "name_variants": [
        "MixATIS"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MixATIS dataset is primarily used to evaluate the out-of-distribution performance of API models in tool-related contexts, such as sequential question-answering. It is also utilized to build and evaluate multi-intent models, extending the capabilities of the original ATIS and SNIPS datasets for more complex natural language processing tasks. This dataset enables researchers to measure and improve the generalizability and multi-intent handling capabilities of API models."
    },
    {
      "display_name": "ToolLLM",
      "normalized_name": "toolllm",
      "name_variants": [
        "ToolLLM"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ToolLLM dataset is used to evaluate and compare the performance of large language models (LLMs) such as FLAN-T5-XXL, MPT-30B, Falcon-40B, and StarCoder-15B, focusing on tool-related and API-based tasks. It benchmarks these models by assessing their effectiveness in handling specific tasks, providing insights into their capabilities and limitations in practical applications."
    },
    {
      "display_name": "Kinetics Human Action Video Dataset",
      "normalized_name": "kineticshumanactionvideodataset",
      "name_variants": [
        "Kinetics Human Action Video Dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Kinetics Human Action Video Dataset is used to train and evaluate models on recognizing human actions in videos. It focuses on enhancing the planning capabilities of large language models to understand and predict complex human behaviors, leveraging the dataset's extensive video content to improve model performance in these tasks."
    },
    {
      "display_name": "M INE CLIP",
      "normalized_name": "mineclip",
      "name_variants": [
        "M INE CLIP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The M INE CLIP dataset is used to train policies with Proximal Policy Optimization (PPO) algorithms, specifically focusing on reward shaping to enhance planning capabilities in large language models. This dataset enables researchers to improve the strategic decision-making and planning performance of LLMs through optimized reward structures."
    },
    {
      "display_name": "Go-rilla",
      "normalized_name": "gorilla",
      "name_variants": [
        "Go-rilla"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Go-rilla dataset utilizes synthetic, single-sequence API data generated from language models, particularly from Deep Learning libraries' APIs. It is used to compare with multi-sequence REST-API data from GPT-4, focusing on the differences and similarities in the structure and functionality of these API sequences. This comparison helps researchers understand the nuances and capabilities of different API data types in the context of language model outputs."
    },
    {
      "display_name": "Unnatural Instructions",
      "normalized_name": "unnaturalinstructions",
      "name_variants": [
        "Unnatural Instructions"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Unnatural Instructions' dataset is used to evaluate the performance and behavior of large language models like GPT-4 and text-davinci-002 under non-standard conditions. It focuses on decoding model answers and assessing the quality and diversity of 68,478 synthesized samples using 3-shot in-context learning. This dataset enables researchers to test and analyze model responses to synthetic and unconventional instructions, providing insights into model robustness and adaptability."
    },
    {
      "display_name": "wikiHow",
      "normalized_name": "wikihow",
      "name_variants": [
        "wikiHow"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The wikiHow dataset is used to provide examples of instructional articles on various topics, focusing on the content and structure of how-to guides. It serves as an initial dataset for analyzing and understanding the format and language of instructional texts, enabling research into the creation and improvement of instructional materials."
    },
    {
      "display_name": "Matterport3D",
      "normalized_name": "matterport3d",
      "name_variants": [
        "Matterport3D"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Matterport3D dataset is used to evaluate multi-object and spatial goal navigation tasks within the Habitat simulator, focusing on embodied AI research. It supports methodologies that test agents' ability to navigate complex 3D environments, addressing research questions related to spatial understanding and goal-directed navigation. The dataset's rich 3D reconstructions enable realistic simulation scenarios, enhancing the robustness of embodied AI models."
    },
    {
      "display_name": "gSCAN test dataset",
      "normalized_name": "gscantest",
      "name_variants": [
        "gSCAN test dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The gSCAN test dataset is used to evaluate the systematic generalization capabilities of models, particularly in language and vision tasks. It is employed across eight different splits to assess how well models can generalize to unseen data, focusing on their ability to understand and apply learned concepts in new contexts. This dataset enables researchers to rigorously test and compare model performance in these specific areas."
    },
    {
      "display_name": "StepGame",
      "normalized_name": "stepgame",
      "name_variants": [
        "StepGame"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The StepGame dataset is used to evaluate and enhance robust multi-hop spatial reasoning in texts, specifically focusing on the planning capabilities of language models. It is employed in evaluating models through complex reasoning tasks and in pretraining SynSup to improve understanding of intricate spatial relationships."
    },
    {
      "display_name": "OlympiadBench",
      "normalized_name": "olympiadbench",
      "name_variants": [
        "OlympiadBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "OlympiadBench is used to evaluate the performance of large language models (LLMs) on complex problem-solving tasks, particularly focusing on logical reasoning, mathematical skills, and quantitative analysis. The dataset assesses the consistency and accuracy of LLMs' solutions across different training checkpoints, enabling researchers to analyze the models' reasoning capabilities and general problem-solving abilities."
    },
    {
      "display_name": "DeepscaleR-40k",
      "normalized_name": "deepscaler40k",
      "name_variants": [
        "DeepscaleR-40k"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DeepscaleR-40k dataset is used to train and fine-tune models, specifically focusing on enhancing the planning capabilities of large language models (LLMs). It involves using 4k randomly selected samples to train a model and fine-tuning the Qwen-7B-Base model with specific training settings to improve planning performance. This dataset enables researchers to evaluate and enhance the strategic and planning abilities of LLMs through targeted training methodologies."
    },
    {
      "display_name": "Wikipedia text",
      "normalized_name": "wikipediatext",
      "name_variants": [
        "Wikipedia text"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Wikipedia text dataset is used in mixtures for training or evaluation, contributing 0.5% to 3.1% of the data. It is primarily employed for language modeling and text generation tasks, and also for robotic manipulation tasks. This dataset enables researchers to enhance model performance in generating coherent text and improving robotic control through diverse textual inputs."
    },
    {
      "display_name": "GQA",
      "normalized_name": "gqa",
      "name_variants": [
        "GQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GQA dataset is used to train DriveLM-Agent by providing questions and answers about images, enhancing the model's ability to reason about visual content. This involves using the dataset's rich visual and textual data to improve the model's understanding and reasoning capabilities in visual contexts."
    },
    {
      "display_name": "DriveLM-Data",
      "normalized_name": "drivelmdata",
      "name_variants": [
        "DriveLM-Data"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DriveLM-Data dataset is used to fine-tune the BLIP-2 model for motion tasks, employing a trajectory tokenization scheme. This approach specifically enhances the model's ability to process and generate trajectories, addressing research questions related to improving motion prediction and planning capabilities in language models. The dataset's trajectory data is crucial for training and evaluating these fine-tuned models."
    },
    {
      "display_name": "OpenLane-V2",
      "normalized_name": "openlanev2",
      "name_variants": [
        "OpenLane-V2"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OpenLane-V2 dataset is used to generate Perception QAs, providing ground truth data for topology reasoning and unified 3D HD mapping. It leverages ground truth annotations to create questions about observational facets of objects within a scene, which are then used to enhance the planning capabilities of LLMs by focusing on topology reasoning and 3D mapping."
    },
    {
      "display_name": "DealOrNotDeal",
      "normalized_name": "dealornotdeal",
      "name_variants": [
        "DealOrNotDeal"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'DealOrNotDeal' dataset is used to implement and study bargaining over multiple issues, with a focus on end-to-end learning of negotiation dialogues. Researchers employ this dataset to develop and evaluate models that can engage in complex negotiations, enhancing the understanding of automated dialogue systems in multi-issue bargaining scenarios."
    },
    {
      "display_name": "GAMA-Bench",
      "normalized_name": "gamabench",
      "name_variants": [
        "GAMA-Bench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GAMA-Bench dataset is used as an implemented environment for the Public Goods Game to evaluate the decision-making and gaming abilities of LLMs in multi-agent settings. It provides a structured framework to assess how LLMs interact and make decisions in complex, multi-agent scenarios, focusing on their strategic behavior and cooperation."
    },
    {
      "display_name": "FOLIO",
      "normalized_name": "folio",
      "name_variants": [
        "FOLIO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The FOLIO dataset is used to assess and enhance LLMs' logical reasoning capabilities, particularly in handling complex logical structures such as first-order logic, quantifiers, and logical connectives. It employs a range of methodologies, including multiple-choice questions, formal proofs, and progressively challenging logical inference problems, to measure and improve LLMs' deductive, inductive, and abductive reasoning skills. This dataset is crucial for evaluating and training models to perform structured reasoning and logical deduction, thereby enhancing their planning capabilities."
    },
    {
      "display_name": "TP-RAG",
      "normalized_name": "tprag",
      "name_variants": [
        "TP-RAG"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The TP-RAG dataset is used to create a travel planning benchmark for retrieval-augmented and spatiotemporal-aware travel planning. Constructed using approximately 1 billion GPT-4 tokens, it enables researchers to evaluate and enhance the capabilities of language models in generating detailed and contextually relevant travel plans. This dataset supports the development and testing of advanced travel planning systems by providing a large-scale, high-quality resource for benchmarking and improving model performance in this specific domain."
    },
    {
      "display_name": "ActivityPrograms knowledge base",
      "normalized_name": "activityprogramsknowledgebase",
      "name_variants": [
        "ActivityPrograms knowledge base"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ActivityPrograms knowledge base is used to generate executable plans for household tasks within the VirtualHome simulation platform. It focuses on enhancing the planning capabilities of systems by providing structured data that enables the simulation of complex household activities. This dataset supports research into automated task planning and execution in virtual environments, specifically addressing how systems can effectively plan and execute sequences of actions to complete household tasks."
    },
    {
      "display_name": "KAMEL",
      "normalized_name": "kamel",
      "name_variants": [
        "KAMEL"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The KAMEL dataset is used to analyze the planning capabilities of language models by leveraging structured knowledge from Wikidata. Researchers employ relation-based question templates to assess how these models utilize and reason with structured data, focusing on their ability to plan and execute complex tasks based on relational information. This approach helps evaluate the models' understanding and application of structured knowledge in planning scenarios."
    },
    {
      "display_name": "Abstraction and Reasoning Corpus (ARC)",
      "normalized_name": "abstractionandreasoningcorpusarc",
      "name_variants": [
        "Abstraction and Reasoning Corpus (ARC)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Abstraction and Reasoning Corpus (ARC) is used to evaluate methods in few-shot visual reasoning, specifically focusing on the ability to solve complex, abstract problems with minimal examples. This dataset enables researchers to assess and improve algorithms' reasoning capabilities by providing a set of tasks that require understanding and generalization from limited data."
    },
    {
      "display_name": "NaturalCodeBench (NCB)",
      "normalized_name": "naturalcodebenchncb",
      "name_variants": [
        "NaturalCodeBench (NCB)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NaturalCodeBench (NCB) dataset is used to measure models' capacities to solve practical programming tasks, particularly focusing on real-world coding challenges. It evaluates programming problems in languages beyond Python, assessing the model's ability to generate correct code solutions. This dataset enables researchers to test and compare the performance of language models in diverse coding scenarios, providing insights into their practical utility and robustness."
    },
    {
      "display_name": "IFEval",
      "normalized_name": "ifeval",
      "name_variants": [
        "IFEval"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The IFEval dataset is used to assess the instruction-following capabilities of large language models, such as GLM-4, by evaluating their proficiency in executing complex tasks and understanding diverse instructions. This dataset enables researchers to measure and compare the performance of different models in adhering to specific instructions, thereby providing insights into their ability to follow and execute complex commands accurately."
    },
    {
      "display_name": "HumanEval-X",
      "normalized_name": "humanevalx",
      "name_variants": [
        "HumanEval-X"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The HumanEval-X dataset is used to evaluate the performance of large language models (LLMs) in solving programming problems across multiple languages. It focuses on assessing LLMs' capabilities in tackling coding challenges, providing a benchmark to measure their effectiveness and identify areas for improvement. This dataset enables researchers to systematically analyze and compare the programming skills of different LLMs."
    },
    {
      "display_name": "VirtualHome-Env",
      "normalized_name": "virtualhomeenv",
      "name_variants": [
        "VirtualHome-Env"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VirtualHome-Env dataset is used to synthesize environment-aware activities in household settings, focusing on daily activities across 7 scenes. These activities are gathered through crowdsourcing, enabling researchers to study and model natural human behaviors in realistic home environments. The dataset facilitates the development and evaluation of algorithms that can understand and predict daily activities in various household contexts."
    },
    {
      "display_name": "PRM12K",
      "normalized_name": "prm12k",
      "name_variants": [
        "PRM12K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PRM12K dataset is used to train large language models (LLMs) to enhance their planning capabilities. Specifically, it generates self-training data that focuses on improving the models' ability to perform consistent step-by-step verification. This dataset enables researchers to refine LLMs' planning processes, ensuring more reliable and structured output."
    },
    {
      "display_name": "LiLA",
      "normalized_name": "lila",
      "name_variants": [
        "LiLA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The LiLA dataset is used to unify various mathematical datasets, enabling the evaluation and enhancement of planning capabilities in large language models (LLMs) for mathematical reasoning tasks. This unification facilitates a more comprehensive assessment of LLMs' ability to plan and reason mathematically, addressing specific research questions related to improving these models' performance in complex mathematical problem-solving."
    },
    {
      "display_name": "MWP datasets",
      "normalized_name": "mwp",
      "name_variants": [
        "MWP datasets"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MWP datasets are used to evaluate the performance of models like PoT+SC on math word problems, specifically focusing on their accuracy and effectiveness in solving complex problems. These datasets enable researchers to assess how well these models can interpret and solve mathematical problems presented in natural language, providing insights into their problem-solving capabilities."
    },
    {
      "display_name": "BIRD",
      "normalized_name": "bird",
      "name_variants": [
        "BIRD"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The BIRD dataset is used to evaluate and enhance the planning capabilities of large language models (LLMs), particularly in interacting with databases. It constructs SELECT-only query instructions and collects interaction trajectories of GPT-4 with databases. The dataset generates correct trajectories for training and evaluating LLMs, focusing on their ability to plan and execute structured data interactions effectively."
    },
    {
      "display_name": "12K-episode dataset",
      "normalized_name": "12kepisode",
      "name_variants": [
        "12K-episode dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 12K-episode dataset is used to fine-tune the HTML-T5 model for web navigation tasks. It specifically supports the comparison of model performance across 56 MiniWoB++ tasks, enabling researchers to evaluate and enhance the model's capabilities in navigating and interacting with web pages. This dataset facilitates the development and assessment of web navigation skills in machine learning models."
    },
    {
      "display_name": "MINT-HotpotQA",
      "normalized_name": "minthotpotqa",
      "name_variants": [
        "MINT-HotpotQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MINT-HotpotQA dataset is used to evaluate models' ability to answer complex questions that require multi-hop reasoning, emphasizing natural language understanding. It focuses on testing the model's capacity to integrate information from multiple sources to derive answers, thereby assessing advanced reasoning skills in natural language processing tasks."
    },
    {
      "display_name": "ObstructedSuite",
      "normalized_name": "obstructedsuite",
      "name_variants": [
        "ObstructedSuite"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ObstructedSuite dataset is used to evaluate agents' planning, movement, and interaction capabilities in obstacle-filled environments. It focuses on reinforcement learning and motion planning, enabling researchers to assess how effectively agents navigate and interact within complex, obstructed settings. This dataset supports the development and testing of algorithms designed to enhance these specific capabilities."
    },
    {
      "display_name": "GSM240K",
      "normalized_name": "gsm240k",
      "name_variants": [
        "GSM240K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GSM240K dataset is used to train and enhance the performance of the SFT baseline model, specifically examining how additional training data impacts model performance. This dataset enables researchers to evaluate and optimize model improvements through extensive training, focusing on empirical performance gains rather than theoretical planning capabilities."
    },
    {
      "display_name": "MBPP",
      "normalized_name": "mbpp",
      "name_variants": [
        "MBPP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MBPP dataset is used to evaluate the effectiveness of ReVISE in enhancing reasoning capabilities within the coding domain, specifically focusing on program synthesis tasks. This involves assessing the model's ability to generate correct code solutions, thereby enabling research into improving the reasoning and synthesis capabilities of language models in programming contexts."
    },
    {
      "display_name": "Blocksworld",
      "normalized_name": "blocksworld",
      "name_variants": [
        "Blocksworld"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Blocksworld dataset is used to evaluate various cognitive and computational tasks, including question answering, logical reasoning, and problem-solving. It assesses the ability to compose sentences, solve complex arithmetic problems, and apply recursive algorithms. Specifically, it tests planning and sequential decision-making in block manipulation environments and the Tower of Hanoi puzzle, emphasizing step-by-step reasoning and strategic thinking."
    },
    {
      "display_name": "Letter Concatenation",
      "normalized_name": "letterconcatenation",
      "name_variants": [
        "Letter Concatenation"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Letter Concatenation' dataset is used to evaluate symbolic reasoning capabilities in large language models, specifically focusing on the ability to concatenate letters logically. It also assesses the effectiveness of revised examples in enhancing date understanding tasks, thereby improving the reasoning capabilities of these models. This dataset enables researchers to test and refine the logical and sequential reasoning skills of language models through targeted evaluation tasks."
    },
    {
      "display_name": "Musique",
      "normalized_name": "musique",
      "name_variants": [
        "Musique"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Musique dataset is primarily used to evaluate and improve models' abilities in complex question answering, focusing on multi-hop reasoning, implicit reasoning strategies, and step-by-step problem-solving. It assesses models on tasks such as answering strategy-based questions, solving math problems, and executing complex web interactions, using decomposed queries, supporting facts, and Wikipedia paragraphs. This dataset emphasizes the logical and inferential reasoning capabilities of models, particularly in handling implicit and multi-step reasoning processes."
    },
    {
      "display_name": "PRM800K",
      "normalized_name": "prm800k",
      "name_variants": [
        "PRM800K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PRM800K dataset is used to train and evaluate large language models (LLMs) on tasks requiring complex reasoning, including solving math problems with step-by-step solutions, answering strategy-based questions with supporting facts, and executing complex web interactions. It emphasizes multi-hop reasoning, decomposing complex questions into sub-questions, and using supporting evidence. The dataset includes natural language solution steps, formulas, and Wikipedia paragraph indices, enabling researchers to assess models' logical reasoning and problem-solving capabilities."
    },
    {
      "display_name": "Mystery Blocksworld",
      "normalized_name": "mysteryblocksworld",
      "name_variants": [
        "Mystery Blocksworld"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Mystery Blocksworld dataset is used to evaluate large language models on planning and reasoning tasks, particularly focusing on the ability to reason about change. It consists of 100 instances drawn from a domain similar to Blocksworld, enabling researchers to assess the models' planning capabilities through specific reasoning challenges."
    },
    {
      "display_name": "data scraped from 4nums.com",
      "normalized_name": "scrapedfrom4numscom",
      "name_variants": [
        "data scraped from 4nums.com"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The dataset scraped from 4nums.com is used to evaluate the problem-solving capabilities of large language models, particularly in numerical reasoning tasks and algorithmic challenges. Researchers employ this dataset to assess how well these models can handle complex mathematical problems, providing insights into their reasoning abilities and identifying areas for improvement."
    },
    {
      "display_name": "Tell Me More",
      "normalized_name": "tellmemore",
      "name_variants": [
        "Tell Me More"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'Tell Me More' dataset is used to evaluate language agents' capabilities in asking clarification and follow-up questions, enhancing user interaction by improving the understanding of ambiguous instructions. It focuses on testing agents' planning capabilities in travel-related and web-based tasks, assuming clear and explicit user instructions to assess performance without additional clarifications. This dataset enables researchers to refine language agents' interaction strategies, making them more effective and user-friendly."
    },
    {
      "display_name": "Clamber",
      "normalized_name": "clamber",
      "name_variants": [
        "Clamber"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Clamber dataset is used to evaluate language agents' abilities in performing web-based tasks, particularly in travel-related scenarios, by assessing their planning capabilities and need for clarification. It focuses on the agents' ability to ask follow-up questions to enhance user interaction and improve the clarity and effectiveness of responses to ambiguous instructions."
    },
    {
      "display_name": "Ask-before-Plan",
      "normalized_name": "askbeforeplan",
      "name_variants": [
        "Ask-before-Plan"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Ask-before-Plan dataset is used to evaluate the planning capabilities of language models, particularly in interactive travel planning scenarios. It serves as a foundational resource, providing travel planning data for both training and evaluating these models. The dataset enables researchers to assess how well language models can handle complex, real-world planning tasks through interactive dialogues."
    }
  ]
}