{
  "summary": {
    "total_unique_datasets": 86,
    "total_dataset_mentions": 160,
    "unique_dataset_names": 86,
    "extraction_successful": 393,
    "extraction_failed": 4156,
    "unique_contexts_processed": 3306,
    "total_citation_instances": 4549,
    "total_processing_time": 205.25112295150757
  },
  "datasets_sorted_by_citation_count": [
    {
      "cited_paper_id": "230799347",
      "citation_count": 0,
      "total_dataset_mentions": 10,
      "unique_datasets": [
        "GSM8K"
      ],
      "dataset_details": [
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate the multi-step reasoning and planning capabilities of LLMs in real-world scenarios. | Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks. | Used to evaluate planning capabilities of LLMs, focusing on task-oriented reasoning and problem-solving skills. | Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations. | Applied to assess the reading comprehension and multi-hop reasoning skills of LLMs in planning contexts. | Utilized to test the robustness and generalization of LLMs in handling diverse and challenging planning tasks. | Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems. | Used to evaluate the enhanced logical reasoning and planning abilities of LLMs with additional constraints. | Used to evaluate foundation models on human-centric tasks, focusing on reasoning and planning capabilities through multi-turn feedback loops. | Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions. | Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences. | Evaluates logical reasoning, focusing on the ability to solve first-order logic problems. | Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems. | Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations. | Tests advanced logical reasoning, focusing on more complex first-order logic problems. | Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains. | Repurposed to assess LLMs' ability to recognize and correct errors in medical question-answering tasks, providing insights into self-reflection and error correction. | Utilized to test the logical reasoning and inference capabilities of LLMs in various planning scenarios. | Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation. | Evaluates the ability to answer questions that require strategic reasoning and multi-step inference. | Applied to assess the ability of LLMs to handle complex workflows and procedural tasks.",
          "citing_paper_id": "277150776",
          "cited_paper_id": null,
          "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Utilized to evaluate simple math word problems, specifically targeting the ability to handle arithmetic operations and basic reasoning. | Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Employed to test commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps. | Used to evaluate implicit reasoning strategies in question answering, specifically assessing the ability to infer and apply logical reasoning.",
          "citing_paper_id": "256846992",
          "cited_paper_id": 12777818,
          "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2302.06706",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2023,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Utilized to evaluate simple math word problems, specifically targeting the ability to handle arithmetic operations and basic reasoning. | Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Employed to test commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps. | Used to evaluate implicit reasoning strategies in question answering, specifically assessing the ability to infer and apply logical reasoning.",
          "citing_paper_id": "256846992",
          "cited_paper_id": 230799347,
          "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2302.06706",
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Utilized to evaluate simple math word problems, specifically targeting the ability to handle arithmetic operations and basic reasoning. | Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills. | Employed to test commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge. | Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps. | Used to evaluate implicit reasoning strategies in question answering, specifically assessing the ability to infer and apply logical reasoning.",
          "citing_paper_id": "256846992",
          "cited_paper_id": 232223322,
          "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2302.06706",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.168",
          "citing_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "cited_paper_url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate in-context learning for solving grade school math problems, focusing on step-by-step reasoning and problem-solving strategies. | Used to assess mathematical reasoning skills with a 4-shot setting, emphasizing problem-solving and explanation generation. | Used to train and evaluate models on solving algebraic word problems and reasoning-based multiple-choice questions, focusing on the ability to generate correct answers and explanations. | Used to assess in-context learning for algebraic word problems, emphasizing the generation of rationales and solution steps. | Used to test in-context learning for arithmetic and quantitative reasoning, focusing on multi-step problem-solving and logical deduction.",
          "citing_paper_id": "276259098",
          "cited_paper_id": 12777818,
          "context_text": "The in-context learning (ICL) examples are adapted from OpenCompass (Contrib-utors, 2023) for GSM8K and MATH500, and from LLM-Reasoner (Hao et al., 2024) for AQUA.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets (GSM8K, MATH500, AQUA) used for in-context learning examples. These datasets are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2502.06813",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/39c1c4eafb2f9afd716b3f49b8f16d91356d4990",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate models' performance on competition-level math problems, emphasizing complex reasoning and problem-solving strategies. | Used to assess models' ability to solve grade-school math problems, focusing on step-by-step reasoning and problem-solving skills.",
          "citing_paper_id": "274150091",
          "cited_paper_id": 239998651,
          "context_text": "Mathematical reasoning datasets like GSM8K and MATH (Cobbe et al., 2021; Hendrycks et al., 2021) assess models’ abilities to solve grade-school and competition-level math problems, while Shi et al. (2022) explore multilingual chain-of-thought reasoning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GSM8K and MATH, which are used to assess models' mathematical reasoning capabilities. These datasets are directly relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2411.13543",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate models' performance on competition-level math problems, emphasizing complex reasoning and problem-solving strategies. | Used to assess models' ability to solve grade-school math problems, focusing on step-by-step reasoning and problem-solving skills.",
          "citing_paper_id": "274150091",
          "cited_paper_id": 252735112,
          "context_text": "Mathematical reasoning datasets like GSM8K and MATH (Cobbe et al., 2021; Hendrycks et al., 2021) assess models’ abilities to solve grade-school and competition-level math problems, while Shi et al. (2022) explore multilingual chain-of-thought reasoning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, GSM8K and MATH, which are used to assess models' mathematical reasoning capabilities. These datasets are directly relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2411.13543",
          "cited_paper_doi": "10.48550/arXiv.2210.03057",
          "citing_paper_url": "https://www.semanticscholar.org/paper/faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
          "cited_paper_url": "https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate mathematical reasoning tasks, focusing on problem-solving and planning capabilities in LLMs. | Used to assess mathematical reasoning, particularly in solving word problems that require planning and step-by-step solutions. | Used to assess models on high-school-level math problems, emphasizing more complex and challenging questions. | Used to evaluate non-calculation-intensive mathematical reasoning tasks, including Prealgebra, Geometry, and Counting & Probability, to focus on planning aspects.",
          "citing_paper_id": "273163663",
          "cited_paper_id": 232223322,
          "context_text": "For mathematical reasoning tasks, we use the full test set of GSM8K and SVAMP, and a non-calculation-intensive subset of MATH, including “Prealgebra,” “Geometry,” “Counting & Probability” since we focus on planning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating mathematical reasoning tasks, which are directly relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2409.12452",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.168",
          "citing_paper_url": "https://www.semanticscholar.org/paper/f023ce0fec7d682a8ba657f786ddbd6f6570f183",
          "cited_paper_url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate mathematical reasoning tasks, focusing on problem-solving and planning capabilities in LLMs. | Used to assess mathematical reasoning, particularly in solving word problems that require planning and step-by-step solutions. | Used to assess models on high-school-level math problems, emphasizing more complex and challenging questions. | Used to evaluate non-calculation-intensive mathematical reasoning tasks, including Prealgebra, Geometry, and Counting & Probability, to focus on planning aspects.",
          "citing_paper_id": "273163663",
          "cited_paper_id": 239998651,
          "context_text": "For mathematical reasoning tasks, we use the full test set of GSM8K and SVAMP, and a non-calculation-intensive subset of MATH, including “Prealgebra,” “Geometry,” “Counting & Probability” since we focus on planning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating mathematical reasoning tasks, which are directly relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2409.12452",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f023ce0fec7d682a8ba657f786ddbd6f6570f183",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "GSM8K",
          "dataset_description": "Used to evaluate the arithmetic reasoning capabilities of LLMs, focusing on solving math word problems with step-by-step solutions. | Used to assess the arithmetic reasoning of LLMs, specifically through multiple-choice questions that require understanding and solving mathematical problems.",
          "citing_paper_id": "276421789",
          "cited_paper_id": 239998651,
          "context_text": "We evaluate the arithmetic reasoning of LLMs, on GSM8K [Cobbe et al., 2021] and AQuA [Ling et al., 2017] benchmark.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, GSM8K and AQuA, which are used to evaluate the arithmetic reasoning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2502.12521",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c1fe3ddae2443ffd11cb62e76537af0ee207833f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "232134851",
      "citation_count": 0,
      "total_dataset_mentions": 9,
      "unique_datasets": [
        "MATH"
      ],
      "dataset_details": [
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs, focusing on multi-step reasoning and task execution. | Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks. | Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations. | Used to assess scientific reasoning capabilities, focusing on abstract and concrete reasoning problems in a variety of domains. | Used to assess the logical reasoning and inference abilities of LLMs in complex scenarios. | Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems. | Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions. | Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences. | Evaluates logical reasoning, focusing on the ability to solve first-order logic problems. | Used to evaluate the automatic planning capabilities of LLMs in various domains and scenarios. | Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems. | Used to test constraint satisfaction and arithmetic reasoning, focusing on solving mathematical puzzles with specific constraints. | Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations. | Used to assess logical reasoning, particularly in more complex and practical scenarios involving first-order logic. | Used to evaluate everyday common sense reasoning, particularly through multistep soft reasoning tasks. | Used to challenge reasoning abilities with difficult tasks, including logical, mathematical, and commonsense reasoning problems. | Tests advanced logical reasoning, focusing on more complex first-order logic problems. | Used to evaluate the chain-of-thought reasoning capabilities of LLMs through multistep soft reasoning tasks. | Used to assess the robustness and generalization of LLMs in a variety of challenging reasoning tasks. | Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains. | Used to evaluate logical reasoning skills, specifically first-order logic inference and problem-solving tasks. | Used to test the multi-step reasoning and planning capabilities of LLMs in natural language understanding tasks. | Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation. | Used to evaluate the ability of LLMs to emulate tools and perform complex tasks requiring planning. | Evaluates the ability to answer questions that require strategic reasoning and multi-step inference. | Used to evaluate the planning and logical reasoning capabilities of LLMs, with a focus on progressive complexity.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 264439655,
          "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2310.16049",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/743ef29a9406c44c835684c7755d423d6ca0b663",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks. | Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations. | Used to assess scientific reasoning capabilities, focusing on abstract and concrete reasoning problems in a variety of domains. | Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems. | Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions. | Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences. | Evaluates logical reasoning, focusing on the ability to solve first-order logic problems. | Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems. | Used to test constraint satisfaction and arithmetic reasoning, focusing on solving mathematical puzzles with specific constraints. | Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations. | Used to assess logical reasoning, particularly in more complex and practical scenarios involving first-order logic. | Used to evaluate everyday common sense reasoning, particularly through multistep soft reasoning tasks. | Used to challenge reasoning abilities with difficult tasks, including logical, mathematical, and commonsense reasoning problems. | Tests advanced logical reasoning, focusing on more complex first-order logic problems. | Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains. | Used to evaluate logical reasoning skills, specifically first-order logic inference and problem-solving tasks. | Used to evaluate planning capabilities of LLMs, focusing on automated planning tasks and benchmarking performance across various planning scenarios. | Used to assess the planning abilities of LLMs, specifically targeting complex planning tasks and evaluating the effectiveness of chain-of-thought reasoning. | Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation. | Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 252917648,
          "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2210.09261",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks. | Used to evaluate multi-step reasoning and problem-solving skills in algebraic word problems, focusing on the ability to generate and follow logical rationales. | Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations. | Applied to assess complex question answering, emphasizing multi-hop reasoning and the ability to integrate information from multiple documents. | Used to evaluate multi-hop question answering, requiring the model to integrate information from multiple paragraphs to answer questions. | Used to assess mathematical reasoning, particularly in solving complex math problems and providing detailed explanations. | Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems. | Used to assess the ability to answer questions that require strategic thinking and multi-step reasoning processes. | Used to test mathematical reasoning through multiple-choice questions, emphasizing problem-solving strategies and logical steps. | Utilized to test scientific reasoning and commonsense knowledge, specifically designed to challenge AI systems with questions that require deep understanding. | Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions. | Employed to evaluate strategic reasoning and planning, focusing on the ability to solve problems that require sequential decision-making and long-term planning. | Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences. | Evaluates logical reasoning, focusing on the ability to solve first-order logic problems. | Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems. | Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations. | Tests advanced logical reasoning, focusing on more complex first-order logic problems. | Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains. | Used to assess numerical reasoning and problem-solving in grade school math problems, emphasizing the ability to handle multi-step calculations and logical sequences. | Used to evaluate reading comprehension and multi-hop reasoning, focusing on identifying correct answers from multiple-choice options based on given passages. | Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation. | Applied to evaluate advanced mathematical reasoning and problem-solving, focusing on complex problems that require deep understanding of mathematical concepts and principles. | Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 12777818,
          "context_text": "…these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)),…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used to evaluate the planning capabilities of LLMs in various domains, including mathematical reasoning and multi-hop question answering.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.18653/v1/P17-1015",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to compare token usage between MCTS and PGTS methods, focusing on efficiency in numerical reasoning tasks. | Used to evaluate PGTS's improvement over CoT in numerical reasoning tasks, specifically measuring accuracy in generating high-quality reasoning chains.",
          "citing_paper_id": "276259098",
          "cited_paper_id": 247595263,
          "context_text": "For instance, on the MATH dataset, PGTS improves accuracy from CoT’s 34.40% to 41.40% in the 8B setting, illustrating its ability to explore high-quality reasoning chains.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MATH dataset, which is used to evaluate the performance of PGTS compared to CoT in numerical reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2502.06813",
          "cited_paper_doi": "10.48550/arXiv.2203.11171",
          "citing_paper_url": "https://www.semanticscholar.org/paper/39c1c4eafb2f9afd716b3f49b8f16d91356d4990",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to collect execution trajectories during training, specifically filtering out incorrect solution paths to improve model performance. | Used as a training corpus to develop models for API execution, focusing on real-world usage patterns and trajectories. | Used to collect trajectories from GPT-4, focusing on mathematical problem-solving skills and planning capabilities. | Used to collect trajectories from ChatGPT, focusing on complex arithmetic and reasoning tasks to evaluate planning capabilities.",
          "citing_paper_id": "266999372",
          "cited_paper_id": null,
          "context_text": "5-turbo-1106 ( OpenAI, 2022) and gpt-4 (OpenAI, 2023b) to collect execution trajectories in the training set of MATH and GSM8K and filter out the trajectories that do not lead to the correct final answer.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the use of MATH and GSM8K datasets for collecting execution trajectories and filtering incorrect ones. These datasets are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2401.07324",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ff61aef2fef3a235bfaa123158a990c4f5f27d1a",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to compare token usage between MCTS and PGTS methods, focusing on efficiency in numerical reasoning tasks. | Used to evaluate PGTS's improvement over CoT in numerical reasoning tasks, specifically measuring accuracy in generating high-quality reasoning chains.",
          "citing_paper_id": "276259098",
          "cited_paper_id": 253801709,
          "context_text": "For instance, on the MATH dataset, PGTS improves accuracy from CoT’s 34.40% to 41.40% in the 8B setting, illustrating its ability to explore high-quality reasoning chains.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the MATH dataset, which is used to evaluate the performance of PGTS compared to CoT in numerical reasoning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2502.06813",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/39c1c4eafb2f9afd716b3f49b8f16d91356d4990",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Applied to assess code generation and execution skills, specifically evaluating the ability to solve programming challenges in a live coding environment. | Used to evaluate mathematical problem-solving capabilities, focusing on complex algebraic and geometric problems in large language models. | Used to evaluate large language models on mathematical problem-solving, focusing on the ability to reason through complex equations and problems.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 221516475,
          "context_text": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MATH' as a benchmark for evaluating LLMs on mathematical problem-solving. The cited paper 'Measuring Mathematical Problem Solving With the MATH Dataset' confirms that MATH is a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/814a4f680b9ba6baba23b93499f4b48af1a27678",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Applied to assess code generation and execution skills, specifically evaluating the ability to solve programming challenges in a live coding environment. | Used to evaluate mathematical problem-solving capabilities, focusing on complex algebraic and geometric problems in large language models. | Used to evaluate large language models on mathematical problem-solving, focusing on the ability to reason through complex equations and problems.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 232134851,
          "context_text": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MATH' as a benchmark for evaluating LLMs on mathematical problem-solving. The cited paper 'Measuring Mathematical Problem Solving With the MATH Dataset' confirms that MATH is a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "MATH",
          "dataset_description": "Used to evaluate large language models on mathematical problem-solving, focusing on the ability to reason through complex equations and problems.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 270380229,
          "context_text": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MATH' as a benchmark for evaluating LLMs on mathematical problem-solving. The cited paper 'Measuring Mathematical Problem Solving With the MATH Dataset' confirms that MATH is a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2406.07381",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9445a7fcf495ba13169533d591cefaddadaf6f43",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "258833055",
      "citation_count": 0,
      "total_dataset_mentions": 9,
      "unique_datasets": [
        "Hot-potQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used to evaluate multi-hop reasoning capabilities in LLMs, focusing on diverse and explainable question answering tasks.",
          "citing_paper_id": "274822434",
          "cited_paper_id": 52822214,
          "context_text": "• HotpotQA [Yang et al., 2018].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "HotpotQA is a specific dataset designed for multi-hop question answering, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2412.13520",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5c6805afcb85167758716152a54cc7bb110fc78e",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Repurposed to assess LLMs' ability to recognize and correct errors in medical question-answering tasks, providing insights into self-reflection and error correction. | Used to evaluate foundation models on human-centric tasks, focusing on reasoning and planning capabilities through multi-turn feedback loops. | Used to evaluate planning capabilities in interactive environments, focusing on aligning text and embodied actions in multi-turn feedback loops. | Used to track success rates in complex question-answering tasks, evaluating the performance of language agents in understanding and reasoning over multiple documents. | Used to evaluate interactive learning in embodied environments, assessing the ability of agents to align text instructions with physical actions in a simulated world. | Used to track success rates in complex question-answering tasks, focusing on the ability of language agents to reason and retrieve information effectively. | Employed to evaluate interactive learning in embodied environments, assessing the alignment between textual instructions and physical actions in a simulated world.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 222208810,
          "context_text": "For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and efficiency.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'HotPotQA' and 'ALFWorld', both of which are specific datasets or environments used for evaluating models. However, 'Reflexion' and 'RAISE' are methods or frameworks, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used to evaluate interactive learning in embodied environments, assessing the ability of agents to align text instructions with physical actions in a simulated world. | Used to track success rates in complex question-answering tasks, focusing on the ability of language agents to reason and retrieve information effectively. | Employed to evaluate interactive learning in embodied environments, assessing the alignment between textual instructions and physical actions in a simulated world. | Used to track success rates in complex question-answering tasks, evaluating the performance of language agents in understanding and reasoning over multiple documents.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 258833055,
          "context_text": "For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and efficiency.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'HotPotQA' and 'ALFWorld', both of which are specific datasets or environments used for evaluating models. However, 'Reflexion' and 'RAISE' are methods or frameworks, not datasets.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used for sequential decision-making tasks, involving interaction with and feedback from the environment to assess the planning capabilities of LLMs. | Used to evaluate multi-step reasoning capabilities in question answering, focusing on diverse and explainable multi-hop questions. | Used for multi-step reasoning tasks, focusing on diverse, explainable multi-hop question answering to evaluate the planning capabilities of LLMs.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 52822214,
          "context_text": "…agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' as an example of a dataset used for multi-step reasoning tasks. The cited paper title confirms it is a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used for sequential decision-making tasks, involving interaction with and feedback from the environment to assess the planning capabilities of LLMs. | Used to evaluate multi-step reasoning capabilities in language models, focusing on complex question answering that requires multiple pieces of evidence. | Used for multi-step reasoning tasks, focusing on diverse, explainable multi-hop question answering to evaluate the planning capabilities of LLMs.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 222208810,
          "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve interaction with and feedback from the environment.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used for sequential decision-making tasks to assess the capabilities of agent frameworks in interactive environments. | Used for multi-step reasoning tasks to evaluate the performance of agent frameworks in complex question-answering scenarios.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 252762395,
          "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively. These are clearly identifiable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Applied for instruction fine-tuning to align with complex agent environments, providing diverse and high-quality instruction-completion data. | Used for instruction fine-tuning to improve Hephaestus's function calling capabilities, enhancing its performance in complex agent environments. | Used to evaluate LLM agents in six distinct environments, focusing on multi-turn, open-ended generation tasks to assess planning capabilities. | Utilized for instruction fine-tuning to improve function calling in LLMs, enhancing interaction with complex agent environments. | Used for instruction fine-tuning to enhance Hephaestus's alignment with complex agent environments, focusing on high-quality instruction-completion tasks. | Used for multi-step reasoning tasks to evaluate the performance of agent frameworks in complex question-answering scenarios. | Used for instruction fine-tuning to enhance instruction-following capabilities, focusing on high-quality instruction-completion pairs. | Used for instruction fine-tuning to refine Hephaestus's planning and execution capabilities in complex agent environments, focusing on high-quality instruction-completion tasks. | Used for sequential decision-making tasks to assess the capabilities of agent frameworks in interactive environments. | Used to fine-tune models for enhanced instruction-following capabilities, focusing on real-world conversations from 70K user data.",
          "citing_paper_id": "276250494",
          "cited_paper_id": null,
          "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively. These are clearly identifiable datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used to evaluate multi-hop reasoning capabilities, specifically requiring the integration of information from multiple documents to answer questions.",
          "citing_paper_id": "270216385",
          "cited_paper_id": 52822214,
          "context_text": "HotpotQA (Yang et al., 2018) is requires finding and reasoning over multiple supporting documents to formulate responses.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions HotpotQA as a dataset requiring reasoning over multiple documents, which aligns with the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2406.00936",
          "cited_paper_doi": "10.18653/v1/D18-1259",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4906654ecb2d42cfee13f46bebe2ec02bb6442b1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Hot-potQA",
          "dataset_description": "Used to assess KNOW A GENT's planning capabilities in an interactive, embodied environment, emphasizing alignment between text and actions. | Used to assess KNOW A GENT's capabilities in interactive learning within embodied environments, emphasizing alignment between text and physical actions. | Used to evaluate KNOW A GENT on complex question-answering tasks, focusing on multi-hop reasoning and evidence retrieval. | Used to assess KNOWAGENT's capabilities in aligning text and embodied environments for interactive learning, showing strong performance. | Used to evaluate KNOW A GENT's performance on complex question answering, focusing on multi-hop reasoning and evidence retrieval. | Used to evaluate KNOWAGENT's performance on complex question answering, demonstrating comparable or superior results to existing baselines.",
          "citing_paper_id": "268248897",
          "cited_paper_id": 222208810,
          "context_text": "We evaluate K NOW A GENT on HotpotQA (Yang et al., 2018) and ALF-World (Shridhar et al., 2021).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, HotpotQA and ALF-World, which are used to evaluate the performance of KNOW A GENT. HotpotQA is a question-answering dataset, and ALF-World is an environment for interactive learning.",
          "citing_paper_doi": "10.48550/arXiv.2403.03101",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/95dad62c52800600e571b4197314578fd441ca28",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "259262186",
      "citation_count": 0,
      "total_dataset_mentions": 6,
      "unique_datasets": [
        "ALFWorld"
      ],
      "dataset_details": [
        {
          "dataset_name": "ALFWorld",
          "dataset_description": "Used to test code generation and problem-solving in programming, emphasizing practical coding tasks. | Used to evaluate the model's performance in interactive learning environments, focusing on accuracy improvements in embodied text-based tasks. | Used to evaluate code generation and execution, focusing on functional correctness and programming challenges. | Used to evaluate steps of the Llama3-8B model, focusing on problem-solving in a text-based environment. | Used to assess advanced mathematical reasoning, covering a range of topics and problem types. | Used to assess planning and decision-making in agent tasks, focusing on closed-loop interactions and problem-solving. | Used to evaluate planning and interaction in text-based environments, focusing on embodied learning and task completion. | Used to evaluate trajectories of the 5-Turbo agent, focusing on interactive learning in embodied environments.",
          "citing_paper_id": "273507611",
          "cited_paper_id": 222208810,
          "context_text": "For example, without using any additional supervision, we achieve a 7.2% improvement on GSM8K (Cobbe et al., 2021) and a 25.3% improvement on AlfWorld (Shridhar et al., 2021) accuracy over ReAct (Yao et al., 2022).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific datasets, GSM8K and AlfWorld, which are used to measure the performance improvements of the model being discussed.",
          "citing_paper_doi": "10.48550/arXiv.2410.17195",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/909efa87bc89b95644935c34bb23c3e7b7563137",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ALFWorld",
          "dataset_description": "Used to validate the SLINVIT algorithm in a planning benchmark, focusing on evaluating planning capabilities in a controlled environment. | Used to validate the SLINVIT algorithm in an interactive coding environment, focusing on standardizing and benchmarking with execution feedback. | Used to validate the SLINVIT algorithm in an embodied environment, focusing on aligning text and interactive learning scenarios.",
          "citing_paper_id": "267938891",
          "cited_paper_id": 222208810,
          "context_text": "We further present a practical algorithm called SLINVIT and empirically validate it in various benchmarks, including ALFWorld (Shridhar et al., 2020), the interactive coding environment InterCode (Yang et al., 2023a), and the planning benchmark BlocksWorld (Valmeekam et al., 2023b).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three benchmarks: ALFWorld, InterCode, and BlocksWorld. ALFWorld and InterCode are specific environments used for empirical validation, while BlocksWorld is a planning benchmark. These are used to validate the SLINVIT algorithm.",
          "citing_paper_doi": "10.48550/arXiv.2402.16181",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/b42713664a72410307839fe44ec51aef8d69c943",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ALFWorld",
          "dataset_description": "Used to validate the SLINVIT algorithm in a planning benchmark, focusing on evaluating planning capabilities in a controlled environment. | Used to validate the SLINVIT algorithm in an embodied environment, focusing on aligning text and interactive learning scenarios. | Used to investigate the sample efficiency of the proposed algorithm in an interactive coding environment with execution feedback, focusing on performance and learning dynamics. | Used to evaluate the planning abilities of large language models using SQL commands as action spaces, focusing on interactive coding tasks with execution feedback. | Used to validate the SLINVIT algorithm in an interactive coding environment, focusing on standardizing and benchmarking with execution feedback. | Used to evaluate the planning abilities of large language models using Bash commands as action spaces, focusing on interactive coding tasks with execution feedback.",
          "citing_paper_id": "267938891",
          "cited_paper_id": 259262186,
          "context_text": "We further present a practical algorithm called SLINVIT and empirically validate it in various benchmarks, including ALFWorld (Shridhar et al., 2020), the interactive coding environment InterCode (Yang et al., 2023a), and the planning benchmark BlocksWorld (Valmeekam et al., 2023b).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three benchmarks: ALFWorld, InterCode, and BlocksWorld. ALFWorld and InterCode are specific environments used for empirical validation, while BlocksWorld is a planning benchmark. These are used to validate the SLINVIT algorithm.",
          "citing_paper_doi": "10.48550/arXiv.2402.16181",
          "cited_paper_doi": "10.48550/arXiv.2306.14898",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b42713664a72410307839fe44ec51aef8d69c943",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f94c040b02bdd6cf1b85f374e3912630c66861c3",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "ALFWorld",
          "dataset_description": "Used to evaluate sequential decision-making in a text-based virtual household environment, focusing on six distinct task types to assess planning capabilities.",
          "citing_paper_id": "273163663",
          "cited_paper_id": 222208810,
          "context_text": "We use one benchmark to evaluate the performance in sequential decision-making scenarios: ALFWorld (Shridhar et al., 2020), a text-based virtual household environment comprising six distinct task types.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "ALFWorld is mentioned as a benchmark for evaluating sequential decision-making scenarios, which aligns with the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2409.12452",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f023ce0fec7d682a8ba657f786ddbd6f6570f183",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "ALFWorld",
          "dataset_description": "Used to fine-tune GPT-2 for robotics planning, demonstrating its effectiveness in inferring detailed plans from high-level instructions.",
          "citing_paper_id": "276409203",
          "cited_paper_id": 160025533,
          "context_text": "For example, Jansen (2020) and Chalvatzaki et al. (2023) fine-tuned GPT-2 (Radford et al., 2019) on ALF-World, demonstrating its effectiveness in robotics planning.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ALF-World' as a dataset used for fine-tuning GPT-2 to demonstrate its effectiveness in robotics planning. The dataset is specific and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2502.11221",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/06552667bd28b30a314de7caefda0e2b8226fab9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "ALFWorld",
          "dataset_description": "Used to fine-tune GPT-2 for robotics planning, demonstrating its effectiveness in inferring detailed plans from high-level instructions.",
          "citing_paper_id": "276409203",
          "cited_paper_id": 222066988,
          "context_text": "For example, Jansen (2020) and Chalvatzaki et al. (2023) fine-tuned GPT-2 (Radford et al., 2019) on ALF-World, demonstrating its effectiveness in robotics planning.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ALF-World' as a dataset used for fine-tuning GPT-2 to demonstrate its effectiveness in robotics planning. The dataset is specific and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2502.11221",
          "cited_paper_doi": "10.18653/v1/2020.findings-emnlp.395",
          "citing_paper_url": "https://www.semanticscholar.org/paper/06552667bd28b30a314de7caefda0e2b8226fab9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0cd0c3c76578deb7dc2c779256cd1b1f48ea2567",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "173188048",
      "citation_count": 0,
      "total_dataset_mentions": 5,
      "unique_datasets": [
        "MathQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "MathQA",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 252383606,
          "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2209.09513",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "MathQA",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 274656164,
          "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2412.09560",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/15c63a1c79c37549979cf8997d01dab226b67bd7",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "MathQA",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 1553193,
          "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.18653/v1/W17-4413",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/932a5de79d8a8ebb75ea0c43493450fd9922e738",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        },
        {
          "dataset_name": "MathQA",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 3922816,
          "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88bb0a28bb58d847183ec505dda89b63771bb495",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "MathQA",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 173188048,
          "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.18653/v1/N19-1245",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/eef7cfe8267954adbb4675576072a1d80ca7a3a8",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "6866988",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "RedPajama"
      ],
      "dataset_details": [
        {
          "dataset_name": "RedPajama",
          "dataset_description": "Used to train and evaluate LLMs in the context of materials science, focusing on the integration of LLMs with tools and databases to enhance materials discovery and evaluation. | Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text. | Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset. | Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content.",
          "citing_paper_id": "280010642",
          "cited_paper_id": null,
          "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        },
        {
          "dataset_name": "RedPajama",
          "dataset_description": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text. | Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset. | Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 6866988,
          "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.1109/ICCV.2015.11",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045",
          "citing_paper_year": 2025,
          "cited_paper_year": 2015
        },
        {
          "dataset_name": "RedPajama",
          "dataset_description": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text. | Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset. | Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 16299141,
          "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd",
          "citing_paper_year": 2025,
          "cited_paper_year": 2016
        },
        {
          "dataset_name": "RedPajama",
          "dataset_description": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text. | Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset. | Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 274140947,
          "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2411.12372",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/fe60274074830556a57ddab2a857adf47e79e57f",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "247939706",
      "citation_count": 0,
      "total_dataset_mentions": 4,
      "unique_datasets": [
        "ALFRED dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "ALFRED dataset",
          "dataset_description": "Used to evaluate planning capabilities in a controlled environment, focusing on behavior execution and task completion in simulated settings. | Used to extract instance-level labels of objects observed in the environment, supporting the development of embodied agents capable of interpreting grounded instructions for everyday tasks. | Used to assess the interpretation of grounded instructions for everyday tasks, emphasizing the ability to understand and execute complex, real-world instructions.",
          "citing_paper_id": "272693849",
          "cited_paper_id": 208617407,
          "context_text": "For ALFRED, we utilize tools from ALFWORLD, which include interactive TextWorld environments [7] that mirror embodied worlds in the ALFRED dataset [23], to extract instance-level labels of objects observed in the image directly in front of the robot in the environment.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the ALFRED dataset, which is used to extract instance-level labels of objects observed in the environment. The ALFRED dataset is relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.1145/3664647.3680661",
          "cited_paper_doi": "10.1109/cvpr42600.2020.01075",
          "citing_paper_url": "https://www.semanticscholar.org/paper/fc409c663357758248eea787afd1c7809f30c6f3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4cf4246f3882aa6337e9c05d5675a3b8463a32e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        },
        {
          "dataset_name": "ALFRED dataset",
          "dataset_description": "Used to extract instance-level labels of objects observed in the environment, supporting the development of embodied agents capable of interpreting grounded instructions for everyday tasks.",
          "citing_paper_id": "272693849",
          "cited_paper_id": null,
          "context_text": "For ALFRED, we utilize tools from ALFWORLD, which include interactive TextWorld environments [7] that mirror embodied worlds in the ALFRED dataset [23], to extract instance-level labels of objects observed in the image directly in front of the robot in the environment.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ALFRED dataset, which is used to extract instance-level labels of objects observed in the environment. The ALFRED dataset is relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.1145/3664647.3680661",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fc409c663357758248eea787afd1c7809f30c6f3",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        },
        {
          "dataset_name": "ALFRED dataset",
          "dataset_description": "Used to evaluate and compare the success rates of state-of-the-art methods with the proposed method in the context of grounding language in robotic affordances.",
          "citing_paper_id": "272693849",
          "cited_paper_id": 247939706,
          "context_text": "We first evaluate the ALFRED dataset to compare the success rates (SR) of state-of-the-art methods [2, 3, 20, 27, 28, 39] with our method.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ALFRED dataset, which is a specific, verifiable dataset used for evaluating success rates of methods in robotic affordances.",
          "citing_paper_doi": "10.1145/3664647.3680661",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/fc409c663357758248eea787afd1c7809f30c6f3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "ALFRED dataset",
          "dataset_description": "Used to derive ALFRED-LTL, focusing on household task instructions executed in a simulated environment to study planning capabilities of LLMs.",
          "citing_paper_id": "280150778",
          "cited_paper_id": 208617407,
          "context_text": "ALFRED-LTL is derived from the ALFRED dataset [17], which consists of household task instructions executed in a simulated environment.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the ALFRED dataset, which is a specific, verifiable dataset used for interpreting grounded instructions for everyday tasks in a simulated environment.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/cvpr42600.2020.01075",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb8b72f0b0301a98c01442d827bce82a76a15e65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4cf4246f3882aa6337e9c05d5675a3b8463a32e",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "52897360",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "MultiWOZ"
      ],
      "dataset_details": [
        {
          "dataset_name": "MultiWOZ",
          "dataset_description": "Used to verify agents' ability to produce accurate, executable scientific code, focusing on research coding tasks curated by scientists. | Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows. | Evaluates agents on setting up and executing tasks from research repositories, assessing their capability to handle scientific workflows. | Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 271270048,
          "context_text": "0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) (§3.4) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation (§5)",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and frameworks, but only MultiWOZ and SciCode are clearly identified as datasets in the cited paper titles.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2407.13168",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/5bf3ea7b0825424c3c01e48e8fcd8215c950677e",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "MultiWOZ",
          "dataset_description": "Applied to assess the performance of conversational agents in handling complex calendar and scheduling tasks, emphasizing natural language processing and dialogue management. | Used to train and evaluate dialogue systems, focusing on multi-domain task-oriented conversations and natural language understanding. | Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows. | Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 275757481,
          "context_text": "…MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al.,…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several named resources, but most are models or benchmarks. Only 'MultiWOZ' and 'SMCalFlow' are datasets used for training and evaluation in dialogue systems.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2501.11067",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7f1bb453aab994967a93ac7a308f5fefb74d8b95",
          "citing_paper_year": 2025,
          "cited_paper_year": 2025
        },
        {
          "dataset_name": "MultiWOZ",
          "dataset_description": "Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows. | Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding. | Used as a benchmark for task-oriented dialogue systems, specifically for evaluating multi-domain dialogue management and natural language understanding.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 52897360,
          "context_text": "Additional examples of crowdsourced task-oriented dialogue benchmarks are MultiWOZ (Budzianowski et al., 2018) and SMCalFlow (Andreas et al., 2020).",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions MultiWOZ and SMCalFlow as examples of crowdsourced task-oriented dialogue benchmarks. MultiWOZ is a specific dataset, while SMCalFlow is excluded as it is not clearly identified as a dataset in the context.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.18653/v1/D18-1547",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4a5503783487eba5c5e34b1d02c09016b244b1d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "232134851",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Game of 24"
      ],
      "dataset_details": [
        {
          "dataset_name": "Game of 24",
          "dataset_description": "Used to evaluate RFF's performance in solving arithmetic problems, focusing on accuracy improvements over baseline methods.",
          "citing_paper_id": "279154984",
          "cited_paper_id": 220047831,
          "context_text": "We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions specific datasets used for evaluating RFF, which are clearly named and relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.03673",
          "cited_paper_doi": "10.18653/v1/2020.acl-main.92",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e18698918e315e6714d4f864fb3347cdf27fdb43",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f13e41d24e5d0a68ca662c1b49de398a6fb68251",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        },
        {
          "dataset_name": "Game of 24",
          "dataset_description": "Used to evaluate performance on simple arithmetic problems, comparing the method's accuracy against other approaches. | Used to evaluate the ability of LLMs to solve grade school math problems, focusing on multi-step reasoning and arithmetic operations. | Used to assess the performance of Chain-of-Thought methods, emphasizing the improvement in solving mathematical problems and focusing on progressive prompting. | Used to evaluate RFF's performance in solving arithmetic problems, focusing on accuracy improvements over baseline methods. | Used to assess performance on complex mathematical problems, demonstrating better relative performance on harder tasks. | Used to evaluate the effectiveness of Chain-of-Thought methods, specifically in enhancing accuracy and attention to detail. | Used to measure mathematical problem-solving capabilities, focusing on the impact of detailed hints and question-answer pairs on complex problems. | Utilized to test the generalization of LLMs across diverse arithmetic problems, including word problems and symbolic expressions. | Used to measure the accuracy improvement of Chain-of-Thought methods, focusing on detailed and relational aspects of problem-solving.",
          "citing_paper_id": "279154984",
          "cited_paper_id": 232134851,
          "context_text": "We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for evaluating RFF, which are clearly named and relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.03673",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e18698918e315e6714d4f864fb3347cdf27fdb43",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Game of 24",
          "dataset_description": "Used to evaluate RFF's performance in solving arithmetic problems, focusing on accuracy improvements over baseline methods.",
          "citing_paper_id": "279154984",
          "cited_paper_id": null,
          "context_text": "We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions specific datasets used for evaluating RFF, which are clearly named and relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2506.03673",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/e18698918e315e6714d4f864fb3347cdf27fdb43",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "266174368",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "BabyAI"
      ],
      "dataset_details": [
        {
          "dataset_name": "BabyAI",
          "dataset_description": "Used to assess the limitations of specialized neural models, highlighting tasks where even extensive in-domain training results in limited progress. | Used to evaluate zero-shot performance of long-context models, focusing on tasks where these models show fair performance without additional training.",
          "citing_paper_id": "274150091",
          "cited_paper_id": 263334319,
          "context_text": "…from tasks where we see fair zero-shot performance by state-of-the-art long-context models (BabyAI) to those where even specialized neural models trained on billions of in-domain datapoints make very limited progress (NetHack) (Piterbarg et al., 2024; Klissarov et al., 2023; Wołczyk et al., 2024).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'BabyAI' and 'NetHack' as specific environments or datasets used to evaluate model performance. These are multi-word proper nouns that fit the criteria for inclusion.",
          "citing_paper_doi": "10.48550/arXiv.2411.13543",
          "cited_paper_doi": "10.48550/arXiv.2310.00166",
          "citing_paper_url": "https://www.semanticscholar.org/paper/faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c3e2bec83b9105b7925aa76c0f38b88d2e337b31",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BabyAI",
          "dataset_description": "Used to assess the limitations of specialized neural models, highlighting tasks where even extensive in-domain training results in limited progress. | Used to evaluate zero-shot performance of long-context models, focusing on tasks where these models show fair performance without additional training.",
          "citing_paper_id": "274150091",
          "cited_paper_id": 266174368,
          "context_text": "…from tasks where we see fair zero-shot performance by state-of-the-art long-context models (BabyAI) to those where even specialized neural models trained on billions of in-domain datapoints make very limited progress (NetHack) (Piterbarg et al., 2024; Klissarov et al., 2023; Wołczyk et al., 2024).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'BabyAI' and 'NetHack' as specific environments or datasets used to evaluate model performance. These are multi-word proper nouns that fit the criteria for inclusion.",
          "citing_paper_doi": "10.48550/arXiv.2411.13543",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e41150e2b0feeb02c3948b34a2dec9d290b9c979",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BabyAI",
          "dataset_description": "Used to compare planning capabilities, specifically evaluating handcrafted policies or solvers in a grid-world environment. | Used to compare planning capabilities, specifically evaluating handcrafted policies or solvers in a simulated home environment.",
          "citing_paper_id": "275133533",
          "cited_paper_id": 259370794,
          "context_text": "Of the datasets that we compare to, only BabyAI [Chevalier-Boisvert et al. , 2019] and ALFRED [Shridhar et al. , 2020] release handcrafted policies or solvers.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'BabyAI' and 'ALFRED' as datasets that release handcrafted policies or solvers. These are specific datasets used for comparing planning capabilities in the research.",
          "citing_paper_doi": "10.48550/arXiv.2412.21033",
          "cited_paper_doi": "10.18653/v1/2023.acl-long.255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a7487e90969663d7d18709cf6b30c025f3b0833c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2a8b771c1645f3a7b7f6aaf9241e3f88d11b14c1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "258865184",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "MMLU"
      ],
      "dataset_details": [
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of LLMs across various tasks and domains. | Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of large language models across various tasks.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 258179056,
          "context_text": "Following Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot demonstrations (Lu et al., 2024), environmental feedback (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2023), and tree-like reasoning procedures (Yao et al., 2024; Zhuang et al., 2024a).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks, but only 'MMLU' is a specific, identifiable dataset. The others are excluded as they are primarily used for score comparison or are not clearly datasets.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": "10.18653/v1/2023.emnlp-main.187",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/19c222d1f18317d58cc85491f37479bc0dc49f41",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to assess function-calling proficiencies of LLM agents, providing a rigorous framework for evaluation and benchmarking. | Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of large language models across various tasks. | Used to evaluate the performance of Hephaestus, focusing on core agentic capabilities and generalization, highlighting improvements from pre-training.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 258865184,
          "context_text": "…Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot…",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks, but only MMLU is a specific, downloadable dataset. The others are excluded as they are primarily used for score comparison.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d8905a1fd288068f12c8347caeabefd36d0dd6c",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MMLU",
          "dataset_description": "Used to evaluate multitask language understanding across 57 diverse subjects, including elementary mathematics, computer science, and law, focusing on comprehensive language capabilities.",
          "citing_paper_id": "273233132",
          "cited_paper_id": 221516475,
          "context_text": "In this context, Hendrycks et al. [6] introduced the Massive Multitask Language Understanding (MMLU) benchmark which consists of 57 tasks spanning subjects such as elementary mathematics, computer science, and law.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MMLU benchmark but does not specify its use as a dataset. It is described as a benchmark for evaluating language understanding across multiple tasks.",
          "citing_paper_doi": "10.48550/arXiv.2410.07765",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/abc377522b96b6f05c4ca0b063f9150a8e1d7198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/814a4f680b9ba6baba23b93499f4b48af1a27678",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "259360665",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "NaturalQuestions-Open"
      ],
      "dataset_details": [
        {
          "dataset_name": "NaturalQuestions-Open",
          "dataset_description": "Used to evaluate the performance of MemGPT in managing multi-session conversations, assessing the system's ability to maintain context across multiple interactions. | Used to test the tiered memory system in MemGPT, focusing on open-domain question answering and long-term context management.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 236034497,
          "context_text": "Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'NaturalQuestions-Open' and 'multi-session chat datasets', which are specific datasets used for evaluating memory systems in language models.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.18653/v1/2022.acl-long.356",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/88064de690af282dbdf222774f03ff070b9df22b",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "NaturalQuestions-Open",
          "dataset_description": "Used to evaluate the performance of MemGPT in managing multi-session conversations, assessing the system's ability to maintain context across multiple interactions. | Used to test the tiered memory system in MemGPT, focusing on open-domain question answering and long-term context management.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 259360665,
          "context_text": "Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'NaturalQuestions-Open' and 'multi-session chat datasets', which are specific datasets used for evaluating memory systems in language models.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.1162/tacl_a_00638",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1733eb7792f7a43dd21f51f4d1017a1bffd217b5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "NaturalQuestions-Open",
          "dataset_description": "Used to evaluate the performance of MemGPT in managing multi-session conversations, assessing the system's ability to maintain context across multiple interactions. | Used to test the tiered memory system in MemGPT, focusing on open-domain question answering and long-term context management.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 263909014,
          "context_text": "Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'NaturalQuestions-Open' and 'multi-session chat datasets', which are specific datasets used for evaluating memory systems in language models.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2310.08560",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/908dad62c0e43d80e3e3cb3c0402f7c71c70499c",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "268032947",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "UrbanKGent"
      ],
      "dataset_details": [
        {
          "dataset_name": "UrbanKGent",
          "dataset_description": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios. | Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 261705716,
          "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2309.06719",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8aab972b0c3a3d581536b0d74339794809dc1a64",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "UrbanKGent",
          "dataset_description": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios. | Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 267750165,
          "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CAI59869.2024.00159",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9c7945def96c4cae7c6c5b16dae36d6a3dd2d01c",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "UrbanKGent",
          "dataset_description": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios. | Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 268032947,
          "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2402.17161",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f95d2c974e6aa2497b85342296b863d0d10c0892",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "52055325",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "Olympiad-Bench"
      ],
      "dataset_details": [
        {
          "dataset_name": "Olympiad-Bench",
          "dataset_description": "Used to evaluate factual and alignment tasks, focusing on the reliability and coherence of language model outputs. | Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used to assess factual and alignment tasks, specifically evaluating the accuracy and consistency of language model responses. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to evaluate multi-choice question answering, particularly in the context of complex reasoning and comprehension tasks. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately. | Used to evaluate conversational question answering, focusing on multi-turn dialogue and context understanding in language models. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 52055325,
          "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1162/tacl_a_00266",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/990a7b4eceedb6e053e6386269481bdfc42a1094",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "Olympiad-Bench",
          "dataset_description": "Used to evaluate models on challenging scientific problems from international olympiads, focusing on bilingual multimodal capabilities and promoting AGI development. | Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used to assess advanced reasoning skills, particularly in solving olympiad-level bilingual multimodal scientific problems, promoting AGI development. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 267770504,
          "context_text": "• OlympiadBench (He et al., 2024): Problems sourced from international olympiads (e.g., IMO and regional contests).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "OlympiadBench is a specific, verifiable dataset used for evaluating models on challenging scientific problems from international olympiads.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2402.14008",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bcf2c7e3f4ed64c8294c35a59220a26dd4f40060",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "Olympiad-Bench",
          "dataset_description": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 279154475,
          "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2506.04178",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d5da014705059c44ed8c7b54a1660815a0eaee26",
          "citing_paper_year": 2025,
          "cited_paper_year": 2025
        }
      ]
    },
    {
      "cited_paper_id": "49317780",
      "citation_count": 0,
      "total_dataset_mentions": 3,
      "unique_datasets": [
        "VirtualHome"
      ],
      "dataset_details": [
        {
          "dataset_name": "VirtualHome",
          "dataset_description": "Adapted to simulate human-like activities in a virtual environment, focusing on modeling household tasks for AI planning capabilities. | Used to simulate household activities via programs, focusing on zero-shot planning capabilities of language models. The dataset provides actionable knowledge for embodied agents.",
          "citing_paper_id": "280150778",
          "cited_paper_id": 49317780,
          "context_text": "For our experiments, we used the VirtualHome [18] dataset from the ZeroShot planner [6].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'VirtualHome' dataset, which is a specific, verifiable resource used in the experiments. The dataset is associated with household activity simulation and is used in the context of zero-shot planning.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb8b72f0b0301a98c01442d827bce82a76a15e65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "VirtualHome",
          "dataset_description": "Used to simulate household activities via programs, focusing on zero-shot planning capabilities of language models. The dataset provides actionable knowledge for embodied agents.",
          "citing_paper_id": "280150778",
          "cited_paper_id": 246035276,
          "context_text": "For our experiments, we used the VirtualHome [18] dataset from the ZeroShot planner [6].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'VirtualHome' dataset, which is a specific, verifiable resource used in the experiments. The dataset is associated with household activity simulation and is used in the context of zero-shot planning.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/bb8b72f0b0301a98c01442d827bce82a76a15e65",
          "cited_paper_url": "https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "VirtualHome",
          "dataset_description": "Used to evaluate the method in a large-scale household simulation, focusing on interactions with various objects, receptacles, and rooms.",
          "citing_paper_id": "265498448",
          "cited_paper_id": 49317780,
          "context_text": "A. Simulation Experiments 1) Experimental settings in Simulation: We evaluate our method using VirtualHome [36], a large-scale household simulation with a variety of interactive objects, receptacles, and rooms.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "VirtualHome is a simulation environment, not a traditional dataset, but it is used as a reusable resource for evaluating methods in household simulations.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a2f1b49c7d909e5a40d0f9ff773a4e9ffe018fee",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2023,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "273662115",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "OpenCity"
      ],
      "dataset_details": [
        {
          "dataset_name": "OpenCity",
          "dataset_description": "Used to simulate urban activities with massive LLM agents, focusing on spatio-temporal multi-agent trajectories and text vectors for planning capabilities. | Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios. | Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
          "citing_paper_id": "280047877",
          "cited_paper_id": null,
          "context_text": "…- Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text - Spatio-Temporal Multi-Agent - OpenCity [198] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent -",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'OpenCity' which is a platform for simulating urban activities with LLM agents. It is used as a database for spatio-temporal multi-agent trajectories and text vectors.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        },
        {
          "dataset_name": "OpenCity",
          "dataset_description": "Used to simulate urban activities with massive LLM agents, focusing on spatio-temporal multi-agent trajectories and text vectors for planning capabilities.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 273662115,
          "context_text": "…- Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text - Spatio-Temporal Multi-Agent - OpenCity [198] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent -",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'OpenCity' which is a platform for simulating urban activities with LLM agents. It is used as a database for spatio-temporal multi-agent trajectories and text vectors.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2410.21286",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2778689dd0a33a22293492409d94e1e30a10f966",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "208290939",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ARC"
      ],
      "dataset_details": [
        {
          "dataset_name": "ARC",
          "dataset_description": "Used for training and evaluating models on math-related Q&A, focusing on problem-solving and reasoning skills. | Used for training and evaluating models on physical interaction Q&A, focusing on understanding and reasoning about physical scenarios. | Used for training and evaluating models on web-based instructions, enhancing the model's ability to follow and generate instructions. | Used for training and evaluating models on advanced reasoning and commonsense Q&A, enhancing the model's ability to handle challenging problems. | Used for training and evaluating models on general Q&A tasks, providing a diverse set of complex explanation traces.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 269605607,
          "context_text": "Several datasets are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used to support IFT, which are specific and identifiable. These datasets are used for training and evaluating models on various types of questions, particularly in the domain of science and mathematics.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2405.03548",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4fcb8b6c466937025d315be6a83b624b10e860b4",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "ARC",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 208290939,
          "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.1609/AAAI.V34I05.6239",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/04f4e55e14150b7c48b0287ba77c7443df76ed45",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "267406800",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "TravelPlanner"
      ],
      "dataset_details": [
        {
          "dataset_name": "TravelPlanner",
          "dataset_description": "Used to evaluate the success rate of a model in real-world planning tasks, specifically comparing performance against GPT models with a focus on planning capabilities.",
          "citing_paper_id": "270063287",
          "cited_paper_id": 252762395,
          "context_text": "It achieved an average success rate of about 42% on TravelPlanner, a significant improvement from the initial 0.6%, and outperformed GPT-4 with ReAct by 14% on API-Bank.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'TravelPlanner' as a benchmark for evaluating success rates. It is used to measure the performance of a model in real-world planning tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/5795319850a8a4bf469dbfe432d56e3a02b2b5b0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "TravelPlanner",
          "dataset_description": "Used to evaluate planning capabilities of language models, focusing on planning capabilities and real-world task execution. The benchmark assesses success rates in complex planning scenarios. | Used to evaluate the success rate of a model in real-world planning tasks, specifically comparing performance against GPT models with a focus on planning capabilities.",
          "citing_paper_id": "270063287",
          "cited_paper_id": 267406800,
          "context_text": "It achieved an average success rate of about 42% on TravelPlanner, a significant improvement from the initial 0.6%, and outperformed GPT-4 with ReAct by 14% on API-Bank.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'TravelPlanner' as a benchmark for evaluating success rates. It is used to measure the performance of a model in real-world planning tasks.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2402.01622",
          "citing_paper_url": "https://www.semanticscholar.org/paper/5795319850a8a4bf469dbfe432d56e3a02b2b5b0",
          "cited_paper_url": "https://www.semanticscholar.org/paper/11155af5ccd1889277f4269f6bb349a7633554f4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "266448819",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MatSci-Instruct"
      ],
      "dataset_details": [
        {
          "dataset_name": "MatSci-Instruct",
          "dataset_description": "Used for instruction-following tasks in materials science, evaluating model ability to perform complex instructions and generate accurate responses.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 263909166,
          "context_text": "…> 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼ 52k T1 MatSciNLP [156] NLP benchmark Text, Q&A ∼ 170k T1, T4, T5 LLM4Mat-Bench [157] LLM benchmark for property…",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several datasets that are used for training and evaluating models in the domain of materials science. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2310.08511",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9c217c50addaee50e91e3481be8778bf62d71df0",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MatSci-Instruct",
          "dataset_description": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines. | Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills. | Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena. | Used for instruction-following tasks in materials science, evaluating the ability of models to understand and execute complex instructions. | Used to evaluate the performance of Llama-2-70B and GPT-3 in answering materials science questions, focusing on the models' planning capabilities and knowledge retention. | Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving. | Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 266448819,
          "context_text": "…literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼ 52k T1 MatSciNLP [156] NLP benchmark Text, Q&A ∼…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets that are used for training and evaluating models in the domain of materials science. These datasets are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.1039/d3dd00188a",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8f8a45cb25936abb541d6b11453e3efe23d33cda",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "234093776",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "SUPER"
      ],
      "dataset_details": [
        {
          "dataset_name": "SUPER",
          "dataset_description": "Used to verify agents' ability to produce accurate, executable scientific code, focusing on research coding tasks curated by scientists. | Used to assess the test-driven development skills of LLMs, specifically their ability to write tests and implement corresponding code. | Evaluates agents on setting up and executing tasks from research repositories, assessing their capability to handle scientific workflows. | Used to evaluate the planning capabilities of LLMs in executing tasks from research repositories, focusing on task setup and execution accuracy. | Used to evaluate agents on setting up and executing tasks from research repositories, focusing on the planning capabilities of LLMs in scientific contexts.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 272593197,
          "context_text": "…et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several named resources, but most are likely benchmarks or challenges rather than traditional datasets. Only 'SUPER' is confirmed as a dataset through the cited paper title.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2409.07440",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/053ef8299988680d47df36224bfccffc817472f1",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "SUPER",
          "dataset_description": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context. | Used to evaluate agents on setting up and executing tasks from research repositories, focusing on the planning capabilities of LLMs in scientific contexts.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 234093776,
          "context_text": "…et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several named resources, but most are likely benchmarks or challenges rather than traditional datasets. Only 'SUPER' is confirmed as a dataset through the cited paper title.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.18653/V1/2021.NAACL-MAIN.365",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "273228858",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "ACPBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "ACPBench",
          "dataset_description": "Used to evaluate LLMs on core reasoning skills, specifically focusing on action, change, and planning. The dataset provides tasks to assess the model's ability to reason about these aspects. | Used to evaluate reasoning about action, change, and planning, focusing on the ability of models to understand and predict the outcomes of actions in dynamic environments.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 273228858,
          "context_text": "ACPBench (Kokel et al., 2024) focuses on evaluating LLMs on core reasoning skills.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ACPBench is mentioned as a benchmark for evaluating LLMs on core reasoning skills, which aligns with the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2410.05669",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7bee11da995d968394477a66e78aaab9721ecdf7",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "ACPBench",
          "dataset_description": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions. | Used to evaluate reasoning about action, change, and planning, focusing on the ability of models to handle complex reasoning tasks involving sequences of actions and their effects. | Used to evaluate reasoning tasks in classical planning domains, focusing on action, change, and planning with 7 atomic tasks across 13 domains. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 273228858,
          "context_text": "• ACPBench (Kokel et al., 2025): It has 7 atomic reasoning tasks around 13 classical planning domains.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "ACPBench is mentioned as a benchmark with specific tasks and domains, which fits the criteria for a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2410.05669",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7bee11da995d968394477a66e78aaab9721ecdf7",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "270223881",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Collection of MOF databases"
      ],
      "dataset_details": [
        {
          "dataset_name": "Collection of MOF databases",
          "dataset_description": "Used to predict and generate metal-organic frameworks, focusing on the integration of LLMs with tools and evaluators to enhance materials design and discovery.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 270223881,
          "context_text": "…BM25 + Contriever + LLM Literature text, web search MatSciKB T1 LLMatDesign [37] MatDeepLearn + TorchMD-Net + LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for materials…",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Collection of MOF databases' which appears to be a specific dataset used in the research. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.1038/s41467-024-48998-4",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/bf27c81faad0dd21bd39c45ee5e85300577fda66",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "Collection of MOF databases",
          "dataset_description": "Utilized to provide structural and textual data for MOF (Metal-Organic Framework) research, supporting the development of LLM-based agents in materials design.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 270620677,
          "context_text": "HoneyComb [36] BM25 + Contriever + LLM Literature text, web search MatSciKB T1 LLMatDesign [37] MatDeepLearn + TorchMD-Net + LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for…",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MatSciKB' and 'Collection of MOF databases', which appear to be specific datasets or collections used in materials science research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2406.13163",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/70bda40fdbb92a00bfd4d965824db59581f3b4f1",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "252693237",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "PrOntoQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "PrOntoQA",
          "dataset_description": "Used to assess performance on graduate-level multiple-choice questions in a 0-shot setting, testing the model's ability to answer complex questions without prior exposure. | Used to evaluate logical deduction in a 5-shot setting, focusing on the ability of language models to perform step-by-step reasoning.",
          "citing_paper_id": "276259098",
          "cited_paper_id": 252693237,
          "context_text": "For logical reasoning, we evaluate the PrOntoQA (Saparov & He, 2022) dataset for logical deduction in a 5-shot setting and the GPQA (Rein et al., 2023) dataset for graduate-level multiple-choice questions in a 0-shot setting.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, PrOntoQA and GPQA, which are used for evaluating logical reasoning capabilities of language models in different settings.",
          "citing_paper_doi": "10.48550/arXiv.2502.06813",
          "cited_paper_doi": "10.48550/arXiv.2210.01240",
          "citing_paper_url": "https://www.semanticscholar.org/paper/39c1c4eafb2f9afd716b3f49b8f16d91356d4990",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "PrOntoQA",
          "dataset_description": "Used to assess performance on graduate-level multiple-choice questions in a 0-shot setting, testing the model's ability to answer complex questions without prior exposure. | Used to evaluate logical deduction in a 5-shot setting, focusing on the ability of language models to perform step-by-step reasoning.",
          "citing_paper_id": "276259098",
          "cited_paper_id": 265295009,
          "context_text": "For logical reasoning, we evaluate the PrOntoQA (Saparov & He, 2022) dataset for logical deduction in a 5-shot setting and the GPQA (Rein et al., 2023) dataset for graduate-level multiple-choice questions in a 0-shot setting.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, PrOntoQA and GPQA, which are used for evaluating logical reasoning capabilities of language models in different settings.",
          "citing_paper_doi": "10.48550/arXiv.2502.06813",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/39c1c4eafb2f9afd716b3f49b8f16d91356d4990",
          "cited_paper_url": "https://www.semanticscholar.org/paper/210b0a3d76e93079cc51b03c4115fde545eea966",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "270923717",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Planetarium benchmark"
      ],
      "dataset_details": [
        {
          "dataset_name": "Planetarium benchmark",
          "dataset_description": "Used to assess LLMs’ planning capabilities by translating text to structured planning languages, focusing on rigorous benchmarking and evaluation of PDDL-based planning tasks. | Used to evaluate LLMs' understanding and generation of actions, problems, and plans in PDDL, complementing the Planetarium benchmark. | Used to assess the ability of models to translate text into structured planning languages, focusing on parsing, generation, and reasoning with PDDL. | Used to evaluate how well models understand and generate actions and problems in PDDL, complementing the Planetarium benchmark. | Used to assess LLMs' ability to translate text into structured planning languages, focusing on parsing, generating, and reasoning with PDDL.",
          "citing_paper_id": "276647180",
          "cited_paper_id": 270923717,
          "context_text": "…in the PDDL language, focusing on their ability to parse, generate, and reason with PDDL. Speciﬁ-cally, we leverage the Planetarium benchmark Zuo et al. (2024) alongside the dataset introduced by Oswald et al. (2024) to assess how well these models understand and generate actions, problems,…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions two specific resources: 'Planetarium benchmark' and a dataset by Oswald et al. (2024). Both are used to assess the performance of models in understanding and generating actions and problems in PDDL.",
          "citing_paper_doi": "10.48550/arXiv.2502.20175",
          "cited_paper_doi": "10.48550/arXiv.2407.03321",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c9f77c68fc65154650d391ee9669e444577059b3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c1d553145cb39bc8768c87b1c134d19d09979e64",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "Planetarium benchmark",
          "dataset_description": "Used to assess LLMs' understanding and generation of PDDL actions, problems, and plans, focusing on parsing, generating, and reasoning capabilities. | Used to assess LLMs’ planning capabilities by translating text to structured planning languages, focusing on rigorous benchmarking and evaluation of PDDL-based planning tasks.",
          "citing_paper_id": "276647180",
          "cited_paper_id": null,
          "context_text": "Novel benchmarks such as PlanBench Valmeekam et al. (2023), AutoPlanBench Stein et al. (2024), Planetarium Zuo et al. (2024), and the domain benchmark from Oswald et al. (2024) have been introduced to assess LLMs’ planning capabilities using PDDL.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several benchmarks, but only 'Planetarium' is a specific, verifiable dataset with a clear reference to a published paper.",
          "citing_paper_doi": "10.48550/arXiv.2502.20175",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/c9f77c68fc65154650d391ee9669e444577059b3",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "267750682",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "nuScenes"
      ],
      "dataset_details": [
        {
          "dataset_name": "nuScenes",
          "dataset_description": "Used to evaluate planning capabilities in autonomous driving, focusing on expert trajectories in real-world sessions to assess model performance.",
          "citing_paper_id": "277824043",
          "cited_paper_id": 266725320,
          "context_text": "Despite the various QA setups, benchmarks that involve planning [10,46,47] still resort to an open-loop setting on real-world sessions (e.g., nuScenes) where expert trajectories are used.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'nuScenes' as a real-world session involving planning capabilities, which is relevant to the research topic. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2405.01533",
          "cited_paper_doi": "10.1109/CVPR52733.2024.01297",
          "citing_paper_url": "https://www.semanticscholar.org/paper/40fbc19197da3e414aa6e460c5e39789cf248100",
          "cited_paper_url": "https://www.semanticscholar.org/paper/cd49101103f73d88a4a3b368898066f03984c339",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        },
        {
          "dataset_name": "nuScenes",
          "dataset_description": "Used to evaluate planning capabilities in autonomous driving, focusing on expert trajectories in real-world sessions to assess model performance.",
          "citing_paper_id": "277824043",
          "cited_paper_id": 267750682,
          "context_text": "Despite the various QA setups, benchmarks that involve planning [10,46,47] still resort to an open-loop setting on real-world sessions (e.g., nuScenes) where expert trajectories are used.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'nuScenes' as a real-world session involving planning capabilities, which is relevant to the research topic. No other specific datasets are mentioned.",
          "citing_paper_doi": "10.48550/arXiv.2405.01533",
          "cited_paper_doi": "10.48550/arXiv.2402.12289",
          "citing_paper_url": "https://www.semanticscholar.org/paper/40fbc19197da3e414aa6e460c5e39789cf248100",
          "cited_paper_url": "https://www.semanticscholar.org/paper/758c2dc290c037a6f211ec503beee70abe2d1197",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "258298051",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "BlocksWorld"
      ],
      "dataset_details": [
        {
          "dataset_name": "BlocksWorld",
          "dataset_description": "Used as a planning benchmark to evaluate the ability of large language models to arrange blocks in specific configurations, focusing on their planning proficiency.",
          "citing_paper_id": "267938891",
          "cited_paper_id": 256846992,
          "context_text": "BlocksWorld (Valmeekam et al., 2023b; Liu et al., 2023a) is another planning benchmark that contains various tasks to arrange blocks in specific configurations.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The term 'BlocksWorld' is mentioned as a planning benchmark containing various tasks. It is specific and relevant to the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2402.16181",
          "cited_paper_doi": "10.48550/arXiv.2302.06706",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b42713664a72410307839fe44ec51aef8d69c943",
          "cited_paper_url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "BlocksWorld",
          "dataset_description": "Used as a planning benchmark to evaluate the ability of large language models to arrange blocks in specific configurations, focusing on their planning proficiency.",
          "citing_paper_id": "267938891",
          "cited_paper_id": 258298051,
          "context_text": "BlocksWorld (Valmeekam et al., 2023b; Liu et al., 2023a) is another planning benchmark that contains various tasks to arrange blocks in specific configurations.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The term 'BlocksWorld' is mentioned as a planning benchmark containing various tasks. It is specific and relevant to the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2402.16181",
          "cited_paper_doi": "10.48550/arXiv.2304.11477",
          "citing_paper_url": "https://www.semanticscholar.org/paper/b42713664a72410307839fe44ec51aef8d69c943",
          "cited_paper_url": "https://www.semanticscholar.org/paper/003ef1cd670d01af05afa0d3c72d72228f494432",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "237142385",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "MBPP"
      ],
      "dataset_details": [
        {
          "dataset_name": "MBPP",
          "dataset_description": "Used to assess the effectiveness of CodeSynth, specifically measuring F1-scores and showing significant improvement over iterations, highlighting the model's capability in solving programming problems.",
          "citing_paper_id": "267681745",
          "cited_paper_id": 266051661,
          "context_text": "CodeSynth significantly improved F1-scores on both HumanEval-X and MBPP datasets, achieving a perfect score of 1.0 by the fifth iteration from initial scores of 0.844 and 0.912 , respectively.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions two datasets, HumanEval-X and MBPP, which are used to evaluate the performance of CodeSynth. These datasets are specific and verifiable.",
          "citing_paper_doi": "10.48550/arXiv.2402.10051",
          "cited_paper_doi": "10.48550/arXiv.2312.04474",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2ae05ad71073dd8d32b9a669c9add15518615c9a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/3a56bc074b8f3f985599627404b70e16fc5bce1b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "MBPP",
          "dataset_description": "Used to evaluate code generation capabilities of large language models, focusing on a 500-problem test set of crowd-sourced Python tasks.",
          "citing_paper_id": "279391526",
          "cited_paper_id": 237142385,
          "context_text": "We use the 500-problem test set of the MBPP dataset (Austin et al., 2021), which consists of crowd-sourced Python code generation tasks.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the MBPP dataset, which is a specific, verifiable dataset used for evaluating code generation tasks.",
          "citing_paper_doi": "10.48550/arXiv.2506.11578",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/6f2a1f397cbbb4f0ea4cc8eecb2911c9df40f454",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "3986974",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "CWQ"
      ],
      "dataset_details": [
        {
          "dataset_name": "CWQ",
          "dataset_description": "Used to test PoG's effectiveness in multi-hop question answering, specifically designed to challenge models with complex reasoning tasks. | Used to evaluate PoG's performance on complex reasoning over knowledge graphs, focusing on multi-hop question answering tasks. | Used to assess PoG's capability in handling multi-hop questions over knowledge graphs, emphasizing complex reasoning and semantic parsing.",
          "citing_paper_id": "273707190",
          "cited_paper_id": 3986974,
          "context_text": "To demonstrate the effectiveness of PoG on complex reasoning over knowledge graphs, we adopt three representative multi-hop KGQA datasets: CWQ [37], WebQSP [56], and GrailQA [17].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating the effectiveness of PoG on complex reasoning over knowledge graphs. These datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2410.23875",
          "cited_paper_doi": "10.18653/v1/N18-1059",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a5997b04bef7c9ccc84c3bd8d8a5d477936931f4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8725f13be7434b69738491c66b45c9225258253",
          "citing_paper_year": 2024,
          "cited_paper_year": 2018
        },
        {
          "dataset_name": "CWQ",
          "dataset_description": "Used to test PoG's effectiveness in multi-hop question answering, specifically designed to challenge models with complex reasoning tasks. | Used to evaluate PoG's performance on complex reasoning over knowledge graphs, focusing on multi-hop question answering tasks. | Used to assess PoG's capability in handling multi-hop questions over knowledge graphs, emphasizing complex reasoning and semantic parsing.",
          "citing_paper_id": "273707190",
          "cited_paper_id": 13905064,
          "context_text": "To demonstrate the effectiveness of PoG on complex reasoning over knowledge graphs, we adopt three representative multi-hop KGQA datasets: CWQ [37], WebQSP [56], and GrailQA [17].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions three specific datasets used for evaluating the effectiveness of PoG on complex reasoning over knowledge graphs. These datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2410.23875",
          "cited_paper_doi": "10.18653/v1/P16-2033",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a5997b04bef7c9ccc84c3bd8d8a5d477936931f4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7fcaa13db8c89ff1f39b5687ba47f8beee107c4",
          "citing_paper_year": 2024,
          "cited_paper_year": 2016
        }
      ]
    },
    {
      "cited_paper_id": "1739732",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "synthesized trajectory data"
      ],
      "dataset_details": [
        {
          "dataset_name": "synthesized trajectory data",
          "dataset_description": "Used to instruction-tune LLMs, focusing on generating and evaluating planning capabilities through synthesized trajectories. The dataset provides structured data for training and testing planning algorithms.",
          "citing_paper_id": "271600631",
          "cited_paper_id": 1739732,
          "context_text": "(iii) Instruction-tuning LLMs with the synthesized trajectory data.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'synthesized trajectory data' which is a plausible dataset name given the research topic on planning capabilities of LLMs. However, it is not a clearly identifiable, publicly accessible dataset.",
          "citing_paper_doi": "10.48550/arXiv.2408.00764",
          "cited_paper_doi": "10.1109/ICRA.2011.5980391",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a392c7ff653068c3ce2daec638a657d047debe45",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9b7ae896675c71ac50fa1fbc555cb19f80863f0e",
          "citing_paper_year": 2024,
          "cited_paper_year": 2011
        },
        {
          "dataset_name": "synthesized trajectory data",
          "dataset_description": "Used to instruction-tune LLMs, focusing on generating and evaluating planning capabilities through synthesized trajectories. The dataset provides structured data for training and testing planning algorithms.",
          "citing_paper_id": "271600631",
          "cited_paper_id": null,
          "context_text": "(iii) Instruction-tuning LLMs with the synthesized trajectory data.",
          "confidence_score": 0.6,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'synthesized trajectory data' which is a plausible dataset name given the research topic on planning capabilities of LLMs. However, it is not a clearly identifiable, publicly accessible dataset.",
          "citing_paper_doi": "10.48550/arXiv.2408.00764",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a392c7ff653068c3ce2daec638a657d047debe45",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "270123654",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Tell Me More"
      ],
      "dataset_details": [
        {
          "dataset_name": "Tell Me More",
          "dataset_description": "Used to test language agents' planning capabilities under the assumption that user instructions are clear and explicit, focusing on task execution rather than clarification. | Used to assess language agents' capability to seek additional information when faced with unclear user instructions, enhancing the agent's conversational skills. | Used to evaluate language agents' ability to ask clarification questions, focusing on improving user interaction through better understanding of ambiguous instructions. | Used to evaluate language agents' generalist capabilities on web tasks, assuming clear and explicit user instructions, focusing on the agent's ability to navigate and interact with web interfaces.",
          "citing_paper_id": "270561990",
          "cited_paper_id": 259129428,
          "context_text": "3 Ask-before-Plan Dataset Existing benchmarks on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets/benchmarks but does not specify their usage in the current research. It only describes their focus areas.",
          "citing_paper_doi": "10.48550/arXiv.2406.12639",
          "cited_paper_doi": "10.48550/arXiv.2306.06070",
          "citing_paper_url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/58f8925a8b87054ad0635a6398a7fe24935b1604",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "Tell Me More",
          "dataset_description": "Used to test language agents' planning capabilities under the assumption that user instructions are clear and explicit, focusing on task execution rather than clarification. | Used to assess language agents' capability to seek additional information when faced with unclear user instructions, enhancing the agent's conversational skills. | Used to evaluate language agents' ability to ask clarification questions, focusing on improving user interaction through better understanding of ambiguous instructions. | Used to evaluate language agents' generalist capabilities on web tasks, assuming clear and explicit user instructions, focusing on the agent's ability to navigate and interact with web interfaces.",
          "citing_paper_id": "270561990",
          "cited_paper_id": 270123654,
          "context_text": "3 Ask-before-Plan Dataset Existing benchmarks on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets/benchmarks but does not specify their usage in the current research. It only describes their focus areas.",
          "citing_paper_doi": "10.48550/arXiv.2406.12639",
          "cited_paper_doi": "10.48550/arXiv.2405.19425",
          "citing_paper_url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c8574be40d6c92f4a108f955969bd43ddde2a31d",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "265220706",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "SUPER-NATURAL-INSTRUCTIONS"
      ],
      "dataset_details": [
        {
          "dataset_name": "SUPER-NATURAL-INSTRUCTIONS",
          "dataset_description": "Used to evaluate the performance of large language models in following instructions, emphasizing multi-dimensional assessment of instruction-following capabilities. | Used to assess the instruction-following capability of large language models, providing a multi-dimensional benchmark that evaluates various aspects of instruction-following performance. | Used to benchmark instruction-following capabilities of large language models, focusing on natural language instructions across various domains and complexities.",
          "citing_paper_id": "279260580",
          "cited_paper_id": 265220706,
          "context_text": "Instruction-following ability has been benchmarked through datasets like SUPER-NATURAL-INSTRUCTIONS [26], AlpacaEval [10], and FollowEval [8].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions datasets used for benchmarking instruction-following abilities of large language models. These are specific, named datasets that are relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2506.08119",
          "cited_paper_doi": "10.48550/arXiv.2311.09829",
          "citing_paper_url": "https://www.semanticscholar.org/paper/3374ffaf24732fb180b74d122e65b239d407e260",
          "cited_paper_url": "https://www.semanticscholar.org/paper/312fed7476dd58b8a14bbc2a15f9faa00423b988",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        },
        {
          "dataset_name": "SUPER-NATURAL-INSTRUCTIONS",
          "dataset_description": "Used to assess the instruction-following capability of large language models, providing a multi-dimensional benchmark that evaluates various aspects of instruction-following performance. | Used to evaluate the performance of large language models in following instructions, emphasizing multi-dimensional assessment of instruction-following capabilities. | Used to benchmark instruction-following capabilities of large language models, focusing on natural language instructions across various domains and complexities.",
          "citing_paper_id": "279260580",
          "cited_paper_id": null,
          "context_text": "Instruction-following ability has been benchmarked through datasets like SUPER-NATURAL-INSTRUCTIONS [26], AlpacaEval [10], and FollowEval [8].",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions datasets used for benchmarking instruction-following abilities of large language models. These are specific, named datasets that are relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2506.08119",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/3374ffaf24732fb180b74d122e65b239d407e260",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "245329531",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "Minecraft Wiki recipe pages"
      ],
      "dataset_details": [
        {
          "dataset_name": "Minecraft Wiki recipe pages",
          "dataset_description": "Used to build a knowledge base for evaluating RAG capabilities, focusing on the integration of web-based information into language model evaluations.",
          "citing_paper_id": "275133533",
          "cited_paper_id": 245329531,
          "context_text": "In Plancraft, we collect the pages of recipes available on the Minecraft Wiki and use them to build a knowledge base to evaluate RAG capabilities.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions collecting recipe pages from the Minecraft Wiki to build a knowledge base for evaluating RAG capabilities. This is a specific, verifiable resource used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2412.21033",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/a7487e90969663d7d18709cf6b30c025f3b0833c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22",
          "citing_paper_year": 2024,
          "cited_paper_year": 2021
        },
        {
          "dataset_name": "Minecraft Wiki recipe pages",
          "dataset_description": "Used to build a knowledge base for evaluating RAG capabilities, focusing on the integration of web-based information into language model evaluations.",
          "citing_paper_id": "275133533",
          "cited_paper_id": 249017698,
          "context_text": "In Plancraft, we collect the pages of recipes available on the Minecraft Wiki and use them to build a knowledge base to evaluate RAG capabilities.",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions collecting recipe pages from the Minecraft Wiki to build a knowledge base for evaluating RAG capabilities. This is a specific, verifiable resource used in the research.",
          "citing_paper_doi": "10.48550/arXiv.2412.21033",
          "cited_paper_doi": "10.48550/arXiv.2205.12255",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a7487e90969663d7d18709cf6b30c025f3b0833c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/354bf043179e3e9f05df73e3f04517e53c326d1f",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "266906759",
      "citation_count": 0,
      "total_dataset_mentions": 2,
      "unique_datasets": [
        "hazardous home scenarios"
      ],
      "dataset_details": [
        {
          "dataset_name": "hazardous home scenarios",
          "dataset_description": "Used to address gaps in academic research by providing over 500 scenarios typically avoided by embodied agents and agent simulations, focusing on planning capabilities of language models.",
          "citing_paper_id": "266551253",
          "cited_paper_id": 250451569,
          "context_text": "The dataset comprises over 500 hazardous home scenarios, specifically curated to fill gaps often ignored in academic research, such as scenarios typically avoided by embodied agents and agent simulations [14, 32, 12, 13, 15, 16, 17].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context describes a specific dataset with a clear identifier and purpose, which is used to address gaps in academic research related to hazardous home scenarios.",
          "citing_paper_doi": "10.1109/ICMEW63481.2024.10645429",
          "cited_paper_doi": "10.48550/arXiv.2207.05608",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e63fca764a07f991a62361bf98e9cd109ab4a40b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        },
        {
          "dataset_name": "hazardous home scenarios",
          "dataset_description": "Used to address gaps in academic research by providing over 500 scenarios typically avoided by embodied agents and agent simulations, focusing on planning capabilities of language models.",
          "citing_paper_id": "266551253",
          "cited_paper_id": 266906759,
          "context_text": "The dataset comprises over 500 hazardous home scenarios, specifically curated to fill gaps often ignored in academic research, such as scenarios typically avoided by embodied agents and agent simulations [14, 32, 12, 13, 15, 16, 17].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context describes a specific dataset with a clear identifier and purpose, which is used to address gaps in academic research related to hazardous home scenarios.",
          "citing_paper_doi": "10.1109/ICMEW63481.2024.10645429",
          "cited_paper_doi": "10.48550/arXiv.2401.12963",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e63fca764a07f991a62361bf98e9cd109ab4a40b",
          "cited_paper_url": "https://www.semanticscholar.org/paper/948c9d605a77d9d3c3959efecaa69d97b4d9a1de",
          "citing_paper_year": 2023,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "232134851",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "BBH"
      ],
      "dataset_details": [
        {
          "dataset_name": "BBH",
          "dataset_description": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks. | Used to evaluate the ability to answer questions that require strategic thinking and multi-step reasoning. | Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations. | Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems. | Used to benchmark planning capabilities in AI models, specifically designed to test sequential decision-making and long-term planning. | Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions. | Applied to assess reasoning abilities in solving algebraic word problems, emphasizing logical steps and explanations. | Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences. | Evaluates logical reasoning, focusing on the ability to solve first-order logic problems. | Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems. | Used to evaluate multi-hop reasoning capabilities in question-answering systems, focusing on complex information retrieval and synthesis. | Applied to assess numerical reasoning and problem-solving skills through a card game, requiring quick and accurate calculations. | Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations. | Tests advanced logical reasoning, focusing on more complex first-order logic problems. | Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains. | Applied to assess commonsense reasoning and knowledge integration in AI models, emphasizing challenging and diverse question types. | Utilized to test multi-hop question answering, requiring the integration of information from multiple paragraphs. | Utilized to evaluate multi-step reasoning and planning in natural language processing tasks, focusing on complex problem-solving scenarios. | Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation. | Evaluates the ability to answer questions that require strategic reasoning and multi-step inference. | Applied to evaluate flow-based reasoning and planning, focusing on tasks that require understanding and manipulating sequences of actions. | Utilized to test strategic reasoning and planning in question-answering tasks, specifically designed to require multi-step logical thinking.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 232134851,
          "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "273654966",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "agent-environment interactions dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "agent-environment interactions dataset",
          "dataset_description": "Used to train LLMs to generate feasible trajectories and respond to changing urban contexts, focusing on urban mobility constraints.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 273654966,
          "context_text": "TrajAgent [29] derives a self-reflection mechanism to generate a large-scale dataset of agent-environment interactions under urban mobility constraints, which is used to train LLMs to generate feasible trajectories and respond to changing urban contexts.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a large-scale dataset of agent-environment interactions generated by TrajAgent, which is used to train LLMs for trajectory generation and urban context response.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2410.20445",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/2bae867f6d0cb906e3dd98d0a4c88f1215465c6f",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "264146527",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "EconAgent"
      ],
      "dataset_details": [
        {
          "dataset_name": "EconAgent",
          "dataset_description": "Used to simulate macroeconomic activities with large language model-empowered agents, focusing on operational state and text data in a multi-agent environment.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 264146527,
          "context_text": "…- D2A [175] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Williams et al. [185] Text - Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text -…",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several multi-agent datasets, but only 'EconAgent' is associated with a specific paper title, which helps disambiguate it as a dataset.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/2024.acl-long.829",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/9b3cc162df43bc999d6cba219e8d9871c28fbdcc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "267199749",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "VisualWebArena"
      ],
      "dataset_details": [
        {
          "dataset_name": "VisualWebArena",
          "dataset_description": "Used to evaluate multimodal agents on realistic visual web tasks, focusing on the performance of agents in complex, real-world scenarios.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 267199749,
          "context_text": "…et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al.,…",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation span mentions several benchmarks and challenges, which are primarily used for score comparison rather than as reusable datasets. However, 'VisualWebArena' is mentioned in a context that suggests it could be a specific, downloadable dataset beyond the leaderboard, especially given the cited paper's title.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2401.13649",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f554b22d2ccf786a6d61d5858f43024ba9115e15",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "268031860",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OmniACT"
      ],
      "dataset_details": [
        {
          "dataset_name": "OmniACT",
          "dataset_description": "Used to test agents' ability to navigate and execute tasks in real-world computer systems, focusing on the coordination of actions across multiple applications. | Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding. | Used to evaluate multimodal generalist autonomous agents in desktop and web environments, testing their ability to perform complex tasks and navigate diverse interfaces. | Used to assess agents' performance in executing complex tasks and coordinating actions across multiple applications, emphasizing real-world computer system navigation. | Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows. | Used to enable multimodal generalist autonomous agents for desktop and web, focusing on evaluating planning capabilities through a diverse set of tasks and environments.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 268031860,
          "context_text": "…AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several frameworks and leaderboards, but only 'OmniACT' is identified as a dataset and benchmark. The other items are excluded as they are likely leaderboards or frameworks.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2402.17553",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/c7d35995a9a4d49011cf47f130535e1391ebeebc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "203837042",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Allensville (1-ﬂoor)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Allensville (1-ﬂoor)",
          "dataset_description": "Used to evaluate the method, focusing on a single-floor environment to test planning capabilities in a simplified setting.",
          "citing_paper_id": "262045061",
          "cited_paper_id": 203837042,
          "context_text": "To evaluate our method, we use Allensville (1-ﬂoor), Benevolence (3-ﬂoor) and Collierville (3-ﬂoor) from the 3D Scene Graph dataset [11].",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions specific locations from the 3D Scene Graph dataset, which is used to evaluate the method. The dataset is clearly identified and used for evaluation purposes.",
          "citing_paper_doi": "10.1109/ICRA57147.2024.10610599",
          "cited_paper_doi": "10.1109/ICCV.2019.00576",
          "citing_paper_url": "https://www.semanticscholar.org/paper/09e0e5f07013466bd119241a4aabef0e6496fdb5",
          "cited_paper_url": "https://www.semanticscholar.org/paper/77b4f542ea56b02804672790b6482df400428c95",
          "citing_paper_year": 2023,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Alpaca dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Alpaca dataset",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs by mixing with another dataset and fine-tuning the model for 3 epochs to improve generalization ability.",
          "citing_paper_id": "270257748",
          "cited_paper_id": null,
          "context_text": "We mixed it with the Alpaca dataset (Taori et al., 2023) and fine-tuned the model with 3 epochs to improve the generalization ability.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions the 'Alpaca dataset' which is a specific, named dataset used for fine-tuning a model. The dataset is clearly identified and its usage is described.",
          "citing_paper_doi": "10.48550/arXiv.2406.02903",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/66c90d03c00c128435825561f4095069eff0d746",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "270620912",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "SWT-Bench"
      ],
      "dataset_details": [
        {
          "dataset_name": "SWT-Bench",
          "dataset_description": "Used to evaluate the agent’s ability to generate tests from user issues in real-world Github repositories, specifically for validating real-world bug-fixes with code agents.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 270620912,
          "context_text": "TDD-Bench Verified (Ahmed et al., 2024) and SWT-Bench (Mündler et al., 2024) evaluate the agent’s ability to generate tests from user issues in real-world Github repositories.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions two benchmarks, TDD-Bench and SWT-Bench, which are used to evaluate the ability of agents to generate tests from user issues in real-world Github repositories.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/81f16a255718a9750e450c1d3738647b3bafa9e1",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "273507547",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Reflection-Bench"
      ],
      "dataset_details": [
        {
          "dataset_name": "Reflection-Bench",
          "dataset_description": "Used to assess LLMs' cognitive reflection capabilities, including perception, memory usage, belief updating, decision-making, counterfactual reasoning, and meta-reflection. | Used to assess LLMs' cognitive reflection capabilities, focusing on components such as perception of new information, memory usage, belief updating, and decision-making adjustments.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 273507547,
          "context_text": "From a cognitive science perspective, Reflection-Bench (Li et al., 2024) was designed to assess LLMs’ cognitive reflection capabilities, breaking down reflection into components like perception of new information, memory usage, belief updating following surprise, decision-making adjustments,…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "Reflection-Bench is mentioned as a tool to assess LLMs' cognitive reflection capabilities, which aligns with the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6875e36ecb05f73d9e5a98729af0b927bb4f94d6",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "272367007",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MatSciKB"
      ],
      "dataset_details": [
        {
          "dataset_name": "MatSciKB",
          "dataset_description": "Used to integrate literature text and web search results for materials science knowledge, enhancing the capabilities of LLMs in materials discovery. | Utilized to provide structural and textual data for MOF (Metal-Organic Framework) research, supporting the development of LLM-based agents in materials design.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 272367007,
          "context_text": "HoneyComb [36] BM25 + Contriever + LLM Literature text, web search MatSciKB T1 LLMatDesign [37] MatDeepLearn + TorchMD-Net + LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for…",
          "confidence_score": 0.85,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions 'MatSciKB' and 'Collection of MOF databases', which appear to be specific datasets or collections used in materials science research.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2409.00135",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a970be54c4df5f04c3fe65b7414e0c2879c55909",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "226965153",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "WebQSP"
      ],
      "dataset_details": [
        {
          "dataset_name": "WebQSP",
          "dataset_description": "Used for few-shot learning experiments in question answering, specifically for path generation with 4 shots. | Used to evaluate the searching success rate and reliable answering rate of the SRP framework, focusing on complex question answering over knowledge bases. | Used to assess SRP’s effectiveness in handling complex web questions, highlighting its superior performance over baselines. | Used to test the ability of SRP to effectively retrieve knowledge needed to answer questions through reliable planning and reflection. | Used to evaluate the effectiveness of reasoning paths generated by SRP in retrieving knowledge for question answering. | Used to evaluate SRP’s performance in complex question answering, demonstrating its reliability and accuracy through extensive testing. | Used to test reasoning capabilities in a question-answering system, focusing on generating and editing reasoning paths with few-shot prompts. | Used to test SRP's capability in answering complex, multi-hop questions over knowledge bases, with a focus on diverse and challenging query types. | Used for few-shot learning in path editing, focusing on 4-shot examples to enhance question answering on knowledge bases. | Used for few-shot learning experiments in question answering, specifically for path generation with 3 shots. | Used to assess the performance of SRP in generating reliable planning and reflection for complex question answering. | Used for few-shot learning in path editing, focusing on 5-shot examples to enhance question answering on knowledge bases. | Used to assess the generalization capabilities of SRP, maintaining over 95% reliability, highlighting the robustness of the system. | Used to assess SRP's performance in handling complex, multi-hop questions over web-scale knowledge graphs, emphasizing reasoning complexity. | Used to assess the effect of reference quality on model performance, demonstrating the superiority of carefully searched references over random ones. | Used to evaluate the reasoning ability of SRP in multi-hop question answering over knowledge bases, focusing on complex query structures.",
          "citing_paper_id": "278905229",
          "cited_paper_id": 226965153,
          "context_text": "• Extensive evaluations on WebQSP, CWQ, and GrailQA datasets demonstrate SRP’s superior performance compared to competitive base-lines, underscoring the reliability and accuracy of SRP.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used for evaluation, which are relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2505.19410",
          "cited_paper_doi": "10.1145/3442381.3449992",
          "citing_paper_url": "https://www.semanticscholar.org/paper/98bc1a0fca0e64f5f12f58ddc1bc0ece7e2f561a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/d21c5a9e34a6290212c68f6ec64f65ad5c5e46f4",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "34953552",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MiniWob"
      ],
      "dataset_details": [
        {
          "dataset_name": "MiniWob",
          "dataset_description": "Used to evaluate agents' performance in navigating and interacting with web pages, emphasizing exploration and task-solving skills. | Extended version of MiniWob, used to enhance and refine the assessment of navigation and task automation capabilities in web-based agents, focusing on more complex tasks. | Extended version of MiniWob, used to assess agents' ability to handle more complex web interactions and tasks. | Evaluates agents' ability to complete e-commerce tasks, such as searching for products and making purchases. | Used to assess navigation and task automation capabilities in web-based agents, providing a foundational framework for evaluating planning and interaction skills. | Used to evaluate web-based agents' performance on a variety of web tasks, focusing on navigation and interaction capabilities. | Assesses agents' ability to understand and execute web-based instructions, focusing on natural language processing and task completion.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 34953552,
          "context_text": "Early benchmarks such as MiniWob (Shi et al., 2017) and MiniWoB++ (Liu et al., 2018) provided fundamental frameworks for assessing navigation and task automation capabilities.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions MiniWob and MiniWoB++ as early benchmarks for assessing navigation and task automation capabilities, which are relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/298a55ddc9777e39c5bad92a750827e1cae98ac1",
          "citing_paper_year": 2025,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "263671594",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Path Planning from Natural Language (PPNL)"
      ],
      "dataset_details": [
        {
          "dataset_name": "Path Planning from Natural Language (PPNL)",
          "dataset_description": "Used to evaluate LLMs' path planning capabilities, focusing on end-to-end navigation while adhering to movement constraints and avoiding obstacles.",
          "citing_paper_id": "279070828",
          "cited_paper_id": 263671594,
          "context_text": "Specialized benchmarks, such as Path Planning from Natural Language (PPNL) (Aghzal et al., 2023) explored LLMs’ ability to perform end-to-end navigation while adhering to movement con-* straints and avoiding obstacles.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions a specialized benchmark called PPNL, which is used to evaluate LLMs' path planning capabilities. The context indicates that the dataset is used to test LLMs' ability to navigate while adhering to constraints and avoiding obstacles.",
          "citing_paper_doi": "10.48550/arXiv.2505.24306",
          "cited_paper_doi": "10.48550/arXiv.2310.03249",
          "citing_paper_url": "https://www.semanticscholar.org/paper/01126edb491ba4ecd315df09a51699b25d569c82",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e45d0221d00cd5b29a0a287d6037428bd53dc5aa",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "260334759",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ToolBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolBench",
          "dataset_description": "Used to conduct real-time evaluations of LLMs on real tasks via RapidAPI, focusing on the test set to assess performance and capability.",
          "citing_paper_id": "266999372",
          "cited_paper_id": 260334759,
          "context_text": "To assess the performance of LLMs for solving real tasks via RapidAPI, we follow the ToolEval method (Qin et al., 2023b) proposed by the Tool-Bench team to conduct a real-time evaluation on the test set of ToolBench.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'ToolBench' as a dataset used for evaluating LLMs on real tasks via RapidAPI. The dataset is specifically used for real-time evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2401.07324",
          "cited_paper_doi": "10.48550/arXiv.2307.16789",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ff61aef2fef3a235bfaa123158a990c4f5f27d1a",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "265294665",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Tabular Math Word Problems"
      ],
      "dataset_details": [
        {
          "dataset_name": "Tabular Math Word Problems",
          "dataset_description": "Used to evaluate the agentic plan caching framework on long-context financial data, focusing on data-intensive reasoning tasks. | Used to assess tabular and mathematical data reasoning, focusing on solving math problems presented in tabular formats. | Used to evaluate the agentic plan caching framework on tabular mathematical data, focusing on data-intensive reasoning tasks.",
          "citing_paper_id": "279447720",
          "cited_paper_id": 265294665,
          "context_text": "We evaluate our system on two data-intensive reasoning benchmarks: FinanceBench [23] (financial and long-context data reasoning) and Tabular Math Word Problems [34] (tabular and mathematical data reasoning).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "FinanceBench and Tabular Math Word Problems are mentioned as benchmarks used for evaluating the system. FinanceBench is specifically noted for financial and long-context data reasoning, while Tabular Math Word Problems is for tabular and mathematical data reasoning.",
          "citing_paper_doi": "10.48550/arXiv.2506.14852",
          "cited_paper_doi": "10.48550/arXiv.2311.11944",
          "citing_paper_url": "https://www.semanticscholar.org/paper/2e0736c159b3b2ff0f38c4e4eb63252936e4b9a3",
          "cited_paper_url": "https://www.semanticscholar.org/paper/89ed7fd00319d45269906a9b05e10c8680bf9cec",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "258108259",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AGIEval"
      ],
      "dataset_details": [
        {
          "dataset_name": "AGIEval",
          "dataset_description": "Repurposed to assess LLMs' ability to recognize and correct errors in medical question-answering tasks, providing insights into self-reflection and error correction. | Used to evaluate foundation models' reasoning and planning capabilities through multi-turn feedback loops, focusing on self-reflection and human-centric tasks. | Used to evaluate foundation models on human-centric tasks, focusing on reasoning and planning capabilities through multi-turn feedback loops.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 258108259,
          "context_text": "Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025).",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and challenges, but they are not specific datasets. However, 'AGIEval' and 'MedMCQA' are named resources that could be considered datasets in the context of evaluating LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2304.06364",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68c834c19cd126bbd6d25a3572d7205cfed76271",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "268230464",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "NetHack Learning Environment"
      ],
      "dataset_details": [
        {
          "dataset_name": "NetHack Learning Environment",
          "dataset_description": "Used to evaluate zero-shot capabilities of LLMs in complex game environments, highlighting the challenges and limitations of current models in mastering human-level tasks.",
          "citing_paper_id": "274150091",
          "cited_paper_id": 268230464,
          "context_text": "…contrast, BAL-ROG fills an important gap by providing a wide range of games at varying difficulties-including the NetHack Learning Environment (K¨uttler et al., 2020), which takes humans years to master, and where zero-shot LLMs struggle greatly, as also seen in prior work (Jeurissen et al., 2024).",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions the NetHack Learning Environment, which is a specific environment used for evaluating LLMs in game-playing scenarios. It is not a traditional dataset but a benchmark environment.",
          "citing_paper_doi": "10.48550/arXiv.2411.13543",
          "cited_paper_doi": "10.1109/CoG60054.2024.10645630",
          "citing_paper_url": "https://www.semanticscholar.org/paper/faf5f373bd9944028664ea3e7da2d6a1fe3bf335",
          "cited_paper_url": "https://www.semanticscholar.org/paper/81881d2589a34df12d5e2fd192d5354dda1f81a8",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "258170084",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DroidTask"
      ],
      "dataset_details": [
        {
          "dataset_name": "DroidTask",
          "dataset_description": "Used to evaluate the planning capabilities of LLMs in automating Android UI interactions, focusing on task completion and efficiency.",
          "citing_paper_id": "270869659",
          "cited_paper_id": 258170084,
          "context_text": "DroidTask (Wen et al., 2023a) and various un-named datasets (Liu et al., 2023b; Wen et al., 2023b) covering various mobile applications have also been established.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'DroidTask' and 'various un-named datasets', but only 'DroidTask' is a specific, verifiable dataset. The other datasets are not named and thus do not meet the criteria.",
          "citing_paper_doi": "10.48550/arXiv.2407.00993",
          "cited_paper_doi": "10.48550/arXiv.2304.07061",
          "citing_paper_url": "https://www.semanticscholar.org/paper/47aacaab789e80388d22598b4810213655e62888",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e64f7611d360022bba415436627cf3e00e4a90bc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "265019477",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PPTC Benchmark"
      ],
      "dataset_details": [
        {
          "dataset_name": "PPTC Benchmark",
          "dataset_description": "Used to evaluate large language models on 279 multi-round dialogue tasks for PPT file operations, focusing on the models' ability to complete specific PowerPoint tasks. | Used to evaluate the performance of LLM-based agents on PowerPoint tasks, focusing on task completion capabilities and assessing the planning and execution skills of the models.",
          "citing_paper_id": "270869659",
          "cited_paper_id": 265019477,
          "context_text": "PPTC Benchmark (Guo et al., 2023) proposed to evaluate the ability of LLM-based agents on PowerPoint tasks.",
          "confidence_score": 0.7,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions the 'PPTC Benchmark' which is a specific benchmark designed to evaluate LLM-based agents on PowerPoint tasks. However, it is primarily a benchmark or challenge rather than a traditional dataset.",
          "citing_paper_doi": "10.48550/arXiv.2407.00993",
          "cited_paper_doi": "10.48550/arXiv.2311.01767",
          "citing_paper_url": "https://www.semanticscholar.org/paper/47aacaab789e80388d22598b4810213655e62888",
          "cited_paper_url": "https://www.semanticscholar.org/paper/33af85c1b2b9325e096a5b8ee36af8d957a45914",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "220483148",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CommonQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CommonQA",
          "dataset_description": "Used to evaluate commonsense reasoning capabilities, focusing on background knowledge not explicitly stated in problems, employing RFF-G methodology. | Used to assess machine reading comprehension with logical reasoning, emphasizing the need for implicit background knowledge, utilizing RFF-G approach. | Used to evaluate various baselines in machine reading comprehension with logical reasoning, focusing on the performance differences between methods like Least-to-Most, Give-me-Hint, RFF, and CR.",
          "citing_paper_id": "279154984",
          "cited_paper_id": 220483148,
          "context_text": "These problems rely on background knowledge, which is usually not explicitly stated in the problems. we have conducted experiments on two widely used commonsense benchmarks: CommonQA (Talmor et al., 2018) and LogiQA (Liu et al., 2020) using RFF-G.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two benchmarks, CommonQA and LogiQA, which are used for evaluating commonsense reasoning. These are specific datasets used in the experiments.",
          "citing_paper_doi": "10.48550/arXiv.2506.03673",
          "cited_paper_doi": "10.24963/ijcai.2020/501",
          "citing_paper_url": "https://www.semanticscholar.org/paper/e18698918e315e6714d4f864fb3347cdf27fdb43",
          "cited_paper_url": "https://www.semanticscholar.org/paper/70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "272368347",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ToolACE"
      ],
      "dataset_details": [
        {
          "dataset_name": "ToolACE",
          "dataset_description": "Applied for instruction fine-tuning to align with complex agent environments, providing diverse and high-quality instruction-completion data. | Used for instruction fine-tuning to improve Hephaestus's function calling capabilities, enhancing its performance in complex agent environments. | Utilized for instruction fine-tuning to improve function calling in LLMs, enhancing interaction with complex agent environments. | Used for instruction fine-tuning to enhance Hephaestus's alignment with complex agent environments, focusing on high-quality instruction-completion tasks. | Used for instruction fine-tuning to enhance instruction-following capabilities, focusing on high-quality instruction-completion pairs. | Used to enhance function-calling capabilities of LLM agents, focusing on single-tool conversations to improve planning and execution in LLMs. | Used for instruction fine-tuning to refine Hephaestus's planning and execution capabilities in complex agent environments, focusing on high-quality instruction-completion tasks.",
          "citing_paper_id": "276250494",
          "cited_paper_id": 272368347,
          "context_text": "To further improve its instruction-following capabilities to align with complex agent environments, Hephaestus undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including ShareGPT (Chiang et al., 2023), ToolACE (Liu et al., 2024c), and AgentFlan (Chen et al., 2024b).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions specific datasets used for instruction fine-tuning of a model called Hephaestus. These datasets are named and appear to be relevant to the topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2502.06589",
          "cited_paper_doi": "10.48550/arXiv.2409.00920",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6ff06d054217164cdda7fb31c93665e431dec554",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0350636522997217df53553ddf3e472338bca97b",
          "citing_paper_year": 2025,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "57825680",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GTB"
      ],
      "dataset_details": [
        {
          "dataset_name": "GTB",
          "dataset_description": "Used to study model-free planning in environments with deceptive paths, focusing on the traversal of different sized levels to understand planning capabilities.",
          "citing_paper_id": "273233132",
          "cited_paper_id": 57825680,
          "context_text": "While there are datasets available from previous games [22, 23], GTB provides many different sized levels with deceptive paths to traverse.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'GTB' as a dataset providing levels with deceptive paths, which is relevant to the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2410.07765",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/abc377522b96b6f05c4ca0b063f9150a8e1d7198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/70dbcbcea2f804dd88f291235ae9781de511643a",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "249889477",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "PlanBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "PlanBench",
          "dataset_description": "Used to evaluate the performance of large language models on planning and reasoning tasks, specifically assessing their capabilities in understanding and generating plans.",
          "citing_paper_id": "273233132",
          "cited_paper_id": 249889477,
          "context_text": "The best-performing LLMs, such as GPT-4-Turbo and Claude-3-Opus , are among the state-of-the-art LLMs as well when it comes to benchmarks for natural language understanding tasks, code generation tasks, as well as on PlanBench [16].",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions PlanBench, which is a benchmark for evaluating LLMs on planning and reasoning tasks. It is used to assess the performance of LLMs in these specific areas.",
          "citing_paper_doi": "10.48550/arXiv.2410.07765",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/abc377522b96b6f05c4ca0b063f9150a8e1d7198",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "260334759",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Spider"
      ],
      "dataset_details": [
        {
          "dataset_name": "Spider",
          "dataset_description": "Used to evaluate text-to-SQL task performance, focusing on the ability of agents to generate SQL queries from natural language instructions. | Used to assess the capability of LLMs to interact with real-world APIs, emphasizing the integration and utilization of external tools. | Used to evaluate multi-hop reasoning skills, focusing on the ability to answer complex questions requiring information from multiple sources.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 260334759,
          "context_text": "StreamBench (Wu et al., 2024a) represents a more challenging setting, evaluating how agents leverage external memory components—including the memory of previous interactions and external feed-5 back—to continuously improve performance over time, with quality and efficiency assessed across diverse datasets including text-to-SQL tasks (e.g., Spider (Yu et al., 2018)), ToolBench (Xu et al., 2023), and HotpotQA (Yang et al., 2018).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific datasets used to evaluate the performance of agents in StreamBench, including Spider, ToolBench, and HotpotQA. These datasets are clearly identified and used for evaluating the planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2307.16789",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "262053695",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MINT"
      ],
      "dataset_details": [
        {
          "dataset_name": "MINT",
          "dataset_description": "Used to evaluate LLMs' planning capabilities in various domains, focusing on task-oriented and goal-directed behavior. | Used to evaluate LLMs in multi-turn interactions with tools and language feedback, focusing on the planning capabilities of LLMs in complex, interactive scenarios. | Used to test LLMs' ability to follow and generate procedural instructions, emphasizing sequential and logical reasoning. | Used to evaluate LLMs' numerical reasoning and strategic planning in solving arithmetic puzzles. | Used to assess LLMs' performance on grade school math problems, emphasizing step-by-step problem-solving skills. | Used to assess LLMs' performance in multi-turn interactions with tools and language feedback, focusing on dynamic and interactive planning. | Used to test LLMs' mathematical reasoning and problem-solving abilities, particularly in complex and multi-step problems. | Used to evaluate LLMs' ability to solve strategic questions, focusing on multi-step reasoning and planning. | Used to evaluate LLMs' performance in financial and operational planning tasks, focusing on real-world application scenarios. | Used to assess LLMs' planning capabilities in personalized financial and operational contexts, emphasizing adaptability and user-specific strategies.",
          "citing_paper_id": "277150776",
          "cited_paper_id": 262053695,
          "context_text": "…(Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use…",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several benchmarks and datasets, but only 'MINT' is a specific dataset used for evaluating LLMs in multi-turn interactions. Others are excluded as they are either benchmarks or tools.",
          "citing_paper_doi": "10.48550/arXiv.2503.16416",
          "cited_paper_doi": "10.48550/arXiv.2309.10691",
          "citing_paper_url": "https://www.semanticscholar.org/paper/1ac6b0d31ad221a6fb6b505585ccdb107d8b92cb",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "261276812",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "AgentMove"
      ],
      "dataset_details": [
        {
          "dataset_name": "AgentMove",
          "dataset_description": "Used to test LLMs' planning capabilities in planning tasks, focusing on spatiotemporal single-agent trajectories.",
          "citing_paper_id": "280047877",
          "cited_paper_id": 261276812,
          "context_text": "…[74] Trajectory Operational State Spatio-Temporal Single-Agent - CoPB [145] Trajectory Operational State Spatio-Temporal Single-Agent - LLM-Mob [173] Trajectory - Spatio-Temporal Single-Agent - AgentMove [39] Trajectory Operational State Spatio-Temporal Single-Agent - TrajAgent [29] Trajectory…",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions several trajectory datasets, which are relevant to the research topic of planning capabilities of LLMs. These datasets are used to evaluate the predictive performance of LLMs in human mobility scenarios.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2308.15197",
          "citing_paper_url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
          "cited_paper_url": "https://www.semanticscholar.org/paper/f4eea3bda63af47bd6ddc0b5a34ab04c558f0da2",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "258832847",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "IFEval"
      ],
      "dataset_details": [
        {
          "dataset_name": "IFEval",
          "dataset_description": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used for evaluating conversational question answering, focusing on multi-turn dialogue and contextual understanding. | Used for assessing factual consistency and alignment in language models, particularly in information extraction tasks. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used for evaluating commonsense reasoning, focusing on multi-choice questions that require understanding of everyday scenarios. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to evaluate hallucination in large language models, focusing on the accuracy and reliability of generated content. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 258832847,
          "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2305.11747",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e0384ba36555232c587d4a80d527895a095a9001",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "204241631",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Talk2Car"
      ],
      "dataset_details": [
        {
          "dataset_name": "Talk2Car",
          "dataset_description": "Used to provide human-like advice for navigating self-driving vehicles, focusing on natural language interactions to guide driving actions.",
          "citing_paper_id": "277824043",
          "cited_paper_id": 204241631,
          "context_text": "HAD and Talk2Car both contain human like advice to best navigate the car [9,22], while LaMPilot contains labels meant to evaluate transition from human commands to drive action [35].",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions HAD, Talk2Car, and LaMPilot as datasets containing human-like advice or labels for evaluating transitions from human commands to driving actions.",
          "citing_paper_doi": "10.48550/arXiv.2405.01533",
          "cited_paper_doi": "10.1109/CVPR.2019.01084",
          "citing_paper_url": "https://www.semanticscholar.org/paper/40fbc19197da3e414aa6e460c5e39789cf248100",
          "cited_paper_url": "https://www.semanticscholar.org/paper/dee86cf4ddab965169297960b55a4a2c0aee58d5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "260164518",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MoralChoice datasets"
      ],
      "dataset_details": [
        {
          "dataset_name": "MoralChoice datasets",
          "dataset_description": "Used to present moral dilemmas involving pedestrians to evaluate the moral beliefs encoded in LLMs, focusing on decision-making scenarios.",
          "citing_paper_id": "270216385",
          "cited_paper_id": 260164518,
          "context_text": "In the MoralChoice datasets (Scherrer et al., 2023), one example of a moral dilemma involves a scenario where the respondent is a driver approaching a pedestrian crossing the street.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MoralChoice datasets' which is a specific, multi-word proper noun. It is used to present moral dilemmas for evaluating moral beliefs in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2406.00936",
          "cited_paper_doi": "10.48550/arXiv.2307.14324",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4906654ecb2d42cfee13f46bebe2ec02bb6442b1",
          "cited_paper_url": "https://www.semanticscholar.org/paper/12acdfc7e32e9d603dc108008bb15e65439e7c79",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "265157752",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "CoQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "CoQA",
          "dataset_description": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to evaluate conversational question answering, focusing on multi-turn dialogue and contextual understanding in large language models. | Used to evaluate the instruction-following capabilities of large language models, focusing on the accuracy and reliability of responses to verifiable instructions. | Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to assess instruction-following capabilities, specifically evaluating the ability of models to execute complex instructions accurately. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used to evaluate the performance of UniReason-Qwen3-14B-SFT-no-think and UniReason-Qwen3-14B(RL) models, focusing on KL divergence in mathematical reasoning tasks. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness. | Used to evaluate multi-choice question answering, focusing on the ability of models to select correct answers from multiple options in various contexts. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to assess the instruction-following capabilities of UniReason-Qwen3-14B-SFT-no-think and UniReason-Qwen3-14B(RL) models, measuring KL divergence in instruction-following tasks. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to evaluate hallucination in language models, focusing on the generation of factually incorrect statements during task execution. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 265157752,
          "context_text": "…such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions several datasets used for evaluating large language models, particularly in non-reasoning tasks. These datasets are explicitly named and are relevant to the topic of planning capabilities.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2311.07911",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1a9b8c545ba9a6779f202e04639c2d67e6d34f63",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "255198985",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Behavior1K"
      ],
      "dataset_details": [
        {
          "dataset_name": "Behavior1K",
          "dataset_description": "Used to create 1,000 tasks tailored to human preferences, balancing task diversity and physical realism in embodied AI research using the OMNIGIBSON platform.",
          "citing_paper_id": "274789097",
          "cited_paper_id": 255198985,
          "context_text": "Behavior1K [14] created 1,000 tasks tailored to human preferences through surveys, achieving a balance between task diversity and physical realism based on the OMNIGIBSON platform.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The citation mentions 'Behavior1K' as a benchmark with 1,000 tasks, which is a specific dataset used for embodied AI research. It is clearly identified and used for creating tasks tailored to human preferences.",
          "citing_paper_doi": "10.48550/arXiv.2412.13178",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/afe1ccdf90a31a7422e8150d008c4fe7d75f9745",
          "cited_paper_url": "https://www.semanticscholar.org/paper/69764fcc646e4c608ac08eeb4c784cf8465268d2",
          "citing_paper_year": 2024,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "258865184",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "APIBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "APIBench",
          "dataset_description": "Mentioned as a public dataset, but specific usage in the research context is not detailed. | Used to connect large language models with a diverse set of machine learning APIs, enhancing the model's capabilities by integrating external services and functionalities.",
          "citing_paper_id": "272827086",
          "cited_paper_id": 258865184,
          "context_text": "APIBench (Patil et al., 2023) collects 1,716 machine learning APIs from 3 public model hubs.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "APIBench is a collection of machine learning APIs, which is not a traditional dataset but a resource. However, it is specific and verifiable, and it is used in the context of connecting large language models with APIs.",
          "citing_paper_doi": "10.48550/arXiv.2409.14826",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/ace1a82d97d024c26e588cef084dcb322f157811",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7d8905a1fd288068f12c8347caeabefd36d0dd6c",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "276344352",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "API-Bank"
      ],
      "dataset_details": [
        {
          "dataset_name": "API-Bank",
          "dataset_description": "Used to evaluate tool-augmented LLMs, specifically assessing performance on 264 annotated dialogues and 568 APIs, focusing on the integration and effectiveness of external tools.",
          "citing_paper_id": "272827086",
          "cited_paper_id": 276344352,
          "context_text": "Tool-augmented LLMs Datasets: The research community collects diverse datasets to facilitate research on tool-enhanced LLMs. API-Bank (Li et al., 2023) provides a benchmark that includes 264 annotated dialogues and 568 APIs.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "API-Bank is mentioned as a benchmark that includes annotated dialogues and APIs, which is relevant to the topic of planning capabilities of LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2409.14826",
          "cited_paper_doi": "10.48550/arXiv.2304.08244",
          "citing_paper_url": "https://www.semanticscholar.org/paper/ace1a82d97d024c26e588cef084dcb322f157811",
          "cited_paper_url": "https://www.semanticscholar.org/paper/6298fb744cb31c0ca3db96833edd57a0c974424b",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "215768690",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Disfluent Nav-igational Instruction Audio Dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Disfluent Nav-igational Instruction Audio Dataset",
          "dataset_description": "Used to conduct experiments evaluating TrustNavGPT's performance in navigation tasks, specifically focusing on disfluent navigational instructions in audio format.",
          "citing_paper_id": "271709656",
          "cited_paper_id": 215768690,
          "context_text": "3) We conduct experiments on a large-scale Disfluent Nav-igational Instruction Audio Dataset [14], RoboTHOR simulation environment [15], and also real-world setup, to show that TrustNavGPT significantly surpasses existing LLM-based navigation techniques, by a 55% improvement in achieving successful…",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a specific dataset 'Disfluent Nav-igational Instruction Audio Dataset' and a simulation environment 'RoboTHOR'. The dataset is used for experiments to evaluate TrustNavGPT's performance in navigation tasks.",
          "citing_paper_doi": "10.1109/IROS58592.2024.10801932",
          "cited_paper_doi": "10.1109/CVPR42600.2020.00323",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cc2ac34739f9a563eaa494760a5dc7140376f45",
          "cited_paper_url": "https://www.semanticscholar.org/paper/98789b46f7be983300a0e93e1c53bab56b36efd1",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "26419660",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MPE"
      ],
      "dataset_details": [
        {
          "dataset_name": "MPE",
          "dataset_description": "Used to evaluate the framework in a fully cooperative game setting, focusing on the performance of agents in a simple spread environment.",
          "citing_paper_id": "273186556",
          "cited_paper_id": 26419660,
          "context_text": "We evaluate our framework in MPE [7] simple spread environment which is a fully cooperative game.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'MPE' which stands for Multi-Agent Particle Environment, a known benchmark for multi-agent reinforcement learning. However, it does not specify a dataset but rather an environment for evaluation.",
          "citing_paper_doi": "10.48550/arXiv.2410.03997",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/f08e6657e93226e67c6cf19a1d76cb46e1535ede",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
          "citing_paper_year": 2024,
          "cited_paper_year": 2017
        }
      ]
    },
    {
      "cited_paper_id": "265295009",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "LiveCodeBench"
      ],
      "dataset_details": [
        {
          "dataset_name": "LiveCodeBench",
          "dataset_description": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to evaluate advanced computational problem-solving abilities, focusing on algorithmic and logical reasoning tasks. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to assess graduate-level reasoning and problem-solving skills, containing complex questions that require deep understanding and critical thinking. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to evaluate code generation and reasoning capabilities, focusing on live coding tasks and problem-solving skills. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 265295009,
          "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
          "confidence_score": 0.9,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/210b0a3d76e93079cc51b03c4115fde545eea966",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "222208810",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ALFWorld Benchmark"
      ],
      "dataset_details": [
        {
          "dataset_name": "ALFWorld Benchmark",
          "dataset_description": "Used to conduct experiments on planning capabilities in LLMs, integrating TextWorld to generate textual scenarios in household settings, focusing on text-based embodied interaction.",
          "citing_paper_id": "275546429",
          "cited_paper_id": 222208810,
          "context_text": "We conducted the experiments on the ALFWorld Benchmark [27], which integrates the TextWorld engine [28] to generate textual scenarios in household settings, thereby providing a text-based embodied interaction environment.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "ALFWorld Benchmark is mentioned as the environment where experiments were conducted, integrating TextWorld for generating textual scenarios.",
          "citing_paper_doi": "10.1109/PRAI62207.2024.10826957",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/4eb4952d69655253f49e25293adbb36d9b9ac1a4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2024,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "271497434",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MuEP"
      ],
      "dataset_details": [
        {
          "dataset_name": "MuEP",
          "dataset_description": "Used for fine-tuning models in complex embodied planning applications, extending the ALF-World dataset to include multimodal data. | Serves as the base dataset for MuEP, providing foundational data for embodied planning tasks.",
          "citing_paper_id": "275546429",
          "cited_paper_id": 271497434,
          "context_text": "These metrics together furnish a comprehensive framework for evaluating agent proficiency in complex embodied planning applications. c) Implementation Details: In this study, we use MuEP [30] demonstration dataset, extended from the ALF-World dataset, for model fine-tuning.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the 'MuEP' and 'ALF-World' datasets, which are used for model fine-tuning in the study. Both are specific, verifiable datasets.",
          "citing_paper_doi": "10.1109/PRAI62207.2024.10826957",
          "cited_paper_doi": "10.24963/ijcai.2024/15",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4eb4952d69655253f49e25293adbb36d9b9ac1a4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/4bf1d4946ec440a00f2521f861d14a29e81712a5",
          "citing_paper_year": 2024,
          "cited_paper_year": 2024
        }
      ]
    },
    {
      "cited_paper_id": "202541184",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MC-TACO"
      ],
      "dataset_details": [
        {
          "dataset_name": "MC-TACO",
          "dataset_description": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue. | Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning. | Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems. | Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge. | Used to evaluate models' temporal commonsense, focusing on aspects such as duration, ordering, typical time, frequency, and stationarity in planning scenarios. | Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference. | Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks. | Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks. | Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately. | Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks. | Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately. | Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions. | Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 202541184,
          "context_text": "• MC-TACO (Zhou et al., 2019): It is a multiple-choice benchmark designed to evaluate models’ temporal commonsense, covering duration, ordering, typical time, frequency, and stationarity.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "MC-TACO is a specific benchmark dataset designed to evaluate temporal commonsense in models, which aligns with the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.18653/v1/D19-1332",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/81b4920ad488affaee27389ff9540b7fea90a4ce",
          "citing_paper_year": 2025,
          "cited_paper_year": 2019
        }
      ]
    },
    {
      "cited_paper_id": "258823112",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "semantic anomaly dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "semantic anomaly dataset",
          "dataset_description": "Used to evaluate self-driving scenario ablations, focusing on detecting semantic anomalies using large language models.",
          "citing_paper_id": "271098044",
          "cited_paper_id": 258823112,
          "context_text": "In our ablations on self-driving scenarios, we adopt the semantic anomaly dataset presented in [12].",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions a 'semantic anomaly dataset' which is specific and plausible. The cited paper title confirms it is a dataset used for semantic anomaly detection.",
          "citing_paper_doi": "10.48550/arXiv.2407.08735",
          "cited_paper_doi": "10.1007/s10514-023-10132-6",
          "citing_paper_url": "https://www.semanticscholar.org/paper/42989a1798d447dbb201bfb5f0e2acdf9958affd",
          "cited_paper_url": "https://www.semanticscholar.org/paper/735ea38114bd2406a8bbc7f060cba1fc7a254d89",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "207167677",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Freebase"
      ],
      "dataset_details": [
        {
          "dataset_name": "Freebase",
          "dataset_description": "Used as an external knowledge graph to provide structured information for the datasets, enhancing the semantic richness of the data.",
          "citing_paper_id": "273707190",
          "cited_paper_id": 207167677,
          "context_text": "All three datasets rely on the external knowledge graph from Freebase [5].",
          "confidence_score": 0.7,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'external knowledge graph from Freebase' which is a specific, verifiable resource. However, it does not specify how it is used in the research context.",
          "citing_paper_doi": "10.48550/arXiv.2410.23875",
          "cited_paper_doi": "10.1145/1376616.1376746",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a5997b04bef7c9ccc84c3bd8d8a5d477936931f4",
          "cited_paper_url": "https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002",
          "citing_paper_year": 2024,
          "cited_paper_year": 2008
        }
      ]
    },
    {
      "cited_paper_id": "237504552",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "MiniGrid"
      ],
      "dataset_details": [
        {
          "dataset_name": "MiniGrid",
          "dataset_description": "Used to evaluate the approach on solving complex, long-horizon tasks, demonstrating faster learning compared to baseline methods.",
          "citing_paper_id": "264558778",
          "cited_paper_id": 237504552,
          "context_text": "We evaluate this approach on several simulated environments (MiniGrid [5], SkillHack [17], and Crafter [11]), showing that it can learn to solve complex, long-horizon tasks much faster than baseline methods.",
          "confidence_score": 0.8,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The citation mentions specific environments used for evaluation, but they are not datasets in the traditional sense. They are simulation environments used to test agent capabilities.",
          "citing_paper_doi": "10.48550/arXiv.2311.05596",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d84c43779a2feabb0070a3affd25db0d71d7711d",
          "cited_paper_url": "https://www.semanticscholar.org/paper/8e128a1b2efb0ddf688902ade4405d22d5b61eec",
          "citing_paper_year": 2023,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Ask-before-Plan"
      ],
      "dataset_details": [
        {
          "dataset_name": "Ask-before-Plan",
          "dataset_description": "Used to build a dataset for evaluating planning capabilities in language models, focusing on interactive travel planning scenarios. | Served as the foundation for the Ask-before-Plan dataset, providing travel planning data for model training and evaluation.",
          "citing_paper_id": "270561990",
          "cited_paper_id": null,
          "context_text": "The Ask-before-Plan dataset was built from the TravelPlanner dataset (Xie et al., 2024), which is publicly available.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two datasets: 'Ask-before-Plan' and 'TravelPlanner'. Both are specific and have clear identifiers.",
          "citing_paper_doi": "10.48550/arXiv.2406.12639",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
          "cited_paper_url": null,
          "citing_paper_year": 2024,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "222208810",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ScienceWorld"
      ],
      "dataset_details": [
        {
          "dataset_name": "ScienceWorld",
          "dataset_description": "Used for textual science experiment tasks, focusing on the planning capabilities of LLMs in solving complex scientific problems through text-based interactions. | Used for embodied household tasks, evaluating the planning capabilities of LLMs in navigating and interacting with simulated environments.",
          "citing_paper_id": "276765428",
          "cited_paper_id": 222208810,
          "context_text": "Datasets We conducted experiments on two representative agent datasets: ScienceWorld (Wang et al., 2022) for textual science experiment tasks and ALFWorld (Shridhar et al., 2020) for embodied household tasks.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, ScienceWorld and ALFWorld, which are used for conducting experiments on textual science experiment tasks and embodied household tasks, respectively.",
          "citing_paper_doi": "10.48550/arXiv.2503.02682",
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/d03c8be41ff513c2bbfd991c068774b92e30d8e9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb",
          "citing_paper_year": 2025,
          "cited_paper_year": 2020
        }
      ]
    },
    {
      "cited_paper_id": "252762102",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Compositional Celebrities"
      ],
      "dataset_details": [
        {
          "dataset_name": "Compositional Celebrities",
          "dataset_description": "Used to evaluate the performance of REBEL compared to GPT3, focusing on question-answering capabilities without external search tools. | Used to evaluate the compositionality gap in language models, focusing on the 'Birthyear NobelLiterature' category to assess model performance on compositional tasks. | Used to evaluate compositionality in language models by posing 8.6k questions about celebrities in different categories, focusing on the model's ability to understand and generate compositional language. | Used to test REBEL's ability to handle compositional reasoning in language models, focusing on the compositionality gap in celebrity-related tasks. | Used to evaluate the REBEL algorithm's performance on reasoning tasks involving current facts, focusing on compositionality in language models.",
          "citing_paper_id": "262084273",
          "cited_paper_id": 252762102,
          "context_text": "We tested REBEL on 3 datasets: Compositional Celebrities (Press et al., 2022), FEVER (Thorne et al.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets, 'Compositional Celebrities' and 'FEVER', which are used to test the REBEL model. These datasets are clearly named and relevant to the research topic.",
          "citing_paper_doi": "10.48550/arXiv.2309.11688",
          "cited_paper_doi": "10.48550/arXiv.2210.03350",
          "citing_paper_url": "https://www.semanticscholar.org/paper/4f06064a27cb38e8fc2e65a67dc8ddb770e1c176",
          "cited_paper_url": "https://www.semanticscholar.org/paper/e070ff286709db28312e08b52b05539debe88146",
          "citing_paper_year": 2023,
          "cited_paper_year": 2022
        }
      ]
    },
    {
      "cited_paper_id": "278768680",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "General-Reasoner"
      ],
      "dataset_details": [
        {
          "dataset_name": "General-Reasoner",
          "dataset_description": "Used to explore the effect of training data distribution for SFT-based reasoning models, containing 232K examples across various reasoning and non-reasoning tasks. | Used to explore the effect of training data distribution on SFT-based reasoning models, containing 232K examples across various reasoning and non-reasoning tasks.",
          "citing_paper_id": "280146966",
          "cited_paper_id": 278768680,
          "context_text": "…explore the effect of training data distribution for SFT-based reasoning models, we also distill a larger and more comprehensive dataset collected from General-Reasoner (Ma et al., 2025), which contains 232K examples across reasoning and non-reasoning tasks (e.g., Math, Chemistry, Business).",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions a specific dataset 'General-Reasoner' with a clear size and content description, which is relevant to the research topic of planning capabilities of LLMs.",
          "citing_paper_doi": null,
          "cited_paper_doi": "10.48550/arXiv.2505.14652",
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": "https://www.semanticscholar.org/paper/04c05c6acc970f2ca89af8e436b8dd8189396146",
          "citing_paper_year": 2025,
          "cited_paper_year": 2025
        }
      ]
    },
    {
      "cited_paper_id": "unknown",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "DeepScaler"
      ],
      "dataset_details": [
        {
          "dataset_name": "DeepScaler",
          "dataset_description": "Used to fine-tune Qwen3-14B, enhancing the model's ability to handle complex mathematical scaling tasks.",
          "citing_paper_id": "280146966",
          "cited_paper_id": null,
          "context_text": "We fine-tune Qwen3-14B (Team, 2025b) on the high-quality math dataset derived from MATH and DeepScaler (Luo et al., 2025).",
          "confidence_score": 0.85,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'high-quality math dataset derived from MATH and DeepScaler'. MATH is a known dataset, and DeepScaler appears to be a specific resource. However, 'high-quality math dataset' is too generic.",
          "citing_paper_doi": null,
          "cited_paper_doi": null,
          "citing_paper_url": "https://www.semanticscholar.org/paper/99d9293e91abce69918aa7c9db8ba2c6ad646cba",
          "cited_paper_url": null,
          "citing_paper_year": 2025,
          "cited_paper_year": null
        }
      ]
    },
    {
      "cited_paper_id": "265351664",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "GAIA"
      ],
      "dataset_details": [
        {
          "dataset_name": "GAIA",
          "dataset_description": "Used to evaluate general AI assistants without restrictions on tools, focusing on the capabilities and performance of AI agents in various tasks.",
          "citing_paper_id": "275133533",
          "cited_paper_id": 265351664,
          "context_text": "Some datasets, such as GAIA [Mialon et al. , 2023], even evaluate agents without any restriction on the tools allowed.",
          "confidence_score": 0.8,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "GAIA is mentioned as a benchmark, but since it is used to evaluate agents and involves specific datasets, it is included as a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2412.21033",
          "cited_paper_doi": "10.48550/arXiv.2311.12983",
          "citing_paper_url": "https://www.semanticscholar.org/paper/a7487e90969663d7d18709cf6b30c025f3b0833c",
          "cited_paper_url": "https://www.semanticscholar.org/paper/ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd",
          "citing_paper_year": 2024,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "230799347",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "StrategyQA"
      ],
      "dataset_details": [
        {
          "dataset_name": "StrategyQA",
          "dataset_description": "Used to evaluate implicit reasoning strategies in question answering, focusing on multi-step reasoning and strategy formulation. | Used to assess RAP on planning tasks in a simulated environment, evaluating its ability to solve block manipulation problems. | Used to evaluate RAP on optimization problems, testing its ability to efficiently pack items into bins. | Used to evaluate RAP on grade school math word problems, focusing on multi-step reasoning and problem-solving strategies. | Used to evaluate RAP on strategic reasoning tasks, assessing its capability to plan and execute multi-step solutions. | Applied to assess complex question answering, emphasizing multi-hop reasoning and evidence aggregation. | Used to assess RAP on arithmetic puzzle solving, focusing on numerical reasoning and strategic planning. | Used to test RAP on complex question answering tasks, evaluating its ability to handle implicit reasoning and contextual understanding. | Used to assess RAP on multiple-choice questions requiring implicit reasoning, emphasizing logical and commonsense inference. | Used to test RAP on algorithmic problem solving, focusing on step-by-step planning to solve the cube.",
          "citing_paper_id": "276421789",
          "cited_paper_id": 230799347,
          "context_text": "We evaluate this type of reasoning using the StrategyQA Geva et al. [2021] and HotPotQA Yang et al. [2018] datasets.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions two specific datasets used for evaluating reasoning capabilities, which are directly relevant to the research topic of planning capabilities in LLMs.",
          "citing_paper_doi": "10.48550/arXiv.2502.12521",
          "cited_paper_doi": "10.1162/tacl_a_00370",
          "citing_paper_url": "https://www.semanticscholar.org/paper/c1fe3ddae2443ffd11cb62e76537af0ee207833f",
          "cited_paper_url": "https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "49317780",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "ActivityPrograms"
      ],
      "dataset_details": [
        {
          "dataset_name": "ActivityPrograms",
          "dataset_description": "Reconstructed to create a pairwise safety preference dataset, focusing on optimizing Safe-Align by simulating household activities via programs. | Used to train models in the Supervised Fine-Tuning (SFT) phase, focusing on simulating household activities via programs. The dataset provides structured activity sequences for training. | Used to evaluate task-planning safety of embodied agents, focusing on simulating household activities via programs. | Used to evaluate task-planning safety of embodied agents, encompassing a wide range of tasks and recording performance.",
          "citing_paper_id": "277955666",
          "cited_paper_id": 49317780,
          "context_text": "To optimize Safe-Align, we reconstruct the existing embodied dataset, ActivityPrograms (Puig et al., 2018), to create a corresponding pairwise safety preference dataset.",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions the reconstruction of an existing embodied dataset, ActivityPrograms, to create a new safety preference dataset. ActivityPrograms is a specific, verifiable dataset.",
          "citing_paper_doi": "10.48550/arXiv.2504.14650",
          "cited_paper_doi": "10.1109/CVPR.2018.00886",
          "citing_paper_url": "https://www.semanticscholar.org/paper/88307eeb6d60a38f3fcc52a5683f29cfaf46d5ce",
          "cited_paper_url": "https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5",
          "citing_paper_year": 2025,
          "cited_paper_year": 2018
        }
      ]
    },
    {
      "cited_paper_id": "235428843",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Autoscale benchmark set"
      ],
      "dataset_details": [
        {
          "dataset_name": "Autoscale benchmark set",
          "dataset_description": "Used for exploratory experiments during pipeline development, focusing on classical planning capabilities across ten distinct domains.",
          "citing_paper_id": "277940718",
          "cited_paper_id": 235428843,
          "context_text": "We used a disjoint set of ten domains from the Autoscale benchmark set [79] for exploratory experiments while developing our pipeline.",
          "confidence_score": 0.9,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The context mentions 'Autoscale benchmark set' which is a specific benchmark set used for exploratory experiments in classical planning. It is a reusable resource and fits the criteria for a dataset.",
          "citing_paper_doi": "10.48550/arXiv.2503.18809",
          "cited_paper_doi": "10.1609/icaps.v31i1.15983",
          "citing_paper_url": "https://www.semanticscholar.org/paper/172d755052521b2ee12a79b73443daf6c9976e28",
          "cited_paper_url": "https://www.semanticscholar.org/paper/68f72c6f4a38257ef10b5eac4ac6b083fcbe120d",
          "citing_paper_year": 2025,
          "cited_paper_year": 2021
        }
      ]
    },
    {
      "cited_paper_id": "259075316",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "OpenOrca"
      ],
      "dataset_details": [
        {
          "dataset_name": "OpenOrca",
          "dataset_description": "Used for training and evaluating models on math-related Q&A, focusing on problem-solving and reasoning skills. | Used for training and evaluating models on physical interaction Q&A, focusing on understanding and reasoning about physical scenarios. | Used for training and evaluating models on web-based instructions, enhancing the model's ability to follow and generate instructions. | Used for training and evaluating models on advanced reasoning and commonsense Q&A, enhancing the model's ability to handle challenging problems. | Used for training and evaluating models on general Q&A tasks, providing a diverse set of complex explanation traces.",
          "citing_paper_id": "280010642",
          "cited_paper_id": 259075316,
          "context_text": "Several datasets are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and…",
          "confidence_score": 1.0,
          "citation_intent": [
            "b",
            "o",
            "r",
            "a",
            " ",
            "s",
            "c",
            "u",
            "l",
            "e"
          ],
          "resource_type": [
            "a",
            "t",
            "d",
            "s",
            "e"
          ],
          "reasoning": "The context mentions several datasets used to support IFT, which are specific and identifiable. These datasets are used for training and evaluating models on various types of questions, particularly in the domain of science and mathematics.",
          "citing_paper_doi": "10.48550/arXiv.2506.20743",
          "cited_paper_doi": "10.48550/arXiv.2306.02707",
          "citing_paper_url": "https://www.semanticscholar.org/paper/6cd8eaab70b0eb3da600651c4141b76e824dea77",
          "cited_paper_url": "https://www.semanticscholar.org/paper/0244aeb7c6927e2fb0c2e668687e160a00737dbe",
          "citing_paper_year": 2025,
          "cited_paper_year": 2023
        }
      ]
    },
    {
      "cited_paper_id": "248366629",
      "citation_count": 0,
      "total_dataset_mentions": 1,
      "unique_datasets": [
        "Delivery dataset"
      ],
      "dataset_details": [
        {
          "dataset_name": "Delivery dataset",
          "dataset_description": "Used to train and evaluate models on delivery route planning, specifically assessing generalization from small-scale (9–17 locations) to larger-scale (70–100 locations) deliveries.",
          "citing_paper_id": "276409203",
          "cited_paper_id": 248366629,
          "context_text": "For example, in the Delivery dataset (Yang et al., 2022), models trained on small-scale deliveries (9–17 locations) can generalize to larger ones (70–100 locations) using the same core strategy.",
          "confidence_score": 1.0,
          "citation_intent": "reusable resource",
          "resource_type": "dataset",
          "reasoning": "The 'Delivery dataset' is mentioned as a specific dataset used to train and evaluate models on delivery route planning tasks.",
          "citing_paper_doi": "10.48550/arXiv.2502.11221",
          "cited_paper_doi": "10.48550/arXiv.2204.10420",
          "citing_paper_url": "https://www.semanticscholar.org/paper/06552667bd28b30a314de7caefda0e2b8226fab9",
          "cited_paper_url": "https://www.semanticscholar.org/paper/a7ae577fca451ea64e19b918bcc2598d7e50eb73",
          "citing_paper_year": 2025,
          "cited_paper_year": 2022
        }
      ]
    }
  ],
  "citation_count_distribution": {
    "2320080": 1,
    "18976919": 2,
    "24331484": 1,
    "37925315": 2,
    "160025533": 7,
    "204838007": 6,
    "218971783": 25,
    "220618602": 1,
    "222066988": 2,
    "239998651": 7,
    "246035276": 17,
    "246485514": 3,
    "248366629": 2,
    "250451569": 8,
    "251979509": 1,
    "257532815": 15,
    "258298051": 10,
    "258564677": 1,
    "258762760": 4,
    "258865812": 8,
    "258865907": 3,
    "258947337": 4,
    "258959321": 3,
    "259138811": 3,
    "259950998": 5,
    "261100610": 1,
    "261245497": 2,
    "262825203": 2,
    "263671594": 2,
    "263829338": 2,
    "263829697": 3,
    "264172681": 1,
    "264405734": 2,
    "265128575": 3,
    "267782588": 2,
    "267897975": 2,
    "269214439": 2,
    "269982497": 1,
    "270357425": 2,
    "270371268": 1,
    "270562133": 1,
    "271745636": 1,
    "272367167": 1,
    "272770270": 1,
    "273374872": 2,
    "273502635": 1,
    "273507560": 1,
    "274234014": 1,
    "274763098": 1,
    "277313841": 1,
    "279621061": 2,
    "16946362": 1,
    "59656859": 9,
    "209485573": 1,
    "218470560": 1,
    "257496672": 2,
    "269626390": 2,
    "11080756": 1,
    "201646309": 4,
    "236258746": 1,
    "247155069": 1,
    "258218323": 1,
    "258535542": 1,
    "3752019": 1,
    "235294299": 2,
    "254854675": 2,
    "51876975": 1,
    "204852201": 1,
    "218869575": 4,
    "222208810": 12,
    "231591445": 9,
    "246411621": 14,
    "252762395": 22,
    "258179774": 1,
    "258833055": 13,
    "261064713": 9,
    "211204736": 1,
    "221191193": 1,
    "254854219": 1,
    "254877499": 1,
    "258832372": 2,
    "259129428": 4,
    "260164780": 3,
    "260351380": 1,
    "261048935": 1,
    "265295561": 1,
    "267406772": 1,
    "7164502": 2,
    "14337532": 1,
    "204241631": 1,
    "231879586": 1,
    "237194835": 1,
    "248476411": 2,
    "252439240": 1,
    "253735327": 1,
    "257364842": 3,
    "257631498": 1,
    "257687420": 2,
    "258587978": 1,
    "261706176": 1,
    "264490392": 1,
    "266725320": 1,
    "267750682": 2,
    "229853": 1,
    "5340880": 1,
    "7828197": 1,
    "10956233": 1,
    "13195796": 1,
    "20817484": 1,
    "38193578": 2,
    "212658007": 1,
    "251647156": 1,
    "257687798": 1,
    "259341893": 1,
    "263830368": 1,
    "264305599": 1,
    "268531200": 1,
    "58377815": 1,
    "208617407": 6,
    "231839855": 1,
    "246275593": 1,
    "252968000": 1,
    "254408960": 7,
    "255198985": 2,
    "259342833": 2,
    "270123427": 1,
    "271212307": 1,
    "271571434": 6,
    "271769045": 1,
    "271854813": 1,
    "15912959": 1,
    "132910614": 1,
    "256697342": 11,
    "2329216": 1,
    "123106881": 2,
    "229363334": 1,
    "247939706": 14,
    "252355542": 10,
    "253801709": 5,
    "257663729": 5,
    "258833332": 1,
    "267406155": 2,
    "536614": 1,
    "2307186": 1,
    "2366653": 1,
    "2738870": 1,
    "4706694": 1,
    "9225837": 1,
    "12316599": 1,
    "13771609": 1,
    "14217178": 1,
    "16238232": 1,
    "16261041": 1,
    "16716972": 1,
    "33551774": 1,
    "49575415": 1,
    "52012313": 1,
    "61817892": 1,
    "160018920": 1,
    "189048448": 1,
    "199405608": 1,
    "201093978": 1,
    "204960852": 1,
    "206805969": 1,
    "210926937": 1,
    "218981884": 1,
    "221308729": 1,
    "235359276": 1,
    "235651885": 1,
    "236034184": 1,
    "236090307": 1,
    "236447420": 1,
    "236945351": 1,
    "236985291": 1,
    "238353829": 1,
    "240302757": 1,
    "245144651": 1,
    "245986665": 1,
    "246240065": 1,
    "247022700": 1,
    "247897432": 1,
    "251253219": 1,
    "251371611": 1,
    "252089119": 1,
    "257365015": 1,
    "257912912": 1,
    "258685266": 1,
    "258762525": 8,
    "258841776": 1,
    "259137864": 1,
    "259241297": 1,
    "259262186": 2,
    "260334759": 14,
    "261030303": 3,
    "261100760": 1,
    "262055731": 1,
    "263671690": 1,
    "263834585": 1,
    "263909014": 2,
    "264930510": 1,
    "265019126": 1,
    "265607979": 2,
    "265659006": 1,
    "266176951": 1,
    "266435934": 1,
    "266693892": 1,
    "267060885": 1,
    "267069400": 1,
    "268239825": 1,
    "271533450": 1,
    "271544382": 1,
    "142610973": 1,
    "256598146": 7,
    "258865184": 7,
    "258967184": 6,
    "259262376": 1,
    "276344352": 2,
    "1397894": 3,
    "40099475": 1,
    "59524001": 1,
    "67788344": 1,
    "84186410": 1,
    "184469323": 1,
    "210971709": 1,
    "220828823": 1,
    "246681316": 1,
    "249461514": 1,
    "252693035": 1,
    "259991167": 1,
    "266051233": 1,
    "266162896": 1,
    "267616761": 1,
    "277626231": 1,
    "1100293": 1,
    "1906145": 1,
    "12198630": 1,
    "15693605": 1,
    "233324485": 2,
    "246426909": 8,
    "257219404": 8,
    "258841029": 1,
    "258887849": 5,
    "259095869": 1,
    "260293142": 2,
    "1553193": 2,
    "52055325": 1,
    "202541184": 1,
    "202660943": 1,
    "221516475": 5,
    "232134851": 5,
    "247011290": 1,
    "247595263": 13,
    "253237047": 1,
    "256415991": 1,
    "258832847": 1,
    "258833200": 1,
    "258967391": 1,
    "261030818": 1,
    "261696697": 1,
    "262084051": 1,
    "264172893": 1,
    "265157752": 1,
    "265295009": 3,
    "265608902": 1,
    "266209760": 1,
    "266359670": 1,
    "267412590": 1,
    "267412607": 2,
    "267770504": 1,
    "269457450": 1,
    "270380229": 1,
    "270560678": 1,
    "273228858": 2,
    "274234789": 1,
    "275789950": 4,
    "275932560": 1,
    "276079693": 1,
    "276116814": 1,
    "276422318": 1,
    "277451641": 1,
    "278715317": 1,
    "278768680": 1,
    "278788524": 1,
    "278911988": 1,
    "279070806": 1,
    "279154475": 1,
    "279447674": 1,
    "280000246": 1,
    "199541993": 1,
    "210836616": 1,
    "218595872": 1,
    "233322292": 1,
    "245023520": 1,
    "252284068": 1,
    "257512189": 1,
    "258558102": 4,
    "269005338": 1,
    "9944290": 1,
    "15120295": 1,
    "250390581": 1,
    "257445349": 1,
    "264491049": 1,
    "267069469": 1,
    "269292966": 1,
    "6138957": 1,
    "28328610": 3,
    "201782024": 1,
    "219964473": 1,
    "254591260": 2,
    "257205781": 2,
    "257757298": 1,
    "258841057": 1,
    "263134620": 1,
    "267068760": 1,
    "9953039": 1,
    "235458009": 4,
    "245329531": 4,
    "249017698": 2,
    "259370794": 1,
    "260682249": 1,
    "265129059": 4,
    "265351664": 2,
    "267406800": 5,
    "277467626": 1,
    "10097708": 1,
    "26553672": 1,
    "60447873": 1,
    "80628296": 1,
    "140952910": 1,
    "189656355": 1,
    "199437458": 1,
    "221668003": 1,
    "253410677": 1,
    "255826858": 1,
    "259737661": 1,
    "259837542": 4,
    "265295011": 1,
    "268777626": 1,
    "1033682": 1,
    "2682274": 1,
    "7338543": 1,
    "9026666": 1,
    "127986044": 1,
    "233296711": 1,
    "248097655": 1,
    "248986576": 1,
    "249926846": 1,
    "253510037": 1,
    "255372955": 1,
    "258059885": 1,
    "258865950": 1,
    "263151865": 1,
    "15238391": 1,
    "222066674": 1,
    "256827430": 1,
    "258179056": 4,
    "263625818": 1,
    "265019021": 1,
    "103456": 2,
    "17746168": 1,
    "42886148": 1,
    "49317780": 5,
    "252519594": 5,
    "258564887": 2,
    "259089333": 1,
    "259141622": 5,
    "259274760": 3,
    "260125969": 1,
    "260438420": 1,
    "8347993": 1,
    "233305616": 1,
    "248986485": 1,
    "249017743": 4,
    "250311260": 1,
    "258960174": 1,
    "261100919": 3,
    "264935717": 2,
    "265185500": 1,
    "266906759": 2,
    "268042527": 3,
    "268531996": 1,
    "246634264": 1,
    "259360395": 1,
    "259936967": 1,
    "261817592": 7,
    "265067168": 1,
    "267412980": 4,
    "269921354": 2,
    "276489554": 1,
    "13756489": 8,
    "230799347": 3,
    "264935408": 1,
    "267094920": 1,
    "267759583": 1,
    "270703266": 1,
    "274788614": 1,
    "4673790": 1,
    "91184540": 2,
    "121987403": 1,
    "254877310": 1,
    "259342058": 3,
    "269293529": 1,
    "270258182": 1,
    "17305": 2,
    "1478442": 1,
    "2126705": 1,
    "2432261": 1,
    "2730999": 2,
    "5642010": 1,
    "5753706": 1,
    "6384075": 1,
    "6552475": 1,
    "7138783": 1,
    "8065594": 1,
    "8623866": 1,
    "10448069": 1,
    "10684261": 1,
    "10791590": 1,
    "11337237": 2,
    "11481542": 1,
    "16652519": 1,
    "17099383": 1,
    "19104534": 1,
    "19182808": 1,
    "21698093": 1,
    "28467205": 1,
    "32556638": 1,
    "126643659": 1,
    "206799161": 3,
    "208512881": 1,
    "210928826": 1,
    "219188633": 1,
    "235349223": 1,
    "235428843": 1,
    "235586367": 1,
    "237581157": 1,
    "245827334": 1,
    "246527904": 1,
    "247778891": 1,
    "248721898": 1,
    "250334236": 1,
    "250406114": 1,
    "254017827": 1,
    "256274690": 1,
    "256846992": 2,
    "259320426": 1,
    "259854663": 1,
    "260440590": 4,
    "262269888": 2,
    "265130666": 1,
    "265957829": 1,
    "266223700": 1,
    "266359688": 1,
    "268297180": 2,
    "268680720": 1,
    "268961846": 1,
    "269614003": 1,
    "270158113": 1,
    "270170904": 1,
    "270620065": 1,
    "271504408": 1,
    "271571035": 3,
    "271719990": 4,
    "273098440": 1,
    "273589540": 1,
    "274776284": 1,
    "276079564": 1,
    "276482470": 1,
    "276618008": 1,
    "6326401": 1,
    "19100351": 1,
    "47012216": 1,
    "53513571": 2,
    "209366827": 1,
    "209515572": 1,
    "214774912": 1,
    "219124336": 1,
    "220280457": 1,
    "220404390": 1,
    "220551746": 1,
    "231698518": 1,
    "231730714": 1,
    "237581476": 1,
    "246575985": 1,
    "258987581": 1,
    "260865987": 1,
    "261395685": 1,
    "261823722": 1,
    "263832826": 1,
    "264128019": 1,
    "264146567": 1,
    "267406588": 1,
    "267411892": 3,
    "267413218": 1,
    "267897865": 1,
    "269539563": 1,
    "270285926": 1,
    "274437509": 1,
    "276450249": 1,
    "276742020": 1,
    "278904757": 1,
    "3628605": 1,
    "10878315": 1,
    "12515065": 1,
    "13253834": 1,
    "17341026": 1,
    "19182772": 1,
    "38899272": 1,
    "207424229": 1,
    "211132951": 1,
    "212703531": 1,
    "237581307": 1,
    "255825556": 1,
    "268856519": 1,
    "13405741": 1,
    "250114239": 1,
    "255124952": 1,
    "267642722": 1,
    "271162268": 1,
    "272559061": 1,
    "272827463": 1,
    "273482255": 1,
    "274823073": 1,
    "276913297": 1,
    "53208380": 1,
    "67000854": 1,
    "237142385": 1,
    "254096365": 1,
    "256105296": 1,
    "264172455": 1,
    "276482377": 1,
    "279586232": 1,
    "256868474": 1,
    "266359151": 3,
    "266844877": 1,
    "266902900": 1,
    "273042295": 1,
    "214754592": 1,
    "227347434": 1,
    "232404173": 1,
    "233324338": 1,
    "236957210": 1,
    "248721683": 1,
    "248913107": 2,
    "256390607": 1,
    "258947447": 1,
    "268379149": 1,
    "268417079": 1,
    "268889483": 1,
    "272367253": 1,
    "273098318": 2,
    "274131897": 1,
    "276580691": 1,
    "277150647": 1,
    "258685337": 2,
    "258987659": 2,
    "260887105": 1,
    "263609132": 3,
    "264590387": 1,
    "265609521": 1,
    "267212063": 1,
    "267320303": 1,
    "12777818": 4,
    "34953552": 1,
    "52897360": 1,
    "233231380": 1,
    "234093776": 1,
    "236034497": 1,
    "252917648": 3,
    "258108259": 1,
    "259360665": 1,
    "262053695": 1,
    "264439655": 1,
    "267199749": 1,
    "267897727": 1,
    "268031860": 1,
    "268363855": 2,
    "269148675": 1,
    "270620912": 1,
    "270738094": 1,
    "271270048": 1,
    "272593197": 1,
    "273507547": 1,
    "275606655": 1,
    "275757481": 1,
    "276421817": 1,
    "226965153": 1,
    "249097975": 3,
    "257900871": 4,
    "258714753": 1,
    "259203671": 1,
    "263605944": 2,
    "268379197": 1,
    "274514455": 1,
    "233289729": 1,
    "257637012": 1,
    "258179336": 1,
    "258959262": 2,
    "259108190": 2,
    "261241602": 1,
    "263310365": 1,
    "264306101": 3,
    "277276996": 1,
    "235755472": 4,
    "258040990": 6,
    "269921148": 2,
    "270371956": 1,
    "265506220": 1,
    "270562306": 1,
    "271212406": 1,
    "271953942": 1,
    "272368347": 2,
    "272424184": 1,
    "5082661": 1,
    "28695052": 1,
    "204972004": 1,
    "257663442": 4,
    "258179085": 1,
    "259144814": 1,
    "259145016": 1,
    "259947046": 1,
    "262053612": 2,
    "263310497": 1,
    "263708879": 1,
    "264146113": 1,
    "267897401": 1,
    "268032502": 1,
    "269502645": 1,
    "269536190": 1,
    "272330427": 2,
    "1767517": 1,
    "265067391": 1,
    "265294665": 1,
    "267211622": 1,
    "267898017": 1,
    "268379413": 1,
    "269283058": 1,
    "270062853": 1,
    "271515809": 1,
    "273502415": 1,
    "276107438": 1,
    "277780948": 1,
    "278237252": 1,
    "202770731": 1,
    "208158225": 2,
    "237504552": 3,
    "248722148": 3,
    "249674500": 1,
    "249848263": 2,
    "252735112": 1,
    "253107613": 1,
    "257532801": 1,
    "259262142": 1,
    "263334319": 1,
    "263909278": 1,
    "266174368": 1,
    "267413027": 1,
    "267617061": 2,
    "267627652": 1,
    "268230464": 2,
    "271064506": 2,
    "274437383": 1,
    "34198369": 1,
    "49470584": 1,
    "225062560": 1,
    "225076003": 1,
    "235606348": 1,
    "237277983": 1,
    "246634577": 1,
    "247618840": 2,
    "252200013": 1,
    "252718704": 1,
    "257079001": 2,
    "257952310": 1,
    "259187664": 1,
    "259308821": 1,
    "259837330": 2,
    "261681849": 1,
    "263218031": 1,
    "263626099": 1,
    "268856732": 1,
    "270440391": 1,
    "3517962": 1,
    "59788741": 1,
    "198953378": 2,
    "231718747": 1,
    "235212182": 1,
    "248496292": 3,
    "254823489": 2,
    "257687695": 1,
    "258170084": 1,
    "264590280": 1,
    "265019477": 1,
    "268248810": 1,
    "268297061": 1,
    "14588414": 1,
    "25648541": 1,
    "52127932": 1,
    "67769538": 1,
    "214802269": 1,
    "218908860": 1,
    "247749019": 1,
    "251719353": 2,
    "252407646": 1,
    "256827239": 1,
    "259129651": 2,
    "259342486": 1,
    "259342528": 1,
    "260202961": 1,
    "261823330": 1,
    "263787277": 1,
    "264406064": 1,
    "267412137": 1,
    "270441137": 1,
    "276248853": 1,
    "259262077": 2,
    "265050948": 1,
    "266044180": 1,
    "267740648": 1,
    "268249090": 1,
    "6628106": 2,
    "33285731": 1,
    "215737187": 2,
    "233427561": 1,
    "235415270": 1,
    "236477750": 1,
    "259187717": 1,
    "259833781": 1,
    "263610099": 1,
    "266521240": 1,
    "266551228": 1,
    "267068458": 1,
    "267211869": 1,
    "267411962": 1,
    "268553748": 1,
    "270870063": 2,
    "1373518": 1,
    "52822214": 4,
    "258060250": 1,
    "258822910": 2,
    "258823133": 2,
    "258865718": 1,
    "259243960": 2,
    "260887189": 1,
    "267548105": 1,
    "268232499": 2,
    "270521777": 1,
    "271532761": 1,
    "273654757": 1,
    "964287": 1,
    "990084": 1,
    "13866508": 1,
    "17756042": 1,
    "18506861": 1,
    "49313245": 3,
    "52183757": 1,
    "62039799": 1,
    "204854503": 1,
    "208290939": 2,
    "221555387": 1,
    "225066679": 1,
    "226278099": 1,
    "237205744": 1,
    "237532606": 1,
    "238531697": 1,
    "239010011": 1,
    "252780839": 1,
    "253080612": 1,
    "256661805": 1,
    "257572753": 1,
    "257952500": 1,
    "258041354": 1,
    "258171641": 1,
    "258556812": 1,
    "258947222": 4,
    "259129531": 1,
    "259129602": 1,
    "259983087": 1,
    "260164518": 1,
    "264107446": 1,
    "264825354": 1,
    "265038146": 1,
    "265104049": 1,
    "266362535": 1,
    "266999586": 1,
    "267406814": 1,
    "267412025": 1,
    "267523467": 1,
    "267750939": 1,
    "204578308": 1,
    "222140924": 1,
    "247451124": 2,
    "258291566": 1,
    "259950380": 1,
    "260063238": 1,
    "265019383": 1,
    "267770308": 1,
    "268417347": 1,
    "269302548": 1,
    "270371213": 1,
    "270620354": 1,
    "46997949": 1,
    "53533716": 1,
    "153257084": 1,
    "202538486": 1,
    "225066700": 1,
    "238408283": 1,
    "253759631": 2,
    "254277011": 1,
    "257833781": 2,
    "258041253": 1,
    "258509586": 1,
    "259088724": 1,
    "262044464": 1,
    "265213232": 1,
    "266690872": 1,
    "267061073": 1,
    "267081115": 1,
    "267938212": 1,
    "268042457": 1,
    "268681670": 1,
    "268819803": 1,
    "268876709": 1,
    "269745421": 1,
    "269757480": 1,
    "269757934": 1,
    "270062501": 1,
    "252596089": 1,
    "258212642": 1,
    "983645": 1,
    "144110805": 1,
    "218665356": 1,
    "235266260": 1,
    "238259902": 2,
    "256808659": 2,
    "257805102": 1,
    "259224572": 1,
    "261049794": 1,
    "263310943": 2,
    "264306288": 1,
    "265610018": 1,
    "547566": 2,
    "2548749": 1,
    "7559418": 1,
    "14827857": 1,
    "17055038": 1,
    "257255456": 2,
    "258968043": 2,
    "259714625": 1,
    "260881158": 1,
    "263829963": 2,
    "266163085": 1,
    "267548095": 1,
    "254246305": 1,
    "258212542": 1,
    "258740978": 1,
    "258947657": 2,
    "258999153": 1,
    "264555202": 1,
    "1916803": 1,
    "15080556": 1,
    "15117338": 1,
    "15184765": 1,
    "118922379": 1,
    "222133157": 2,
    "234680400": 1,
    "247951931": 5,
    "251881108": 2,
    "254877723": 1,
    "254877753": 4,
    "256416127": 1,
    "258740735": 2,
    "258762577": 2,
    "263098481": 1,
    "263868305": 1,
    "208910339": 1,
    "254823156": 1,
    "260887370": 1,
    "261076387": 1,
    "263671701": 2,
    "264490502": 1,
    "268042522": 1,
    "268307249": 1,
    "1543962": 1,
    "5182891": 1,
    "7038773": 1,
    "21740766": 1,
    "31204656": 1,
    "52198622": 1,
    "53715584": 1,
    "64316736": 1,
    "205223625": 1,
    "215768690": 1,
    "220714040": 1,
    "257378363": 1,
    "258079021": 2,
    "258967487": 1,
    "264146047": 1,
    "265294541": 2,
    "279052856": 1,
    "26419660": 1,
    "232092445": 2,
    "235417602": 1,
    "256615643": 2,
    "262055166": 1,
    "267411736": 1,
    "3883991": 1,
    "29167732": 1,
    "49670925": 1,
    "60035920": 1,
    "116908168": 1,
    "208527654": 1,
    "233289456": 1,
    "250287185": 1,
    "256389594": 1,
    "258297976": 1,
    "246652372": 2,
    "253264914": 1,
    "258841328": 1,
    "262824801": 1,
    "266191787": 1,
    "268532485": 1,
    "271497434": 1,
    "11309330": 1,
    "259982665": 1,
    "262828493": 1,
    "263620434": 1,
    "264564300": 1,
    "265149884": 1,
    "273811174": 1,
    "5276660": 1,
    "19135734": 1,
    "53115163": 1,
    "216559511": 1,
    "255569874": 1,
    "256846700": 1,
    "263608611": 2,
    "263830494": 4,
    "18709606": 1,
    "60440549": 1,
    "213176860": 1,
    "254685791": 1,
    "256783608": 1,
    "257219432": 2,
    "260682436": 1,
    "270440269": 1,
    "270576866": 1,
    "273025556": 1,
    "273695554": 1,
    "276482111": 1,
    "276766708": 1,
    "6936600": 1,
    "9885884": 1,
    "266198147": 1,
    "266435868": 1,
    "267311877": 1,
    "270561990": 1,
    "271270974": 1,
    "258947250": 1,
    "267413178": 2,
    "267637077": 1,
    "67872126": 1,
    "76665670": 1,
    "259243610": 1,
    "259501163": 1,
    "259689601": 1,
    "260378734": 1,
    "261898118": 1,
    "267212047": 1,
    "268531565": 1,
    "270621087": 1,
    "32893583": 1,
    "57366648": 1,
    "119181611": 1,
    "214770841": 1,
    "238634419": 1,
    "246863587": 1,
    "249304070": 1,
    "252904698": 1,
    "253098091": 1,
    "253954372": 1,
    "258841284": 1,
    "5550767": 1,
    "7646250": 1,
    "32182109": 1,
    "52967399": 3,
    "174803437": 1,
    "195984066": 1,
    "213662188": 1,
    "215786368": 1,
    "218889832": 1,
    "232045968": 1,
    "235795331": 1,
    "237420687": 1,
    "237589920": 1,
    "253734315": 1,
    "256826987": 1,
    "258461606": 1,
    "258823112": 1,
    "259203115": 1,
    "264935466": 1,
    "266550843": 1,
    "253708270": 3,
    "254853816": 1,
    "263831032": 1,
    "266051661": 1,
    "1762453": 1,
    "15252590": 1,
    "15597128": 1,
    "21174343": 1,
    "36102848": 2,
    "199370376": 1,
    "238857096": 2,
    "264305982": 1,
    "3986974": 1,
    "7278297": 1,
    "13905064": 1,
    "158046772": 1,
    "202539519": 1,
    "207167677": 1,
    "208006241": 1,
    "237562927": 1,
    "239011786": 1,
    "247082469": 1,
    "251518434": 1,
    "259095910": 1,
    "259138909": 1,
    "259924559": 1,
    "259936842": 1,
    "260315904": 1,
    "260350986": 1,
    "261076103": 1,
    "261214582": 1,
    "264995471": 1,
    "266176297": 1,
    "267411800": 1,
    "268296710": 1,
    "268417087": 1,
    "268691587": 1,
    "268697551": 1,
    "269687228": 1,
    "269704509": 1,
    "274060374": 1,
    "3694591": 1,
    "6627476": 1,
    "70350059": 1,
    "173991054": 1,
    "189998275": 2,
    "207870268": 1,
    "220042384": 1,
    "232170216": 1,
    "244921108": 1,
    "207716": 1,
    "9926549": 1,
    "10248021": 1,
    "232335456": 1,
    "237364324": 1,
    "251979702": 1,
    "258947227": 1,
    "259765919": 1,
    "260925901": 1,
    "261034676": 1,
    "261075905": 1,
    "1739732": 1,
    "59536625": 1,
    "249889477": 2,
    "253734939": 1,
    "258741194": 1,
    "259164815": 1,
    "262217135": 1,
    "268249221": 1,
    "270357954": 1,
    "270823634": 1,
    "3752864": 1,
    "235574660": 1,
    "252814432": 1,
    "254070076": 1,
    "240088413": 1,
    "246867455": 1,
    "257636839": 1,
    "257834038": 2,
    "216356": 1,
    "3674213": 1,
    "4662765": 1,
    "5210390": 2,
    "7228830": 1,
    "15654531": 1,
    "18394514": 1,
    "18433083": 1,
    "44451104": 1,
    "144296456": 1,
    "210023849": 1,
    "222379602": 1,
    "225588887": 1,
    "227305659": 1,
    "233224125": 1,
    "233547699": 1,
    "233987077": 1,
    "235599489": 1,
    "237416499": 1,
    "239020696": 1,
    "245062830": 1,
    "245212848": 1,
    "248177934": 1,
    "248377511": 1,
    "255941863": 1,
    "259139714": 1,
    "262041623": 1,
    "266999465": 1,
    "269329951": 1,
    "270638927": 1,
    "272201868": 1,
    "274131547": 1,
    "275954224": 1,
    "202773799": 1,
    "214148289": 1,
    "237492197": 1,
    "250264533": 2,
    "252918278": 1,
    "257038525": 1,
    "257920051": 1,
    "258479667": 1,
    "259949693": 1,
    "267365239": 1,
    "267897510": 1,
    "268732915": 1,
    "269757778": 1,
    "270123654": 1,
    "248118878": 1,
    "261697361": 1,
    "268536974": 1,
    "270558898": 1,
    "248986239": 2,
    "252715485": 1,
    "252762102": 2,
    "258180548": 1,
    "3297437": 1,
    "17362777": 1,
    "30196678": 1,
    "127986954": 1,
    "232223322": 3,
    "256416408": 1,
    "257427208": 1,
    "260866107": 1,
    "265128672": 1,
    "270620509": 1,
    "271923851": 1,
    "2188447": 1,
    "13905150": 1,
    "49902052": 1,
    "52035171": 1,
    "235593036": 1,
    "237263814": 1,
    "248426863": 1,
    "249209900": 1,
    "256868369": 1,
    "257482793": 1,
    "263830880": 1,
    "266435584": 2,
    "266335848": 2,
    "270379625": 1,
    "270688227": 1,
    "273234147": 1,
    "274024100": 1,
    "278532827": 1,
    "5034059": 1,
    "269294048": 1,
    "270440318": 1,
    "272986898": 1,
    "218551201": 1,
    "246634179": 1,
    "265220706": 1,
    "271039484": 1,
    "3098522": 3,
    "7646504": 1,
    "14105338": 1,
    "20483647": 1,
    "34600346": 1,
    "37954891": 1,
    "96452977": 1,
    "205763978": 1,
    "219179089": 1,
    "238583814": 1,
    "238634478": 1,
    "247957917": 1,
    "248085271": 1,
    "258187415": 1,
    "258991231": 1,
    "265157583": 1,
    "266899650": 1,
    "268512756": 1,
    "273229503": 1,
    "273374421": 1,
    "260126067": 2,
    "262825568": 1,
    "268248325": 1,
    "268296913": 1,
    "268296997": 1,
    "273821674": 1,
    "274859421": 2,
    "277824060": 1,
    "12508462": 1,
    "259202547": 1,
    "261065228": 1,
    "263611068": 1,
    "266844635": 1,
    "270257715": 1,
    "271092420": 1,
    "273228480": 1,
    "273821797": 1,
    "273970146": 1,
    "274823033": 1,
    "275515996": 1,
    "276482713": 1,
    "276742133": 1,
    "276884818": 1,
    "276937204": 1,
    "277113131": 1,
    "277634095": 1,
    "277993666": 1,
    "277999502": 1,
    "278602855": 1,
    "32417": 1,
    "27250165": 1,
    "85499781": 1,
    "140254691": 1,
    "144787755": 1,
    "196127175": 1,
    "211258652": 1,
    "260775783": 1,
    "264555641": 1,
    "270226047": 1,
    "270923717": 2,
    "7575616": 1,
    "16355120": 1,
    "18688952": 1,
    "54767605": 1,
    "63614656": 1,
    "142669959": 1,
    "143781031": 1,
    "146578268": 1,
    "153933608": 1,
    "221904092": 1,
    "231693275": 1,
    "239009871": 1,
    "239459629": 1,
    "247085270": 1,
    "248780453": 1,
    "250311134": 1,
    "253420678": 1,
    "258179241": 1,
    "258217017": 1,
    "258217984": 1,
    "258833277": 1,
    "260697887": 1,
    "262826094": 1,
    "268533038": 1,
    "9426935": 1,
    "57825693": 1,
    "236586513": 1,
    "252409485": 1,
    "253499063": 1,
    "260939243": 1,
    "2218552": 1,
    "11039301": 1,
    "76660838": 1,
    "202540203": 1,
    "212645349": 1,
    "246411155": 1,
    "249642147": 1,
    "252873467": 1,
    "258865984": 1,
    "261049680": 1,
    "261245264": 1,
    "264288947": 2,
    "265067352": 1,
    "267334785": 1,
    "267523037": 1,
    "267770255": 1,
    "268041615": 1,
    "269032933": 1,
    "269363075": 2,
    "269484643": 1,
    "270062331": 1,
    "270064259": 1,
    "270703648": 1,
    "270710703": 1,
    "271404773": 1,
    "271860017": 1,
    "272770285": 1,
    "273023170": 1,
    "273098148": 1,
    "273098808": 1,
    "273345961": 1,
    "273901309": 1,
    "274422541": 1,
    "274859535": 1,
    "275907122": 1,
    "276094378": 1,
    "276235546": 1,
    "276482449": 1,
    "277857467": 1,
    "278165315": 1,
    "279071173": 1,
    "3110807": 1,
    "7435203": 1,
    "11552928": 1,
    "14526353": 1,
    "14758992": 1,
    "15202972": 1,
    "16224144": 1,
    "36031679": 1,
    "43543364": 1,
    "61059855": 1,
    "61567783": 1,
    "206875703": 1,
    "256358611": 1,
    "268033675": 1,
    "269005522": 1,
    "271488373": 1,
    "235899403": 1,
    "251135295": 1,
    "255595818": 1,
    "255912658": 1,
    "263136146": 1,
    "263605637": 1,
    "266521410": 1,
    "267028230": 1,
    "269009845": 1,
    "327319": 1,
    "3922816": 1,
    "4897444": 1,
    "5445756": 1,
    "6675645": 1,
    "6866988": 1,
    "12358271": 1,
    "13838309": 1,
    "15367821": 1,
    "16299141": 1,
    "21722946": 1,
    "24604537": 1,
    "29799633": 1,
    "37065565": 1,
    "40344783": 1,
    "51903808": 1,
    "53232749": 1,
    "53787096": 1,
    "54434517": 1,
    "94929253": 1,
    "108374190": 1,
    "119413748": 1,
    "125046626": 1,
    "135497159": 1,
    "173188048": 1,
    "174797974": 1,
    "174802445": 1,
    "195069360": 1,
    "195791303": 1,
    "198999551": 1,
    "201754999": 1,
    "204105343": 1,
    "204960716": 1,
    "206756462": 1,
    "207220720": 1,
    "211146177": 1,
    "212415210": 1,
    "215768677": 1,
    "217680306": 1,
    "218487092": 1,
    "219955663": 1,
    "220347237": 1,
    "224803102": 1,
    "224803470": 1,
    "226059428": 1,
    "230528669": 1,
    "232104726": 1,
    "233444273": 1,
    "234207025": 1,
    "235220930": 1,
    "237476761": 1,
    "237578994": 1,
    "237747003": 1,
    "238226735": 1,
    "238634584": 1,
    "243865204": 1,
    "245960612": 1,
    "248065796": 1,
    "249674746": 1,
    "252030240": 1,
    "252383606": 1,
    "252544920": 1,
    "253237094": 1,
    "254877346": 1,
    "256389950": 1,
    "256416564": 1,
    "256827363": 1,
    "257637220": 1,
    "258021826": 1,
    "258059792": 1,
    "258213830": 1,
    "258381985": 1,
    "258587994": 1,
    "258685532": 1,
    "259075316": 1,
    "259316643": 1,
    "259501816": 1,
    "259766332": 1,
    "261681740": 1,
    "261925357": 1,
    "263134037": 1,
    "263835191": 1,
    "263890629": 1,
    "263908945": 1,
    "263909166": 1,
    "264591559": 1,
    "264973960": 1,
    "265041119": 1,
    "265104899": 1,
    "265308533": 1,
    "265308911": 1,
    "265505046": 1,
    "265505419": 1,
    "265544461": 1,
    "265551641": 1,
    "265601068": 1,
    "265698016": 1,
    "266448819": 1,
    "266693920": 1,
    "266862195": 1,
    "267500390": 1,
    "267522772": 1,
    "267547479": 1,
    "268297055": 1,
    "268509923": 1,
    "268531713": 1,
    "268680784": 1,
    "269605576": 1,
    "269605607": 1,
    "269626268": 1,
    "269626676": 1,
    "270223881": 1,
    "270358007": 1,
    "270620677": 1,
    "270878784": 1,
    "271064894": 1,
    "271544431": 1,
    "271865537": 1,
    "271874369": 1,
    "272367007": 1,
    "272886429": 1,
    "273375070": 1,
    "273798212": 1,
    "273993329": 1,
    "273993701": 1,
    "274023350": 1,
    "274140377": 1,
    "274140947": 1,
    "274220905": 1,
    "274656164": 1,
    "274966586": 1,
    "276422100": 1,
    "276741674": 1,
    "276855039": 1,
    "277356221": 1,
    "278220803": 1,
    "245353475": 2,
    "215827489": 1,
    "258418299": 1,
    "261696947": 1,
    "268247491": 1,
    "272911169": 1,
    "33081038": 1,
    "198967841": 1,
    "220769401": 1,
    "229298019": 1,
    "246210055": 1,
    "252693237": 1,
    "254017497": 2,
    "258426922": 2,
    "258509700": 1,
    "258865395": 1,
    "268230308": 1,
    "270620269": 2,
    "271051190": 1,
    "275133600": 1,
    "1680775": 1,
    "2711679": 1,
    "6257157": 1,
    "7498624": 1,
    "12246493": 1,
    "12490249": 1,
    "15299054": 1,
    "15975768": 1,
    "31002043": 1,
    "34088202": 1,
    "54480269": 1,
    "64008149": 1,
    "142242370": 1,
    "145184426": 1,
    "213174884": 1,
    "256459681": 1,
    "258686311": 1,
    "2316322": 1,
    "6107832": 1,
    "195218865": 1,
    "209324317": 1,
    "220919878": 1,
    "232379138": 1,
    "269527804": 1,
    "267782782": 1,
    "273638611": 1,
    "276250494": 1,
    "143424870": 1,
    "210861095": 2,
    "211204954": 1,
    "225067460": 1,
    "233444226": 1,
    "261064970": 1,
    "261242328": 1,
    "265050721": 1,
    "265498394": 1,
    "266436034": 1,
    "271038835": 1,
    "280272508": 1,
    "2069491": 1,
    "220047831": 1,
    "220483148": 1,
    "226096901": 1,
    "275820389": 1,
    "25784016": 1,
    "120478295": 1,
    "121328056": 1,
    "198229805": 1,
    "210931542": 1,
    "221806544": 1,
    "224911624": 1,
    "226227046": 1,
    "235435927": 1,
    "249335344": 1,
    "250471540": 1,
    "252125685": 1,
    "256274581": 1,
    "258192578": 1,
    "259881565": 1,
    "265132727": 1,
    "271963189": 1,
    "272344390": 1,
    "273662196": 1,
    "274192378": 1,
    "276635791": 1,
    "4076251": 1,
    "203837042": 3,
    "231632715": 1,
    "257985497": 2,
    "258060965": 1,
    "259262396": 1,
    "4807711": 1,
    "13928442": 1,
    "16623047": 1,
    "19449905": 1,
    "40992894": 1,
    "205261034": 1,
    "206853161": 1,
    "212945787": 1,
    "245218671": 1,
    "247058662": 1,
    "256900800": 1,
    "263605885": 1,
    "267211867": 1,
    "268364153": 1,
    "268819892": 1,
    "259601920": 1,
    "9968823": 1,
    "23892230": 1,
    "210861196": 2,
    "634360": 1,
    "246411325": 1,
    "247778764": 1,
    "5160783": 1,
    "9377590": 1,
    "271693407": 1,
    "57825680": 1,
    "236493269": 1,
    "253734854": 1,
    "258960371": 1,
    "259064000": 1,
    "263672149": 1,
    "268264163": 1,
    "19706": 1,
    "232269696": 1,
    "237091588": 1,
    "257900969": 1,
    "258212828": 1,
    "258291425": 1,
    "258832478": 1,
    "258841249": 1,
    "258947425": 1,
    "264146554": 1,
    "264591439": 1,
    "247500268": 1,
    "249097545": 1,
    "252681067": 1,
    "255341638": 1,
    "256358492": 1,
    "265715696": 1,
    "266176989": 1,
    "268520018": 1,
    "13753671": 1,
    "52099638": 1,
    "220845633": 1,
    "229156165": 1,
    "250426345": 1,
    "252846548": 1,
    "1563120": 1,
    "3334421": 1,
    "4635503": 1,
    "9082946": 1,
    "10242377": 1,
    "13029170": 1,
    "13241655": 1,
    "14709421": 1,
    "15551372": 1,
    "18595016": 1,
    "53026976": 1,
    "57189241": 1,
    "60121883": 1,
    "109965942": 1,
    "114735598": 1,
    "130096094": 1,
    "143983583": 1,
    "153312312": 1,
    "155295771": 1,
    "155298218": 1,
    "189762063": 1,
    "195847962": 1,
    "204142413": 1,
    "206456257": 1,
    "207215142": 1,
    "207231872": 1,
    "212705090": 1,
    "213389645": 1,
    "215791332": 1,
    "218907828": 1,
    "221342993": 1,
    "229156802": 1,
    "229923103": 1,
    "231877730": 1,
    "232269781": 1,
    "233592247": 1,
    "237416585": 1,
    "239768231": 1,
    "243476338": 1,
    "246634238": 1,
    "246863881": 1,
    "250596535": 1,
    "251518264": 1,
    "251518408": 1,
    "251719382": 1,
    "252683292": 1,
    "257632892": 1,
    "257767087": 1,
    "259129807": 1,
    "259251823": 1,
    "260499637": 1,
    "260499706": 1,
    "260548557": 1,
    "260775522": 1,
    "261276812": 1,
    "261705716": 1,
    "263322443": 1,
    "263830637": 1,
    "263908782": 1,
    "264146527": 1,
    "264590333": 1,
    "264615596": 1,
    "265609836": 1,
    "266362729": 1,
    "266375217": 1,
    "266693228": 1,
    "266844108": 1,
    "266933252": 1,
    "267034871": 1,
    "267069344": 1,
    "267617080": 1,
    "267627170": 1,
    "267682348": 1,
    "267750165": 1,
    "267782436": 1,
    "268032947": 1,
    "268063792": 1,
    "268091158": 1,
    "268091311": 1,
    "268379562": 1,
    "268560147": 1,
    "268680708": 1,
    "268722652": 1,
    "268724471": 1,
    "269449477": 1,
    "269605040": 1,
    "270045529": 1,
    "270081134": 1,
    "270199466": 1,
    "270379543": 1,
    "270440657": 1,
    "270440703": 1,
    "270619481": 1,
    "270619971": 1,
    "271051324": 1,
    "271064200": 1,
    "271329051": 1,
    "271533766": 1,
    "271571077": 1,
    "271854887": 1,
    "271954939": 1,
    "271955275": 1,
    "271961973": 1,
    "273229366": 1,
    "273404602": 1,
    "273506998": 1,
    "273654966": 1,
    "273662115": 1,
    "273680557": 1,
    "273811763": 1,
    "273822178": 1,
    "274117080": 1,
    "274198497": 1,
    "274464979": 1,
    "274656234": 1,
    "274776648": 1,
    "274822996": 1,
    "274964904": 1,
    "275133196": 1,
    "275936018": 1,
    "276250403": 1,
    "276317785": 1,
    "276333529": 1,
    "276569762": 1,
    "276617519": 1,
    "276708938": 1,
    "276742424": 1,
    "276885142": 1,
    "276885258": 1,
    "276993547": 1,
    "277042081": 1,
    "277104104": 1,
    "277740771": 1,
    "279074991": 1,
    "270391544": 1,
    "272910918": 1,
    "276161350": 1,
    "276625983": 1,
    "56552072": 1,
    "257404891": 1,
    "258823123": 1,
    "259859069": 1,
    "269790943": 1,
    "2014543": 1,
    "2449317": 1,
    "11518222": 1,
    "49552345": 1,
    "216553223": 1,
    "265221159": 1,
    "259129398": 1,
    "7866026": 1,
    "11243437": 1,
    "18116614": 1,
    "49865049": 1,
    "70219313": 1,
    "215998752": 1,
    "244352923": 1,
    "267740683": 1,
    "268554279": 1,
    "268723699": 1,
    "791679": 1,
    "1934251": 1,
    "18941952": 1,
    "207609497": 1,
    "700325": 1,
    "1842081": 1,
    "4300572": 1,
    "7188883": 1,
    "7846914": 1,
    "13202755": 1,
    "16900887": 1,
    "27294004": 1,
    "61897015": 1,
    "206852649": 1,
    "214429751": 1,
    "252212141": 1,
    "265609805": 1,
    "4959317": 1,
    "5803269": 1,
    "53691145": 1,
    "202558505": 1,
    "222341867": 1,
    "252542956": 1,
    "256504063": 1,
    "258331653": 1,
    "258960339": 2,
    "259089245": 1,
    "211171605": 1,
    "216284142": 1,
    "245218925": 1,
    "258841374": 1,
    "260775975": 1,
    "263829588": 1,
    "263830033": 1
  },
  "merged_dataset_groups": [
    {
      "display_name": "GSM8K",
      "normalized_name": "gsm8k",
      "name_variants": [
        "GSM8K"
      ],
      "mention_count": 10,
      "cited_papers_count": 6,
      "topic_summary": "The GSM8K dataset is primarily used to evaluate the multi-step reasoning, planning, and problem-solving capabilities of large language models (LLMs). It focuses on tasks such as solving complex math problems, generating explanations, and handling algebraic word problems. The dataset assesses both simple and advanced logical reasoning, including first-order logic, arithmetic, and scientific reasoning. It is also utilized to test reading comprehension, multi-hop reasoning, and the ability to handle diverse and challenging planning tasks, often emphasizing step-by-step problem-solving and the generation of logical rationales."
    },
    {
      "display_name": "MATH",
      "normalized_name": "math",
      "name_variants": [
        "MATH"
      ],
      "mention_count": 9,
      "cited_papers_count": 9,
      "topic_summary": "The MATH dataset is extensively used to evaluate the reasoning and problem-solving capabilities of large language models (LLMs), focusing on multi-step reasoning, mathematical problem-solving, and logical inference. It assesses the ability to handle complex tasks such as algebraic word problems, first-order logic, and multi-hop question answering. The dataset is also employed to test planning capabilities, constraint satisfaction, and the generation of detailed explanations. It is used in both evaluation and training contexts, helping to benchmark and improve LLM performance in various reasoning tasks."
    },
    {
      "display_name": "Hot-potQA",
      "normalized_name": "hotpotqa",
      "name_variants": [
        "Hot-potQA",
        "HotPotQA",
        "HotpotQA"
      ],
      "mention_count": 9,
      "cited_papers_count": 5,
      "topic_summary": "The Hot-potQA dataset is primarily used to evaluate multi-hop reasoning capabilities in language models, focusing on complex question-answering tasks that require integrating information from multiple documents. It is also employed to assess the planning and interactive learning capabilities of agents in embodied environments, where the alignment between textual instructions and physical actions is crucial. The dataset supports sequential decision-making tasks and instruction fine-tuning, enhancing the performance of models in both complex question-answering and interactive, multi-turn scenarios."
    },
    {
      "display_name": "ALFWorld",
      "normalized_name": "alfworld",
      "name_variants": [
        "ALF-World",
        "ALFWorld",
        "AlfWorld"
      ],
      "mention_count": 6,
      "cited_papers_count": 4,
      "topic_summary": "The ALFWorld dataset is primarily used to evaluate and validate planning and decision-making capabilities in various models, particularly in interactive and embodied environments. It focuses on assessing code generation, problem-solving, and functional correctness in programming tasks, as well as enhancing performance in text-based and interactive learning scenarios. The dataset supports research on planning algorithms, such as SLINVIT, by providing a benchmark for evaluating planning capabilities and sample efficiency in controlled and interactive settings. It also facilitates the fine-tuning of large language models for robotics planning and the assessment of sequential decision-making in virtual environments."
    },
    {
      "display_name": "MathQA",
      "normalized_name": "mathqa",
      "name_variants": [
        "MathQA"
      ],
      "mention_count": 5,
      "cited_papers_count": 5,
      "topic_summary": "The MathQA dataset is used to enhance and evaluate models' reasoning capabilities across a variety of scientific and mathematical domains. It focuses on solving complex science questions and math word problems, improving general science knowledge, and enhancing physical commonsense understanding. The dataset employs operation-based formalisms to ensure interpretable solutions, enabling researchers to assess and refine models' problem-solving and reasoning skills."
    },
    {
      "display_name": "RedPajama",
      "normalized_name": "redpajama",
      "name_variants": [
        "RedPajama"
      ],
      "mention_count": 4,
      "cited_papers_count": 4,
      "topic_summary": "The RedPajama dataset is primarily used for pretraining large language models (LLMs) with diverse sources of information, including Wikipedia articles, general knowledge, and web-scale data. This enhances the models' factual knowledge, coherence, and exposure to a wide range of content. It is also utilized in materials science to integrate LLMs with tools and databases, aiding in materials discovery and evaluation. The dataset's large scale and open-source nature make it a valuable resource for these applications."
    },
    {
      "display_name": "ALFRED dataset",
      "normalized_name": "alfred",
      "name_variants": [
        "ALFRED",
        "ALFRED dataset"
      ],
      "mention_count": 4,
      "cited_papers_count": 3,
      "topic_summary": "The ALFRED dataset is used to evaluate and develop planning capabilities in embodied agents, particularly in the context of interpreting and executing grounded instructions for everyday tasks in simulated environments. It supports the extraction of instance-level object labels, enhancing the agents' ability to understand and complete complex, real-world tasks. The dataset facilitates the comparison of success rates between state-of-the-art methods and new approaches in grounding language in robotic affordances, and it is also utilized to derive ALFRED-LTL, which focuses on household task instructions to study the planning capabilities of large language models."
    },
    {
      "display_name": "MultiWOZ",
      "normalized_name": "multiwoz",
      "name_variants": [
        "MultiWOZ"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The MultiWOZ dataset is primarily used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding. It is employed to assess the performance of conversational agents in handling complex tasks such as scientific coding, calendar management, and executing research workflows. The dataset serves as a benchmark for evaluating dialogue management and natural language processing capabilities in AI systems."
    },
    {
      "display_name": "Game of 24",
      "normalized_name": "gameof24",
      "name_variants": [
        "Game of 24"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The 'Game of 24' dataset is used to evaluate the performance of models, particularly LLMs, in solving arithmetic problems. It focuses on accuracy improvements over baseline methods, multi-step reasoning, and the effectiveness of Chain-of-Thought methods. The dataset assesses both simple and complex mathematical problems, including word problems and symbolic expressions, to measure the impact of detailed hints and progressive prompting on problem-solving capabilities."
    },
    {
      "display_name": "BabyAI",
      "normalized_name": "babyai",
      "name_variants": [
        "BabyAI"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The BabyAI dataset is used to assess the limitations of specialized neural models and evaluate their zero-shot performance on long-context tasks. It also compares planning capabilities of handcrafted policies or solvers in grid-world and simulated home environments, highlighting tasks where extensive in-domain training yields limited progress. This dataset enables researchers to analyze model performance and planning strategies in controlled, task-specific scenarios."
    },
    {
      "display_name": "MMLU",
      "normalized_name": "mmlu",
      "name_variants": [
        "MMLU"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The MMLU dataset is primarily used to evaluate the performance of large language models (LLMs) across various tasks and domains, including multitask language understanding in 57 diverse subjects such as mathematics, computer science, and law. It is employed in scaling law experiments to assess benchmark loss and in evaluating function-calling proficiencies and core agentic capabilities of LLM agents, providing a rigorous framework for comprehensive performance assessment and benchmarking."
    },
    {
      "display_name": "NaturalQuestions-Open",
      "normalized_name": "naturalquestionsopen",
      "name_variants": [
        "NaturalQuestions-Open"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The NaturalQuestions-Open dataset is used to evaluate the performance of MemGPT in managing multi-session conversations and maintaining context over time. It tests the system's tiered memory capabilities, particularly in open-domain question answering and long-term context management. This dataset enables researchers to assess how effectively MemGPT handles complex, extended interactions."
    },
    {
      "display_name": "UrbanKGent",
      "normalized_name": "urbankgent",
      "name_variants": [
        "UrbanKGent"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The UrbanKGent dataset is utilized to enhance participatory urban planning by serving as a training or evaluation dataset for LLMs in urban planning scenarios. It supports urban planning tasks through structured data, enabling knowledge graph-based applications. This dataset facilitates the development of LLMs that can assist in urban planning, focusing on improving participatory processes and decision-making."
    },
    {
      "display_name": "Olympiad-Bench",
      "normalized_name": "olympiadbench",
      "name_variants": [
        "Olympiad-Bench",
        "OlympiadBench"
      ],
      "mention_count": 3,
      "cited_papers_count": 3,
      "topic_summary": "The Olympiad-Bench dataset is used to evaluate a wide range of capabilities in large language models (LLMs), including factual and alignment tasks, conversational question answering, mathematical reasoning, and problem-solving on olympiad-level questions. It assesses the models' ability to handle complex reasoning, domain-specific knowledge, and multi-turn dialogues. The dataset also evaluates code generation, instruction-following, and argument generation, emphasizing logical consistency and persuasiveness. It is used to promote AGI development by assessing advanced reasoning skills in scientific and bilingual multimodal contexts."
    },
    {
      "display_name": "VirtualHome",
      "normalized_name": "virtualhome",
      "name_variants": [
        "VirtualHome"
      ],
      "mention_count": 3,
      "cited_papers_count": 2,
      "topic_summary": "The VirtualHome dataset is used to simulate household activities in a virtual environment, focusing on the zero-shot planning capabilities of language models and the interactions of embodied agents with various objects, receptacles, and rooms. It provides actionable knowledge for AI planning and evaluation in large-scale household simulations."
    },
    {
      "display_name": "OpenCity",
      "normalized_name": "opencity",
      "name_variants": [
        "OpenCity"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The OpenCity dataset is used to simulate urban activities with massive LLM agents, focusing on spatio-temporal multi-agent trajectories and text vectors to enhance planning capabilities. It supports urban planning tasks by providing structured data for knowledge graph-based applications and serves as a training or evaluation dataset for LLMs in urban planning scenarios."
    },
    {
      "display_name": "ARC",
      "normalized_name": "arc",
      "name_variants": [
        "ARC"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The ARC dataset is used for training and evaluating models on a variety of Q&A tasks, including math, physical interaction, web-based instructions, advanced reasoning, and general science questions. It focuses on enhancing and evaluating the model's problem-solving, reasoning, and comprehension skills across multiple domains. The dataset provides complex explanation traces and challenges models to handle diverse and intricate problems, improving their ability to reason and generate interpretable solutions."
    },
    {
      "display_name": "TravelPlanner",
      "normalized_name": "travelplanner",
      "name_variants": [
        "TravelPlanner"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The TravelPlanner dataset is used to evaluate the planning capabilities of language models, particularly in real-world planning tasks. It assesses the success rates of models in complex planning scenarios, often comparing their performance against GPT models. This benchmark focuses on the models' ability to execute real-world tasks effectively, providing insights into their planning capabilities."
    },
    {
      "display_name": "MatSci-Instruct",
      "normalized_name": "matsciinstruct",
      "name_variants": [
        "MatSci-Instruct"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MatSci-Instruct dataset is primarily used for instruction-following tasks in materials science, evaluating models' abilities to understand and execute complex instructions. It is also employed to enhance and assess models' general science knowledge, reasoning skills, and physical commonsense across multiple scientific disciplines. Specifically, it helps in solving math word problems with interpretable solutions and reasoning processes, and it is used to evaluate the performance of large language models like Llama-2-70B and GPT-3 in answering materials science questions, focusing on their planning capabilities and knowledge retention."
    },
    {
      "display_name": "SUPER",
      "normalized_name": "super",
      "name_variants": [
        "SUPER"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The SUPER dataset is used to evaluate the planning capabilities of LLMs in scientific contexts, specifically focusing on their ability to set up and execute tasks from research repositories. It assesses LLMs' skills in producing accurate, executable scientific code, writing tests, and implementing corresponding code. The dataset enables researchers to verify agents' performance in handling scientific workflows and test-driven development tasks."
    },
    {
      "display_name": "ACPBench",
      "normalized_name": "acpbench",
      "name_variants": [
        "ACPBench"
      ],
      "mention_count": 2,
      "cited_papers_count": 1,
      "topic_summary": "ACPBench is used to evaluate the reasoning and problem-solving capabilities of large language models (LLMs) across various domains. It assesses core reasoning skills, including action, change, and planning, through tasks that test the model's ability to understand and predict outcomes of actions in dynamic environments. The dataset also evaluates graduate-level reasoning, conversational question answering, mathematical and olympiad-level problem-solving, commonsense reasoning, story comprehension, agent planning, code generation, and argument generation. It includes 7 atomic tasks across 13 domains, enabling comprehensive evaluation of complex reasoning and execution capabilities."
    },
    {
      "display_name": "Collection of MOF databases",
      "normalized_name": "collectionofmofdatabases",
      "name_variants": [
        "Collection of MOF databases"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The 'Collection of MOF databases' is used to predict and generate metal-organic frameworks (MOFs), integrating LLMs with tools and evaluators to enhance materials design and discovery. It provides structural and textual data, supporting the development of LLM-based agents in MOF research. This dataset enables researchers to leverage LLMs for more efficient and innovative materials design processes."
    },
    {
      "display_name": "PrOntoQA",
      "normalized_name": "prontoqa",
      "name_variants": [
        "PrOntoQA"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "PrOntoQA is used to evaluate the performance of language models on complex, graduate-level multiple-choice questions in both 0-shot and 5-shot settings. It assesses the models' ability to answer questions without prior exposure and their capacity for logical deduction and step-by-step reasoning. This dataset enables researchers to test and compare the reasoning capabilities of language models under different conditions."
    },
    {
      "display_name": "Planetarium benchmark",
      "normalized_name": "planetarium",
      "name_variants": [
        "Planetarium",
        "Planetarium benchmark"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Planetarium benchmark is used to assess LLMs' planning capabilities by translating text to structured planning languages, particularly focusing on PDDL. It evaluates models' abilities to parse, generate, and reason with PDDL actions, problems, and plans, providing rigorous benchmarking and evaluation of PDDL-based planning tasks. This dataset complements other benchmarks by focusing on the robustness and accuracy of LLMs in handling structured planning languages."
    },
    {
      "display_name": "nuScenes",
      "normalized_name": "nuscenes",
      "name_variants": [
        "nuScenes"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The nuScenes dataset is used to evaluate planning capabilities in autonomous driving systems. Researchers focus on analyzing expert trajectories from real-world driving sessions to assess the performance of models. This dataset provides detailed, real-world driving scenarios that enable the evaluation of how well models can predict and plan vehicle movements."
    },
    {
      "display_name": "BlocksWorld",
      "normalized_name": "blocksworld",
      "name_variants": [
        "BlocksWorld"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The BlocksWorld dataset is used as a planning benchmark to evaluate the ability of large language models to arrange blocks in specific configurations. It focuses on assessing the planning proficiency of these models, providing a standardized test environment to measure their performance in sequential decision-making tasks."
    },
    {
      "display_name": "MBPP",
      "normalized_name": "mbpp",
      "name_variants": [
        "MBPP"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The MBPP dataset is used to evaluate the code generation capabilities of large language models, particularly in solving programming problems. It focuses on a 500-problem test set of crowd-sourced Python tasks, assessing metrics like F1-scores to measure model performance and improvements over iterations. This dataset enables researchers to benchmark and enhance the effectiveness of models in generating functional code."
    },
    {
      "display_name": "CWQ",
      "normalized_name": "cwq",
      "name_variants": [
        "CWQ"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The CWQ dataset is used to evaluate the effectiveness of models, particularly PoG, in multi-hop question answering and complex reasoning over knowledge graphs. It focuses on assessing the model's capability to handle multi-hop questions, emphasizing complex reasoning and semantic parsing. This dataset enables researchers to test and improve the performance of models in challenging reasoning tasks."
    },
    {
      "display_name": "synthesized trajectory data",
      "normalized_name": "synthesizedtrajectory",
      "name_variants": [
        "synthesized trajectory data"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The synthesized trajectory data dataset is used to instruction-tune large language models (LLMs), specifically focusing on enhancing and evaluating their planning capabilities. This dataset provides structured, synthesized trajectories that serve as input for training and testing planning algorithms, enabling researchers to assess the LLMs' ability to generate and evaluate complex plans."
    },
    {
      "display_name": "Tell Me More",
      "normalized_name": "tellmemore",
      "name_variants": [
        "Tell Me More"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The 'Tell Me More' dataset is used to evaluate language agents' planning and conversational capabilities, particularly in handling user instructions. It tests agents' ability to execute tasks with clear instructions, seek additional information when instructions are ambiguous, and navigate web interfaces. The dataset enhances research by providing a framework to assess and improve agents' interaction and understanding in real-world scenarios."
    },
    {
      "display_name": "SUPER-NATURAL-INSTRUCTIONS",
      "normalized_name": "supernaturalinstructions",
      "name_variants": [
        "SUPER-NATURAL-INSTRUCTIONS"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The SUPER-NATURAL-INSTRUCTIONS dataset is used to evaluate and benchmark the instruction-following capabilities of large language models. It provides a multi-dimensional assessment, evaluating various aspects of performance such as accuracy, coherence, and context understanding. The dataset focuses on natural language instructions across diverse domains and complexities, enabling researchers to comprehensively assess how well these models follow instructions in different scenarios."
    },
    {
      "display_name": "Minecraft Wiki recipe pages",
      "normalized_name": "minecraftwikirecipepages",
      "name_variants": [
        "Minecraft Wiki recipe pages"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The Minecraft Wiki recipe pages dataset is used to build a knowledge base for evaluating Retrieval-Augmented Generation (RAG) capabilities. It focuses on integrating web-based information into language model evaluations, enabling researchers to assess how effectively models can incorporate and utilize external knowledge sources. This dataset provides structured, web-derived content that is essential for testing the retrieval and integration mechanisms of language models."
    },
    {
      "display_name": "hazardous home scenarios",
      "normalized_name": "hazardoushomescenarios",
      "name_variants": [
        "hazardous home scenarios"
      ],
      "mention_count": 2,
      "cited_papers_count": 2,
      "topic_summary": "The 'hazardous home scenarios' dataset is used to address gaps in academic research by providing over 500 scenarios typically avoided by embodied agents and agent simulations. It focuses on enhancing the planning capabilities of language models, enabling researchers to test and improve how these models handle complex, real-world situations in home environments."
    },
    {
      "display_name": "BBH",
      "normalized_name": "bbh",
      "name_variants": [
        "BBH"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The BBH dataset is used to evaluate and benchmark advanced reasoning capabilities in AI models, particularly focusing on multi-step and strategic reasoning. It tests a wide range of skills, including mathematical problem-solving, logical reasoning, multi-hop question answering, and commonsense reasoning. The dataset is designed to challenge models with complex tasks that require understanding multiple pieces of evidence, generating explanations, and making sequential decisions. It is applied to assess planning capabilities, flow-based reasoning, and the integration of diverse information in natural language processing tasks."
    },
    {
      "display_name": "agent-environment interactions dataset",
      "normalized_name": "agentenvironmentinteractions",
      "name_variants": [
        "agent-environment interactions dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'agent-environment interactions dataset' is used to train large language models (LLMs) to generate feasible trajectories and respond to dynamic urban contexts, particularly focusing on urban mobility constraints. This dataset enables researchers to enhance the planning capabilities of LLMs by providing real-world scenarios and constraints, ensuring that the generated trajectories are practical and contextually appropriate."
    },
    {
      "display_name": "EconAgent",
      "normalized_name": "econagent",
      "name_variants": [
        "EconAgent"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The EconAgent dataset is used to simulate macroeconomic activities by empowering agents with large language models, focusing on operational state and text data within a multi-agent environment. This approach allows researchers to explore complex interactions and emergent behaviors in economic systems, leveraging the dataset's detailed operational and textual information to enhance the realism and depth of simulations."
    },
    {
      "display_name": "VisualWebArena",
      "normalized_name": "visualwebarena",
      "name_variants": [
        "VisualWebArena"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The VisualWebArena dataset is used to evaluate the performance of multimodal agents on realistic visual web tasks, particularly in complex, real-world scenarios. This dataset enables researchers to assess how well these agents can navigate and interact with web environments, providing insights into their capabilities and limitations in practical settings."
    },
    {
      "display_name": "OmniACT",
      "normalized_name": "omniact",
      "name_variants": [
        "OmniACT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OmniACT dataset is used to train and evaluate agents in navigating and executing complex tasks across real-world computer systems, including desktop and web environments. It focuses on the coordination of actions across multiple applications, multimodal interactions, and natural language understanding. The dataset is employed to assess agents' performance in scientific coding tasks and their ability to understand and execute complex research workflows, emphasizing planning capabilities and multi-domain interactions."
    },
    {
      "display_name": "Allensville (1-ﬂoor)",
      "normalized_name": "allensville1oor",
      "name_variants": [
        "Allensville (1-ﬂoor)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Allensville (1-floor) dataset is used to evaluate planning capabilities in a simplified single-floor environment. It focuses on testing methods for planning in constrained settings, enabling researchers to assess the effectiveness of their approaches in a controlled scenario. This dataset facilitates the development and validation of algorithms designed for navigation and planning tasks in indoor spaces."
    },
    {
      "display_name": "Alpaca dataset",
      "normalized_name": "alpaca",
      "name_variants": [
        "Alpaca dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Alpaca dataset is used to evaluate the planning capabilities of large language models (LLMs). It is mixed with another dataset and the combined data is used to fine-tune the LLM for 3 epochs, enhancing its generalization ability. This approach helps researchers assess how well the model can plan and execute tasks in diverse contexts."
    },
    {
      "display_name": "SWT-Bench",
      "normalized_name": "swtbench",
      "name_variants": [
        "SWT-Bench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The SWT-Bench dataset is used to evaluate the performance of code agents in generating tests from user issues in real-world GitHub repositories. Specifically, it assesses the agent's ability to validate real-world bug-fixes. The dataset enables researchers to test and improve the effectiveness of these agents in practical software development scenarios, focusing on the generation and validation of test cases."
    },
    {
      "display_name": "Reflection-Bench",
      "normalized_name": "reflectionbench",
      "name_variants": [
        "Reflection-Bench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Reflection-Bench dataset is used to assess the cognitive reflection capabilities of large language models (LLMs). It focuses on evaluating components such as perception of new information, memory usage, belief updating, decision-making adjustments, counterfactual reasoning, and meta-reflection. This dataset enables researchers to systematically analyze and measure these cognitive processes, providing insights into the LLMs' ability to reflect and adapt their responses based on new information and internal reasoning."
    },
    {
      "display_name": "MatSciKB",
      "normalized_name": "matscikb",
      "name_variants": [
        "MatSciKB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MatSciKB dataset is used to integrate literature text and web search results, enhancing LLM capabilities in materials discovery. It provides structural and textual data for MOF research, supporting the development of LLM-based agents in materials design. This integration aids in the discovery and design of new materials by leveraging comprehensive and diverse data sources."
    },
    {
      "display_name": "WebQSP",
      "normalized_name": "webqsp",
      "name_variants": [
        "WebQSP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The WebQSP dataset is primarily used for evaluating and enhancing the performance of question-answering systems, particularly in complex, multi-hop scenarios over knowledge bases and web-scale knowledge graphs. It is employed in few-shot learning experiments for path generation and editing, using 3-5 shot examples to improve the system's ability to generate and refine reasoning paths. The dataset supports the assessment of searching success rates, reliable answering rates, and the generalization capabilities of the SRP framework, highlighting its effectiveness in handling diverse and challenging query types. It also aids in testing the impact of reference quality on model performance, demonstrating the importance of carefully selected references."
    },
    {
      "display_name": "MiniWob",
      "normalized_name": "miniwob",
      "name_variants": [
        "MiniWob"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MiniWob dataset is used to evaluate agents' performance in navigating and interacting with web pages, focusing on exploration, task-solving, and automation capabilities. It assesses agents' ability to handle complex web interactions, complete e-commerce tasks, and follow web-based instructions. The dataset provides a foundational framework for evaluating planning, interaction, and natural language processing skills in web-based agents."
    },
    {
      "display_name": "Path Planning from Natural Language (PPNL)",
      "normalized_name": "pathplanningfromnaturallanguageppnl",
      "name_variants": [
        "Path Planning from Natural Language (PPNL)"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Path Planning from Natural Language (PPNL) dataset is used to evaluate the path planning capabilities of large language models (LLMs). It focuses on assessing LLMs' ability to perform end-to-end navigation tasks, including adhering to movement constraints and avoiding obstacles. This dataset enables researchers to test and analyze the effectiveness of LLMs in natural language-guided navigation scenarios."
    },
    {
      "display_name": "ToolBench",
      "normalized_name": "toolbench",
      "name_variants": [
        "ToolBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ToolBench dataset is used to conduct real-time evaluations of large language models (LLMs) on real tasks via RapidAPI. It focuses on the test set to assess the performance and capabilities of these models. This dataset enables researchers to evaluate LLMs in practical, real-world scenarios, providing insights into their effectiveness and limitations."
    },
    {
      "display_name": "Tabular Math Word Problems",
      "normalized_name": "tabularmathwordproblems",
      "name_variants": [
        "Tabular Math Word Problems"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Tabular Math Word Problems dataset is used to evaluate frameworks and models on data-intensive reasoning tasks, particularly in solving math problems presented in tabular formats. It focuses on assessing the agentic plan caching framework and tabular mathematical data reasoning, enabling research into how models handle complex, structured data and long-context financial information."
    },
    {
      "display_name": "AGIEval",
      "normalized_name": "agieval",
      "name_variants": [
        "AGIEval"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AGIEval dataset is used to assess the reasoning and planning capabilities of foundation models, particularly in human-centric tasks. It evaluates LLMs' ability to recognize and correct errors, especially in medical question-answering, through multi-turn feedback loops. This methodology provides insights into self-reflection and error correction, enhancing the understanding of LLMs' performance in complex, interactive scenarios."
    },
    {
      "display_name": "NetHack Learning Environment",
      "normalized_name": "nethacklearningenvironment",
      "name_variants": [
        "NetHack Learning Environment"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The NetHack Learning Environment is used to evaluate the zero-shot capabilities of large language models (LLMs) in complex game environments. Researchers employ this dataset to highlight the challenges and limitations of current LLMs in achieving human-level performance, focusing on the models' ability to navigate and make decisions in unfamiliar and intricate game scenarios."
    },
    {
      "display_name": "DroidTask",
      "normalized_name": "droidtask",
      "name_variants": [
        "DroidTask"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DroidTask dataset is used to evaluate the planning capabilities of large language models (LLMs) in automating Android UI interactions. Research focuses on task completion and efficiency, employing methodologies that assess how effectively LLMs can navigate and interact with Android user interfaces. This dataset enables researchers to measure the performance and reliability of LLMs in real-world mobile application scenarios."
    },
    {
      "display_name": "PPTC Benchmark",
      "normalized_name": "pptcbenchmark",
      "name_variants": [
        "PPTC Benchmark"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PPTC Benchmark dataset is used to evaluate large language models on 279 multi-round dialogue tasks focused on PPT file operations. It assesses the models' task completion capabilities, particularly their planning and execution skills in performing specific PowerPoint tasks. This dataset enables researchers to systematically measure and compare the performance of LLM-based agents in complex, multi-step operations."
    },
    {
      "display_name": "CommonQA",
      "normalized_name": "commonqa",
      "name_variants": [
        "CommonQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CommonQA dataset is used to evaluate commonsense reasoning and machine reading comprehension with a focus on logical reasoning and implicit background knowledge. Researchers employ the RFF-G methodology and compare various baselines such as Least-to-Most, Give-me-Hint, RFF, and CR to assess performance differences in these tasks. This dataset enables the examination of how models handle unexplicitly stated information, crucial for advancing natural language understanding."
    },
    {
      "display_name": "ToolACE",
      "normalized_name": "toolace",
      "name_variants": [
        "ToolACE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ToolACE dataset is primarily used for instruction fine-tuning to enhance the function-calling and instruction-following capabilities of language models, particularly in complex agent environments. It provides high-quality instruction-completion data, focusing on refining planning and execution tasks. This dataset is crucial for improving the interaction and alignment of LLMs like Hephaestus with complex environments, ensuring better performance in single-tool conversations and complex tasks."
    },
    {
      "display_name": "GTB",
      "normalized_name": "gtb",
      "name_variants": [
        "GTB"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GTB dataset is used to study model-free planning in environments with deceptive paths. Researchers employ this dataset to analyze how models traverse different sized levels, focusing on understanding the planning capabilities in complex and misleading environments. This dataset enables the examination of decision-making processes and the effectiveness of planning algorithms in navigating challenging scenarios."
    },
    {
      "display_name": "PlanBench",
      "normalized_name": "planbench",
      "name_variants": [
        "PlanBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The PlanBench dataset is used to evaluate the performance of large language models (LLMs) on planning and reasoning tasks. It assesses LLMs' capabilities in understanding and generating plans, focusing on their ability to reason and create coherent, logical sequences of actions. This dataset enables researchers to systematically test and compare the planning capabilities of different LLMs, providing insights into their strengths and limitations in complex reasoning tasks."
    },
    {
      "display_name": "Spider",
      "normalized_name": "spider",
      "name_variants": [
        "Spider"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Spider dataset is used to evaluate the performance of language models in generating SQL queries from natural language instructions, assessing their ability to interact with real-world APIs, and testing multi-hop reasoning skills. It focuses on the models' capabilities to handle complex tasks, such as integrating external tools and answering questions that require information from multiple sources. This dataset enables researchers to measure and improve the practical utility and reasoning abilities of language models in database-related tasks."
    },
    {
      "display_name": "MINT",
      "normalized_name": "mint",
      "name_variants": [
        "MINT"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MINT dataset is used to evaluate LLMs' planning capabilities across various domains, including task-oriented and goal-directed behavior, multi-turn interactions with tools and language feedback, and procedural instruction generation. It assesses LLMs' numerical reasoning, strategic planning, and problem-solving skills, particularly in complex, multi-step scenarios such as arithmetic puzzles, grade school math problems, and financial and operational planning tasks. The dataset emphasizes dynamic, interactive, and user-specific planning, enabling researchers to test LLMs' adaptability and logical reasoning in real-world applications."
    },
    {
      "display_name": "AgentMove",
      "normalized_name": "agentmove",
      "name_variants": [
        "AgentMove"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The AgentMove dataset is used to test the planning capabilities of large language models (LLMs) in planning tasks, specifically focusing on spatiotemporal single-agent trajectories. Researchers employ this dataset to evaluate how well LLMs can generate and predict agent movements over time and space, addressing questions related to the models' ability to plan and reason about dynamic environments."
    },
    {
      "display_name": "IFEval",
      "normalized_name": "ifeval",
      "name_variants": [
        "IFEval"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The IFEval dataset is used to evaluate various capabilities of large language models (LLMs), including conversational question answering, multi-turn dialogue, and contextual understanding. It assesses reasoning and problem-solving skills at graduate and olympiad levels, covering domains like mathematics, medicine, and science. The dataset also evaluates commonsense reasoning, story comprehension, and the ability to reason about actions and changes. Additionally, it is used to assess factual consistency, argument generation, code generation, and instruction-following capabilities, focusing on accuracy, reliability, and logical consistency."
    },
    {
      "display_name": "Talk2Car",
      "normalized_name": "talk2car",
      "name_variants": [
        "Talk2Car"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Talk2Car dataset is used to enhance natural language interactions for guiding self-driving vehicles. It focuses on providing human-like advice to navigate driving actions, enabling researchers to develop and test systems that can interpret and respond to natural language commands. This dataset supports the development of more intuitive and user-friendly interfaces for autonomous vehicles, emphasizing the integration of human-like communication in navigation tasks."
    },
    {
      "display_name": "MoralChoice datasets",
      "normalized_name": "moralchoice",
      "name_variants": [
        "MoralChoice datasets"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MoralChoice datasets are used to present moral dilemmas involving pedestrians to evaluate the moral beliefs encoded in language models (LLMs). Researchers focus on decision-making scenarios to assess how LLMs handle ethical choices, providing insights into the models' moral reasoning capabilities. This dataset enables the examination of LLM responses to complex ethical situations, facilitating the analysis of their decision-making processes in morally charged contexts."
    },
    {
      "display_name": "CoQA",
      "normalized_name": "coqa",
      "name_variants": [
        "CoQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The CoQA dataset is primarily used to evaluate conversational question answering in large language models, focusing on multi-turn dialogue, contextual understanding, and the ability to handle follow-up questions. It also assesses instruction-following capabilities, reasoning about actions and changes, and complex problem-solving skills, including mathematical reasoning and code generation. The dataset supports research on logical consistency, argument generation, and commonsense reasoning, enabling comprehensive evaluation of LLMs in diverse cognitive tasks."
    },
    {
      "display_name": "Behavior1K",
      "normalized_name": "behavior1k",
      "name_variants": [
        "Behavior1K"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Behavior1K dataset is used in embodied AI research to create 1,000 tasks that align with human preferences, ensuring a balance of task diversity and physical realism. This dataset, integrated with the OMNIGIBSON platform, supports the development and evaluation of AI agents capable of performing complex, realistic tasks in simulated environments."
    },
    {
      "display_name": "APIBench",
      "normalized_name": "apibench",
      "name_variants": [
        "APIBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "APIBench is used to integrate large language models with a diverse set of machine learning APIs, enhancing the models' capabilities by connecting them with external services and functionalities. This integration allows researchers to explore how language models can leverage external tools to perform more complex tasks, though specific research questions and methodologies are not detailed. The dataset's key feature is its collection of APIs, which enables the expansion of model functionalities beyond their inherent capabilities."
    },
    {
      "display_name": "API-Bank",
      "normalized_name": "apibank",
      "name_variants": [
        "API-Bank"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The API-Bank dataset is used to evaluate the performance of tool-augmented language models (LLMs) by assessing their integration and effectiveness with external tools. It consists of 264 annotated dialogues and 568 APIs, enabling researchers to analyze how well LLMs can utilize these tools in practical scenarios. This dataset supports research focused on enhancing the capabilities of LLMs through tool augmentation."
    },
    {
      "display_name": "Disfluent Nav-igational Instruction Audio Dataset",
      "normalized_name": "disfluentnavigationalinstructionaudiodataset",
      "name_variants": [
        "Disfluent Nav-igational Instruction Audio Dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Disfluent Nav-igational Instruction Audio Dataset is used to evaluate TrustNavGPT's performance in navigation tasks, focusing on disfluent navigational instructions in audio format. This dataset enables researchers to assess how well models handle and interpret disfluent speech, enhancing the robustness of navigational assistance systems. The dataset's audio format and inclusion of disfluencies are crucial for testing real-world navigational instruction scenarios."
    },
    {
      "display_name": "MPE",
      "normalized_name": "mpe",
      "name_variants": [
        "MPE"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MPE dataset is used to evaluate the performance of agents in a fully cooperative game setting, specifically in a simple spread environment. It focuses on assessing the effectiveness of the framework in coordinating agent actions. This dataset enables researchers to analyze and improve multi-agent cooperation strategies in controlled environments."
    },
    {
      "display_name": "LiveCodeBench",
      "normalized_name": "livecodebench",
      "name_variants": [
        "LiveCodeBench"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "LiveCodeBench is used to evaluate the advanced reasoning and problem-solving capabilities of large language models (LLMs) across various domains. It assesses graduate-level reasoning, mathematical and computational problem-solving, conversational question answering, and code generation and execution. The dataset includes complex questions, live coding tasks, and real-world programming challenges, enabling researchers to test LLMs' ability to handle sophisticated reasoning, follow complex instructions, and generate coherent arguments."
    },
    {
      "display_name": "ALFWorld Benchmark",
      "normalized_name": "alfworldbenchmark",
      "name_variants": [
        "ALFWorld Benchmark"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ALFWorld Benchmark dataset is used to conduct experiments on the planning capabilities of large language models (LLMs), specifically by integrating TextWorld to generate textual scenarios in household settings. This integration facilitates text-based embodied interaction, enabling researchers to evaluate how LLMs handle complex, interactive tasks in simulated environments. The dataset's focus on household scenarios provides a rich context for assessing LLMs' ability to plan and execute sequences of actions in response to textual inputs."
    },
    {
      "display_name": "MuEP",
      "normalized_name": "muep",
      "name_variants": [
        "MuEP"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MuEP dataset is used for fine-tuning models in complex embodied planning applications, extending the ALF-World dataset to incorporate multimodal data. It serves as a foundational resource for embodied planning tasks, enabling researchers to enhance model performance in environments that require interaction and decision-making based on visual and textual inputs."
    },
    {
      "display_name": "MC-TACO",
      "normalized_name": "mctaco",
      "name_variants": [
        "MC-TACO"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MC-TACO dataset is used to evaluate a wide range of capabilities in large language models (LLMs), including conversational question answering, graduate-level reasoning, mathematical problem-solving, and olympiad-level complex reasoning. It assesses temporal commonsense, narrative understanding, agent planning, code generation, and argument generation. The dataset enables researchers to test LLMs' abilities in executing instructions, generating coherent dialogues, and solving domain-specific problems, providing a comprehensive evaluation of their reasoning and problem-solving skills."
    },
    {
      "display_name": "semantic anomaly dataset",
      "normalized_name": "semanticanomaly",
      "name_variants": [
        "semantic anomaly dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The semantic anomaly dataset is used to evaluate the detection of semantic anomalies in self-driving scenarios, employing large language models. This dataset helps researchers assess the effectiveness of these models in identifying unusual or incorrect situations within driving contexts, enhancing the safety and reliability of autonomous systems."
    },
    {
      "display_name": "Freebase",
      "normalized_name": "freebase",
      "name_variants": [
        "Freebase"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Freebase dataset is used as an external knowledge graph to enhance the semantic richness of data in research. It provides structured information that enriches datasets, supporting methodologies that require detailed and interconnected data. This enhances the context and depth of the data, which is crucial for research requiring comprehensive and semantically rich information."
    },
    {
      "display_name": "MiniGrid",
      "normalized_name": "minigrid",
      "name_variants": [
        "MiniGrid"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The MiniGrid dataset is used to evaluate approaches in solving complex, long-horizon tasks, particularly focusing on demonstrating faster learning compared to baseline methods. It enables researchers to test and compare the efficiency and effectiveness of different algorithms in navigating and solving intricate grid-world environments."
    },
    {
      "display_name": "Ask-before-Plan",
      "normalized_name": "askbeforeplan",
      "name_variants": [
        "Ask-before-Plan"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Ask-before-Plan dataset is used to evaluate the planning capabilities of language models, particularly in interactive travel planning scenarios. It serves as a foundational resource, providing travel planning data for both training and evaluating these models. The dataset enables researchers to assess how effectively language models can handle complex, real-world planning tasks through interactive dialogues."
    },
    {
      "display_name": "ScienceWorld",
      "normalized_name": "scienceworld",
      "name_variants": [
        "ScienceWorld"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The ScienceWorld dataset is used to evaluate the planning capabilities of large language models (LLMs) in both textual science experiment tasks and embodied household tasks. It focuses on how LLMs solve complex scientific problems through text-based interactions and navigate simulated environments, assessing their ability to plan and execute tasks effectively."
    },
    {
      "display_name": "Compositional Celebrities",
      "normalized_name": "compositionalcelebrities",
      "name_variants": [
        "Compositional Celebrities"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Compositional Celebrities dataset is used to evaluate the compositionality and reasoning capabilities of language models, particularly focusing on the performance of REBEL compared to GPT3. It assesses models' ability to answer 8.6k questions about celebrities in various categories, such as 'Birthyear NobelLiterature,' without external search tools. The dataset highlights the compositionality gap in language models, emphasizing their understanding and generation of compositional language and handling of current facts."
    },
    {
      "display_name": "General-Reasoner",
      "normalized_name": "generalreasoner",
      "name_variants": [
        "General-Reasoner"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The General-Reasoner dataset, comprising 232K examples across various reasoning and non-reasoning tasks, is used to explore the effects of training data distribution on SFT-based reasoning models. Researchers employ this dataset to analyze how different distributions of training data influence model performance, focusing on the nuances of reasoning capabilities. This dataset enables a deeper understanding of how training data composition impacts the effectiveness of reasoning models."
    },
    {
      "display_name": "DeepScaler",
      "normalized_name": "deepscaler",
      "name_variants": [
        "DeepScaler"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The DeepScaler dataset is used to fine-tune the Qwen3-14B model, specifically enhancing its capability to handle complex mathematical scaling tasks. This involves training the model to improve its performance in scaling-related problems, which is crucial for applications requiring precise and efficient mathematical computations. The dataset's focus on scaling tasks enables researchers to evaluate and enhance the model's precision and efficiency in these areas."
    },
    {
      "display_name": "GAIA",
      "normalized_name": "gaia",
      "name_variants": [
        "GAIA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The GAIA dataset is used to evaluate the capabilities and performance of general AI assistants across various tasks, without restrictions on tools. It focuses on assessing AI agents' effectiveness in diverse scenarios, enabling researchers to analyze and compare their performance comprehensively. This dataset supports the development and refinement of AI systems by providing a robust framework for evaluation."
    },
    {
      "display_name": "StrategyQA",
      "normalized_name": "strategyqa",
      "name_variants": [
        "StrategyQA"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The StrategyQA dataset is used to evaluate reasoning and planning capabilities in various tasks, including multi-step reasoning, strategy formulation, and problem-solving. It assesses systems like RAP on complex question answering, arithmetic puzzles, grade school math problems, and algorithmic challenges, emphasizing implicit reasoning, logical inference, and efficient planning. The dataset enables researchers to test and improve models' abilities to handle multi-hop reasoning, evidence aggregation, and strategic execution in diverse contexts."
    },
    {
      "display_name": "ActivityPrograms",
      "normalized_name": "activityprograms",
      "name_variants": [
        "ActivityPrograms"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The 'ActivityPrograms' dataset is used to simulate household activities via programs, primarily for training and evaluating models in task-planning safety for embodied agents. It is reconstructed into a pairwise safety preference dataset to optimize Safe-Align and used in the Supervised Fine-Tuning (SFT) phase, providing structured activity sequences. The dataset encompasses a wide range of tasks, enabling comprehensive performance evaluation."
    },
    {
      "display_name": "Autoscale benchmark set",
      "normalized_name": "autoscale",
      "name_variants": [
        "Autoscale benchmark set"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Autoscale benchmark set is used for exploratory experiments during pipeline development, specifically to evaluate classical planning capabilities across ten distinct domains. This dataset enables researchers to test and refine planning algorithms, focusing on their performance and adaptability in diverse scenarios. The dataset's multi-domain nature supports comprehensive analysis and validation of planning systems."
    },
    {
      "display_name": "OpenOrca",
      "normalized_name": "openorca",
      "name_variants": [
        "OpenOrca"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The OpenOrca dataset is used for training and evaluating models across various Q&A tasks, including math problem-solving, physical interaction reasoning, web-based instruction following, advanced reasoning, and commonsense Q&A. It enhances models' abilities to solve complex problems, understand physical scenarios, and generate instructions. The dataset provides diverse and complex explanation traces, enabling researchers to assess and improve models' reasoning and problem-solving capabilities."
    },
    {
      "display_name": "Delivery dataset",
      "normalized_name": "delivery",
      "name_variants": [
        "Delivery dataset"
      ],
      "mention_count": 1,
      "cited_papers_count": 1,
      "topic_summary": "The Delivery dataset is used to train and evaluate models on delivery route planning, focusing on the model's ability to generalize from small-scale (9–17 locations) to larger-scale (70–100 locations) delivery scenarios. This dataset enables researchers to assess the effectiveness and scalability of routing algorithms in practical logistics applications."
    }
  ]
}